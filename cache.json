{"2023-03-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07320v1","updated":"2023-03-13T17:41:57Z","published":"2023-03-13T17:41:57Z","title":"Model-tuning Via Prompts Makes NLP Models Adversarially Robust","summary":"  In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations, such\nas word-level synonym substitutions. In this work, we demonstrate surprising\ngains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an\nalternative method of adapting to downstream tasks. Rather than modifying the\nmodel (by appending an MLP head), MVP instead modifies the input (by appending\na prompt template). Across three classification datasets, MVP improves\nperformance against adversarial word-level synonym substitutions by an average\nof 8% over standard methods and even outperforms adversarial training-based\nstate-of-art defenses by 3.5%. By combining MVP with adversarial training, we\nachieve further improvements in robust accuracy while maintaining clean\naccuracy. Finally, we conduct ablations to investigate the mechanism underlying\nthese gains. Notably, we find that the main causes of vulnerability of MLP can\nbe attributed to the misalignment between pre-training and fine-tuning tasks,\nand the randomly initialized MLP parameters. Code is available at\nhttps://github.com/acmi-lab/mvp\n","authors":["Mrigank Raman","Pratyush Maini","J. Zico Kolter","Zachary C. Lipton","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2303.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07295v1","updated":"2023-03-13T17:17:11Z","published":"2023-03-13T17:17:11Z","title":"Meet in the Middle: A New Pre-training Paradigm","summary":"  Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.\n","authors":["Anh Nguyen","Nikos Karampatziakis","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07295v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07292v1","updated":"2023-03-13T17:12:03Z","published":"2023-03-13T17:12:03Z","title":"Transformer-based approaches to Sentiment Detection","summary":"  The use of transfer learning methods is largely responsible for the present\nbreakthrough in Natural Learning Processing (NLP) tasks across multiple\ndomains. In order to solve the problem of sentiment detection, we examined the\nperformance of four different types of well-known state-of-the-art transformer\nmodels for text classification. Models such as Bidirectional Encoder\nRepresentations from Transformers (BERT), Robustly Optimized BERT Pre-training\nApproach (RoBERTa), a distilled version of BERT (DistilBERT), and a large\nbidirectional neural network architecture (XLNet) were proposed. The\nperformance of the four models that were used to detect disaster in the text\nwas compared. All the models performed well enough, indicating that\ntransformer-based models are suitable for the detection of disaster in text.\nThe RoBERTa transformer model performs best on the test dataset with a score of\n82.6% and is highly recommended for quality predictions. Furthermore, we\ndiscovered that the learning algorithms' performance was influenced by the\npre-processing techniques, the nature of words in the vocabulary, unbalanced\nlabeling, and the model parameters.\n","authors":["Olumide Ebenezer Ojo","Hoang Thang Ta","Alexander Gelbukh","Hiram Calvo","Olaronke Oluwayemisi Adebanji","Grigori Sidorov"],"pdf_url":"https://arxiv.org/pdf/2303.07292v1.pdf","comment":"Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330\n  Cham, Switzerland Published in Book Titled: Recent Developments and the New\n  Directions of Research, Foundations, and Applications: Selected Papers of the\n  8th World Conference on Soft Computing, February 03-05, 2022, Baku,\n  Azerbaijan"},{"id":"http://arxiv.org/abs/2303.07274v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07247v1","updated":"2023-03-13T16:20:33Z","published":"2023-03-13T16:20:33Z","title":"Are Models Trained on Indian Legal Data Fair?","summary":"  Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.\n","authors":["Sahil Girhepuje","Anmol Goel","Gokul Krishnan","Shreya Goyal","Satyendra Pandey","Ponnurangam Kumaraguru","Balaram Ravindran"],"pdf_url":"https://arxiv.org/pdf/2303.07247v1.pdf","comment":"Presented at the Symposium on AI and Law (SAIL) 2023"},{"id":"http://arxiv.org/abs/2303.05737v2","updated":"2023-03-13T16:19:43Z","published":"2023-03-10T06:46:23Z","title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition\n  Performance in Clinical Settings","summary":"  Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n","authors":["Joel Shor","Ruyue Agnes Bi","Subhashini Venugopalan","Steven Ibara","Roman Goldenberg","Ehud Rivlin"],"pdf_url":"https://arxiv.org/pdf/2303.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07027v2","updated":"2023-03-13T16:14:52Z","published":"2023-02-14T13:09:23Z","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained\n  Language Models","summary":"  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n","authors":["Alexandra Chronopoulou","Matthew E. Peters","Alexander Fraser","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2302.07027v2.pdf","comment":"Accepted at EACL 2023; camera-ready version"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.10488v4","updated":"2023-03-13T16:05:11Z","published":"2022-10-19T11:53:13Z","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective","summary":"  Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2210.10488v4.pdf","comment":"Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023"},{"id":"http://arxiv.org/abs/2303.07226v1","updated":"2023-03-13T16:00:31Z","published":"2023-03-13T16:00:31Z","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","summary":"  The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.\n","authors":["Sheng Shen","Zhewei Yao","Chunyuan Li","Trevor Darrell","Kurt Keutzer","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.07226v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2211.05100v3","updated":"2023-03-13T15:55:30Z","published":"2022-11-09T18:48:09Z","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","summary":"  Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n","authors":["BigScience Workshop"," :","Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ilić","Daniel Hesslow","Roman Castagné","Alexandra Sasha Luccioni","François Yvon","Matthias Gallé","Jonathan Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Benoît Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Laurençon","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris Emezue","Christopher Klamm","Colin Leong","Daniel van Strien","David Ifeoluwa Adelani","Dragomir Radev","Eduardo González Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Bar Natan","Francesco De Toni","Gérard Dupont","Germán Kruszewski","Giada Pistilli","Hady Elsahar","Hamza Benyamina","Hieu Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","Jörg Frohberg","Joseph Tobing","Joydeep Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro Von Werra","Leon Weber","Long Phan","Loubna Ben allal","Ludovic Tanguy","Manan Dey","Manuel Romero Muñoz","Maraim Masoud","María Grandury","Mario Šaško","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","Mohammad A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","Roberto Luis López","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","Shanya Sharma","Shayne Longpre","Somaieh Nikpoor","Stanislav Silberberg","Suhas Pai","Sydney Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","Valentin Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","Vrinda Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Davut Emre Taşar","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-shaibani","Matteo Manica","Nihal Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","Tali Bers","Thibault Fevry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiangru Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Yallow Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","Deepak Narayanan","Hatim Bourfoune","Jared Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","Mohammad Shoeybi","Myriam Peyrounette","Nicolas Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre François Lavallée","Rémi Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","Stéphane Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aurélie Névéol","Charles Lovering","Dan Garrette","Deepak Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","Ekaterina Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","Jessica Zosa Forde","Jordan Clive","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","Oleg Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","Shani Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","Vladislav Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zdeněk Kasner","Alice Rueda","Amanda Pestana","Amir Feizpour","Ammar Khan","Amy Faranak","Ana Santos","Anthony Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","Aycha Tammour","Azadeh HajiHosseini","Bahareh Behroozi","Benjamin Ajibade","Bharat Saxena","Carlos Muñoz Ferrandis","Danish Contractor","David Lansky","Davis David","Douwe Kiela","Duong A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","Fatima Mirza","Frankline Ononiwu","Habib Rezanejad","Hessie Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","Isar Nejadgholi","Jesse Passmore","Josh Seltzer","Julio Bonis Sanz","Livia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","Muhammed Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","Nour Fahmy","Olanrewaju Samuel","Ran An","Rasmus Kromann","Ryan Hao","Samira Alizadeh","Sarmad Shubber","Silas Wang","Sourav Roy","Sylvain Viguier","Thanh Le","Tobi Oyebade","Trieu Le","Yoyo Yang","Zach Nguyen","Abhinav Ramesh Kashyap","Alfredo Palasciano","Alison Callahan","Anima Shukla","Antonio Miranda-Escalada","Ayush Singh","Benjamin Beilharz","Bo Wang","Caio Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Clémentine Fourrier","Daniel León Periñán","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","Imane Bello","Ishani Dash","Jihyun Kang","John Giorgi","Jonas Golde","Jose David Posada","Karthik Rangasai Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc Pàmies","Maria A Castillo","Marianna Nezhurina","Mario Sänger","Matthias Samwald","Michael Cullan","Michael Weinberg","Michiel De Wolf","Mina Mihaljcic","Minna Liu","Moritz Freidank","Myungsun Kang","Natasha Seelam","Nathan Dahlberg","Nicholas Michio Broad","Nikolaus Muellner","Pascale Fung","Patrick Haller","Ramya Chandrasekhar","Renata Eisenberg","Robert Martin","Rodrigo Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","Sushil Bharati","Tanmay Laud","Théo Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yash Shailesh Bajaj","Yash Venkatraman","Yifan Xu","Yingxin Xu","Yu Xu","Zhe Tan","Zhongli Xie","Zifan Ye","Mathilde Bras","Younes Belkada","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2211.05100v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07196v1","updated":"2023-03-13T15:34:19Z","published":"2023-03-13T15:34:19Z","title":"A Comprehensive Empirical Evaluation of Existing Word Embedding\n  Approaches","summary":"  Vector-based word representations help countless Natural Language Processing\n(NLP) tasks capture both semantic and syntactic regularities of the language.\nIn this paper, we present the characteristics of existing word embedding\napproaches and analyze them with regards to many classification tasks. We\ncategorize the methods into two main groups - Traditional approaches mostly use\nmatrix factorization to produce word representations, and they are not able to\ncapture the semantic and syntactic regularities of the language very well.\nNeural-Network based approaches, on the other hand, can capture sophisticated\nregularities of the language and preserve the word relationships in the\ngenerated word representations. We report experimental results on multiple\nclassification tasks and highlight the scenarios where one approach performs\nbetter than the rest.\n","authors":["Obaidullah Zaland","Muhammad Abulaish","Mohd. Fazil"],"pdf_url":"https://arxiv.org/pdf/2303.07196v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2211.16270v2","updated":"2023-03-13T14:18:42Z","published":"2022-11-29T14:57:23Z","title":"Neural Transducer Training: Reduced Memory Consumption with Sample-wise\n  Computation","summary":"  The neural transducer is an end-to-end model for automatic speech recognition\n(ASR). While the model is well-suited for streaming ASR, the training process\nremains challenging. During training, the memory requirements may quickly\nexceed the capacity of state-of-the-art GPUs, limiting batch size and sequence\nlengths. In this work, we analyze the time and space complexity of a typical\ntransducer training setup. We propose a memory-efficient training method that\ncomputes the transducer loss and gradients sample by sample. We present\noptimizations to increase the efficiency and parallelism of the sample-wise\nmethod. In a set of thorough benchmarks, we show that our sample-wise method\nsignificantly reduces memory usage, and performs at competitive speed when\ncompared to the default batched computation. As a highlight, we manage to\ncompute the transducer loss and gradients for a batch size of 1024, and audio\nlength of 40 seconds, using only 6 GB of memory.\n","authors":["Stefan Braun","Erik McDermott","Roger Hsiao"],"pdf_url":"https://arxiv.org/pdf/2211.16270v2.pdf","comment":"5 pages, 4 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2303.07146v1","updated":"2023-03-13T14:16:59Z","published":"2023-03-13T14:16:59Z","title":"NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective\n  Reasoning","summary":"  We present a new AI task and baseline solution for Inter-Subjective\nReasoning. We define inter-subjective information, to be a mixture of objective\nand subjective information possibly shared by different parties. Examples may\ninclude commodities and their objective properties as reported by IR\n(Information Retrieval) systems, that need to be cross-referenced with\nsubjective user reviews from an online forum. For an AI system to successfully\nreason about both, it needs to be able to combine symbolic reasoning of\nobjective facts with the shared consensus found on subjective user reviews. To\nthis end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as\na baseline solution for this problem. NeuroQL is a neuro-symbolic language that\nextends logical unification with neural primitives for extraction and\nretrieval. It can function as a target for automatic translation of\ninter-subjective questions (posed in natural language) into the neuro-symbolic\ncode that can answer them.\n","authors":["Nick Papoulias"],"pdf_url":"https://arxiv.org/pdf/2303.07146v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07142v1","updated":"2023-03-13T14:09:53Z","published":"2023-03-13T14:09:53Z","title":"Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification","summary":"  This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n","authors":["Benjamin Clavié","Alexandru Ciceu","Frederick Naylor","Guillaume Soulié","Thomas Brightwell"],"pdf_url":"https://arxiv.org/pdf/2303.07142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2211.05103v2","updated":"2023-03-13T13:35:10Z","published":"2022-11-09T18:53:59Z","title":"Accidental Learners: Spoken Language Identification in Multilingual\n  Self-Supervised Models","summary":"  In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.\n","authors":["Travis M. Bartley","Fei Jia","Krishna C. Puvvada","Samuel Kriman","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2211.05103v2.pdf","comment":"Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.04029v2","updated":"2023-03-13T13:18:44Z","published":"2022-10-08T14:09:58Z","title":"EDU-level Extractive Summarization with Varying Summary Lengths","summary":"  Extractive models usually formulate text summarization as extracting fixed\ntop-$k$ salient sentences from the document as a summary. Few works exploited\nextracting finer-grained Elementary Discourse Unit (EDU) with little analysis\nand justification for the extractive unit selection. Further, the selection\nstrategy of the fixed top-$k$ salient sentences fits the summarization need\npoorly, as the number of salient sentences in different documents varies and\ntherefore a common or best $k$ does not exist in reality. To fill these gaps,\nthis paper first conducts the comparison analysis of oracle summaries based on\nEDUs and sentences, which provides evidence from both theoretical and\nexperimental perspectives to justify and quantify that EDUs make summaries with\nhigher automatic evaluation scores than sentences. Then, considering this merit\nof EDUs, this paper further proposes an EDU-level extractive model with Varying\nsummary Lengths and develops the corresponding learning algorithm. EDU-VL\nlearns to encode and predict probabilities of EDUs in the document, generate\nmultiple candidate summaries with varying lengths based on various $k$ values,\nand encode and score candidate summaries, in an end-to-end training manner.\nFinally, EDU-VL is experimented on single and multi-document benchmark datasets\nand shows improved performances on ROUGE scores in comparison with\nstate-of-the-art extractive models, and further human evaluation suggests that\nEDU-constituent summaries maintain good grammaticality and readability.\n","authors":["Yuping Wu","Ching-Hsun Tseng","Jiayu Shang","Shengzhong Mao","Goran Nenadic","Xiao-Jun Zeng"],"pdf_url":"https://arxiv.org/pdf/2210.04029v2.pdf","comment":"Accepted to EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.07069v1","updated":"2023-03-13T12:45:01Z","published":"2023-03-13T12:45:01Z","title":"Generating multiple-choice questions for medical question answering with\n  distractors and cue-masking","summary":"  Medical multiple-choice question answering (MCQA) is particularly difficult.\nQuestions may describe patient symptoms and ask for the correct diagnosis,\nwhich requires domain knowledge and complex reasoning. Standard language\nmodeling pretraining alone is not sufficient to achieve the best results.\n\\citet{jin2020disease} showed that focusing masked language modeling on disease\nname prediction when using medical encyclopedic paragraphs as input leads to\nconsiderable MCQA accuracy improvement. In this work, we show that (1)\nfine-tuning on generated MCQA dataset outperforms the masked language modeling\nbased objective and (2) correctly masking the cues to the answers is critical\nfor good performance. We release new pretraining datasets and achieve\nstate-of-the-art results on 4 MCQA datasets, notably +5.7\\% with base-size\nmodel on MedQA-USMLE.\n","authors":["Damien Sileo","Kanimozhi Uma","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2303.07069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00258v2","updated":"2023-03-13T12:40:23Z","published":"2022-04-30T13:03:53Z","title":"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language\n  Processing","summary":"  The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).\n","authors":["Chengyu Wang","Minghui Qiu","Chen Shi","Taolin Zhang","Tingting Liu","Lei Li","Jianing Wang","Ming Wang","Jun Huang","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2205.00258v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.00969v2","updated":"2023-03-13T12:14:30Z","published":"2023-03-02T05:06:44Z","title":"Rethinking the Reasonability of the Test Set for Simultaneous Machine\n  Translation","summary":"  Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.\n","authors":["Mengge Liu","Wen Zhang","Xiang Li","Jian Luan","Bin Wang","Yuhang Guo","Shuoying Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00969v2.pdf","comment":"Accepted by 48th IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.07024v1","updated":"2023-03-13T11:41:28Z","published":"2023-03-13T11:41:28Z","title":"Addressing Biases in the Texts using an End-to-End Pipeline Approach","summary":"  The concept of fairness is gaining popularity in academia and industry.\nSocial media is especially vulnerable to media biases and toxic language and\ncomments. We propose a fair ML pipeline that takes a text as input and\ndetermines whether it contains biases and toxic content. Then, based on\npre-trained word embeddings, it suggests a set of new words by substituting the\nbi-ased words, the idea is to lessen the effects of those biases by replacing\nthem with alternative words. We compare our approach to existing fairness\nmodels to determine its effectiveness. The results show that our proposed\npipeline can de-tect, identify, and mitigate biases in social media data\n","authors":["Shaina Raza","Syed Raza Bashir"," Sneha","Urooj Qamar"],"pdf_url":"https://arxiv.org/pdf/2303.07024v1.pdf","comment":"Accepted in Bias @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.06944v1","updated":"2023-03-13T09:22:48Z","published":"2023-03-13T09:22:48Z","title":"A Human Subject Study of Named Entity Recognition (NER) in\n  Conversational Music Recommendation Queries","summary":"  We conducted a human subject study of named entity recognition on a noisy\ncorpus of conversational music recommendation queries, with many irregular and\nnovel named entities. We evaluated the human NER linguistic behaviour in these\nchallenging conditions and compared it with the most common NER systems\nnowadays, fine-tuned transformers. Our goal was to learn about the task to\nguide the design of better evaluation methods and NER algorithms. The results\nshowed that NER in our context was quite hard for both human and algorithms\nunder a strict evaluation schema; humans had higher precision, while the model\nhigher recall because of entity exposure especially during pre-training; and\nentity types had different error patterns (e.g. frequent typing errors for\nartists). The released corpus goes beyond predefined frames of interaction and\ncan support future work in conversational music recommendation.\n","authors":["Elena V. Epure","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2303.06944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06904v1","updated":"2023-03-13T07:46:41Z","published":"2023-03-13T07:46:41Z","title":"Contextually-rich human affect perception using multimodal scene\n  information","summary":"  The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.\n","authors":["Digbalay Bose","Rajat Hebbar","Krishna Somandepalli","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2303.06904v1.pdf","comment":"Accepted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2023"},{"id":"http://arxiv.org/abs/2301.13294v2","updated":"2023-03-13T06:43:18Z","published":"2023-01-30T21:17:15Z","title":"Adaptive Machine Translation with Large Language Models","summary":"  Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, GPT-3.5 can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n","authors":["Yasmin Moslem","Rejwanul Haque","John D. Kelleher","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2301.13294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06878v1","updated":"2023-03-13T05:53:42Z","published":"2023-03-13T05:53:42Z","title":"The System Description of dun_oscar team for The ICPR MSR Challenge","summary":"  This paper introduces the system submitted by dun_oscar team for the ICPR MSR\nChallenge. Three subsystems for task1-task3 are descripted respectively. In\ntask1, we develop a visual system which includes a OCR model, a text tracker,\nand a NLP classifier for distinguishing subtitles and non-subtitles. In task2,\nwe employ an ASR system which includes an AM with 18 layers and a 4-gram LM.\nSemi-supervised learning on unlabeled data is also vital. In task3, we employ\nthe ASR system to improve the visual system, some false subtitles can be\ncorrected by a fusion module.\n","authors":["Binbin Du","Rui Deng","Yingxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06574v3","updated":"2023-03-13T05:51:27Z","published":"2022-02-14T09:36:50Z","title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning","summary":"  Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n","authors":["Ziyang Luo","Zhipeng Hu","Yadong Xi","Rongsheng Zhang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2202.06574v3.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06841v1","updated":"2023-03-13T04:15:33Z","published":"2023-03-13T04:15:33Z","title":"Learning Transductions and Alignments with RNN Seq2seq Models","summary":"  The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four string-to-string transduction\ntasks: identity, reversal, total reduplication, and input-specified\nreduplication. These transductions are traditionally well studied under finite\nstate transducers and attributed with varying complexity. We find that RNN\nseq2seq models are only able to approximate a mapping that fits the training or\nin-distribution data. Attention helps significantly, but does not solve the\nout-of-distribution generalization limitation. Task complexity and RNN variants\nalso play a role in the results. Our results are best understood in terms of\nthe complexity hierarchy of formal languages as opposed to that of string\ntransductions.\n","authors":["Zhengxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06841v1.pdf","comment":"24 pages; 9 figures; 7 tables"},{"id":"http://arxiv.org/abs/2207.01063v3","updated":"2023-03-13T02:13:38Z","published":"2022-07-03T15:07:41Z","title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech","summary":"  The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.\n","authors":["Keon Lee","Kyumin Park","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2207.01063v3.pdf","comment":"5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2204.00175v2","updated":"2023-03-13T01:49:33Z","published":"2022-04-01T02:51:22Z","title":"Alternate Intermediate Conditioning with Syllable-level and\n  Character-level Targets for Japanese ASR","summary":"  End-to-end automatic speech recognition directly maps input speech to\ncharacters. However, the mapping can be problematic when several different\npronunciations should be mapped into one character or when one pronunciation is\nshared among many different characters. Japanese ASR suffers the most from such\nmany-to-one and one-to-many mapping problems due to Japanese kanji characters.\nTo alleviate the problems, we introduce explicit interaction between characters\nand syllables using Self-conditioned connectionist temporal classification\n(CTC), in which the upper layers are ``self-conditioned'' on the intermediate\npredictions from the lower layers. The proposed method utilizes character-level\nand syllable-level intermediate predictions as conditioning features to deal\nwith mutual dependency between characters and syllables. Experimental results\non Corpus of Spontaneous Japanese show that the proposed method outperformed\nthe conventional multi-task and Self-conditioned CTC methods.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Yusuke Kida"],"pdf_url":"https://arxiv.org/pdf/2204.00175v2.pdf","comment":"SLT 2022"},{"id":"http://arxiv.org/abs/2105.11115v3","updated":"2023-03-13T01:47:55Z","published":"2021-05-24T06:42:58Z","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages","summary":"  Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n","authors":["Shunyu Yao","Binghui Peng","Christos Papadimitriou","Karthik Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2105.11115v3.pdf","comment":"ACL 2021. 19 pages with extended appendix. Fixed a small typo in the\n  formula at the end of page 5 (thank to Gabriel Faria). Code:\n  https://github.com/princeton-nlp/dyck-transformer"},{"id":"http://arxiv.org/abs/2303.06806v1","updated":"2023-03-13T01:28:55Z","published":"2023-03-13T01:28:55Z","title":"Neural Diarization with Non-autoregressive Intermediate Attractors","summary":"  End-to-end neural diarization (EEND) with encoder-decoder-based attractors\n(EDA) is a promising method to handle the whole speaker diarization problem\nsimultaneously with a single neural network. While the EEND model can produce\nall frame-level speaker labels simultaneously, it disregards output label\ndependency. In this work, we propose a novel EEND model that introduces the\nlabel dependency between frames. The proposed method generates\nnon-autoregressive intermediate attractors to produce speaker labels at the\nlower layers and conditions the subsequent layers with these labels. While the\nproposed model works in a non-autoregressive manner, the speaker labels are\nrefined by referring to the whole sequence of intermediate labels. The\nexperiments with the two-speaker CALLHOME dataset show that the intermediate\nlabels with the proposed non-autoregressive intermediate attractors boost the\ndiarization performance. The proposed method with the deeper network benefits\nmore from the intermediate labels, resulting in better performance and training\nthroughput than EEND-EDA.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Robin Scheibler","Yusuke Kida","Tetsuji Ogawa"],"pdf_url":"https://arxiv.org/pdf/2303.06806v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07522v1","updated":"2023-03-13T23:17:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v1.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.07519v1","updated":"2023-03-13T23:11:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100\\%\nrate. Accuracy shows great improvement when scaling the models, with the\nlargest model (GPT-J) yielding impressive accuracy ranging between 25% to over\n80% for different prompt categories. We open source the finetuned Architext\nmodels and our synthetic dataset, hoping to inspire experimentation in this\nexciting area of design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01860v2","updated":"2023-03-13T21:41:39Z","published":"2023-02-03T17:07:23Z","title":"GLADIS: A General and Large Acronym Disambiguation Benchmark","summary":"  Acronym Disambiguation (AD) is crucial for natural language understanding on\nvarious sources, including biomedical reports, scientific papers, and search\nengine queries. However, existing acronym disambiguation benchmarks and tools\nare limited to specific domains, and the size of prior benchmarks is rather\nsmall. To accelerate the research on acronym disambiguation, we construct a new\nbenchmark named GLADIS with three components: (1) a much larger acronym\ndictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus\nwith 160 million sentences; (3) three datasets that cover the general,\nscientific, and biomedical domains. We then pre-train a language model,\n\\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,\nand show the challenges and values of our new benchmark.\n","authors":["Lihu Chen","Gaël Varoquaux","Fabian M. Suchanek"],"pdf_url":"https://arxiv.org/pdf/2302.01860v2.pdf","comment":"Long paper at EACL 23"},{"id":"http://arxiv.org/abs/2303.07457v1","updated":"2023-03-13T20:34:56Z","published":"2023-03-13T20:34:56Z","title":"AMOM: Adaptive Masking over Masking for Conditional Masked Language\n  Model","summary":"  Transformer-based autoregressive (AR) methods have achieved appealing\nperformance for varied sequence-to-sequence generation tasks, e.g., neural\nmachine translation, summarization, and code generation, but suffer from low\ninference efficiency. To speed up the inference stage, many non-autoregressive\n(NAR) strategies have been proposed in the past few years. Among them, the\nconditional masked language model (CMLM) is one of the most versatile\nframeworks, as it can support many different sequence generation scenarios and\nachieve very competitive performance on these tasks. In this paper, we further\nintroduce a simple yet effective adaptive masking over masking strategy to\nenhance the refinement capability of the decoder and make the encoder\noptimization easier. Experiments on \\textbf{3} different tasks (neural machine\ntranslation, summarization, and code generation) with \\textbf{15} datasets in\ntotal confirm that our proposed simple method achieves significant performance\nimprovement over the strong CMLM model. Surprisingly, our proposed model yields\nstate-of-the-art performance on neural machine translation (\\textbf{34.62} BLEU\non WMT16 EN$\\to$RO, \\textbf{34.82} BLEU on WMT16 RO$\\to$EN, and \\textbf{34.84}\nBLEU on IWSLT De$\\to$En) and even better performance than the \\textbf{AR}\nTransformer on \\textbf{7} benchmark datasets with at least \\textbf{2.2$\\times$}\nspeedup. Our code is available at GitHub.\n","authors":["Yisheng Xiao","Ruiyang Xu","Lijun Wu","Juntao Li","Tao Qin","Yan-Tie Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07457v1.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2303.07354v1","updated":"2023-03-13T06:39:38Z","published":"2023-03-13T06:39:38Z","title":"MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer\n  Adapters","summary":"  State-sponsored trolls are the main actors of influence campaigns on social\nmedia and automatic troll detection is important to combat misinformation at\nscale. Existing troll detection models are developed based on training data for\nknown campaigns (e.g.\\ the influence campaign by Russia's Internet Research\nAgency on the 2016 US Election), and they fall short when dealing with {\\em\nnovel} campaigns with new targets. We propose MetaTroll, a text-based troll\ndetection model based on the meta-learning framework that enables high\nportability and parameter-efficient adaptation to new campaigns using only a\nhandful of labelled samples for few-shot transfer. We introduce\n\\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''\ncampaign-specific knowledge so as to tackle catastrophic forgetting, where a\nmodel ``forgets'' how to detect trolls from older campaigns due to continual\nadaptation. Our experiments demonstrate that MetaTroll substantially\noutperforms baselines and state-of-the-art few-shot text classification models.\nLastly, we explore simple approaches to extend MetaTroll to multilingual and\nmultimodal detection. Source code for MetaTroll is available at:\nhttps://github.com/ltian678/metatroll-code.git.\n","authors":["Lin Tian","Xiuzhen Zhang","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2303.07354v1.pdf","comment":"11 pages, 2 figures, Accepted by the Web Conference 2023 (WWW 2023)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.07347v1","updated":"2023-03-13T17:59:59Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v1.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07345v1","updated":"2023-03-13T17:59:55Z","published":"2023-03-13T17:59:55Z","title":"Erasing Concepts from Diffusion Models","summary":"  Motivated by recent advancements in text-to-image diffusion, we study erasure\nof specific concepts from the model's weights. While Stable Diffusion has shown\npromise in producing explicit or realistic artwork, it has raised concerns\nregarding its potential for misuse. We propose a fine-tuning method that can\nerase a visual concept from a pre-trained diffusion model, given only the name\nof the style and using negative guidance as a teacher. We benchmark our method\nagainst previous approaches that remove sexually explicit content and\ndemonstrate its effectiveness, performing on par with Safe Latent Diffusion and\ncensored training. To evaluate artistic style removal, we conduct experiments\nerasing five modern artists from the network and conduct a user study to assess\nthe human perception of the removed styles. Unlike previous methods, our\napproach can remove concepts from a diffusion model permanently rather than\nmodifying the output at the inference time, so it cannot be circumvented even\nif a user has access to model weights. Our code, data, and results are\navailable at https://erasing.baulab.info/\n","authors":["Rohit Gandikota","Joanna Materzynska","Jaden Fiotto-Kaufman","David Bau"],"pdf_url":"https://arxiv.org/pdf/2303.07345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07338v1","updated":"2023-03-13T17:59:02Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred. In this work, we revisit CIL with\nPTMs and argue that the core factors in CIL are adaptivity for model updating\nand generalizability for knowledge transferring. 1) We first reveal that frozen\nPTM can already provide generalizable embeddings for CIL. Surprisingly, a\nsimple baseline (SimpleCIL) which continually sets the classifiers of PTM to\nprototype features can beat state-of-the-art even without training on the\ndownstream task. 2) Due to the distribution gap between pre-trained and\ndownstream datasets, PTM can be further cultivated with adaptivity via model\nadapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of\nPTM and adapted models for classifier construction. ADAM is a general framework\nthat can be orthogonally combined with any parameter-efficient tuning method,\nwhich holds the advantages of PTM's generalizability and adapted model's\nadaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the\nera of PTM due to data overlapping and propose four new benchmarks for\nassessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive\nexperiments validate the effectiveness of ADAM with a unified and concise\nframework.\n","authors":["Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v1.pdf","comment":"Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2303.07337v1","updated":"2023-03-13T17:58:54Z","published":"2023-03-13T17:58:54Z","title":"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in\n  Human Pose and Shape Estimation","summary":"  Human pose and shape (HPS) estimation methods achieve remarkable results.\nHowever, current HPS benchmarks are mostly designed to test models in scenarios\nthat are similar to the training data. This can lead to critical situations in\nreal-world applications when the observed data differs significantly from the\ntraining data and hence is out-of-distribution (OOD). It is therefore important\nto test and improve the OOD robustness of HPS methods. To address this\nfundamental problem, we develop a simulator that can be controlled in a\nfine-grained manner using interpretable parameters to explore the manifold of\nimages of human pose, e.g. by varying poses, shapes, and clothes. We introduce\na learning-based testing method, termed PoseExaminer, that automatically\ndiagnoses HPS algorithms by searching over the parameter space of human pose\nimages to find the failure modes. Our strategy for exploring this\nhigh-dimensional parameter space is a multi-agent reinforcement learning\nsystem, in which the agents collaborate to explore different parts of the\nparameter space. We show that our PoseExaminer discovers a variety of\nlimitations in current state-of-the-art models that are relevant in real-world\nscenarios but are missed by current benchmarks. For example, it finds large\nregions of realistic human poses that are not predicted correctly, as well as\nreduced performance for humans with skinny and corpulent body shapes. In\naddition, we show that fine-tuning HPS methods by exploiting the failure modes\nfound by PoseExaminer improve their robustness and even their performance on\nstandard benchmarks by a significant margin. The code are available for\nresearch purposes.\n","authors":["Qihao Liu","Adam Kortylewski","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2303.07337v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07335v1","updated":"2023-03-13T17:57:59Z","published":"2023-03-13T17:57:59Z","title":"Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR","summary":"  Recent DEtection TRansformer-based (DETR) models have obtained remarkable\nperformance. Its success cannot be achieved without the re-introduction of\nmulti-scale feature fusion in the encoder. However, the excessively increased\ntokens in multi-scale features, especially for about 75\\% of low-level\nfeatures, are quite computationally inefficient, which hinders real\napplications of DETR models. In this paper, we present Lite DETR, a simple yet\nefficient end-to-end object detection framework that can effectively reduce the\nGFLOPs of the detection head by 60\\% while keeping 99\\% of the original\nperformance. Specifically, we design an efficient encoder block to update\nhigh-level features (corresponding to small-resolution feature maps) and\nlow-level features (corresponding to large-resolution feature maps) in an\ninterleaved way. In addition, to better fuse cross-scale features, we develop a\nkey-aware deformable attention to predict more reliable attention weights.\nComprehensive experiments validate the effectiveness and efficiency of the\nproposed Lite DETR, and the efficient encoder strategy can generalize well\nacross existing DETR-based models. The code will be available in\n\\url{https://github.com/IDEA-Research/Lite-DETR}.\n","authors":["Feng Li","Ailing Zeng","Shilong Liu","Hao Zhang","Hongyang Li","Lei Zhang","Lionel M. Ni"],"pdf_url":"https://arxiv.org/pdf/2303.07335v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07336v1","updated":"2023-03-13T17:57:59Z","published":"2023-03-13T17:57:59Z","title":"MP-Former: Mask-Piloted Transformer for Image Segmentation","summary":"  We present a mask-piloted Transformer which improves masked-attention in\nMask2Former for image segmentation. The improvement is based on our observation\nthat Mask2Former suffers from inconsistent mask predictions between consecutive\ndecoder layers, which leads to inconsistent optimization goals and low\nutilization of decoder queries. To address this problem, we propose a\nmask-piloted training approach, which additionally feeds noised ground-truth\nmasks in masked-attention and trains the model to reconstruct the original\nones. Compared with the predicted masks used in mask-attention, the\nground-truth masks serve as a pilot and effectively alleviate the negative\nimpact of inaccurate mask predictions in Mask2Former. Based on this technique,\nour \\M achieves a remarkable performance improvement on all three image\nsegmentation tasks (instance, panoptic, and semantic), yielding $+2.3$AP and\n$+1.6$mIoU on the Cityscapes instance and semantic segmentation tasks with a\nResNet-50 backbone. Our method also significantly speeds up the training,\noutperforming Mask2Former with half of the number of training epochs on ADE20K\nwith both a ResNet-50 and a Swin-L backbones. Moreover, our method only\nintroduces little computation during training and no extra computation during\ninference. Our code will be released at\n\\url{https://github.com/IDEA-Research/MP-Former}.\n","authors":["Hao Zhang","Feng Li","Huaizhe Xu","Shijia Huang","Shilong Liu","Lionel M. Ni","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07336v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07327v1","updated":"2023-03-13T17:45:39Z","published":"2023-03-13T17:45:39Z","title":"Unsupervised HDR Image and Video Tone Mapping via Contrastive Learning","summary":"  Capturing high dynamic range (HDR) images (videos) is attractive because it\ncan reveal the details in both dark and bright regions. Since the mainstream\nscreens only support low dynamic range (LDR) content, tone mapping algorithm is\nrequired to compress the dynamic range of HDR images (videos). Although image\ntone mapping has been widely explored, video tone mapping is lagging behind,\nespecially for the deep-learning-based methods, due to the lack of HDR-LDR\nvideo pairs. In this work, we propose a unified framework (IVTMNet) for\nunsupervised image and video tone mapping. To improve unsupervised training, we\npropose domain and instance based contrastive learning loss. Instead of using a\nuniversal feature extractor, such as VGG to extract the features for similarity\nmeasurement, we propose a novel latent code, which is an aggregation of the\nbrightness and contrast of extracted features, to measure the similarity of\ndifferent pairs. We totally construct two negative pairs and three positive\npairs to constrain the latent codes of tone mapped results. For video tone\nmapping, we propose a temporal-feature-replaced (TFR) module to efficiently\nutilize the temporal correlation and improve the temporal consistency of video\ntone-mapped results. We construct a large-scale unpaired HDR-LDR video dataset\nto facilitate the unsupervised training process for video tone mapping.\nExperimental results demonstrate that our method outperforms state-of-the-art\nimage and video tone mapping methods. Our code and dataset will be released\nafter the acceptance of this work.\n","authors":["Cong Cao","Huanjing Yue","Xin Liu","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07327v1.pdf","comment":"12 pages,5 figures"},{"id":"http://arxiv.org/abs/2212.03241v2","updated":"2023-03-13T17:43:16Z","published":"2022-12-06T18:59:58Z","title":"PØDA: Prompt-driven Zero-shot Domain Adaptation","summary":"  Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a single general textual description of the target domain, i.e., a\nprompt. First, we leverage a pretrained contrastive vision-language model\n(CLIP) to optimize affine transformations of source features, steering them\ntowards target text embeddings, while preserving their content and semantics.\nSecond, we show that augmented features can be used to perform zero-shot domain\nadaptation for semantic segmentation. Experiments demonstrate that our method\nsignificantly outperforms CLIP-based style transfer baselines on several\ndatasets for the downstream task at hand. Our prompt-driven approach even\noutperforms one-shot unsupervised domain adaptation on some datasets, and gives\ncomparable results on others. Our code is available at\nhttps://github.com/astra-vision/PODA.\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Pérez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2212.03241v2.pdf","comment":"Project page: https://astra-vision.github.io/PODA/"},{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07321v1","updated":"2023-03-13T17:42:11Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07317v1","updated":"2023-03-13T17:38:58Z","published":"2023-03-13T17:38:58Z","title":"Nearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos","summary":"  Contrastive learning has recently narrowed the gap between self-supervised\nand supervised methods in image and video domain. State-of-the-art video\ncontrastive learning methods such as CVRL and $\\rho$-MoCo spatiotemporally\naugment two clips from the same video as positives. By only sampling positive\nclips locally from a single video, these methods neglect other semantically\nrelated videos that can also be useful. To address this limitation, we leverage\nnearest-neighbor videos from the global space as additional positive pairs,\nthus improving positive key diversity and introducing a more relaxed notion of\nsimilarity that extends beyond video and even class boundaries. Our method,\nInter-Intra Video Contrastive Learning (IIVCL), improves performance on a range\nof video tasks.\n","authors":["David Fan","Deyu Yang","Xinyu Li","Vimal Bhat","Rohith MV"],"pdf_url":"https://arxiv.org/pdf/2303.07317v1.pdf","comment":"Accepted to the ICLR 2023 Workshop on Mathematical and Empirical\n  Understanding of Foundation Models"},{"id":"http://arxiv.org/abs/2303.07308v1","updated":"2023-03-13T17:30:43Z","published":"2023-03-13T17:30:43Z","title":"NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial\n  Understanding with Objects","summary":"  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and\nillustrate how it supports object SLAM for consistent spatial understanding\nwith long-term scene changes. NeuSE is a set of latent object embeddings\ncreated from partial object observations. It serves as a compact point cloud\nsurrogate for complete object models, encoding full shape information while\ntransforming SE(3)-equivariantly in tandem with the object in the physical\nworld. With NeuSE, relative frame transforms can be directly derived from\ninferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape\nand pose characterization, can operate independently or in conjunction with\ntypical SLAM systems. It directly infers SE(3) camera pose constraints that are\ncompatible with general SLAM pose graph optimization, while also maintaining a\nlightweight object-centric map that adapts to real-world changes. Our approach\nis evaluated on synthetic and real-world sequences featuring changed objects\nand shows improved localization accuracy and change-aware mapping capability,\nwhen working either standalone or jointly with a common SLAM pipeline.\n","authors":["Jiahui Fu","Yilun Du","Kurran Singh","Joshua B. Tenenbaum","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2303.07308v1.pdf","comment":"Project webpage: https://neuse-slam.github.io/neuse/"},{"id":"http://arxiv.org/abs/2211.11208v2","updated":"2023-03-13T17:08:01Z","published":"2022-11-21T06:40:46Z","title":"Next3D: Generative Neural Texture Rasterization for 3D-Aware Head\n  Avatars","summary":"  3D-aware generative adversarial networks (GANs) synthesize high-fidelity and\nmulti-view-consistent facial images using only collections of single-view 2D\nimagery. Towards fine-grained control over facial attributes, recent efforts\nincorporate 3D Morphable Face Model (3DMM) to describe deformation in\ngenerative radiance fields either explicitly or implicitly. Explicit methods\nprovide fine-grained expression control but cannot handle topological changes\ncaused by hair and accessories, while implicit ones can model varied topologies\nbut have limited generalization caused by the unconstrained deformation fields.\nWe propose a novel 3D GAN framework for unsupervised learning of generative,\nhigh-quality and 3D-consistent facial avatars from unstructured 2D images. To\nachieve both deformation accuracy and topological flexibility, we propose a 3D\nrepresentation called Generative Texture-Rasterized Tri-planes. The proposed\nrepresentation learns Generative Neural Textures on top of parametric mesh\ntemplates and then projects them into three orthogonal-viewed feature planes\nthrough rasterization, forming a tri-plane feature representation for volume\nrendering. In this way, we combine both fine-grained expression control of\nmesh-guided explicit deformation and the flexibility of implicit volumetric\nrepresentation. We further propose specific modules for modeling mouth interior\nwhich is not taken into account by 3DMM. Our method demonstrates\nstate-of-the-art 3D-aware synthesis quality and animation ability through\nextensive experiments. Furthermore, serving as 3D prior, our animatable 3D\nrepresentation boosts multiple applications including one-shot facial avatars\nand 3D-aware stylization.\n","authors":["Jingxiang Sun","Xuan Wang","Lizhen Wang","Xiaoyu Li","Yong Zhang","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2211.11208v2.pdf","comment":"Accepted by CVPR 2023. Project page:\n  https://mrtornado24.github.io/Next3D/"},{"id":"http://arxiv.org/abs/2303.07284v1","updated":"2023-03-13T17:01:42Z","published":"2023-03-13T17:01:42Z","title":"Align and Attend: Multimodal Summarization with Dual Contrastive Losses","summary":"  The goal of multimodal summarization is to extract the most important\ninformation from different modalities to form summaries. Unlike unimodal\nsummarization, the multimodal summarization task explicitly leverages\ncross-modal information to help generate more reliable and high-quality\nsummaries. However, existing methods fail to leverage the temporal\ncorrespondence between different modalities and ignore the intrinsic\ncorrelation between different samples. To address this issue, we introduce\nAlign and Attend Multimodal Summarization (A2Summ), a unified multimodal\ntransformer-based model which can effectively align and attend the multimodal\ninput. In addition, we propose two novel contrastive losses to model both\ninter-sample and intra-sample correlations. Extensive experiments on two\nstandard video summarization datasets (TVSum and SumMe) and two multimodal\nsummarization datasets (Daily Mail and CNN) demonstrate the superiority of\nA2Summ, achieving state-of-the-art performances on all datasets. Moreover, we\ncollected a large-scale multimodal summarization dataset BLiSS, which contains\nlivestream videos and transcribed texts with annotated summaries. Our code and\ndataset are publicly available at ~\\url{https://boheumd.github.io/A2Summ/}.\n","authors":["Bo He","Jun Wang","Jielin Qiu","Trung Bui","Abhinav Shrivastava","Zhaowen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07284v1.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2303.07280v1","updated":"2023-03-13T16:54:11Z","published":"2023-03-13T16:54:11Z","title":"Vision-Language Models as Success Detectors","summary":"  Detecting successful behaviour is crucial for training intelligent agents. As\nsuch, generalisable reward models are a prerequisite for agents that can learn\nto generalise their behaviour. In this work we focus on developing robust\nsuccess detectors that leverage large, pretrained vision-language models\n(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we\ntreat success detection as a visual question answering (VQA) problem, denoted\nSuccessVQA. We study success detection across three vastly different domains:\n(i) interactive language-conditioned agents in a simulated household, (ii) real\nworld robotic manipulation, and (iii) \"in-the-wild\" human egocentric videos. We\ninvestigate the generalisation properties of a Flamingo-based success detection\nmodel across unseen language and visual changes in the first two domains, and\nfind that the proposed method is able to outperform bespoke reward models in\nout-of-distribution test scenarios with either variation. In the last domain of\n\"in-the-wild\" human videos, we show that success detection on unseen real\nvideos presents an even more challenging generalisation task warranting future\nwork. We hope our initial results encourage further work in real world success\ndetection and reward modelling.\n","authors":["Yuqing Du","Ksenia Konyushkova","Misha Denil","Akhil Raju","Jessica Landon","Felix Hill","Nando de Freitas","Serkan Cabi"],"pdf_url":"https://arxiv.org/pdf/2303.07280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10999v2","updated":"2023-03-13T16:51:03Z","published":"2022-11-20T15:27:55Z","title":"LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders","summary":"  Audio-visual speech enhancement aims to extract clean speech from a noisy\nenvironment by leveraging not only the audio itself but also the target\nspeaker's lip movements. This approach has been shown to yield improvements\nover audio-only speech enhancement, particularly for the removal of interfering\nspeech. Despite recent advances in speech synthesis, most audio-visual\napproaches continue to use spectral mapping/masking to reproduce the clean\naudio, often resulting in visual backbones added to existing speech enhancement\narchitectures. In this work, we propose LA-VocE, a new two-stage approach that\npredicts mel-spectrograms from noisy audio-visual speech via a\ntransformer-based architecture, and then converts them into waveform audio\nusing a neural vocoder (HiFi-GAN). We train and evaluate our framework on\nthousands of speakers and 11+ different languages, and study our model's\nability to adapt to different levels of background noise and speech\ninterference. Our experiments show that LA-VocE outperforms existing methods\naccording to multiple metrics, particularly under very noisy scenarios.\n","authors":["Rodrigo Mira","Buye Xu","Jacob Donley","Anurag Kumar","Stavros Petridis","Vamsi Krishna Ithapu","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2211.10999v2.pdf","comment":"accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07274v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07269v1","updated":"2023-03-13T16:45:41Z","published":"2023-03-13T16:45:41Z","title":"InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised\n  Learning","summary":"  Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL)\nrely on confidence-based pseudo-labeling with consistency regularization. To\nobtain high-quality pseudo-labels, a high confidence threshold is typically\nadopted. However, it has been shown that softmax-based confidence scores in\ndeep networks can be arbitrarily high for samples far from the training data,\nand thus, the pseudo-labels for even high-confidence unlabeled samples may\nstill be unreliable. In this work, we present a new perspective of\npseudo-labeling for imbalanced SSL. Without relying on model confidence, we\npropose to measure whether an unlabeled sample is likely to be\n``in-distribution''; i.e., close to the current training data. To decide\nwhether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'',\nwe adopt the energy score from out-of-distribution detection literature. As\ntraining progresses and more unlabeled samples become in-distribution and\ncontribute to training, the combined labeled and pseudo-labeled data can better\napproximate the true class distribution to improve the model. Experiments\ndemonstrate that our energy-based pseudo-labeling method, \\textbf{InPL}, albeit\nconceptually simple, significantly outperforms confidence-based methods on\nimbalanced SSL benchmarks. For example, it produces around 3\\% absolute\naccuracy improvement on CIFAR10-LT. When combined with state-of-the-art\nlong-tailed SSL methods, further improvements are attained. In particular, in\none of the most challenging scenarios, InPL achieves a 6.9\\% accuracy\nimprovement over the best competitor.\n","authors":["Zhuoran Yu","Yin Li","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2303.07269v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.07264v1","updated":"2023-03-13T16:44:15Z","published":"2023-03-13T16:44:15Z","title":"A Surface-normal Based Neural Framework for Colonoscopy Reconstruction","summary":"  Reconstructing a 3D surface from colonoscopy video is challenging due to\nillumination and reflectivity variation in the video frame that can cause\ndefective shape predictions. Aiming to overcome this challenge, we utilize the\ncharacteristics of surface normal vectors and develop a two-step neural\nframework that significantly improves the colonoscopy reconstruction quality.\nThe normal-based depth initialization network trained with self-supervised\nnormal consistency loss provides depth map initialization to the normal-depth\nrefinement module, which utilizes the relationship between illumination and\nsurface normals to refine the frame-wise normal and depth predictions\nrecursively. Our framework's depth accuracy performance on phantom colonoscopy\ndata demonstrates the value of exploiting the surface normals in colonoscopy\nreconstruction, especially on en face views. Due to its low depth error, the\nprediction result from our framework will require limited post-processing to be\nclinically applicable for real-time colonoscopy reconstruction.\n","authors":["Shuxian Wang","Yubo Zhang","Sarah K. McGill","Julian G. Rosenman","Jan-Michael Frahm","Soumyadip Sengupta","Stephen M. Pizer"],"pdf_url":"https://arxiv.org/pdf/2303.07264v1.pdf","comment":"Accepted at IPMI 2023; first two authors contributed equally"},{"id":"http://arxiv.org/abs/2205.08745v2","updated":"2023-03-13T16:37:07Z","published":"2022-05-18T06:38:18Z","title":"Validation of a photogrammetric approach for the objective study of\n  ancient bowed instruments","summary":"  Some early violins have been reduced during their history to fit imposed\nmorphological standards, while more recent ones have been built directly to\nthese standards. We propose an objective photogrammetric approach to\ndifferentiate between a reduced and an unreduced instrument, whereby a\nthree-dimensional mesh is studied geometrically by examining 2D slices. Our\ncontribution is twofold. First, we validate the quality of the photogrammetric\nmesh through a comparison with reference images obtained by medical imaging,\nand conclude that a sub-millimetre accuracy is achieved. Then, we show how\nquantitative and qualitative features such as contour lines, channel of minima\nand a measure of asymmetry between the upper and lower surfaces of a violin can\nbe automatically extracted from the validated photogrammetric meshes, allowing\nto successfully highlight differences between instruments.\n","authors":["Philémon Beghin","Anne-Emmanuelle Ceulemans","Paul Fisette","François Glineur"],"pdf_url":"https://arxiv.org/pdf/2205.08745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.02328v2","updated":"2023-03-13T16:04:17Z","published":"2023-03-04T05:23:11Z","title":"Decompose, Adjust, Compose: Effective Normalization by Playing with\n  Frequency for Domain Generalization","summary":"  Domain generalization (DG) is a principal task to evaluate the robustness of\ncomputer vision models. Many previous studies have used normalization for DG.\nIn normalization, statistics and normalized features are regarded as style and\ncontent, respectively. However, it has a content variation problem when\nremoving style because the boundary between content and style is unclear. This\nstudy addresses this problem from the frequency domain perspective, where\namplitude and phase are considered as style and content, respectively. First,\nwe verify the quantitative phase variation of normalization through the\nmathematical derivation of the Fourier transform formula. Then, based on this,\nwe propose a novel normalization method, PCNorm, which eliminates style only as\nthe preserving content through spectral decomposition. Furthermore, we propose\nadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of\nvariations in content and style, respectively. Thus, they can learn\ndomain-agnostic representations for DG. With the normalization methods, we\npropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain\ngap. The proposed models outperform other recent DG methods. The DAC-SC\nachieves an average state-of-the-art performance of 65.6% on five datasets:\nPACS, VLCS, Office-Home, DomainNet, and TerraIncognita.\n","authors":["Sangrok Lee","Jongseong Bae","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2303.02328v2.pdf","comment":"10 pages,6 figures, Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2303.07226v1","updated":"2023-03-13T16:00:31Z","published":"2023-03-13T16:00:31Z","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","summary":"  The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.\n","authors":["Sheng Shen","Zhewei Yao","Chunyuan Li","Trevor Darrell","Kurt Keutzer","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.07226v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.07224v1","updated":"2023-03-13T15:58:15Z","published":"2023-03-13T15:58:15Z","title":"Efficient Semantic Segmentation by Altering Resolutions for Compressed\n  Videos","summary":"  Video semantic segmentation (VSS) is a computationally expensive task due to\nthe per-frame prediction for videos of high frame rates. In recent work,\ncompact models or adaptive network strategies have been proposed for efficient\nVSS. However, they did not consider a crucial factor that affects the\ncomputational cost from the input side: the input resolution. In this paper, we\npropose an altering resolution framework called AR-Seg for compressed videos to\nachieve efficient VSS. AR-Seg aims to reduce the computational cost by using\nlow resolution for non-keyframes. To prevent the performance degradation caused\nby downsampling, we design a Cross Resolution Feature Fusion (CReFF) module,\nand supervise it with a novel Feature Similarity Training (FST) strategy.\nSpecifically, CReFF first makes use of motion vectors stored in a compressed\nvideo to warp features from high-resolution keyframes to low-resolution\nnon-keyframes for better spatial alignment, and then selectively aggregates the\nwarped features with local attention mechanism. Furthermore, the proposed FST\nsupervises the aggregated features with high-resolution features through an\nexplicit similarity loss and an implicit constraint from the shared decoding\nlayer. Extensive experiments on CamVid and Cityscapes show that AR-Seg achieves\nstate-of-the-art performance and is compatible with different segmentation\nbackbones. On CamVid, AR-Seg saves 67% computational cost (measured in GFLOPs)\nwith the PSPNet18 backbone while maintaining high segmentation accuracy. Code:\nhttps://github.com/THU-LYJ-Lab/AR-Seg.\n","authors":["Yubin Hu","Yuze He","Yanghao Li","Jisheng Li","Yuxing Han","Jiangtao Wen","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07224v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07223v1","updated":"2023-03-13T15:58:00Z","published":"2023-03-13T15:58:00Z","title":"PromptFusion: Decoupling Stability and Plasticity for Continual Learning","summary":"  Continual learning refers to the capability of continuously learning from a\nstream of data. Current research mainly focuses on relieving catastrophic\nforgetting, and most of their success is at the cost of limiting the\nperformance of newly incoming tasks. Such a trade-off is referred to as the\nstabilityplasticity dilemma and is a more general and challenging problem for\ncontinual learning. However, the inherent conflict between these two concepts\nmakes it seemingly impossible to devise a satisfactory solution to both of them\nsimultaneously. Therefore, we ask, \"is it possible to divide them into two\nproblems to conquer independently?\" To this end, we propose a\nprompt-tuning-based method termed PromptFusion to enable the decoupling of\nstability and plasticity. Specifically, PromptFusion consists of a carefully\ndesigned Stabilizer module that deals with catastrophic forgetting and a\nBooster module to learn new knowledge concurrently. During training,\nPromptFusion first passes an input image to the two modules separately. Then\nthe resulting logits are further fused with a learnable weight parameter.\nFinally, a weight mask is applied to the derived logits to balance between old\nand new classes. Extensive experiments show that our method achieves promising\nresults on popular continual learning datasets for both class-incremental and\ndomain incremental settings. Especially on Split-Imagenet-R, one of the most\nchallenging datasets for class-incremental learning, our method exceeds\nstate-of-the-art prompt-based methods L2P and DualPrompt by more than 10%.\n","authors":["Haoran Chen","Zuxuan Wu","Xintong Han","Menglin Jia","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07216v1","updated":"2023-03-13T15:51:38Z","published":"2023-03-13T15:51:38Z","title":"Parallel Vertex Diffusion for Unified Visual Grounding","summary":"  Unified visual grounding pursues a simple and generic technical route to\nleverage multi-task data with less task-specific design. The most advanced\nmethods typically present boxes and masks as vertex sequences to model\nreferring detection and segmentation as an autoregressive sequential vertex\ngeneration paradigm. However, generating high-dimensional vertex sequences\nsequentially is error-prone because the upstream of the sequence remains static\nand cannot be refined based on downstream vertex information, even if there is\na significant location gap. Besides, with limited vertexes, the inferior\nfitting of objects with complex contours restricts the performance upper bound.\nTo deal with this dilemma, we propose a parallel vertex generation paradigm for\nsuperior high-dimension scalability with a diffusion model by simply modifying\nthe noise dimension. An intuitive materialization of our paradigm is Parallel\nVertex Diffusion (PVD) to directly set vertex coordinates as the generation\ntarget and use a diffusion model to train and infer. We claim that it has two\nflaws: (1) unnormalized coordinate caused a high variance of loss value; (2)\nthe original training objective of PVD only considers point consistency but\nignores geometry consistency. To solve the first flaw, Center Anchor Mechanism\n(CAM) is designed to convert coordinates as normalized offset values to\nstabilize the training loss value. For the second flaw, Angle summation loss\n(ASL) is designed to constrain the geometry difference of prediction and ground\ntruth vertexes for geometry-level consistency. Empirical results show that our\nPVD achieves state-of-the-art in both referring detection and segmentation, and\nour paradigm is more scalable and efficient than sequential vertex generation\nwith high-dimension data.\n","authors":["Zesen Cheng","Kehan Li","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07189v1","updated":"2023-03-13T15:30:28Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death\nworldwide, yet early detection and treatment can prevent the progression of the\ndisease. In contrast to the conventional method of detecting COPD with\nspirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a\nmeasure of morphological changes in the lung. It has been shown that automated\ndetection of COPD can be performed with deep learning models. However, the\npotential of incorporating optimal window setting selection, typically carried\nout by clinicians during examination of CT scans for COPD, is generally\noverlooked in deep learning approaches. We aim to optimize the binary\nclassification of COPD with densely connected convolutional neural networks\n(DenseNets) through implementation of manual and automated Window-Setting\nOptimization (WSO) steps. Our dataset consisted of 78 CT scans from the\nKlinikum rechts der Isar research hospital. Repeated inference on the test set\nshowed that without WSO, the plain DenseNet resulted in a mean slice-level AUC\nof 0.80$\\pm$0.05. With input images manually adjusted to the emphysema window\nsetting, the plain DenseNet model predicted COPD with a mean AUC of\n0.86$\\pm$0.04. By automating the WSO through addition of a customized layer to\nthe DenseNet, an optimal window setting in the proximity of the emphysema\nwindow setting was learned and a mean AUC of 0.82$\\pm$0.04 was achieved.\nDetection of COPD with DenseNet models was optimized by WSO of CT data to the\nemphysema window setting range, demonstrating the importance of implementing\noptimal window setting selection in the deep learning pipeline.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Johannes Thalhammer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13096v2","updated":"2023-03-13T15:28:11Z","published":"2022-04-27T17:46:55Z","title":"3D Magic Mirror: Clothing Reconstruction from a Single Image via a\n  Causal Perspective","summary":"  This research aims to study a self-supervised 3D clothing reconstruction\nmethod, which recovers the geometry shape and texture of human clothing from a\nsingle image. Compared with existing methods, we observe that three primary\nchallenges remain: (1) 3D ground-truth meshes of clothing are usually\ninaccessible due to annotation difficulties and time costs; (2) Conventional\ntemplate-based methods are limited to modeling non-rigid objects, e.g.,\nhandbags and dresses, which are common in fashion images; (3) The inherent\nambiguity compromises the model training, such as the dilemma between a large\nshape with a remote camera or a small shape with a close camera.\n  In an attempt to address the above limitations, we propose a causality-aware\nself-supervised learning method to adaptively reconstruct 3D non-rigid objects\nfrom 2D images without 3D annotations. In particular, to solve the inherent\nambiguity among four implicit variables, i.e., camera position, shape, texture,\nand illumination, we introduce an explainable structural causal map (SCM) to\nbuild our model. The proposed model structure follows the spirit of the causal\nmap, which explicitly considers the prior template in the camera estimation and\nshape prediction. When optimization, the causality intervention tool, i.e., two\nexpectation-maximization loops, is deeply embedded in our algorithm to (1)\ndisentangle four encoders and (2) facilitate the prior template. Extensive\nexperiments on two 2D fashion benchmarks (ATR and Market-HQ) show that the\nproposed method could yield high-fidelity 3D reconstruction. Furthermore, we\nalso verify the scalability of the proposed method on a fine-grained bird\ndataset, i.e., CUB. The code is available at https://github.com/layumi/\n3D-Magic-Mirror .\n","authors":["Zhedong Zheng","Jiayin Zhu","Wei Ji","Yi Yang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2204.13096v2.pdf","comment":"Update results. Report person re-id performance. Add details in\n  Appendix"},{"id":"http://arxiv.org/abs/2303.07182v1","updated":"2023-03-13T15:24:06Z","published":"2023-03-13T15:24:06Z","title":"Mobile Mapping Mesh Change Detection and Update","summary":"  Mobile mapping, in particular, Mobile Lidar Scanning (MLS) is increasingly\nwidespread to monitor and map urban scenes at city scale with unprecedented\nresolution and accuracy. The resulting point cloud sampling of the scene\ngeometry can be meshed in order to create a continuous representation for\ndifferent applications: visualization, simulation, navigation, etc. Because of\nthe highly dynamic nature of these urban scenes, long term mapping should rely\non frequent map updates. A trivial solution is to simply replace old data with\nnewer data each time a new acquisition is made. However it has two drawbacks:\n1) the old data may be of higher quality (resolution, precision) than the new\nand 2) the coverage of the scene might be different in various acquisitions,\nincluding varying occlusions. In this paper, we propose a fully automatic\npipeline to address these two issues by formulating the problem of merging\nmeshes with different quality, coverage and acquisition time. Our method is\nbased on a combined distance and visibility based change detection, a time\nseries analysis to assess the sustainability of changes, a mesh mosaicking\nbased on a global boolean optimization and finally a stitching of the resulting\nmesh pieces boundaries with triangle strips. Finally, our method is\ndemonstrated on Robotcar and Stereopolis datasets.\n","authors":["Teng Wu","Bruno Vallet","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2303.07182v1.pdf","comment":"6 pages without reference"},{"id":"http://arxiv.org/abs/2303.07180v1","updated":"2023-03-13T15:22:50Z","published":"2023-03-13T15:22:50Z","title":"Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-\n  and Category-Aware Transformers","summary":"  As we all know, multi-view data is more expressive than single-view data and\nmulti-label annotation enjoys richer supervision information than single-label,\nwhich makes multi-view multi-label learning widely applicable for various\npattern recognition tasks. In this complex representation learning problem,\nthree main challenges can be characterized as follows: i) How to learn\nconsistent representations of samples across all views? ii) How to exploit and\nutilize category correlations of multi-label to guide inference? iii) How to\navoid the negative impact resulting from the incompleteness of views or labels?\nTo cope with these problems, we propose a general multi-view multi-label\nlearning framework named label-guided masked view- and category-aware\ntransformers in this paper. First, we design two transformer-style based\nmodules for cross-view features aggregation and multi-label classification,\nrespectively. The former aggregates information from different views in the\nprocess of extracting view-specific features, and the latter learns subcategory\nembedding to improve classification performance. Second, considering the\nimbalance of expressive power among views, an adaptively weighted view fusion\nmodule is proposed to obtain view-consistent embedding features. Third, we\nimpose a label manifold constraint in sample-level representation learning to\nmaximize the utilization of supervised information. Last but not least, all the\nmodules are designed under the premise of incomplete views and labels, which\nmakes our method adaptable to arbitrary multi-view and multi-label data.\nExtensive experiments on five datasets confirm that our method has clear\nadvantages over other state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Xiaoling Luo","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07180v1.pdf","comment":"Accepted to AAAI-23"},{"id":"http://arxiv.org/abs/2302.07570v2","updated":"2023-03-13T15:22:31Z","published":"2023-02-15T10:21:38Z","title":"Super-Resolution of BVOC Maps by Adapting Deep Learning Methods","summary":"  Biogenic Volatile Organic Compounds (BVOCs) play a critical role in\nbiosphere-atmosphere interactions, being a key factor in the physical and\nchemical properties of the atmosphere and climate. Acquiring large and\nfine-grained BVOC emission maps is expensive and time-consuming, so most\navailable BVOC data are obtained on a loose and sparse sampling grid or on\nsmall regions. However, high-resolution BVOC data are desirable in many\napplications, such as air quality, atmospheric chemistry, and climate\nmonitoring. In this work, we investigate the possibility of enhancing BVOC\nacquisitions, further explaining the relationships between the environment and\nthese compounds. We do so by comparing the performances of several\nstate-of-the-art neural networks proposed for image Super-Resolution (SR),\nadapting them to overcome the challenges posed by the large dynamic range of\nthe emission and reduce the impact of outliers in the prediction. Moreover, we\nalso consider realistic scenarios, considering both temporal and geographical\nconstraints. Finally, we present possible future developments regarding SR\ngeneralization, considering the scale-invariance property and super-resolving\nemissions from unseen compounds.\n","authors":["Antonio Giganti","Sara Mandelli","Paolo Bestagini","Marco Marcon","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2302.07570v2.pdf","comment":"5 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2204.09804v3","updated":"2023-03-13T15:20:30Z","published":"2022-04-20T22:48:05Z","title":"Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object\n  Detection","summary":"  Background modeling is widely used for intelligent surveillance systems to\ndetect moving targets by subtracting the static background components. Most\nroadside LiDAR object detection methods filter out foreground points by\ncomparing new data points to pre-trained background references based on\ndescriptive statistics over many frames (e.g., voxel density, number of\nneighbors, maximum distance). However, these solutions are inefficient under\nheavy traffic, and parameter values are hard to transfer from one scenario to\nanother. In early studies, the probabilistic background modeling methods widely\nused for the video-based system were considered unsuitable for roadside LiDAR\nsurveillance systems due to the sparse and unstructured point cloud data. In\nthis paper, the raw LiDAR data were transformed into a structured\nrepresentation based on the elevation and azimuth value of each LiDAR point.\nWith this high-order tensor representation, we break the barrier to allow\nefficient high-dimensional multivariate analysis for roadside LiDAR background\nmodeling. The Bayesian Nonparametric (BNP) approach integrates the intensity\nvalue and 3D measurements to exploit the measurement data using 3D and\nintensity info entirely. The proposed method was compared against two\nstate-of-the-art roadside LiDAR background models, computer vision benchmark,\nand deep learning baselines, evaluated at point, object, and path levels under\nheavy traffic and challenging weather. This multimodal Weighted Bayesian\nGaussian Mixture Model (GMM) can handle dynamic backgrounds with noisy\nmeasurements and substantially enhances the infrastructure-based LiDAR object\ndetection, whereby various 3D modeling for smart city applications could be\ncreated.\n","authors":["Tianya Zhang","Yi Ge","Peter J. Jin"],"pdf_url":"https://arxiv.org/pdf/2204.09804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07172v1","updated":"2023-03-13T15:14:26Z","published":"2023-03-13T15:14:26Z","title":"Evaluating Visual Number Discrimination in Deep Neural Networks","summary":"  The ability to discriminate between large and small quantities is a core\naspect of basic numerical competence in both humans and animals. In this work,\nwe examine the extent to which the state-of-the-art neural networks designed\nfor vision exhibit this basic ability. Motivated by studies in animal and\ninfant numerical cognition, we use the numerical bisection procedure to test\nnumber discrimination in different families of neural architectures. Our\nresults suggest that vision-specific inductive biases are helpful in numerosity\ndiscrimination, as models with such biases have lowest test errors on the task,\nand often have psychometric curves that qualitatively resemble those of humans\nand animals performing the task. However, even the strongest models, as\nmeasured on standard metrics of performance, fail to discriminate quantities in\ntransfer experiments with differing training and testing conditions, indicating\nthat such inductive biases might not be sufficient.\n","authors":["Ivana Kajić","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2303.07172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07169v1","updated":"2023-03-13T15:12:30Z","published":"2023-03-13T15:12:30Z","title":"Dynamic Event-based Optical Flow Identification and Communication","summary":"  Optical identification is often done with spatial or temporal visual pattern\nrecognition and localization. Temporal pattern recognition, depending on the\ntechnology, involves a trade-off between communication frequency, range and\naccurate tracking. We propose a solution with light-emitting beacons that\nimproves this trade-off by exploiting fast event-based cameras and, for\ntracking, sparse neuromorphic optical flow computed with spiking neurons. In an\nasset monitoring use case, we demonstrate that the system, embedded in a\nsimulated drone, is robust to relative movements and enables simultaneous\ncommunication with, and tracking of, multiple moving beacons. Finally, in a\nhardware lab prototype, we achieve state-of-the-art optical camera\ncommunication frequencies in the kHz magnitude.\n","authors":["Axel von Arnim","Jules Lecomte","Stanislaw Wozniak","Naima Elosegui Borras","Angeliki Pantazi"],"pdf_url":"https://arxiv.org/pdf/2303.07169v1.pdf","comment":"5 pages, 6 figures and 1 table"},{"id":"http://arxiv.org/abs/2303.07151v1","updated":"2023-03-13T14:25:39Z","published":"2023-03-13T14:25:39Z","title":"Amélioration de la qualité d'images avec un algorithme\n  d'optimisation inspirée par la nature","summary":"  Reproducible images preprocessing is important in the field of computer\nvision, for efficient algorithms comparison or for new images corpus\npreparation. In this paper, we propose a method to obtain an explicit and\nordered sequence of transformations that improves a given image: the\ncomputation is performed via a nature-inspired optimization algorithm based on\nquality assessment techniques. Preliminary tests show the impact of the\napproach on different state-of-the-art data sets.\n  --\n  L'application de pr\\'etraitements explicites et reproductibles est\nfondamentale dans le domaine de la vision par ordinateur, pour pouvoir comparer\nefficacement des algorithmes ou pour pr\\'eparer un nouveau corpus d'images.\nDans cet article, nous proposons une m\\'ethode pour obtenir une s\\'equence\nreproductible de transformations qui am\\'eliore une image donn\\'ee: le calcul\nest r\\'ealis\\'e via un algorithme d'optimisation inspir\\'ee par la nature et\nbas\\'e sur des techniques d'\\'evaluation de la qualit\\'e. Des tests montrent\nl'impact de l'approche sur diff\\'erents ensembles d'images de l'\\'etat de\nl'art.\n","authors":["Olivier Parisot","Thomas Tamisier"],"pdf_url":"https://arxiv.org/pdf/2303.07151v1.pdf","comment":"8 pages, in French language"},{"id":"http://arxiv.org/abs/2303.07150v1","updated":"2023-03-13T14:23:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07141v1","updated":"2023-03-13T14:09:40Z","published":"2023-03-13T14:09:40Z","title":"An Improved Baseline Framework for Pose Estimation Challenge at ECCV\n  2022 Visual Perception for Navigation in Human Environments Workshop","summary":"  This technical report describes our first-place solution to the pose\nestimation challenge at ECCV 2022 Visual Perception for Navigation in Human\nEnvironments Workshop. In this challenge, we aim to estimate human poses from\nin-the-wild stitched panoramic images. Our method is built based on Faster\nR-CNN for human detection, and HRNet for human pose estimation. We describe\ntechnical details for the JRDB-Pose dataset, together with some experimental\nresults. In the competition, we achieved 0.303 $\\text{OSPA}_{\\text{IOU}}$ and\n64.047\\% $\\text{AP}_{\\text{0.5}}$ on the test set of JRDB-Pose.\n","authors":["Jiajun Fu","Yonghao Dang","Ruoqi Yin","Shaojie Zhang","Feng Zhou","Wending Zhao","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2303.07141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03127v3","updated":"2023-03-13T14:09:06Z","published":"2022-11-06T14:08:04Z","title":"StuArt: Individualized Classroom Observation of Students with Automatic\n  Behavior Recognition and Tracking","summary":"  Each student matters, but it is hardly for instructors to observe all the\nstudents during the courses and provide helps to the needed ones immediately.\nIn this paper, we present StuArt, a novel automatic system designed for the\nindividualized classroom observation, which empowers instructors to concern the\nlearning status of each student. StuArt can recognize five representative\nstudent behaviors (hand-raising, standing, sleeping, yawning, and smiling) that\nare highly related to the engagement and track their variation trends during\nthe course. To protect the privacy of students, all the variation trends are\nindexed by the seat numbers without any personal identification information.\nFurthermore, StuArt adopts various user-friendly visualization designs to help\ninstructors quickly understand the individual and whole learning status.\nExperimental results on real classroom videos have demonstrated the superiority\nand robustness of the embedded algorithms. We expect our system promoting the\ndevelopment of large-scale individualized guidance of students. More\ninformation is in https://github.com/hnuzhy/StuArt.\n","authors":["Huayi Zhou","Fei Jiang","Jiaxin Si","Lili Xiong","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2211.03127v3.pdf","comment":"accepted by ICASSP2023. Novel pedagogical approaches in signal\n  processing for K-12 education"},{"id":"http://arxiv.org/abs/2212.07652v2","updated":"2023-03-13T14:06:14Z","published":"2022-12-15T08:19:02Z","title":"Body-Part Joint Detection and Association via Extended Object\n  Representation","summary":"  The detection of human body and its related parts (e.g., face, head or hands)\nhave been intensively studied and greatly improved since the breakthrough of\ndeep CNNs. However, most of these detectors are trained independently, making\nit a challenging task to associate detected body parts with people. This paper\nfocuses on the problem of joint detection of human body and its corresponding\nparts. Specifically, we propose a novel extended object representation that\nintegrates the center location offsets of body or its parts, and construct a\ndense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part\nassociations in BPJDet are embedded into the unified representation which\ncontains both the semantic and geometric information. Therefore, BPJDet does\nnot suffer from error-prone association post-matching, and has a better\naccuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to\njointly detect any body part. To verify the effectiveness and superiority of\nour method, we conduct extensive experiments on the CityPersons, CrowdHuman and\nBodyHands datasets. The proposed BPJDet detector achieves state-of-the-art\nassociation performance on these three benchmarks while maintains high accuracy\nof detection. Code is in https://github.com/hnuzhy/BPJDet.\n","authors":["Huayi Zhou","Fei Jiang","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2212.07652v2.pdf","comment":"accepted by ICME2023"},{"id":"http://arxiv.org/abs/2303.07130v1","updated":"2023-03-13T13:59:47Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) of COVID-19 can be categorized based on the extent of\nlung involvement observed on a CT scan. This paper proposes a domain\nknowledge-based pipeline to extract the infection regions using diverse\nimage-processing algorithms and a pre-trained UNET model. An ensemble of three\nmachine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),\nand Support Vector Machine (SVM), is employed to classify the CT scans into\ndifferent severity classes. The proposed system achieved a macro F1 score of\n57.47% on the validation dataset in the AI-Enabled Medical Image Analysis\nWorkshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07126v1","updated":"2023-03-13T13:57:29Z","published":"2023-03-13T13:57:29Z","title":"Mirror U-Net: Marrying Multimodal Fission with Multi-task Learning for\n  Semantic Segmentation in Medical Imaging","summary":"  Positron Emission Tomography (PET) and Computer Tomography (CT) are routinely\nused together to detect tumors. PET/CT segmentation models can automate tumor\ndelineation, however, current multimodal models do not fully exploit the\ncomplementary information in each modality, as they either concatenate PET and\nCT data or fuse them at the decision level. To combat this, we propose Mirror\nU-Net, which replaces traditional fusion methods with multimodal fission by\nfactorizing the multimodal representation into modality-specific branches and\nan auxiliary multimodal decoder. At these branches, Mirror U-Net assigns a task\ntailored to each modality to reinforce unimodal features while preserving\nmultimodal features in the shared representation. In contrast to previous\nmethods that use either fission or multi-task learning, Mirror U-Net combines\nboth paradigms in a unified framework. We explore various task combinations and\nexamine which parameters to share in the model. We evaluate Mirror U-Net on the\nAutoPET PET/CT and on the multimodal MSD BrainTumor datasets, demonstrating its\neffectiveness in multimodal segmentation and achieving state-of-the-art\nperformance on both datasets. Our code will be made publicly available.\n","authors":["Zdravko Marinov","Simon Reiß","David Kersting","Jens Kleesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.07126v1.pdf","comment":"8 pages; 8 figures; 5 tables"},{"id":"http://arxiv.org/abs/2210.04087v2","updated":"2023-03-13T13:56:31Z","published":"2022-10-08T18:49:58Z","title":"Symmetry Defense Against CNN Adversarial Perturbation Attacks","summary":"  Convolutional neural network classifiers (CNNs) are susceptible to\nadversarial attacks that perturb original samples to fool classifiers such as\nan autonomous vehicle's road sign image classifier. CNNs also lack invariance\nin the classification of symmetric samples because CNNs can classify symmetric\nsamples differently. Considered together, the CNN lack of adversarial\nrobustness and the CNN lack of invariance mean that the classification of\nsymmetric adversarial samples can differ from their incorrect classification.\nCould symmetric adversarial samples revert to their correct classification?\nThis paper answers this question by designing a symmetry defense that inverts\nor horizontally flips adversarial samples before classification against\nadversaries unaware of the defense. Against adversaries aware of the defense,\nthe defense devises a Klein four symmetry subgroup that includes the horizontal\nflip and pixel inversion symmetries. The symmetry defense uses the subgroup\nsymmetries in accuracy evaluation and the subgroup closure property to confine\nthe transformations that an adaptive adversary can apply before or after\ngenerating the adversarial sample. Without changing the preprocessing,\nparameters, or model, the proposed symmetry defense counters the Projected\nGradient Descent (PGD) and AutoAttack attacks with near-default accuracies for\nImageNet. Without using attack knowledge or adversarial samples, the proposed\ndefense exceeds the current best defense, which trains on adversarial samples.\nThe defense maintains and even improves the classification accuracy of\nnon-adversarial samples.\n","authors":["Blerta Lindqvist"],"pdf_url":"https://arxiv.org/pdf/2210.04087v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07125v1","updated":"2023-03-13T13:56:20Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\n\\url{https://github.com/ai-med/PANIC}.\n","authors":["Tom Nuno Wolf","Sebastian Pölster","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07123v1","updated":"2023-03-13T13:56:11Z","published":"2023-03-13T13:56:11Z","title":"Modality-Agnostic Debiasing for Single Domain Generalization","summary":"  Deep neural networks (DNNs) usually fail to generalize well to outside of\ndistribution (OOD) data, especially in the extreme case of single domain\ngeneralization (single-DG) that transfers DNNs from single domain to multiple\nunseen domains. Existing single-DG techniques commonly devise various\ndata-augmentation algorithms, and remould the multi-source domain\ngeneralization methodology to learn domain-generalized (semantic) features.\nNevertheless, these methods are typically modality-specific, thereby being only\napplicable to one single modality (e.g., image). In contrast, we target a\nversatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that\nenables generalization for different modalities. Technically, MAD introduces a\nnovel two-branch classifier: a biased-branch encourages the classifier to\nidentify the domain-specific (superficial) features, and a general-branch\ncaptures domain-generalized features based on the knowledge from biased-branch.\nOur MAD is appealing in view that it is pluggable to most single-DG models. We\nvalidate the superiority of our MAD in a variety of single-DG scenarios with\ndifferent modalities, including recognition on 1D texts, 2D images, 3D point\nclouds, and semantic segmentation on 2D images. More remarkably, for\nrecognition on 3D point clouds and semantic segmentation on 2D images, MAD\nimproves DSU by 2.82\\% and 1.5\\% in accuracy and mIOU.\n","authors":["Sanqing Qu","Yingwei Pan","Guang Chen","Ting Yao","Changjun Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2303.07123v1.pdf","comment":"To appear in CVPR-2023"},{"id":"http://arxiv.org/abs/2303.04940v2","updated":"2023-03-13T13:56:09Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in misaligned hazy and clear image pairs. In this\npaper, we propose a non-aligned supervision framework that consists of three\nnetworks - dehazing, airlight, and transmission. In particular, we explore a\nnon-alignment setting by utilizing a clear reference image that is not aligned\nwith the hazy input image to supervise the dehazing network through a\nmulti-scale reference loss that compares the features of the two images. Our\nsetting makes it easier to collect hazy/clear image pairs in real-world\nenvironments, even under conditions of misalignment and shift views. To\ndemonstrate this, we have created a new hazy dataset called \"Phone-Hazy\", which\nwas captured using mobile phones in both rural and urban areas. Additionally,\nwe present a mean and variance self-attention network to model the infinite\nairlight using dark channel prior as position guidance, and employ a channel\nattention network to estimate the three-channel transmission. Experimental\nresults show that our framework outperforms current state-of-the-art methods in\nthe real-world image dehazing. Phone-Hazy and code will be available at\nhttps://github.com/hello2377/NSDNet.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2303.07115v1","updated":"2023-03-13T13:47:57Z","published":"2023-03-13T13:47:57Z","title":"NeurEPDiff: Neural Operators to Predict Geodesics in Deformation Spaces","summary":"  This paper presents NeurEPDiff, a novel network to fast predict the geodesics\nin deformation spaces generated by a well known Euler-Poincar\\'e differential\nequation (EPDiff). To achieve this, we develop a neural operator that for the\nfirst time learns the evolving trajectory of geodesic deformations\nparameterized in the tangent space of diffeomorphisms(a.k.a velocity fields).\nIn contrast to previous methods that purely fit the training images, our\nproposed NeurEPDiff learns a nonlinear mapping function between the\ntime-dependent velocity fields. A composition of integral operators and smooth\nactivation functions is formulated in each layer of NeurEPDiff to effectively\napproximate such mappings. The fact that NeurEPDiff is able to rapidly provide\nthe numerical solution of EPDiff (given any initial condition) results in a\nsignificantly reduced computational cost of geodesic shooting of\ndiffeomorphisms in a high-dimensional image space. Additionally, the properties\nof discretiztion/resolution-invariant of NeurEPDiff make its performance\ngeneralizable to multiple image resolutions after being trained offline. We\ndemonstrate the effectiveness of NeurEPDiff in registering two image datasets:\n2D synthetic data and 3D brain resonance imaging (MRI). The registration\naccuracy and computational efficiency are compared with the state-of-the-art\ndiffeomophic registration algorithms with geodesic shooting.\n","authors":["Nian Wu","Miaomiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07110v1","updated":"2023-03-13T13:44:04Z","published":"2023-03-13T13:44:04Z","title":"Upcycling Models under Domain and Category Shift","summary":"  Deep neural networks (DNNs) often perform poorly in the presence of domain\nshift and category shift. How to upcycle DNNs and adapt them to the target task\nremains an important open problem. Unsupervised Domain Adaptation (UDA),\nespecially recently proposed Source-free Domain Adaptation (SFDA), has become a\npromising technology to address this issue. Nevertheless, existing SFDA methods\nrequire that the source domain and target domain share the same label space,\nconsequently being only applicable to the vanilla closed-set setting. In this\npaper, we take one step further and explore the Source-free Universal Domain\nAdaptation (SF-UniDA). The goal is to identify \"known\" data samples under both\ndomain and category shift, and reject those \"unknown\" data samples (not present\nin source classes), with only the knowledge from standard pre-trained source\nmodel. To this end, we introduce an innovative global and local clustering\nlearning technique (GLC). Specifically, we design a novel, adaptive one-vs-all\nglobal clustering algorithm to achieve the distinction across different target\nclasses and introduce a local k-NN clustering strategy to alleviate negative\ntransfer. We examine the superiority of our GLC on multiple benchmarks with\ndifferent category shift scenarios, including partial-set, open-set, and\nopen-partial-set DA. Remarkably, in the most challenging open-partial-set DA\nscenario, GLC outperforms UMAD by 14.8\\% on the VisDA benchmark. The code is\navailable at https://github.com/ispc-lab/GLC.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Roehrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07110v1.pdf","comment":"To appear in CVPR 2023. The code has been made public"},{"id":"http://arxiv.org/abs/2303.07100v1","updated":"2023-03-13T13:40:09Z","published":"2023-03-13T13:40:09Z","title":"A Feature-based Approach for the Recognition of Image Quality\n  Degradation in Automotive Applications","summary":"  Cameras play a crucial role in modern driver assistance systems and are an\nessential part of the sensor technology for automated driving. The quality of\nimages captured by in-vehicle cameras highly influences the performance of\nvisual perception systems. This paper presents a feature-based algorithm to\ndetect certain effects that can degrade image quality in automotive\napplications. The algorithm is based on an intelligent selection of significant\nfeatures. Due to the small number of features, the algorithm performs well even\nwith small data sets. Experiments with different data sets show that the\nalgorithm can detect soiling adhering to camera lenses and classify different\ntypes of image degradation.\n","authors":["Florian Bauer"],"pdf_url":"https://arxiv.org/pdf/2303.07100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07096v1","updated":"2023-03-13T13:30:59Z","published":"2023-03-13T13:30:59Z","title":"Prototype-based Embedding Network for Scene Graph Generation","summary":"  Current Scene Graph Generation (SGG) methods explore contextual information\nto predict relationships among entity pairs. However, due to the diverse visual\nappearance of numerous possible subject-object combinations, there is a large\nintra-class variation within each predicate category, e.g., \"man-eating-pizza,\ngiraffe-eating-leaf\", and the severe inter-class similarity between different\nclasses, e.g., \"man-holding-plate, man-eating-pizza\", in model's latent space.\nThe above challenges prevent current SGG methods from acquiring robust features\nfor reliable relation prediction. In this paper, we claim that the predicate's\ncategory-inherent semantics can serve as class-wise prototypes in the semantic\nspace for relieving the challenges. To the end, we propose the Prototype-based\nEmbedding Network (PE-Net), which models entities/predicates with\nprototype-aligned compact and distinctive representations and thereby\nestablishes matching between entity pairs and predicates in a common embedding\nspace for relation recognition. Moreover, Prototype-guided Learning (PL) is\nintroduced to help PE-Net efficiently learn such entitypredicate matching, and\nPrototype Regularization (PR) is devised to relieve the ambiguous\nentity-predicate matching caused by the predicate's semantic overlap. Extensive\nexperiments demonstrate that our method gains superior relation recognition\ncapability on SGG, achieving new state-of-the-art performances on both Visual\nGenome and Open Images datasets.\n","authors":["Chaofan Zheng","Xinyu Lyu","Lianli Gao","Bo Dai","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2303.07096v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.07182v6","updated":"2023-03-13T13:27:02Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk Pflüger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v6.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2208.02998v3","updated":"2023-03-13T13:25:27Z","published":"2022-08-05T05:48:28Z","title":"Localized Sparse Incomplete Multi-view Clustering","summary":"  Incomplete multi-view clustering, which aims to solve the clustering problem\non the incomplete multi-view data with partial view missing, has received more\nand more attention in recent years. Although numerous methods have been\ndeveloped, most of the methods either cannot flexibly handle the incomplete\nmulti-view data with arbitrary missing views or do not consider the negative\nfactor of information imbalance among views. Moreover, some methods do not\nfully explore the local structure of all incomplete views. To tackle these\nproblems, this paper proposes a simple but effective method, named localized\nsparse incomplete multi-view clustering (LSIMVC). Different from the existing\nmethods, LSIMVC intends to learn a sparse and structured consensus latent\nrepresentation from the incomplete multi-view data by optimizing a sparse\nregularized and novel graph embedded multi-view matrix factorization model.\nSpecifically, in such a novel model based on the matrix factorization, a l1\nnorm based sparse constraint is introduced to obtain the sparse low-dimensional\nindividual representations and the sparse consensus representation. Moreover, a\nnovel local graph embedding term is introduced to learn the structured\nconsensus representation. Different from the existing works, our local graph\nembedding term aggregates the graph embedding task and consensus representation\nlearning task into a concise term. Furthermore, to reduce the imbalance factor\nof incomplete multi-view learning, an adaptive weighted learning scheme is\nintroduced to LSIMVC. Finally, an efficient optimization strategy is given to\nsolve the optimization problem of our proposed model. Comprehensive\nexperimental results performed on six incomplete multi-view databases verify\nthat the performance of our LSIMVC is superior to the state-of-the-art IMC\napproaches. The code is available in https://github.com/justsmart/LSIMVC.\n","authors":["Chengliang Liu","Zhihao Wu","Jie Wen","Chao Huang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2208.02998v3.pdf","comment":"Published in IEEE Transactions on Multimedia (TMM). The code is\n  available at Github https://github.com/justsmart/LSIMVC"},{"id":"http://arxiv.org/abs/2303.07093v1","updated":"2023-03-13T13:23:57Z","published":"2023-03-13T13:23:57Z","title":"Weakly Unsupervised Domain Adaptation for Vestibular Schwannoma\n  Segmentation","summary":"  Vestibular schwannoma (VS) is a non-cancerous tumor located next to the ear\nthat can cause hearing loss. Most brain MRI images acquired from patients are\ncontrast-enhanced T1 (ceT1), with a growing interest in high-resolution T2\nimages (hrT2) to replace ceT1, which involves the use of a contrast agent. As\nhrT2 images are currently scarce, it is less likely to train robust machine\nlearning models to segment VS or other brain structures. In this work, we\npropose a weakly supervised machine learning approach that learns from only\nceT1 scans and adapts to segment two structures from hrT2 scans: the VS and the\ncochlea from the crossMoDA dataset. Our model 1) generates fake hrT2 scans from\nceT1 images and segmentation masks, 2) is trained using the fake hrT2 scans, 3)\npredicts the augmented real hrT2 scans, and 4) is retrained again using both\nthe fake and real hrT2. The final result of this model has been computed on an\nunseen testing dataset provided by the 2022 crossMoDA challenge organizers. The\nmean dice score and average symmetric surface distance (ASSD) are 0.78 and\n0.46, respectively. The predicted segmentation masks achieved a dice score of\n0.83 and an ASSD of 0.56 on the VS, and a dice score of 0.74 and an ASSD of\n0.35 on the cochleas.\n","authors":["Shahad Hardan","Hussain Alasmawi","Xiangjian Hou","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2303.07093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10916v3","updated":"2023-03-13T13:14:55Z","published":"2022-11-20T09:20:32Z","title":"ECM-OPCC: Efficient Context Model for Octree-based Point Cloud\n  Compression","summary":"  Recently, deep learning methods have shown promising results in point cloud\ncompression. For octree-based point cloud compression, previous works show that\nthe information of ancestor nodes and sibling nodes are equally important for\npredicting current node. However, those works either adopt insufficient context\nor bring intolerable decoding complexity (e.g. >600s). To address this problem,\nwe propose a sufficient yet efficient context model and design an efficient\ndeep learning codec for point clouds. Specifically, we first propose a\nwindow-constrained multi-group coding strategy to exploit the autoregressive\ncontext while maintaining decoding efficiency. Then, we propose a dual\ntransformer architecture to utilize the dependency of current node on its\nancestors and siblings. We also propose a random-masking pre-train method to\nenhance our model. Experimental results show that our approach achieves\nstate-of-the-art performance for both lossy and lossless point cloud\ncompression. Moreover, our multi-group coding strategy saves 98% decoding time\ncompared with previous octree-based compression method.\n","authors":["Yiqi Jin","Ziyu Zhu","Tongda Xu","Yuhuan Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07084v1","updated":"2023-03-13T13:08:20Z","published":"2023-03-13T13:08:20Z","title":"The challenge of representation learning: Improved accuracy in deep\n  vision models does not come with better predictions of perceptual similarity","summary":"  Over the last years, advancements in deep learning models for computer vision\nhave led to a dramatic improvement in their image classification accuracy.\nHowever, models with a higher accuracy in the task they were trained on do not\nnecessarily develop better image representations that allow them to also\nperform better in other tasks they were not trained on. In order to investigate\nthe representation learning capabilities of prominent high-performing computer\nvision models, we investigated how well they capture various indices of\nperceptual similarity from large-scale behavioral datasets. We find that higher\nimage classification accuracy rates are not associated with a better\nperformance on these datasets, and in fact we observe no improvement in\nperformance since GoogLeNet (released 2015) and VGG-M (released 2014). We\nspeculate that more accurate classification may result from hyper-engineering\ntowards very fine-grained distinctions between highly similar classes, which\ndoes not incentivize the models to capture overall perceptual similarities.\n","authors":["Fritz Günther","Marco Marelli","Marco Alessandro Petilli"],"pdf_url":"https://arxiv.org/pdf/2303.07084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07080v1","updated":"2023-03-13T13:05:33Z","published":"2023-03-13T13:05:33Z","title":"Bag of Tricks with Quantized Convolutional Neural Networks for image\n  classification","summary":"  Deep neural networks have been proven effective in a wide range of tasks.\nHowever, their high computational and memory costs make them impractical to\ndeploy on resource-constrained devices. To address this issue, quantization\nschemes have been proposed to reduce the memory footprint and improve inference\nspeed. While numerous quantization methods have been proposed, they lack\nsystematic analysis for their effectiveness. To bridge this gap, we collect and\nimprove existing quantization methods and propose a gold guideline for\npost-training quantization. We evaluate the effectiveness of our proposed\nmethod with two popular models, ResNet50 and MobileNetV2, on the ImageNet\ndataset. By following our guidelines, no accuracy degradation occurs even after\ndirectly quantizing the model to 8-bits without additional training. A\nquantization-aware training based on the guidelines can further improve the\naccuracy in lower-bits quantization. Moreover, we have integrated a multi-stage\nfine-tuning strategy that works harmoniously with existing pruning techniques\nto reduce costs even further. Remarkably, our results reveal that a quantized\nMobileNetV2 with 30\\% sparsity actually surpasses the performance of the\nequivalent full-precision model, underscoring the effectiveness and resilience\nof our proposed scheme.\n","authors":["Jie Hu","Mengze Zeng","Enhua Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07080v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07077v1","updated":"2023-03-13T12:59:53Z","published":"2023-03-13T12:59:53Z","title":"Spatial Attention and Syntax Rule Enhanced Tree Decoder for Offine\n  Handwritten Mathematical Expression Recognition","summary":"  Offline Handwritten Mathematical Expression Recognition (HMER) has been\ndramatically advanced recently by employing tree decoders as part of the\nencoder-decoder method. Despite the tree decoder-based methods regard the\nexpressions as a tree and parse 2D spatial structure to the tree nodes\nsequence, the performance of existing works is still poor due to the inevitable\ntree nodes prediction errors. Besides, they lack syntax rules to regulate the\noutput of expressions. In this paper, we propose a novel model called Spatial\nAttention and Syntax Rule Enhanced Tree Decoder (SS-TD), which is equipped with\nspatial attention mechanism to alleviate the prediction error of tree structure\nand use syntax masks (obtained from the transformation of syntax rules) to\nconstrain the occurrence of ungrammatical mathematical expression. In this way,\nour model can effectively describe tree structure and increase the accuracy of\noutput expression. Experiments show that SS-TD achieves better recognition\nperformance than prior models on CROHME 14/16/19 datasets, demonstrating the\neffectiveness of our model.\n","authors":["Zihao Lin","Jinrong Li","Fan Yang","Shuangping Huang","Xu Yang","Jianmin Lin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07065v1","updated":"2023-03-13T12:39:59Z","published":"2023-03-13T12:39:59Z","title":"MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object\n  ReID","summary":"  Neural Architecture Search (NAS) has been increasingly appealing to the\nsociety of object Re-Identification (ReID), for that task-specific\narchitectures significantly improve the retrieval performance. Previous works\nexplore new optimizing targets and search spaces for NAS ReID, yet they neglect\nthe difference of training schemes between image classification and ReID. In\nthis work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more\nappropriate supervision for ReID architecture search. TCM reduces the category\noverlaps between the training and validation data, and assists NAS in\nsimulating real-world ReID training schemes. We then design a Multi-Scale\nInteraction (MSI) search space to search for rational interaction operations\nbetween multi-scale features. In addition, we introduce a Spatial Alignment\nModule (SAM) to further enhance the attention consistency confronted with\nimages from different sources. Under the proposed NAS scheme, a specific\narchitecture is automatically searched, named as MSINet. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art ReID methods on both\nin-domain and cross-domain scenarios. Source code available in\nhttps://github.com/vimar-gu/MSINet.\n","authors":["Jianyang Gu","Kai Wang","Hao Luo","Chen Chen","Wei Jiang","Yuqiang Fang","Shanghang Zhang","Yang You","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07065v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07064v1","updated":"2023-03-13T12:38:07Z","published":"2023-03-13T12:38:07Z","title":"A Generalized Multi-Modal Fusion Detection Framework","summary":"  LiDAR point clouds have become the most common data source in autonomous\ndriving. However, due to the sparsity of point clouds, accurate and reliable\ndetection cannot be achieved in specific scenarios. Because of their\ncomplementarity with point clouds, images are getting increasing attention.\nAlthough with some success, existing fusion methods either perform hard fusion\nor do not fuse in a direct manner. In this paper, we propose a generic 3D\ndetection framework called MMFusion, using multi-modal features. The framework\naims to achieve accurate fusion between LiDAR and images to improve 3D\ndetection in complex scenes. Our framework consists of two separate streams:\nthe LiDAR stream and the camera stream, which can be compatible with any\nsingle-modal feature extraction network. The Voxel Local Perception Module in\nthe LiDAR stream enhances local feature representation, and then the\nMulti-modal Feature Fusion Module selectively combines feature output from\ndifferent streams to achieve better fusion. Extensive experiments have shown\nthat our framework not only outperforms existing benchmarks but also improves\ntheir detection, especially for detecting cyclists and pedestrians on KITTI\nbenchmarks, with strong robustness and generalization capabilities. Hopefully,\nour work will stimulate more research into multi-modal fusion for autonomous\ndriving tasks.\n","authors":["Leichao Cui","Xiuxian Li","Min Meng","Xiaoyu Mo"],"pdf_url":"https://arxiv.org/pdf/2303.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01162v4","updated":"2023-03-13T12:37:15Z","published":"2023-02-02T15:37:46Z","title":"Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using\n  Pixel-aligned Reconstruction Priors","summary":"  Fast generation of high-quality 3D digital humans is important to a vast\nnumber of applications ranging from entertainment to professional concerns.\nRecent advances in differentiable rendering have enabled the training of 3D\ngenerative models without requiring 3D ground truths. However, the quality of\nthe generated 3D humans still has much room to improve in terms of both\nfidelity and diversity. In this paper, we present Get3DHuman, a novel 3D human\nframework that can significantly boost the realism and diversity of the\ngenerated outcomes by only using a limited budget of 3D ground-truth data. Our\nkey observation is that the 3D generator can profit from human-related priors\nlearned through 2D human generators and 3D reconstructors. Specifically, we\nbridge the latent space of Get3DHuman with that of StyleGAN-Human via a\nspecially-designed prior network, where the input latent code is mapped to the\nshape and texture feature volumes spanned by the pixel-aligned 3D\nreconstructor. The outcomes of the prior network are then leveraged as the\nsupervisory signals for the main generator network. To ensure effective\ntraining, we further propose three tailored losses applied to the generated\nfeature volumes and the intermediate feature maps. Extensive experiments\ndemonstrate that Get3DHuman greatly outperforms the other state-of-the-art\napproaches and can support a wide range of applications including shape\ninterpolation, shape re-texturing, and single-view reconstruction through\nlatent inversion.\n","authors":["Zhangyang Xiong","Di Kang","Derong Jin","Weikai Chen","Linchao Bao","Shuguang Cui","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2302.01162v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.06592v2","updated":"2023-03-13T12:21:16Z","published":"2021-12-13T12:18:43Z","title":"CR-FIQA: Face Image Quality Assessment by Learning Sample Relative\n  Classifiability","summary":"  The quality of face images significantly influences the performance of\nunderlying face recognition algorithms. Face image quality assessment (FIQA)\nestimates the utility of the captured image in achieving reliable and accurate\nrecognition performance. In this work, we propose a novel learning paradigm\nthat learns internal network observations during the training process. Based on\nthat, our proposed CR-FIQA uses this paradigm to estimate the face image\nquality of a sample by predicting its relative classifiability. This\nclassifiability is measured based on the allocation of the training sample\nfeature representation in angular space with respect to its class center and\nthe nearest negative class center. We experimentally illustrate the correlation\nbetween the face image quality and the sample relative classifiability. As such\nproperty is only observable for the training dataset, we propose to learn this\nproperty from the training dataset and utilize it to predict the quality\nmeasure on unseen samples. This training is performed simultaneously while\noptimizing the class centers by an angular margin penalty-based softmax loss\nused for face recognition model training. Through extensive evaluation\nexperiments on eight benchmarks and four face recognition models, we\ndemonstrate the superiority of our proposed CR-FIQA over state-of-the-art\n(SOTA) FIQA algorithms.\n","authors":["Fadi Boutros","Meiling Fang","Marcel Klemt","Biying Fu","Naser Damer"],"pdf_url":"https://arxiv.org/pdf/2112.06592v2.pdf","comment":"Accepted at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023 (CVPR2023)"},{"id":"http://arxiv.org/abs/2303.07035v1","updated":"2023-03-13T11:54:16Z","published":"2023-03-13T11:54:16Z","title":"FireRisk: A Remote Sensing Dataset for Fire Risk Assessment with\n  Benchmarks Using Supervised and Self-supervised Learning","summary":"  In recent decades, wildfires, as widespread and extremely destructive natural\ndisasters, have caused tremendous property losses and fatalities, as well as\nextensive damage to forest ecosystems. Many fire risk assessment projects have\nbeen proposed to prevent wildfires, but GIS-based methods are inherently\nchallenging to scale to different geographic areas due to variations in data\ncollection and local conditions. Inspired by the abundance of publicly\navailable remote sensing projects and the burgeoning development of deep\nlearning in computer vision, our research focuses on assessing fire risk using\nremote sensing imagery.\n  In this work, we propose a novel remote sensing dataset, FireRisk, consisting\nof 7 fire risk classes with a total of 91872 labelled images for fire risk\nassessment. This remote sensing dataset is labelled with the fire risk classes\nsupplied by the Wildfire Hazard Potential (WHP) raster dataset, and remote\nsensing images are collected using the National Agriculture Imagery Program\n(NAIP), a high-resolution remote sensing imagery program. On FireRisk, we\npresent benchmark performance for supervised and self-supervised\nrepresentations, with Masked Autoencoders (MAE) pre-trained on ImageNet1k\nachieving the highest classification accuracy, 65.29%.\n  This remote sensing dataset, FireRisk, provides a new direction for fire risk\nassessment, and we make it publicly available on\nhttps://github.com/CharmonyShen/FireRisk.\n","authors":["Shuchang Shen","Sachith Seneviratne","Xinye Wanyan","Michael Kirley"],"pdf_url":"https://arxiv.org/pdf/2303.07035v1.pdf","comment":"10 pages, 6 figures, 1 table, 1 equation"},{"id":"http://arxiv.org/abs/2303.07034v1","updated":"2023-03-13T11:53:40Z","published":"2023-03-13T11:53:40Z","title":"Pretrained ViTs Yield Versatile Representations For Medical Images","summary":"  Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis, pushing the\nstate-of-the-art in classification, detection and segmentation tasks. Over the\nlast years, vision transformers (ViTs) have appeared as a competitive\nalternative to CNNs, yielding impressive levels of performance in the natural\nimage domain, while possessing several interesting properties that could prove\nbeneficial for medical imaging tasks. In this work, we explore the benefits and\ndrawbacks of transformer-based models for medical image classification. We\nconduct a series of experiments on several standard 2D medical image benchmark\ndatasets and tasks. Our findings show that, while CNNs perform better if\ntrained from scratch, off-the-shelf vision transformers can perform on par with\nCNNs when pretrained on ImageNet, both in a supervised and self-supervised\nsetting, rendering them as a viable alternative to CNNs.\n","authors":["Christos Matsoukas","Johan Fredin Haslum","Magnus Söderberg","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2303.07034v1.pdf","comment":"Extended version of \"Is it Time to Replace CNNs with Transformers for\n  Medical Images?\" (Matsoukas et al. 2022) originally published at the ICCV\n  2021 Workshop on Computer Vision for Automated Medical Diagnosis"},{"id":"http://arxiv.org/abs/2303.07033v1","updated":"2023-03-13T11:47:24Z","published":"2023-03-13T11:47:24Z","title":"SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency","summary":"  This work presents an effective depth-consistency self-prompt Transformer for\nimage dehazing. It is motivated by an observation that the estimated depths of\nan image with haze residuals and its clear counterpart vary. Enforcing the\ndepth consistency of dehazed images with clear ones, therefore, is essential\nfor dehazing. For this purpose, we develop a prompt based on the features of\ndepth differences between the hazy input images and corresponding clear\ncounterparts that can guide dehazing models for better restoration.\nSpecifically, we first apply deep features extracted from the input images to\nthe depth difference features for generating the prompt that contains the haze\nresidual information in the input. Then we propose a prompt embedding module\nthat is designed to perceive the haze residuals, by linearly adding the prompt\nto the deep features. Further, we develop an effective prompt attention module\nto pay more attention to haze residuals for better removal. By incorporating\nthe prompt, prompt embedding, and prompt attention into an encoder-decoder\nnetwork based on VQGAN, we can achieve better perception quality. As the depths\nof clear images are not available at inference, and the dehazed images with\none-time feed-forward execution may still contain a portion of haze residuals,\nwe propose a new continuous self-prompt inference that can iteratively correct\nthe dehazing model towards better haze-free image generation. Extensive\nexperiments show that our method performs favorably against the\nstate-of-the-art approaches on both synthetic and real-world datasets in terms\nof perception metrics including NIQE, PI, and PIQE.\n","authors":["Cong Wang","Jinshan Pan","Wanyu Lin","Jiangxin Dong","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07016v1","updated":"2023-03-13T11:25:32Z","published":"2023-03-13T11:25:32Z","title":"HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using\n  Inertial Sensing","summary":"  Current Virtual Reality systems are designed for interaction under visual\ncontrol. Using built-in cameras, headsets track the user's hands or hand-held\ncontrollers while they are inside the field of view. Current systems thus\nignore the user's interaction with off-screen content -- virtual objects that\nthe user could quickly access through proprioception without requiring\nlaborious head motions to bring them into focus. In this paper, we present\nHOOV, a wrist-worn sensing method that allows VR users to interact with objects\noutside their field of view. Based on the signals of a single wrist-worn\ninertial sensor, HOOV continuously estimates the user's hand position in\n3-space to complement the headset's tracking as the hands leave the tracking\nrange. Our novel data-driven method predicts hand positions and trajectories\nfrom just the continuous estimation of hand orientation, which by itself is\nstable based solely on inertial observations. Our inertial sensing\nsimultaneously detects finger pinching to register off-screen selection events,\nconfirms them using a haptic actuator inside our wrist device, and thus allows\nusers to select, grab, and drop virtual content. We compared HOOV's performance\nwith a camera-based optical motion capture system in two folds. In the first\nevaluation, participants interacted based on tracking information from the\nmotion capture system to assess the accuracy of their proprioceptive input,\nwhereas in the second, they interacted based on HOOV's real-time estimations.\nWe found that HOOV's target-agnostic estimations had a mean tracking error of\n7.7 cm, which allowed participants to reliably access virtual objects around\ntheir body without first bringing them into focus. We demonstrate several\napplications that leverage the larger input space HOOV opens up for quick\nproprioceptive interaction, and conclude by discussing the potential of our\ntechnique.\n","authors":["Paul Streli","Rayan Armani","Yi Fei Cheng","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2303.07016v1.pdf","comment":"Accepted at 2023 CHI Conference on Human Factors in Computing Systems"},{"id":"http://arxiv.org/abs/2212.00935v2","updated":"2023-03-13T11:24:49Z","published":"2022-12-02T02:47:30Z","title":"Dunhuang murals contour generation network based on convolution and\n  self-attention fusion","summary":"  Dunhuang murals are a collection of Chinese style and national style, forming\na self-contained Chinese-style Buddhist art. It has very high historical and\ncultural value and research significance. Among them, the lines of Dunhuang\nmurals are highly general and expressive. It reflects the character's\ndistinctive character and complex inner emotions. Therefore, the outline\ndrawing of murals is of great significance to the research of Dunhuang Culture.\nThe contour generation of Dunhuang murals belongs to image edge detection,\nwhich is an important branch of computer vision, aims to extract salient\ncontour information in images. Although convolution-based deep learning\nnetworks have achieved good results in image edge extraction by exploring the\ncontextual and semantic features of images. However, with the enlargement of\nthe receptive field, some local detail information is lost. This makes it\nimpossible for them to generate reasonable outline drawings of murals. In this\npaper, we propose a novel edge detector based on self-attention combined with\nconvolution to generate line drawings of Dunhuang murals. Compared with\nexisting edge detection methods, firstly, a new residual self-attention and\nconvolution mixed module (Ramix) is proposed to fuse local and global features\nin feature maps. Secondly, a novel densely connected backbone extraction\nnetwork is designed to efficiently propagate rich edge feature information from\nshallow layers into deep layers. Compared with existing methods, it is shown on\ndifferent public datasets that our method is able to generate sharper and\nricher edge maps. In addition, testing on the Dunhuang mural dataset shows that\nour method can achieve very competitive performance.\n","authors":["Baokai Liu","Fengjie He","Shiqiang Du","Kaiwu Zhang","Jianhua Wang"],"pdf_url":"https://arxiv.org/pdf/2212.00935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07014v1","updated":"2023-03-13T11:22:37Z","published":"2023-03-13T11:22:37Z","title":"Reference-Guided Large-Scale Face Inpainting with Identity and Texture\n  Control","summary":"  Face inpainting aims at plausibly predicting missing pixels of face images\nwithin a corrupted region. Most existing methods rely on generative models\nlearning a face image distribution from a big dataset, which produces\nuncontrollable results, especially with large-scale missing regions. To\nintroduce strong control for face inpainting, we propose a novel\nreference-guided face inpainting method that fills the large-scale missing\nregion with identity and texture control guided by a reference face image.\nHowever, generating high-quality results under imposing two control signals is\nchallenging. To tackle such difficulty, we propose a dual control one-stage\nframework that decouples the reference image into two levels for flexible\ncontrol: High-level identity information and low-level texture information,\nwhere the identity information figures out the shape of the face and the\ntexture information depicts the component-aware texture. To synthesize\nhigh-quality results, we design two novel modules referred to as Half-AdaIN and\nComponent-Wise Style Injector (CWSI) to inject the two kinds of control\ninformation into the inpainting processing. Our method produces realistic\nresults with identity and texture control faithful to reference images. To the\nbest of our knowledge, it is the first work to concurrently apply identity and\ncomponent-level controls in face inpainting to promise more precise and\ncontrollable results. Code is available at\nhttps://github.com/WuyangLuo/RefFaceInpainting\n","authors":["Wuyang Luo","Su Yang","Weishan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07014v1.pdf","comment":"accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology"},{"id":"http://arxiv.org/abs/2303.07012v1","updated":"2023-03-13T11:18:41Z","published":"2023-03-13T11:18:41Z","title":"AGTGAN: Unpaired Image Translation for Photographic Ancient Character\n  Generation","summary":"  The study of ancient writings has great value for archaeology and philology.\nEssential forms of material are photographic characters, but manual\nphotographic character recognition is extremely time-consuming and\nexpertise-dependent. Automatic classification is therefore greatly desired.\nHowever, the current performance is limited due to the lack of annotated data.\nData generation is an inexpensive but useful solution for data scarcity.\nNevertheless, the diverse glyph shapes and complex background textures of\nphotographic ancient characters make the generation task difficult, leading to\nthe unsatisfactory results of existing methods. In this paper, we propose an\nunsupervised generative adversarial network called AGTGAN. By the explicit\nglobal and local glyph shape style modeling followed by the stroke-aware\ntexture transfer, as well as an associate adversarial learning mechanism, our\nmethod can generate characters with diverse glyphs and realistic textures. We\nevaluate our approach on the photographic ancient character datasets, e.g.,\nOBC306 and CSDD. Our method outperforms the state-of-the-art approaches in\nvarious metrics and performs much better in terms of the diversity and\nauthenticity of generated samples. With our generated images, experiments on\nthe largest photographic oracle bone character dataset show that our method can\nachieve a significant increase in classification accuracy, up to 16.34%.\n","authors":["Hongxiang Huang","Daihui Yang","Gang Dai","Zhen Han","Yuyi Wang","Kin-Man Lam","Fan Yang","Shuangping Huang","Yongge Liu","Mengchao He"],"pdf_url":"https://arxiv.org/pdf/2303.07012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07011v1","updated":"2023-03-13T11:11:41Z","published":"2023-03-13T11:11:41Z","title":"OSIS: Efficient One-stage Network for 3D Instance Segmentation","summary":"  Current 3D instance segmentation models generally use multi-stage methods to\nextract instance objects, including clustering, feature extraction, and\npost-processing processes. However, these multi-stage approaches rely on\nhyperparameter settings and hand-crafted processes, which restrict the\ninference speed of the model. In this paper, we propose a new 3D point cloud\ninstance segmentation network, named OSIS. OSIS is a one-stage network, which\ndirectly segments instances from 3D point cloud data using neural network. To\nsegment instances directly from the network, we propose an instance decoder,\nwhich decodes instance features from the network into instance segments. Our\nproposed OSIS realizes the end-to-end training by bipartite matching,\ntherefore, our network does not require computationally expensive\npost-processing steps such as non maximum suppression (NMS) and clustering\nduring inference. The results show that our network finally achieves excellent\nperformance in the commonly used indoor scene instance segmentation dataset,\nand the inference speed of our network is only an average of 138ms per scene,\nwhich substantially exceeds the previous fastest method.\n","authors":["Chuan Tang","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10420v2","updated":"2023-03-13T11:00:33Z","published":"2023-02-21T03:16:22Z","title":"HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection","summary":"  Very-high-resolution (VHR) remote sensing (RS) image change detection (CD)\nhas been a challenging task for its very rich spatial information and sample\nimbalance problem. In this paper, we have proposed a hierarchical change\nguiding map network (HCGMNet) for change detection. The model uses hierarchical\nconvolution operations to extract multiscale features, continuously merges\nmulti-scale features layer by layer to improve the expression of global and\nlocal information, and guides the model to gradually refine edge features and\ncomprehensive performance by a change guide module (CGM), which is a\nself-attention with changing guide map. Extensive experiments on two CD\ndatasets show that the proposed HCGMNet architecture achieves better CD\nperformance than existing state-of-the-art (SOTA) CD methods.\n","authors":["Chengxi Han","Chen Wu","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2302.10420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06999v1","updated":"2023-03-13T10:54:52Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., when considering 200\nproposals from our method, we detect label errors with a precision for a) of up\nto 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06994v1","updated":"2023-03-13T10:49:59Z","published":"2023-03-13T10:49:59Z","title":"Synthesizing Realistic Image Restoration Training Pairs: A Diffusion\n  Approach","summary":"  In supervised image restoration tasks, one key issue is how to obtain the\naligned high-quality (HQ) and low-quality (LQ) training image pairs.\nUnfortunately, such HQ-LQ training pairs are hard to capture in practice, and\nhard to synthesize due to the complex unknown degradation in the wild. While\nseveral sophisticated degradation models have been manually designed to\nsynthesize LQ images from their HQ counterparts, the distribution gap between\nthe synthesized and real-world LQ images remains large. We propose a new\napproach to synthesizing realistic image restoration training pairs using the\nemerging denoising diffusion probabilistic model (DDPM).\n  First, we train a DDPM, which could convert a noisy input into the desired LQ\nimage, with a large amount of collected LQ images, which define the target data\ndistribution. Then, for a given HQ image, we synthesize an initial LQ image by\nusing an off-the-shelf degradation model, and iteratively add proper Gaussian\nnoises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM\nto obtain the final LQ image, which falls into the target distribution of\nreal-world LQ images. Thanks to the strong capability of DDPM in distribution\napproximation, the synthesized HQ-LQ image pairs can be used to train robust\nmodels for real-world image restoration tasks, such as blind face image\nrestoration and blind image super-resolution. Experiments demonstrated the\nsuperiority of our proposed approach to existing degradation models. Code and\ndata will be released.\n","authors":["Tao Yang","Peiran Ren","Xuansong xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17042v2","updated":"2023-03-13T10:31:11Z","published":"2022-11-30T14:43:35Z","title":"Spatio-Temporal Crop Aggregation for Video Representation Learning","summary":"  We propose Spatio-temporal Crop Aggregation for video representation LEarning\n(SCALE), a novel method that enjoys high scalability at both training and\ninference time. Our model builds long-range video features by learning from\nsets of video clip-level features extracted with a pre-trained backbone. To\ntrain the model, we propose a self-supervised objective consisting of masked\nclip feature prediction. We apply sparsity to both the input, by extracting a\nrandom set of video clips, and to the loss function, by only reconstructing the\nsparse inputs. Moreover, we use dimensionality reduction by working in the\nlatent space of a pre-trained backbone applied to single video clips. These\ntechniques make our method not only extremely efficient to train but also\nhighly effective in transfer learning. We demonstrate that our video\nrepresentation yields state-of-the-art performance with linear, non-linear, and\nKNN probing on common action classification and video understanding datasets.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2211.17042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14310v2","updated":"2023-03-13T10:25:22Z","published":"2022-11-25T18:59:54Z","title":"Efficient 3D Reconstruction, Streaming and Visualization of Static and\n  Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale\n  Environments","summary":"  Despite the impressive progress of telepresence systems for room-scale scenes\nwith static and dynamic scene entities, expanding their capabilities to\nscenarios with larger dynamic environments beyond a fixed size of a few\nsquare-meters remains challenging.\n  In this paper, we aim at sharing 3D live-telepresence experiences in\nlarge-scale environments beyond room scale with both static and dynamic scene\nentities at practical bandwidth requirements only based on light-weight scene\ncapture with a single moving consumer-grade RGB-D camera. To this end, we\npresent a system which is built upon a novel hybrid volumetric scene\nrepresentation in terms of the combination of a voxel-based scene\nrepresentation for the static contents, that not only stores the reconstructed\nsurface geometry but also contains information about the object semantics as\nwell as their accumulated dynamic movement over time, and a point-cloud-based\nrepresentation for dynamic scene parts, where the respective separation from\nstatic parts is achieved based on semantic and instance information extracted\nfor the input frames. With an independent yet simultaneous streaming of both\nstatic and dynamic content, where we seamlessly integrate potentially moving\nbut currently static scene entities in the static model until they are becoming\ndynamic again, as well as the fusion of static and dynamic data at the remote\nclient, our system is able to achieve VR-based live-telepresence at close to\nreal-time rates. Our evaluation demonstrates the potential of our novel\napproach in terms of visual quality, performance, and ablation studies\nregarding involved design choices.\n","authors":["Leif Van Holland","Patrick Stotko","Stefan Krumpen","Reinhard Klein","Michael Weinmann"],"pdf_url":"https://arxiv.org/pdf/2211.14310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04788v2","updated":"2023-03-13T10:13:00Z","published":"2022-04-10T22:58:02Z","title":"Representation Learning by Detecting Incorrect Location Embeddings","summary":"  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2204.04788v2.pdf","comment":"accepted at AAAI2023, https://github.com/Separius/DILEMMA"},{"id":"http://arxiv.org/abs/2303.06088v2","updated":"2023-03-13T10:05:01Z","published":"2023-03-10T17:09:04Z","title":"Improving Domain-Invariance in Self-Supervised Learning via Batch Styles\n  Standardization","summary":"  The recent rise of Self-Supervised Learning (SSL) as one of the preferred\nstrategies for learning with limited labeled data, and abundant unlabeled data\nhas led to the widespread use of these models. They are usually pretrained,\nfinetuned, and evaluated on the same data distribution, i.e., within an\nin-distribution setting. However, they tend to perform poorly in\nout-of-distribution evaluation scenarios, a challenge that Unsupervised Domain\nGeneralization (UDG) seeks to address.\n  This paper introduces a novel method to standardize the styles of images in a\nbatch. Batch styles standardization, relying on Fourier-based augmentations,\npromotes domain invariance in SSL by preventing spurious correlations from\nleaking into the features. The combination of batch styles standardization with\nthe well-known contrastive-based method SimCLR leads to a novel UDG method\nnamed CLaSSy ($\\textbf{C}$ontrastive $\\textbf{L}$e$\\textbf{a}$rning with\n$\\textbf{S}$tandardized $\\textbf{S}$t$\\textbf{y}$les). CLaSSy offers serious\nadvantages over prior methods, as it does not rely on domain labels and is\nscalable to handle a large number of domains. Experimental results on various\nUDG datasets demonstrate the superior performance of CLaSSy compared to\nexisting UDG methods. Finally, the versatility of the proposed batch styles\nstandardization is demonstrated by extending respectively the contrastive-based\nand non-contrastive-based SSL methods, SWaV and MSN, while considering\ndifferent backbone architectures (convolutional-based, transformers-based).\n","authors":["Marin Scalbert","Maria Vakalopoulou","Florent Couzinié-Devy"],"pdf_url":"https://arxiv.org/pdf/2303.06088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10984v2","updated":"2023-03-13T09:50:01Z","published":"2022-11-20T14:08:47Z","title":"DAQE: Enhancing the Quality of Compressed Images by Exploiting the\n  Inherent Characteristic of Defocus","summary":"  Image defocus is inherent in the physics of image formation caused by the\noptical aberration of lenses, providing plentiful information on image quality.\nUnfortunately, existing quality enhancement approaches for compressed images\nneglect the inherent characteristic of defocus, resulting in inferior\nperformance. This paper finds that in compressed images, significantly\ndefocused regions have better compression quality, and two regions with\ndifferent defocus values possess diverse texture patterns. These observations\nmotivate our defocus-aware quality enhancement (DAQE) approach. Specifically,\nwe propose a novel dynamic region-based deep learning architecture of the DAQE\napproach, which considers the regionwise defocus difference of compressed\nimages in two aspects. (1) The DAQE approach employs fewer computational\nresources to enhance the quality of significantly defocused regions and more\nresources to enhance the quality of other regions; (2) The DAQE approach learns\nto separately enhance diverse texture patterns for regions with different\ndefocus values, such that texture-specific enhancement can be achieved.\nExtensive experiments validate the superiority of our DAQE approach over\nstate-of-the-art approaches in terms of quality enhancement and resource\nsavings.\n","authors":["Qunliang Xing","Mai Xu","Xin Deng","Yichen Guo"],"pdf_url":"https://arxiv.org/pdf/2211.10984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06949v1","updated":"2023-03-13T09:34:08Z","published":"2023-03-13T09:34:08Z","title":"Improving Table Structure Recognition with Visual-Alignment Sequential\n  Coordinate Modeling","summary":"  Table structure recognition aims to extract the logical and physical\nstructure of unstructured table images into a machine-readable format. The\nlatest end-to-end image-to-text approaches simultaneously predict the two\nstructures by two decoders, where the prediction of the physical structure (the\nbounding boxes of the cells) is based on the representation of the logical\nstructure. However, the previous methods struggle with imprecise bounding boxes\nas the logical representation lacks local visual information. To address this\nissue, we propose an end-to-end sequential modeling framework for table\nstructure recognition called VAST. It contains a novel coordinate sequence\ndecoder triggered by the representation of the non-empty cell from the logical\nstructure decoder. In the coordinate sequence decoder, we model the bounding\nbox coordinates as a language sequence, where the left, top, right and bottom\ncoordinates are decoded sequentially to leverage the inter-coordinate\ndependency. Furthermore, we propose an auxiliary visual-alignment loss to\nenforce the logical representation of the non-empty cells to contain more local\nvisual details, which helps produce better cell bounding boxes. Extensive\nexperiments demonstrate that our proposed method can achieve state-of-the-art\nresults in both logical and physical structure recognition. The ablation study\nalso validates that the proposed coordinate sequence decoder and the\nvisual-alignment loss are the keys to the success of our method.\n","authors":["Yongshuai Huang","Ning Lu","Dapeng Chen","Yibo Li","Zecheng Xie","Shenggao Zhu","Liangcai Gao","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.06949v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.06942v1","updated":"2023-03-13T09:17:56Z","published":"2023-03-13T09:17:56Z","title":"Guiding the Guidance: A Comparative Analysis of User Guidance Signals\n  for Interactive Segmentation of Volumetric Images","summary":"  Interactive segmentation reduces the annotation time of medical images and\nallows annotators to iteratively refine labels with corrective interactions,\nsuch as clicks. While existing interactive models transform clicks into user\nguidance signals, which are combined with images to form (image, guidance)\npairs, the question of how to best represent the guidance has not been fully\nexplored. To address this, we conduct a comparative study of existing guidance\nsignals by training interactive models with different signals and parameter\nsettings to identify crucial parameters for the model's design. Based on our\nfindings, we design a guidance signal that retains the benefits of other\nsignals while addressing their limitations. We propose an adaptive Gaussian\nheatmaps guidance signal that utilizes the geodesic distance transform to\ndynamically adapt the radius of each heatmap when encoding clicks. We conduct\nour study on the MSD Spleen and the AutoPET datasets to explore the\nsegmentation of both anatomy (spleen) and pathology (tumor lesions). Our\nresults show that choosing the guidance signal is crucial for interactive\nsegmentation as we improve the performance by 14% Dice with our adaptive\nheatmaps on the challenging AutoPET dataset when compared to non-interactive\nmodels. This brings interactive models one step closer to deployment on\nclinical workflows. We will make our code publically available.\n","authors":["Zdravko Marinov","Rainer Stiefelhagen","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2303.06942v1.pdf","comment":"8 pages; 2 figures; 2 tables"},{"id":"http://arxiv.org/abs/2207.02625v5","updated":"2023-03-13T09:12:31Z","published":"2022-07-06T12:34:33Z","title":"$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of\n  Features","summary":"  In this paper, we show that the difference in $l_2$ norms of sample features\ncan hinder batch normalization from obtaining more distinguished inter-class\nfeatures and more compact intra-class features. To address this issue, we\npropose an intuitive but effective method to equalize the $l_2$ norms of sample\nfeatures. Concretely, we $l_2$-normalize each sample feature before feeding\nthem into batch normalization, and therefore the features are of the same\nmagnitude. Since the proposed method combines the $l_2$ normalization and batch\nnormalization, we name our method $L_2$BN. The $L_2$BN can strengthen the\ncompactness of intra-class features and enlarge the discrepancy of inter-class\nfeatures. The $L_2$BN is easy to implement and can exert its effect without any\nadditional parameters or hyper-parameters. Therefore, it can be used as a basic\nnormalization method for neural networks. We evaluate the effectiveness of\n$L_2$BN through extensive experiments with various models on image\nclassification and acoustic scene classification tasks. The results demonstrate\nthat the $L_2$BN can boost the generalization ability of various neural network\nmodels and achieve considerable performance improvements.\n","authors":["Zhennan Wang","Kehan Li","Runyi Yu","Yian Zhao","Pengchong Qiao","Fan Xu","Guoli Song","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.02625v5.pdf","comment":"Equation (4) and Figure 5 are confusing"},{"id":"http://arxiv.org/abs/2301.01123v2","updated":"2023-03-13T08:59:01Z","published":"2023-01-03T14:43:40Z","title":"MGTAB: A Multi-Relational Graph-Based Twitter Account Detection\n  Benchmark","summary":"  The development of social media user stance detection and bot detection\nmethods rely heavily on large-scale and high-quality benchmarks. However, in\naddition to low annotation quality, existing benchmarks generally have\nincomplete user relationships, suppressing graph-based account detection\nresearch. To address these issues, we propose a Multi-Relational Graph-Based\nTwitter Account Detection Benchmark (MGTAB), the first standardized graph-based\nbenchmark for account detection. To our knowledge, MGTAB was built based on the\nlargest original data in the field, with over 1.55 million users and 130\nmillion tweets. MGTAB contains 10,199 expert-annotated users and 7 types of\nrelationships, ensuring high-quality annotation and diversified relations. In\nMGTAB, we extracted the 20 user property features with the greatest information\ngain and user tweet features as the user features. In addition, we performed a\nthorough evaluation of MGTAB and other public datasets. Our experiments found\nthat graph-based approaches are generally more effective than feature-based\napproaches and perform better when introducing multiple relations. By analyzing\nexperiment results, we identify effective approaches for account detection and\nprovide potential future research directions in this field. Our benchmark and\nstandardized evaluation procedures are freely available at:\nhttps://github.com/GraphDetec/MGTAB.\n","authors":["Shuhao Shi","Kai Qiao","Jian Chen","Shuai Yang","Jie Yang","Baojie Song","Linyuan Wang","Bin Yan"],"pdf_url":"https://arxiv.org/pdf/2301.01123v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.07157v2","updated":"2023-03-13T08:57:52Z","published":"2022-11-14T07:22:55Z","title":"ParCNetV2: Oversized Kernel with Enhanced Attention","summary":"  Transformers have achieved tremendous success in various computer vision\ntasks. By borrowing design concepts from transformers, many studies\nrevolutionized CNNs and showed remarkable results. This paper falls in this\nline of studies. More specifically, we introduce a convolutional neural network\narchitecture named ParCNetV2, which extends position-aware circular convolution\n(ParCNet) with oversized convolutions and strengthens attention through\nbifurcate gate units. The oversized convolution utilizes a kernel with\n$2\\times$ the input size to model long-range dependencies through a global\nreceptive field. Simultaneously, it achieves implicit positional encoding by\nremoving the shift-invariant property from convolutional kernels, i.e., the\neffective kernels at different spatial locations are different when the kernel\nsize is twice as large as the input size. The bifurcate gate unit implements an\nattention mechanism similar to self-attention in transformers. It splits the\ninput into two branches, one serves as feature transformation while the other\nserves as attention weights. The attention is applied through element-wise\nmultiplication of the two branches. Besides, we introduce a unified\nlocal-global convolution block to unify the design of the early and late stage\nconvolutional blocks. Extensive experiments demonstrate that our method\noutperforms other pure convolutional neural networks as well as neural networks\nhybridizing CNNs and transformers.\n","authors":["Ruihan Xu","Haokui Zhang","Wenze Hu","Shiliang Zhang","Xiaoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2211.07157v2.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.06930v1","updated":"2023-03-13T08:53:47Z","published":"2023-03-13T08:53:47Z","title":"Twin Contrastive Learning with Noisy Labels","summary":"  Learning from noisy data is a challenging task that significantly degenerates\nthe model performance. In this paper, we present TCL, a novel twin contrastive\nlearning model to learn robust representations and handle noisy labels for\nclassification. Specifically, we construct a Gaussian mixture model (GMM) over\nthe representations by injecting the supervised model predictions into GMM to\nlink label-free latent variables in GMM with label-noisy annotations. Then, TCL\ndetects the examples with wrong labels as the out-of-distribution examples by\nanother two-component GMM, taking into account the data distribution. We\nfurther propose a cross-supervision with an entropy regularization loss that\nbootstraps the true targets from model predictions to handle the noisy labels.\nAs a result, TCL can learn discriminative representations aligned with\nestimated labels through mixup and contrastive learning. Extensive experimental\nresults on several standard benchmarks and real-world datasets demonstrate the\nsuperior performance of TCL. In particular, TCL achieves 7.5\\% improvements on\nCIFAR-10 with 90\\% noisy label -- an extremely noisy scenario. The source code\nis available at \\url{https://github.com/Hzzone/TCL}.\n","authors":["Zhizhong Huang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2303.06930v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06925v1","updated":"2023-03-13T08:41:24Z","published":"2023-03-13T08:41:24Z","title":"Super-Resolution Information Enhancement For Crowd Counting","summary":"  Crowd counting is a challenging task due to the heavy occlusions, scales, and\ndensity variations. Existing methods handle these challenges effectively while\nignoring low-resolution (LR) circumstances. The LR circumstances weaken the\ncounting performance deeply for two crucial reasons: 1) limited detail\ninformation; 2) overlapping head regions accumulate in density maps and result\nin extreme ground-truth values. An intuitive solution is to employ\nsuper-resolution (SR) pre-processes for the input LR images. However, it\ncomplicates the inference steps and thus limits application potentials when\nrequiring real-time. We propose a more elegant method termed Multi-Scale\nSuper-Resolution Module (MSSRM). It guides the network to estimate the lost de\ntails and enhances the detailed information in the feature space. Noteworthy\nthat the MSSRM is plug-in plug-out and deals with the LR problems with no\ninference cost. As the proposed method requires SR labels, we further propose a\nSuper-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on\nthree datasets demonstrate the superiority of our method. The code will be\navailable at https://github.com/PRIS-CV/MSSRM.git.\n","authors":["Jiahao Xie","Wei Xu","Dingkang Liang","Zhanyu Ma","Kongming Liang","Weidong Liu","Rui Wang","Ling Jin"],"pdf_url":"https://arxiv.org/pdf/2303.06925v1.pdf","comment":"Accepted by ICASSP 2023. The code will be available at\n  https://github.com/PRIS-CV/MSSRM.git"},{"id":"http://arxiv.org/abs/2211.12194v2","updated":"2023-03-13T08:40:32Z","published":"2022-11-22T11:35:07Z","title":"SadTalker: Learning Realistic 3D Motion Coefficients for Stylized\n  Audio-Driven Single Image Talking Face Animation","summary":"  Generating talking head videos through a face image and a piece of speech\naudio still contains many challenges. ie, unnatural head movement, distorted\nexpression, and identity modification. We argue that these issues are mainly\nbecause of learning from the coupled 2D motion fields. On the other hand,\nexplicitly using 3D information also suffers problems of stiff expression and\nincoherent video. We present SadTalker, which generates 3D motion coefficients\n(head pose, expression) of the 3DMM from audio and implicitly modulates a novel\n3D-aware face render for talking head generation. To learn the realistic motion\ncoefficients, we explicitly model the connections between audio and different\ntypes of motion coefficients individually. Precisely, we present ExpNet to\nlearn the accurate facial expression from audio by distilling both coefficients\nand 3D-rendered faces. As for the head pose, we design PoseVAE via a\nconditional VAE to synthesize head motion in different styles. Finally, the\ngenerated 3D motion coefficients are mapped to the unsupervised 3D keypoints\nspace of the proposed face render, and synthesize the final video. We conducted\nextensive experiments to demonstrate the superiority of our method in terms of\nmotion and video quality.\n","authors":["Wenxuan Zhang","Xiaodong Cun","Xuan Wang","Yong Zhang","Xi Shen","Yu Guo","Ying Shan","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12194v2.pdf","comment":"Accepted by CVPR 2023, Project page: https://sadtalker.github.io,\n  Code: https://github.com/Winfredy/SadTalker"},{"id":"http://arxiv.org/abs/2303.06920v1","updated":"2023-03-13T08:37:59Z","published":"2023-03-13T08:37:59Z","title":"Pixel-wise Gradient Uncertainty for Convolutional Neural Networks\n  applied to Out-of-Distribution Segmentation","summary":"  In recent years, deep neural networks have defined the state-of-the-art in\nsemantic segmentation where their predictions are constrained to a predefined\nset of semantic classes. They are to be deployed in applications such as\nautomated driving, although their categorically confined expressive power runs\ncontrary to such open world scenarios. Thus, the detection and segmentation of\nobjects from outside their predefined semantic space, i.e., out-of-distribution\n(OoD) objects, is of highest interest. Since uncertainty estimation methods\nlike softmax entropy or Bayesian models are sensitive to erroneous predictions,\nthese methods are a natural baseline for OoD detection. Here, we present a\nmethod for obtaining uncertainty scores from pixel-wise loss gradients which\ncan be computed efficiently during inference. Our approach is simple to\nimplement for a large class of models, does not require any additional training\nor auxiliary data and can be readily used on pre-trained segmentation models.\nOur experiments show the ability of our method to identify wrong pixel\nclassifications and to estimate prediction quality. In particular, we observe\nsuperior performance in terms of OoD segmentation to comparable baselines on\nthe SegmentMeIfYouCan benchmark, clearly outperforming methods which are\nsimilarly flexible to implement.\n","authors":["Kira Maag","Tobias Riedlinger"],"pdf_url":"https://arxiv.org/pdf/2303.06920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06919v1","updated":"2023-03-13T08:36:30Z","published":"2023-03-13T08:36:30Z","title":"NeRFLiX: High-Quality Neural View Synthesis by Learning a\n  Degradation-Driven Inter-viewpoint MiXer","summary":"  Neural radiance fields (NeRF) show great success in novel view synthesis.\nHowever, in real-world scenes, recovering high-quality details from the source\nimages is still challenging for the existing NeRF-based approaches, due to the\npotential imperfect calibration information and scene representation\ninaccuracy. Even with high-quality training frames, the synthetic novel views\nproduced by NeRF models still suffer from notable rendering artifacts, such as\nnoise, blur, etc. Towards to improve the synthesis quality of NeRF-based\napproaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by\nlearning a degradation-driven inter-viewpoint mixer. Specially, we design a\nNeRF-style degradation modeling approach and construct large-scale training\ndata, enabling the possibility of effectively removing NeRF-native rendering\nartifacts for existing deep neural networks. Moreover, beyond the degradation\nremoval, we propose an inter-viewpoint aggregation framework that is able to\nfuse highly related high-quality training images, pushing the performance of\ncutting-edge NeRF models to entirely new levels and producing highly\nphoto-realistic synthetic views.\n","authors":["Kun Zhou","Wenbo Li","Yi Wang","Tao Hu","Nianjuan Jiang","Xiaoguang Han","Jiangbo Lu"],"pdf_url":"https://arxiv.org/pdf/2303.06919v1.pdf","comment":"Accepted to CVPR 2023; Project Page: see\n  https://redrock303.github.io/nerflix/"},{"id":"http://arxiv.org/abs/2303.06911v1","updated":"2023-03-13T08:02:12Z","published":"2023-03-13T08:02:12Z","title":"ViM: Vision Middleware for Unified Downstream Transferring","summary":"  Foundation models are pre-trained on massive data and transferred to\ndownstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a\nnew learning paradigm that targets unified transferring from a single\nfoundation model to a variety of downstream tasks. ViM consists of a zoo of\nlightweight plug-in modules, each of which is independently learned on a\nmidstream dataset with a shared frozen backbone. Downstream tasks can then\nbenefit from an adequate aggregation of the module zoo thanks to the rich\nknowledge inherited from midstream tasks. There are three major advantages of\nsuch a design. From the efficiency aspect, the upstream backbone can be trained\nonly once and reused for all downstream tasks without tuning. From the\nscalability aspect, we can easily append additional modules to ViM with no\ninfluence on existing modules. From the performance aspect, ViM can include as\nmany midstream tasks as possible, narrowing the task gap between upstream and\ndownstream. Considering these benefits, we believe that ViM, which the\ncommunity could maintain and develop together, would serve as a powerful tool\nto assist foundation models.\n","authors":["Yutong Feng","Biao Gong","Jianwen Jiang","Yiliang Lv","Yujun Shen","Deli Zhao","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06908v1","updated":"2023-03-13T07:54:29Z","published":"2023-03-13T07:54:29Z","title":"CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale\n  Attention","summary":"  While features of different scales are perceptually important to visual\ninputs, existing vision transformers do not yet take advantage of them\nexplicitly. To this end, we first propose a cross-scale vision transformer,\nCrossFormer. It introduces a cross-scale embedding layer (CEL) and a long-short\ndistance attention (LSDA). On the one hand, CEL blends each token with multiple\npatches of different scales, providing the self-attention module itself with\ncross-scale features. On the other hand, LSDA splits the self-attention module\ninto a short-distance one and a long-distance counterpart, which not only\nreduces the computational burden but also keeps both small-scale and\nlarge-scale features in the tokens. Moreover, through experiments on\nCrossFormer, we observe another two issues that affect vision transformers'\nperformance, i.e. the enlarging self-attention maps and amplitude explosion.\nThus, we further propose a progressive group size (PGS) paradigm and an\namplitude cooling layer (ACL) to alleviate the two issues, respectively. The\nCrossFormer incorporating with PGS and ACL is called CrossFormer++. Extensive\nexperiments show that CrossFormer++ outperforms the other vision transformers\non image classification, object detection, instance segmentation, and semantic\nsegmentation tasks. The code will be available at:\nhttps://github.com/cheerss/CrossFormer.\n","authors":["Wenxiao Wang","Wei Chen","Qibo Qiu","Long Chen","Boxi Wu","Binbin Lin","Xiaofei He","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06908v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.05095v2","updated":"2023-03-13T07:50:57Z","published":"2023-03-09T08:13:06Z","title":"Trajectory-Aware Body Interaction Transformer for Multi-Person Pose\n  Forecasting","summary":"  Multi-person pose forecasting remains a challenging problem, especially in\nmodeling fine-grained human body interaction in complex crowd scenarios.\nExisting methods typically represent the whole pose sequence as a temporal\nseries, yet overlook interactive influences among people based on skeletal body\nparts. In this paper, we propose a novel Trajectory-Aware Body Interaction\nTransformer (TBIFormer) for multi-person pose forecasting via effectively\nmodeling body part interactions. Specifically, we construct a Temporal Body\nPartition Module that transforms all the pose sequences into a Multi-Person\nBody-Part sequence to retain spatial and temporal information based on body\nsemantics. Then, we devise a Social Body Interaction Self-Attention (SBI-MSA)\nmodule, utilizing the transformed sequence to learn body part dynamics for\ninter- and intra-individual interactions. Furthermore, different from prior\nEuclidean distance-based spatial encodings, we present a novel and efficient\nTrajectory-Aware Relative Position Encoding for SBI-MSA to offer discriminative\nspatial information and additional interactive clues. On both short- and\nlong-term horizons, we empirically evaluate our framework on CMU-Mocap,\nMuPoTS-3D as well as synthesized datasets (6 ~ 10 persons), and demonstrate\nthat our method greatly outperforms the state-of-the-art methods. Code will be\nmade publicly available upon acceptance.\n","authors":["Xiaogang Peng","Siyuan Mao","Zizhao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.05095v2.pdf","comment":"Accepted by CVPR2023, 8 pages, 6 figures. arXiv admin note: text\n  overlap with arXiv:2208.09224"},{"id":"http://arxiv.org/abs/2303.06907v1","updated":"2023-03-13T07:48:46Z","published":"2023-03-13T07:48:46Z","title":"ST360IQ: No-Reference Omnidirectional Image Quality Assessment with\n  Spherical Vision Transformers","summary":"  Omnidirectional images, aka 360 images, can deliver immersive and interactive\nvisual experiences. As their popularity has increased dramatically in recent\nyears, evaluating the quality of 360 images has become a problem of interest\nsince it provides insights for capturing, transmitting, and consuming this new\nmedia. However, directly adapting quality assessment methods proposed for\nstandard natural images for omnidirectional data poses certain challenges.\nThese models need to deal with very high-resolution data and implicit\ndistortions due to the spherical form of the images. In this study, we present\na method for no-reference 360 image quality assessment. Our proposed ST360IQ\nmodel extracts tangent viewports from the salient parts of the input\nomnidirectional image and employs a vision-transformers based module processing\nsaliency selective patches/tokens that estimates a quality score from each\nviewport. Then, it aggregates these scores to give a final quality score. Our\nexperiments on two benchmark datasets, namely OIQA and CVIQ datasets,\ndemonstrate that as compared to the state-of-the-art, our approach predicts the\nquality of an omnidirectional image correlated with the human-perceived image\nquality. The code has been available on\nhttps://github.com/Nafiseh-Tofighi/ST360IQ\n","authors":["Nafiseh Jabbari Tofighi","Mohamed Hedi Elfkir","Nevrez Imamoglu","Cagri Ozcinar","Erkut Erdem","Aykut Erdem"],"pdf_url":"https://arxiv.org/pdf/2303.06907v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06905v1","updated":"2023-03-13T07:47:18Z","published":"2023-03-13T07:47:18Z","title":"DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal\n  from Varicolored Haze Scenes","summary":"  Varicolored haze caused by chromatic casts poses haze removal and depth\nestimation challenges. Recent learning-based depth estimation methods are\nmainly targeted at dehazing first and estimating depth subsequently from\nhaze-free scenes. This way, the inner connections between colored haze and\nscene depth are lost. In this paper, we propose a real-time transformer for\nsimultaneous single image Depth Estimation and Haze Removal (DEHRFormer).\nDEHRFormer consists of a single encoder and two task-specific decoders. The\ntransformer decoders with learnable queries are designed to decode coupling\nfeatures from the task-agnostic encoder and project them into clean image and\ndepth map, respectively. In addition, we introduce a novel learning paradigm\nthat utilizes contrastive learning and domain consistency learning to tackle\nweak-generalization problem for real-world dehazing, while predicting the same\ndepth map from the same scene with varicolored haze. Experiments demonstrate\nthat DEHRFormer achieves significant performance improvement across diverse\nvaricolored haze scenes over previous depth estimation networks and dehazing\napproaches.\n","authors":["Sixiang Chen","Tian Ye","Jun Shi","Yun Liu","JingXia Jiang","Erkang Chen","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06905v1.pdf","comment":"Accepted to ICASSP'2023"},{"id":"http://arxiv.org/abs/2303.06904v1","updated":"2023-03-13T07:46:41Z","published":"2023-03-13T07:46:41Z","title":"Contextually-rich human affect perception using multimodal scene\n  information","summary":"  The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.\n","authors":["Digbalay Bose","Rajat Hebbar","Krishna Somandepalli","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2303.06904v1.pdf","comment":"Accepted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2023"},{"id":"http://arxiv.org/abs/2208.12506v3","updated":"2023-03-13T07:35:42Z","published":"2022-08-26T08:56:33Z","title":"EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning","summary":"  The standard diagnostic procedures for targeted therapies in lung cancer\ntreatment involve histological subtyping and subsequent detection of key driver\nmutations, such as EGFR. Even though molecular profiling can uncover the driver\nmutation, the process is often expensive and time-consuming. Deep\nlearning-oriented image analysis offers a more economical alternative for\ndiscovering driver mutations directly from whole slide images (WSIs). In this\nwork, we used customized deep learning pipelines with weak supervision to\nidentify the morphological correlates of EGFR mutation from hematoxylin and\neosin-stained WSIs, in addition to detecting tumor and histologically subtyping\nit. We demonstrate the effectiveness of our pipeline by conducting rigorous\nexperiments and ablation studies on two lung cancer datasets - TCGA and a\nprivate dataset from India. With our pipeline, we achieved an average area\nunder the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological\nsubtyping between adenocarcinoma and squamous cell carcinoma on the TCGA\ndataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA\ndataset and 0.783 on the dataset from India. Our key learning points include\nthe following. Firstly, there is no particular advantage of using a feature\nextractor layers trained on histology, if one is going to fine-tune the feature\nextractor on the target dataset. Secondly, selecting patches with high\ncellularity, presumably capturing tumor regions, is not always helpful, as the\nsign of a disease class may be present in the tumor-adjacent stroma.\n","authors":["Ravi Kant Gupta","Shivani Nandgaonkar","Nikhil Cherian Kurian","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2208.12506v3.pdf","comment":"We need to improve"},{"id":"http://arxiv.org/abs/2210.00215v4","updated":"2023-03-13T07:09:20Z","published":"2022-10-01T07:36:51Z","title":"Differentiable Parsing and Visual Grounding of Natural Language\n  Instructions for Object Placement","summary":"  We present a new method, PARsing And visual GrOuNding (ParaGon), for\ngrounding natural language in object placement tasks. Natural language\ngenerally describes objects and spatial relations with compositionality and\nambiguity, two major obstacles to effective language grounding. For\ncompositionality, ParaGon parses a language instruction into an object-centric\ngraph representation to ground objects individually. For ambiguity, ParaGon\nuses a novel particle-based graph neural network to reason about object\nplacements with uncertainty. Essentially, ParaGon integrates a parsing\nalgorithm into a probabilistic, data-driven learning framework. It is fully\ndifferentiable and trained end-to-end from data for robustness against complex,\nambiguous language input.\n","authors":["Zirui Zhao","Wee Sun Lee","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2210.00215v4.pdf","comment":"To appear in ICRA 2023"},{"id":"http://arxiv.org/abs/2303.05715v2","updated":"2023-03-13T07:09:03Z","published":"2023-03-10T05:46:25Z","title":"Context-Based Trit-Plane Coding for Progressive Image Compression","summary":"  Trit-plane coding enables deep progressive image compression, but it cannot\nuse autoregressive context models. In this paper, we propose the context-based\ntrit-plane coding (CTC) algorithm to achieve progressive compression more\ncompactly. First, we develop the context-based rate reduction module to\nestimate trit probabilities of latent elements accurately and thus encode the\ntrit-planes compactly. Second, we develop the context-based distortion\nreduction module to refine partial latent tensors from the trit-planes and\nimprove the reconstructed image quality. Third, we propose a retraining scheme\nfor the decoder to attain better rate-distortion tradeoffs. Extensive\nexperiments show that CTC outperforms the baseline trit-plane codec\nsignificantly in BD-rate on the Kodak lossless dataset, while increasing the\ntime complexity only marginally. Our codes are available at\nhttps://github.com/seungminjeon-github/CTC.\n","authors":["Seungmin Jeon","Kwang Pyo Choi","Youngo Park","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2303.05715v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2211.03177v2","updated":"2023-03-13T07:06:55Z","published":"2022-11-06T17:05:04Z","title":"Measurement-Consistent Networks via a Deep Implicit Layer for Solving\n  Inverse Problems","summary":"  End-to-end deep neural networks (DNNs) have become the state-of-the-art\n(SOTA) for solving inverse problems. Despite their outstanding performance,\nduring deployment, such networks are sensitive to minor variations in the\ntesting pipeline and often fail to reconstruct small but important details, a\nfeature critical in medical imaging, astronomy, or defence. Such instabilities\nin DNNs can be explained by the fact that they ignore the forward measurement\nmodel during deployment, and thus fail to enforce consistency between their\noutput and the input measurements. To overcome this, we propose a framework\nthat transforms any DNN for inverse problems into a measurement-consistent one.\nThis is done by appending to it an implicit layer (or deep equilibrium network)\ndesigned to solve a model-based optimization problem. The implicit layer\nconsists of a shallow learnable network that can be integrated into the\nend-to-end training while keeping the SOTA DNN fixed. Experiments on\nsingle-image super-resolution show that the proposed framework leads to\nsignificant improvements in reconstruction quality and robustness over the SOTA\nDNNs.\n","authors":["Rahul Mourya","João F. C. Mota"],"pdf_url":"https://arxiv.org/pdf/2211.03177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01567v3","updated":"2023-03-13T07:05:21Z","published":"2022-11-03T03:12:52Z","title":"Galaxy Image Deconvolution for Weak Gravitational Lensing with Unrolled\n  Plug-and-Play ADMM","summary":"  Removing optical and atmospheric blur from galaxy images significantly\nimproves galaxy shape measurements for weak gravitational lensing and galaxy\nevolution studies. This ill-posed linear inverse problem is usually solved with\ndeconvolution algorithms enhanced by regularisation priors or deep learning. We\nintroduce a so-called \"physics-informed deep learning\" approach to the Point\nSpread Function (PSF) deconvolution problem in galaxy surveys. We apply\nalgorithm unrolling and the Plug-and-Play technique to the Alternating\nDirection Method of Multipliers (ADMM), in which a neural network learns\nappropriate hyperparameters and denoising priors from simulated galaxy images.\nWe characterise the time-performance trade-off of several methods for galaxies\nof differing brightness levels as well as our method's robustness to systematic\nPSF errors and network ablations. We show an improvement in reduced shear\nellipticity error of 38.6% (SNR=20)/45.0% (SNR=200) compared to classic methods\nand 7.4% (SNR=20)/33.2% (SNR=200) compared to modern methods.\n","authors":["Tianao Li","Emma Alexander"],"pdf_url":"https://arxiv.org/pdf/2211.01567v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12047v2","updated":"2023-03-13T07:02:14Z","published":"2022-01-28T11:23:29Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Deep learning models as an emerging topic have shown great progress in\nvarious fields. Especially, visualization tools such as class activation\nmapping methods provided visual explanation on the reasoning of convolutional\nneural networks (CNNs). By using the gradients of the network layers, it is\npossible to demonstrate where the networks pay attention during a specific\nimage recognition task. Moreover, these gradients can be integrated with CNN\nfeatures for localizing more generalized task dependent attentive (salient)\nobjects in scenes. Despite this progress, there is not much explicit usage of\nthis gradient (network attention) information to integrate with CNN\nrepresentations for object semantics. This can be very useful for visual tasks\nsuch as simultaneous localization and mapping (SLAM) where CNN representations\nof spatially attentive object locations may lead to improved performance.\nTherefore, in this work, we propose the use of task specific network attention\nfor RGB-D indoor SLAM. To do so, we integrate layer-wise object attention\ninformation (layer gradients) with CNN layer representations to improve frame\nassociation performance in an RGB-D indoor SLAM method. Experiments show\npromising results with improved performance over the baseline.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Weimin Wang","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2201.12047v2.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2301.04011v2","updated":"2023-03-13T06:37:31Z","published":"2023-01-08T09:27:41Z","title":"Learning Support and Trivial Prototypes for Interpretable Image\n  Classification","summary":"  Prototypical part network (ProtoPNet) methods have been designed to achieve\ninterpretable classification by associating predictions with a set of training\nprototypes, which we refer to as trivial prototypes because they are trained to\nlie far from the classification boundary in the feature space. Note that it is\npossible to make an analogy between ProtoPNet and support vector machine (SVM)\ngiven that the classification from both methods relies on computing similarity\nwith a set of training points (i.e., trivial prototypes in ProtoPNet, and\nsupport vectors in SVM). However, while trivial prototypes are located far from\nthe classification boundary, support vectors are located close to this\nboundary, and we argue that this discrepancy with the well-established SVM\ntheory can result in ProtoPNet models with inferior classification accuracy. In\nthis paper, we aim to improve the classification of ProtoPNet with a new method\nto learn support prototypes that lie near the classification boundary in the\nfeature space, as suggested by the SVM theory. In addition, we target the\nimprovement of classification results with a new model, named ST-ProtoPNet,\nwhich exploits our support prototypes and the trivial prototypes to provide\nmore effective classification. Experimental results on CUB-200-2011, Stanford\nCars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves\nstate-of-the-art classification accuracy and interpretability results. We also\nshow that the proposed support prototypes tend to be better localised in the\nobject of interest rather than in the background region.\n","authors":["Chong Wang","Yuyuan Liu","Yuanhong Chen","Fengbei Liu","Yu Tian","Davis J. McCarthy","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2301.04011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05021v2","updated":"2023-03-13T06:18:14Z","published":"2023-03-09T03:48:24Z","title":"DiffusionDepth: Diffusion Denoising Approach for Monocular Depth\n  Estimation","summary":"  Monocular depth estimation is a challenging task that predicts the pixel-wise\ndepth from a single 2D image. Current methods typically model this problem as a\nregression or classification task. We propose DiffusionDepth, a new approach\nthat reformulates monocular depth estimation as a denoising diffusion process.\nIt learns an iterative denoising process to `denoise' random depth distribution\ninto a depth map with the guidance of monocular visual conditions. The process\nis performed in the latent space encoded by a dedicated depth encoder and\ndecoder. Instead of diffusing ground truth (GT) depth, the model learns to\nreverse the process of diffusing the refined depth of itself into random depth\ndistribution. This self-diffusion formulation overcomes the difficulty of\napplying generative models to sparse GT depth scenarios. The proposed approach\nbenefits this task by refining depth estimation step by step, which is superior\nfor generating accurate and highly detailed depth maps. Experimental results on\nKITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion\napproach could reach state-of-the-art performance in both indoor and outdoor\nscenarios with acceptable inference time.\n","authors":["Yiqun Duan","Zheng Zhu","Xianda Guo"],"pdf_url":"https://arxiv.org/pdf/2303.05021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06885v1","updated":"2023-03-13T06:05:18Z","published":"2023-03-13T06:05:18Z","title":"DR2: Diffusion-based Robust Degradation Remover for Blind Face\n  Restoration","summary":"  Blind face restoration usually synthesizes degraded low-quality data with a\npre-defined degradation model for training, while more complex cases could\nhappen in the real world. This gap between the assumed and actual degradation\nhurts the restoration performance where artifacts are often observed in the\noutput. However, it is expensive and infeasible to include every type of\ndegradation to cover real-world cases in the training data. To tackle this\nrobustness issue, we propose Diffusion-based Robust Degradation Remover (DR2)\nto first transform the degraded image to a coarse but degradation-invariant\nprediction, then employ an enhancement module to restore the coarse prediction\nto a high-quality image. By leveraging a well-performing denoising diffusion\nprobabilistic model, our DR2 diffuses input images to a noisy status where\nvarious types of degradation give way to Gaussian noise, and then captures\nsemantic information through iterative denoising steps. As a result, DR2 is\nrobust against common degradation (e.g. blur, resize, noise and compression)\nand compatible with different designs of enhancement modules. Experiments in\nvarious settings show that our framework outperforms state-of-the-art methods\non heavily degraded synthetic and real-world datasets.\n","authors":["Zhixin Wang","Xiaoyun Zhang","Ziying Zhang","Huangjie Zheng","Mingyuan Zhou","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06885v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06884v1","updated":"2023-03-13T06:03:23Z","published":"2023-03-13T06:03:23Z","title":"SCPNet: Semantic Scene Completion on Point Cloud","summary":"  Training deep models for semantic scene completion (SSC) is challenging due\nto the sparse and incomplete input, a large quantity of objects of diverse\nscales as well as the inherent label noise for moving objects. To address the\nabove-mentioned problems, we propose the following three solutions: 1)\nRedesigning the completion sub-network. We design a novel completion\nsub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate\nmulti-scale features and is free from the lossy downsampling operations. 2)\nDistilling rich knowledge from the multi-frame model. We design a novel\nknowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation\n(DSKD). It transfers the dense, relation-based semantic knowledge from the\nmulti-frame teacher to the single-frame student, significantly improving the\nrepresentation learning of the single-frame model. 3) Completion label\nrectification. We propose a simple yet effective label rectification strategy,\nwhich uses off-the-shelf panoptic segmentation labels to remove the traces of\ndynamic objects in completion labels, greatly improving the performance of deep\nmodels especially for those moving objects. Extensive experiments are conducted\nin two public SSC benchmarks, i.e., SemanticKITTI and SemanticPOSS. Our SCPNet\nranks 1st on SemanticKITTI semantic scene completion challenge and surpasses\nthe competitive S3CNet by 7.2 mIoU. SCPNet also outperforms previous completion\nalgorithms on the SemanticPOSS dataset. Besides, our method also achieves\ncompetitive results on SemanticKITTI semantic segmentation tasks, showing that\nknowledge learned in the scene completion is beneficial to the segmentation\ntask.\n","authors":["Zhaoyang Xia","Youquan Liu","Xin Li","Xinge Zhu","Yuexin Ma","Yikang Li","Yuenan Hou","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.06884v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06881v1","updated":"2023-03-13T05:56:36Z","published":"2023-03-13T05:56:36Z","title":"OverlapNetVLAD: A Coarse-to-Fine Framework for LiDAR-based Place\n  Recognition","summary":"  Place recognition is a challenging yet crucial task in robotics. Existing 3D\nLiDAR place recognition methods suffer from limited feature representation\ncapability and long search times. To address these challenges, we propose a\nnovel coarse-to-fine framework for 3D LiDAR place recognition that combines\nBirds' Eye View (BEV) feature extraction, coarse-grained matching, and\nfine-grained verification. In the coarse stage, our framework leverages the\nrich contextual information contained in BEV features to produce global\ndescriptors. Then the top-\\textit{K} most similar candidates are identified via\ndescriptor matching, which is fast but coarse-grained. In the fine stage, our\noverlap estimation network reuses the corresponding BEV features to predict the\noverlap region, enabling meticulous and precise matching. Experimental results\non the KITTI odometry benchmark demonstrate that our framework achieves leading\nperformance compared to state-of-the-art methods. Our code is available at:\n\\url{https://github.com/fcchit/OverlapNetVLAD}.\n","authors":["Chencan Fu","Lin Li","Linpeng Peng","Yukai Ma","Xiangrui Zhao","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06881v1.pdf","comment":"Submitted to IROS2023"},{"id":"http://arxiv.org/abs/2303.06880v1","updated":"2023-03-13T05:54:13Z","published":"2023-03-13T05:54:13Z","title":"Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection","summary":"  Current 3D object detection models follow a single dataset-specific training\nand testing paradigm, which often faces a serious detection accuracy drop when\nthey are directly deployed in another dataset. In this paper, we study the task\nof training a unified 3D detector from multiple datasets. We observe that this\nappears to be a challenging task, which is mainly due to that these datasets\npresent substantial data-level differences and taxonomy-level variations caused\nby different LiDAR types and data acquisition standards. Inspired by such\nobservation, we present a Uni3D which leverages a simple data-level correction\noperation and a designed semantic-level coupling-and-recoupling module to\nalleviate the unavoidable data-level and taxonomy-level differences,\nrespectively. Our method is simple and easily combined with many 3D object\ndetection baselines such as PV-RCNN and Voxel-RCNN, enabling them to\neffectively learn from multiple off-the-shelf 3D datasets to obtain more\ndiscriminative and generalizable representations. Experiments are conducted on\nmany dataset consolidation settings including Waymo-nuScenes, nuScenes-KITTI,\nWaymo-KITTI, and Waymo-nuScenes-KITTI consolidations. Their results demonstrate\nthat Uni3D exceeds a series of individual detectors trained on a single\ndataset, with a 1.04x parameter increase over a selected baseline detector. We\nexpect this work will inspire the research of 3D generalization since it will\npush the limits of perceptual performance.\n","authors":["Bo Zhang","Jiakang Yuan","Botian Shi","Tao Chen","Yikang Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.06880v1.pdf","comment":"Accepted by CVPR-2023, and our code is available at\n  https://github.com/PJLab-ADG/3DTrans"},{"id":"http://arxiv.org/abs/2303.06879v1","updated":"2023-03-13T05:54:01Z","published":"2023-03-13T05:54:01Z","title":"Spacecraft Anomaly Detection with Attention Temporal Convolution Network","summary":"  Spacecraft faces various situations when carrying out exploration missions in\ncomplex space, thus monitoring the anomaly status of spacecraft is crucial to\nthe development of \\textcolor{blue}{the} aerospace industry. The time series\ntelemetry data generated by on-orbit spacecraft \\textcolor{blue}{contains}\nimportant information about the status of spacecraft. However, traditional\ndomain knowledge-based spacecraft anomaly detection methods are not effective\ndue to high dimensionality and complex correlation among variables. In this\nwork, we propose an anomaly detection framework for spacecraft multivariate\ntime-series data based on temporal convolution networks (TCNs). First, we\nemploy dynamic graph attention to model the complex correlation among variables\nand time series. Second, temporal convolution networks with parallel processing\nability are used to extract multidimensional \\textcolor{blue}{features} for\n\\textcolor{blue}{the} downstream prediction task. Finally, many potential\nanomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL\nspacecraft datasets show the superiority of our proposed model with respect to\nstate-of-the-art methods.\n","authors":["Liang Liu","Ling Tian","Zhao Kang","Tianqi Wan"],"pdf_url":"https://arxiv.org/pdf/2303.06879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06877v1","updated":"2023-03-13T05:53:11Z","published":"2023-03-13T05:53:11Z","title":"Progressive Open Space Expansion for Open-Set Model Attribution","summary":"  Despite the remarkable progress in generative technology, the Janus-faced\nissues of intellectual property protection and malicious content supervision\nhave arisen. Efforts have been paid to manage synthetic images by attributing\nthem to a set of potential source models. However, the closed-set\nclassification setting limits the application in real-world scenarios for\nhandling contents generated by arbitrary models. In this study, we focus on a\nchallenging task, namely Open-Set Model Attribution (OSMA), to simultaneously\nattribute images to known models and identify those from unknown ones. Compared\nto existing open-set recognition (OSR) tasks focusing on semantic novelty, OSMA\nis more challenging as the distinction between images from known and unknown\nmodels may only lie in visually imperceptible traces. To this end, we propose a\nProgressive Open Space Expansion (POSE) solution, which simulates open-set\nsamples that maintain the same semantics as closed-set samples but embedded\nwith different imperceptible traces. Guided by a diversity constraint, the open\nspace is simulated progressively by a set of lightweight augmentation models.\nWe consider three real-world scenarios and construct an OSMA benchmark dataset,\nincluding unknown models trained with different random seeds, architectures,\nand datasets from known ones. Extensive experiments on the dataset demonstrate\nPOSE is superior to both existing model attribution methods and off-the-shelf\nOSR methods.\n","authors":["Tianyun Yang","Danding Wang","Fan Tang","Xinying Zhao","Juan Cao","Sheng Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06877v1.pdf","comment":"accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.01904v3","updated":"2023-03-13T05:52:56Z","published":"2023-03-03T13:05:30Z","title":"EcoTTA: Memory-Efficient Continual Test-time Adaptation via\n  Self-distilled Regularization","summary":"  This paper presents a simple yet effective approach that improves continual\ntest-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be\nconducted on edge devices with limited memory, so reducing memory is crucial\nbut has been overlooked in previous TTA studies. In addition, long-term\nadaptation often leads to catastrophic forgetting and error accumulation, which\nhinders applying TTA in real-world deployments. Our approach consists of two\ncomponents to address these issues. First, we present lightweight meta networks\nthat can adapt the frozen original networks to the target domain. This novel\narchitecture minimizes memory consumption by decreasing the size of\nintermediate activations required for backpropagation. Second, our novel\nself-distilled regularization controls the output of the meta networks not to\ndeviate significantly from the output of the frozen original networks, thereby\npreserving well-trained knowledge from the source domain. Without additional\nmemory, this regularization prevents error accumulation and catastrophic\nforgetting, resulting in stable performance even in long-term test-time\nadaptation. We demonstrate that our simple yet effective strategy outperforms\nother state-of-the-art methods on various benchmarks for image classification\nand semantic segmentation tasks. Notably, our proposed method with ResNet-50\nand WideResNet-40 takes 86% and 80% less memory than the recent\nstate-of-the-art method, CoTTA.\n","authors":["Junha Song","Jungsoo Lee","In So Kweon","Sungha Choi"],"pdf_url":"https://arxiv.org/pdf/2303.01904v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06876v1","updated":"2023-03-13T05:51:35Z","published":"2023-03-13T05:51:35Z","title":"Revisiting model self-interpretability in a decision-theoretic way for\n  binary medical image classification","summary":"  Interpretability is highly desired for deep neural network-based classifiers,\nespecially when addressing high-stake decisions in medical imaging. Commonly\nused post-hoc interpretability methods may not be always useful because\ndifferent such methods can produce several plausible but different\ninterpretations of a given model, leading to confusion about which one to\nchoose. {In this work, an {inherently} interpretable encoder-decoder model\ncoupled with a single-layer fully connected network with unity weights is\nproposed for binary medical image classification problems. The feature\nextraction component of a trained black-box network for the same task is\nemployed as the pre-trained encoder of the interpretable model. The model is\ntrained to estimate the decision statistic of the given trained black-box deep\nbinary classifier to maintain a similar accuracy.} The decoder output\nrepresents a transformed version of the to-be-classified image that, when\nprocessed by the fixed fully connected layer, produces the same decision\nstatistic value as the original classifier. This is accomplished by minimizing\nthe mean squared error between the decision statistic values of the black-box\nmodel and encoder-decoder based model during training. The decoder output image\nis referred to as an equivalency map. Because the single-layer network is fully\ninterpretable, the equivalency map provides a visualization of the transformed\nimage features that contribute to the decision statistic value and, moreover,\npermits quantification of their relative contributions. Unlike the traditional\npost-hoc interpretability methods, the proposed method is inherently\ninterpretable, quantitative, and fundamentally based on decision theory.\n","authors":["Sourya Sengupta","Mark A. Anastasio"],"pdf_url":"https://arxiv.org/pdf/2303.06876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06574v3","updated":"2023-03-13T05:51:27Z","published":"2022-02-14T09:36:50Z","title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning","summary":"  Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n","authors":["Ziyang Luo","Zhipeng Hu","Yadong Xi","Rongsheng Zhang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2202.06574v3.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06873v1","updated":"2023-03-13T05:46:56Z","published":"2023-03-13T05:46:56Z","title":"Interventional Bag Multi-Instance Learning On Whole-Slide Pathological\n  Images","summary":"  Multi-instance learning (MIL) is an effective paradigm for whole-slide\npathological images (WSIs) classification to handle the gigapixel resolution\nand slide-level label. Prevailing MIL methods primarily focus on improving the\nfeature extractor and aggregator. However, one deficiency of these methods is\nthat the bag contextual prior may trick the model into capturing spurious\ncorrelations between bags and labels. This deficiency is a confounder that\nlimits the performance of existing MIL methods. In this paper, we propose a\nnovel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve\ndeconfounded bag-level prediction. Unlike traditional likelihood-based\nstrategies, the proposed scheme is based on the backdoor adjustment to achieve\nthe interventional training, thus is capable of suppressing the bias caused by\nthe bag contextual prior. Note that the principle of IBMIL is orthogonal to\nexisting bag MIL methods. Therefore, IBMIL is able to bring consistent\nperformance boosting to existing schemes, achieving new state-of-the-art\nperformance. Code is available at https://github.com/HHHedo/IBMIL.\n","authors":["Tiancheng Lin","Zhimiao Yu","Hongyu Hu","Yi Xu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06873v1.pdf","comment":"Accepted by CVPR 2023; Code at https://github.com/HHHedo/IBMIL"},{"id":"http://arxiv.org/abs/2303.06872v1","updated":"2023-03-13T05:46:21Z","published":"2023-03-13T05:46:21Z","title":"FusionLoc: Camera-2D LiDAR Fusion Using Multi-Head Self-Attention for\n  End-to-End Serving Robot Relocalization","summary":"  With the recent development of autonomous driving technology, as the pursuit\nof efficiency for repetitive tasks and the value of non-face-to-face services\nincrease, mobile service robots such as delivery robots and serving robots\nattract attention, and their demands are increasing day by day. However, when\nsomething goes wrong, most commercial serving robots need to return to their\nstarting position and orientation to operate normally again. In this paper, we\nfocus on end-to-end relocalization of serving robots to address the problem. It\nis to predict robot pose directly from only the onboard sensor data using\nneural networks. In particular, we propose a deep neural network architecture\nfor the relocalization based on camera-2D LiDAR sensor fusion. We call the\nproposed method FusionLoc. In the proposed method, the multi-head\nself-attention complements different types of information captured by the two\nsensors. Our experiments on a dataset collected by a commercial serving robot\ndemonstrate that FusionLoc can provide better performances than previous\nrelocalization methods taking only a single image or a 2D LiDAR point cloud as\nwell as a straightforward fusion method concatenating their features.\n","authors":["Jieun Lee","Hakjun Lee","Jiyong Oh"],"pdf_url":"https://arxiv.org/pdf/2303.06872v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.06870v1","updated":"2023-03-13T05:37:46Z","published":"2023-03-13T05:37:46Z","title":"Three Guidelines You Should Know for Universally Slimmable\n  Self-Supervised Learning","summary":"  We propose universally slimmable self-supervised learning (dubbed as US3L) to\nachieve better accuracy-efficiency trade-offs for deploying self-supervised\nmodels across different devices. We observe that direct adaptation of\nself-supervised learning (SSL) to universally slimmable networks misbehaves as\nthe training process frequently collapses. We then discover that temporal\nconsistent guidance is the key to the success of SSL for universally slimmable\nnetworks, and we propose three guidelines for the loss design to ensure this\ntemporal consistency from a unified gradient perspective. Moreover, we propose\ndynamic sampling and group regularization strategies to simultaneously improve\ntraining efficiency and accuracy. Our US3L method has been empirically\nvalidated on both convolutional neural networks and vision transformers. With\nonly once training and one copy of weights, our method outperforms various\nstate-of-the-art methods (individually trained or not) on benchmarks including\nrecognition, object detection and instance segmentation. Our code is available\nat https://github.com/megvii-research/US3L-CVPR2023.\n","authors":["Yun-Hao Cao","Peiqin Sun","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06870v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06869v1","updated":"2023-03-13T05:37:40Z","published":"2023-03-13T05:37:40Z","title":"Adaptive Data-Free Quantization","summary":"  Data-free quantization (DFQ) recovers the performance of quantized network\n(Q) without accessing the real data, but generates the fake sample via a\ngenerator (G) by learning from full-precision network (P) instead. However,\nsuch sample generation process is totally independent of Q, overlooking the\nadaptability of the knowledge from generated samples, i.e., informative or not\nto the learning process of Q, resulting into the overflow of generalization\nerror. Building on this, several critical questions -- how to measure the\nsample adaptability to Q under varied bit-width scenarios? how to generate the\nsamples with large adaptability to improve Q's generalization? whether the\nlargest adaptability is the best? To answer the above questions, in this paper,\nwe propose an Adaptive Data-Free Quantization (AdaDFQ) method, which\nreformulates DFQ as a zero-sum game upon the sample adaptability between two\nplayers -- a generator and a quantized network. Following this viewpoint, we\nfurther define the disagreement and agreement samples to form two boundaries,\nwhere the margin is optimized to address the over-and-under fitting issues, so\nas to generate the samples with the desirable adaptability to Q. Our AdaDFQ\nreveals: 1) the largest adaptability is NOT the best for sample generation to\nbenefit Q's generalization; 2) the knowledge of the generated sample should not\nbe informative to Q only, but also related to the category and distribution\ninformation of the training data for P. The theoretical and empirical analysis\nvalidate the advantages of AdaDFQ over the state-of-the-arts. Our code is\navailable at https: github.com/hfutqian/AdaDFQ.\n","authors":["Biao Qian","Yang Wang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06869v1.pdf","comment":"9 pages, 6 figures, accepted by CVPR 2023. arXiv admin note:\n  substantial text overlap with arXiv:2302.09572"},{"id":"http://arxiv.org/abs/2303.06868v1","updated":"2023-03-13T05:33:28Z","published":"2023-03-13T05:33:28Z","title":"Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's\n  Disease Using 3D Comprehensive Visual Stimuli","summary":"  Alzheimer's Disease (AD) causes a continuous decline in memory, thinking, and\njudgment. Traditional diagnoses are usually based on clinical experience, which\nis limited by some realistic factors. In this paper, we focus on exploiting\ndeep learning techniques to diagnose AD based on eye-tracking behaviors. Visual\nattention, as typical eye-tracking behavior, is of great clinical value to\ndetect cognitive abnormalities in AD patients. To better analyze the\ndifferences in visual attention between AD patients and normals, we first\nconduct a 3D comprehensive visual task on a non-invasive eye-tracking system to\ncollect visual attention heatmaps. We then propose a multi-layered comparison\nconvolution neural network (MC-CNN) to distinguish the visual attention\ndifferences between AD patients and normals. In MC-CNN, the multi-layered\nrepresentations of heatmaps are obtained by hierarchical convolution to better\nencode eye-movement behaviors, which are further integrated into a distance\nvector to benefit the comprehensive visual task. Extensive experimental results\non the collected dataset demonstrate that MC-CNN achieves consistent validity\nin classifying AD patients and normals with eye-tracking data.\n","authors":["Fangyu Zuo","Peiguang Jing","Jinglin Sun"," Jizhong"," Duan","Yong Ji","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09590v4","updated":"2023-03-13T05:19:15Z","published":"2022-11-17T15:36:48Z","title":"Hypergraph Transformer for Skeleton-based Action Recognition","summary":"  Skeleton-based action recognition aims to recognize human actions given human\njoint coordinates with skeletal interconnections. By defining a graph with\njoints as vertices and their natural connections as edges, previous works\nsuccessfully adopted Graph Convolutional networks (GCNs) to model joint\nco-occurrences and achieved superior performance. More recently, a limitation\nof GCNs is identified, i.e., the topology is fixed after training. To relax\nsuch a restriction, Self-Attention (SA) mechanism has been adopted to make the\ntopology of GCNs adaptive to the input, resulting in the state-of-the-art\nhybrid models. Concurrently, attempts with plain Transformers have also been\nmade, but they still lag behind state-of-the-art GCN-based methods due to the\nlack of structural prior. Unlike hybrid models, we propose a more elegant\nsolution to incorporate the bone connectivity into Transformer via a graph\ndistance embedding. Our embedding retains the information of skeletal structure\nduring training, whereas GCNs merely use it for initialization. More\nimportantly, we reveal an underlying issue of graph models in general, i.e.,\npairwise aggregation essentially ignores the high-order kinematic dependencies\nbetween body joints. To fill this gap, we propose a new self-attention (SA)\nmechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to\nincorporate intrinsic higher-order relations into the model. We name the\nresulting model Hyperformer, and it beats state-of-the-art graph models w.r.t.\naccuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA\ndatasets.\n","authors":["Yuxuan Zhou","Zhi-Qi Cheng","Chao Li","Yanwen Fang","Yifeng Geng","Xuansong Xie","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2211.09590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06862v1","updated":"2023-03-13T05:13:47Z","published":"2023-03-13T05:13:47Z","title":"OTOV2: Automatic, Generic, User-Friendly","summary":"  The existing model compression methods via structured pruning typically\nrequire complicated multi-stage procedures. Each individual stage necessitates\nnumerous engineering efforts and domain-knowledge from the end-users which\nprevent their wider applications onto broader scenarios. We propose the second\ngeneration of Only-Train-Once (OTOv2), which first automatically trains and\ncompresses a general DNN only once from scratch to produce a more compact model\nwith competitive performance without fine-tuning. OTOv2 is automatic and\npluggable into various deep learning applications, and requires almost minimal\nengineering efforts from the users. Methodologically, OTOv2 proposes two major\nimprovements: (i) Autonomy: automatically exploits the dependency of general\nDNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and\nconstructs the compressed model; and (ii) Dual Half-Space Projected Gradient\n(DHSPG): a novel optimizer to more reliably solve structured-sparsity problems.\nNumerically, we demonstrate the generality and autonomy of OTOv2 on a variety\nof model architectures such as VGG, ResNet, CARN, ConvNeXt, DenseNet and\nStackedUnets, the majority of which cannot be handled by other methods without\nextensive handcrafting efforts. Together with benchmark datasets including\nCIFAR10/100, DIV2K, Fashion-MNIST, SVNH and ImageNet, its effectiveness is\nvalidated by performing competitively or even better than the\nstate-of-the-arts. The source code is available at\nhttps://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Luming Liang","Tianyu Ding","Zhihui Zhu","Ilya Zharkov"],"pdf_url":"https://arxiv.org/pdf/2303.06862v1.pdf","comment":"Published on ICLR 2023. Remark here that a few images of dependency\n  graphs can not be included in arXiv due to exceeding size limit"},{"id":"http://arxiv.org/abs/2303.06860v1","updated":"2023-03-13T05:08:25Z","published":"2023-03-13T05:08:25Z","title":"View Adaptive Light Field Deblurring Networks with Depth Perception","summary":"  The Light Field (LF) deblurring task is a challenging problem as the blur\nimages are caused by different reasons like the camera shake and the object\nmotion. The single image deblurring method is a possible way to solve this\nproblem. However, since it deals with each view independently and cannot\neffectively utilize and maintain the LF structure, the restoration effect is\nusually not ideal. Besides, the LF blur is more complex because the degree is\naffected by the views and depth. Therefore, we carefully designed a novel LF\ndeblurring network based on the LF blur characteristics. On one hand, since the\nblur degree varies a lot in different views, we design a novel view adaptive\nspatial convolution to deblur blurred LFs, which calculates the exclusive\nconvolution kernel for each view. On the other hand, because the blur degree\nalso varies with the depth of the object, a depth perception view attention is\ndesigned to deblur different depth areas by selectively integrating information\nfrom different views. Besides, we introduce an angular position embedding to\nmaintain the LF structure better, which ensures the model correctly restores\nthe view information. Quantitative and qualitative experimental results on\nsynthetic and real images show that the deblurring effect of our method is\nbetter than other state-of-the-art methods.\n","authors":["Zeqi Shen","Shuo Zhang","Zhuhao Zhang","Qihua Chen","Xueyao Dong","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.06860v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.06859v1","updated":"2023-03-13T05:04:18Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Casual-IRDIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.06857v1","updated":"2023-03-13T05:02:34Z","published":"2023-03-13T05:02:34Z","title":"An automated pipeline to create an atlas of in situ hybridization gene\n  expression data in the adult marmoset brain","summary":"  We present the first automated pipeline to create an atlas of in situ\nhybridization gene expression in the adult marmoset brain in the same\nstereotaxic space. The pipeline consists of segmentation of gene expression\nfrom microscopy images and registration of images to a standard space.\nAutomation of this pipeline is necessary to analyze the large volume of data in\nthe genome-wide whole-brain dataset, and to process images that have varying\nintensity profiles and expression patterns with minimal human bias. To reduce\nthe number of labelled images required for training, we develop a\nsemi-supervised segmentation model. We further develop an iterative algorithm\nto register images to a standard space, enabling comparative analysis between\ngenes and concurrent visualization with other datasets, thereby facilitating a\nmore holistic understanding of primate brain structure and function.\n","authors":["Charissa Poon","Muhammad Febrian Rachmadi","Michal Byra","Matthias Schlachter","Binbin Xu","Tomomi Shimogori","Henrik Skibbe"],"pdf_url":"https://arxiv.org/pdf/2303.06857v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.06856v1","updated":"2023-03-13T05:01:50Z","published":"2023-03-13T05:01:50Z","title":"Dynamic Neural Network for Multi-Task Learning Searching across Diverse\n  Network Topologies","summary":"  In this paper, we present a new MTL framework that searches for structures\noptimized for multiple tasks with diverse graph topologies and shares features\namong tasks. We design a restricted DAG-based central network with\nread-in/read-out layers to build topologically diverse task-adaptive structures\nwhile limiting search space and time. We search for a single optimized network\nthat serves as multiple task adaptive sub-networks using our three-stage\ntraining process. To make the network compact and discretized, we propose a\nflow-based reduction algorithm and a squeeze loss used in the training process.\nWe evaluate our optimized network on various public MTL datasets and show ours\nachieves state-of-the-art performance. An extensive ablation study\nexperimentally validates the effectiveness of the sub-module and schemes in our\nframework.\n","authors":["Wonhyeok Choi","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2303.06856v1.pdf","comment":"Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06852v1","updated":"2023-03-13T04:47:15Z","published":"2023-03-13T04:47:15Z","title":"One-Shot Segmentation of Novel White Matter Tracts via Extensive Data\n  Augmentation","summary":"  Deep learning based methods have achieved state-of-the-art performance for\nautomated white matter (WM) tract segmentation. In these methods, the\nsegmentation model needs to be trained with a large number of manually\nannotated scans, which can be accumulated throughout time. When novel WM\ntracts, i.e., tracts not included in the existing annotated WM tracts, are to\nbe segmented, additional annotations of these novel WM tracts need to be\ncollected. Since tract annotation is time-consuming and costly, it is desirable\nto make only a few annotations of novel WM tracts for training the segmentation\nmodel, and previous work has addressed this problem by transferring the\nknowledge learned for segmenting existing WM tracts to the segmentation of\nnovel WM tracts. However, accurate segmentation of novel WM tracts can still be\nchallenging in the one-shot setting, where only one scan is annotated for the\nnovel WM tracts. In this work, we explore the problem of one-shot segmentation\nof novel WM tracts. Since in the one-shot setting the annotated training data\nis extremely scarce, based on the existing knowledge transfer framework, we\npropose to further perform extensive data augmentation for the single annotated\nscan, where synthetic annotated training data is produced. We have designed\nseveral different strategies that mask out regions in the single annotated scan\nfor data augmentation. Our method was evaluated on public and in-house\ndatasets. The experimental results show that our method improves the accuracy\nof one-shot segmentation of novel WM tracts.\n","authors":["Wan Liu","Qi Lu","ZhiZheng Zhuo","Yaou Liu","Chuyang Ye"],"pdf_url":"https://arxiv.org/pdf/2303.06852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05970v2","updated":"2023-03-13T04:41:36Z","published":"2023-03-10T15:01:51Z","title":"Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D\n  Perception","summary":"  Long-term temporal fusion is a crucial but often overlooked technique in\ncamera-based Bird's-Eye-View (BEV) 3D perception. Existing methods are mostly\nin a parallel manner. While parallel fusion can benefit from long-term\ninformation, it suffers from increasing computational and memory overheads as\nthe fusion window size grows. Alternatively, BEVFormer adopts a recurrent\nfusion pipeline so that history information can be efficiently integrated, yet\nit fails to benefit from longer temporal frames. In this paper, we explore an\nembarrassingly simple long-term recurrent fusion strategy built upon the\nLSS-based methods and find it already able to enjoy the merits from both sides,\ni.e., rich long-term information and efficient fusion pipeline. A temporal\nembedding module is further proposed to improve the model's robustness against\noccasionally missed frames in practical scenarios. We name this simple but\neffective fusing pipeline VideoBEV. Experimental results on the nuScenes\nbenchmark show that VideoBEV obtains leading performance on various\ncamera-based 3D perception tasks, including object detection (55.4% mAP and\n62.9% NDS), segmentation (48.6% vehicle mIoU), tracking (54.8% AMOTA), and\nmotion prediction (0.80m minADE and 0.463 EPA). Code will be available.\n","authors":["Chunrui Han","Jianjian Sun","Zheng Ge","Jinrong Yang","Runpei Dong","Hongyu Zhou","Weixin Mao","Yuang Peng","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06842v1","updated":"2023-03-13T04:16:42Z","published":"2023-03-13T04:16:42Z","title":"Scene Graph Generation from Hierarchical Relationship Reasoning","summary":"  This paper describes a novel approach to deducing relationships between\nobjects in a visual scene. It explicitly exploits an informative hierarchical\nstructure that can be imposed to divide the object and relationship categories\ninto disjoint super-categories. Specifically, our proposed scheme implements a\nBayes prediction head to jointly predict the super-category or type of\nrelationship between the two objects, along with the detailed relationship\nwithin that super-category. This design reduces the impact of class imbalance\nproblems. We present experimental results on the Visual Genome and OpenImage V6\ndatasets showing that this factorized approach allows a relatively simple model\nto achieve competitive performance, especially on predicate classification and\nzero-shot tasks.\n","authors":["Bowen Jiang","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2303.06842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06840v1","updated":"2023-03-13T04:06:42Z","published":"2023-03-13T04:06:42Z","title":"DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion","summary":"  Multi-modality image fusion aims to combine different modalities to produce\nfused images that retain the complementary features of each modality, such as\nfunctional highlights and texture details. To leverage strong generative priors\nand address challenges such as unstable training and lack of interpretability\nfor GAN-based generative methods, we propose a novel fusion algorithm based on\nthe denoising diffusion probabilistic model (DDPM). The fusion task is\nformulated as a conditional generation problem under the DDPM sampling\nframework, which is further divided into an unconditional generation subproblem\nand a maximum likelihood subproblem. The latter is modeled in a hierarchical\nBayesian manner with latent variables and inferred by the\nexpectation-maximization algorithm. By integrating the inference solution into\nthe diffusion sampling iteration, our method can generate high-quality fused\nimages with natural image generative priors and cross-modality information from\nsource images. Note that all we required is an unconditional pre-trained\ngenerative model, and no fine-tuning is needed. Our extensive experiments\nindicate that our approach yields promising fusion results in infrared-visible\nimage fusion and medical image fusion. The code will be released.\n","authors":["Zixiang Zhao","Haowen Bai","Yuanzhi Zhu","Jiangshe Zhang","Shuang Xu","Yulun Zhang","Kai Zhang","Deyu Meng","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.06840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00648v3","updated":"2023-03-13T04:06:24Z","published":"2022-12-01T16:49:53Z","title":"One-shot recognition of any material anywhere using contrastive learning\n  with physics-based rendering","summary":"  Visual recognition of materials and their states is essential for\nunderstanding most aspects of the world, from determining whether food is\ncooked, metal is rusted, or a chemical reaction has occurred. However, current\nimage recognition methods are limited to specific classes and properties and\ncan't handle the vast number of material states and textures in the world. To\naddress this, we present MatSim: the first dataset and benchmark for computer\nvision-based recognition of similarities and transitions between materials and\ntextures, focusing on identifying any material under any conditions using one\nor a few examples. The dataset contains synthetic and real images. The\nsynthetic images were rendered using giant collections of textures, objects,\nand environments generated by computer graphics artists. We use mixtures and\ngradual transitions between materials to allow the system to learn cases with\nsmooth transitions between states (like gradually cooked food). We also render\nimages with materials inside transparent containers to support beverage and\nchemistry lab use cases. We use this dataset to train a siamese net that\nidentifies the same material in different objects, mixtures, and environments.\nThe descriptor generated by this net can be used to identify the states of\nmaterials and their subclasses using a single image. We also present the first\nfew-shot material recognition benchmark with images from a wide range of\nfields, including the state of metals and chemical reactions types of ground\nand many other use cases. We show that a net trained on the MatSim synthetic\ndataset outperforms state-of-the-art models like Clip on the benchmark and also\nachieves good results on other unsupervised material classification tasks.\n","authors":["Manuel S. Drehwald","Sagi Eppel","Jolina Li","Han Hao","Alan Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2212.00648v3.pdf","comment":"for associated code and dataset, see\n  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or\n  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX\n  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF"},{"id":"http://arxiv.org/abs/2303.06834v1","updated":"2023-03-13T03:31:29Z","published":"2023-03-13T03:31:29Z","title":"DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep\n  Inconsistency Prior","summary":"  RGB-NIR fusion is a promising method for low-light imaging. However,\nhigh-intensity noise in low-light images amplifies the effect of structure\ninconsistency between RGB-NIR images, which fails existing algorithms. To\nhandle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net\n(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior\n(DIP). The Deep Structure extracts clear structure details in deep multiscale\nfeature space rather than raw input space, which is more robust to noisy\ninputs. Based on the deep structures from both RGB and NIR domains, we\nintroduce the DIP to leverage the structure inconsistency to guide the fusion\nof RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality\nlowlight images without the visual artifacts. We also propose a new dataset\ncalled Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as\nthe first public RGBNIR fusion benchmark. Quantitative and qualitative results\non the proposed benchmark show that DVN significantly outperforms other\ncomparison algorithms in PSNR and SSIM, especially in extremely low light\nconditions.\n","authors":["Shuangping Jin","Bingbing Yu","Minhao Jing","Yi Zhou","Jiajun Liang","Renhe Ji"],"pdf_url":"https://arxiv.org/pdf/2303.06834v1.pdf","comment":"Accepted to AAAI 2022"},{"id":"http://arxiv.org/abs/2212.13906v2","updated":"2023-03-13T03:21:43Z","published":"2022-12-24T17:59:01Z","title":"DiP: Learning Discriminative Implicit Parts for Person Re-Identification","summary":"  In person re-identification (ReID) tasks, many works explore the learning of\npart features to improve the performance over global image features. Existing\nmethods explicitly extract part features by either using a hand-designed image\ndivision or keypoints obtained with external visual systems. In this work, we\npropose to learn Discriminative implicit Parts (DiPs) which are decoupled from\nexplicit body parts. Therefore, DiPs can learn to extract any discriminative\nfeatures that can benefit in distinguishing identities, which is beyond\npredefined body parts (such as accessories). Moreover, we propose a novel\nimplicit position to give a geometric interpretation for each DiP. The implicit\nposition can also serve as a learning signal to encourage DiPs to be more\nposition-equivariant with the identity in the image. Lastly, an additional DiP\nweighting is introduced to handle the invisible or occluded situation and\nfurther improve the feature representation of DiPs. Extensive experiments show\nthat the proposed method achieves state-of-the-art performance on multiple\nperson ReID benchmarks.\n","authors":["Dengjie Li","Siyu Chen","Yujie Zhong","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2212.13906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06823v1","updated":"2023-03-13T02:49:50Z","published":"2023-03-13T02:49:50Z","title":"Instate: Predicting the State of Residence From Last Name","summary":"  India has twenty-two official languages. Serving such a diverse language base\nis a challenge for survey statisticians, call center operators, software\ndevelopers, and other such service providers. To help provide better services\nto different language communities via better localization, we introduce a new\nmachine learning model that predicts the language(s) that the user can speak\nfrom their name. Using nearly 438M records spanning 33 Indian states and 1.13M\nunique last names from the Indian Electoral Rolls Corpus (?), we build a\ncharacter-level transformer-based machine-learning model that predicts the\nstate of residence based on the last name. The model has a top-3 accuracy of\n85.3% on unseen names. We map the states to languages using the Indian census\nto infer languages understood by the respondent. We provide open-source\nsoftware that implements the method discussed in the paper.\n","authors":["Atul Dhingra","Gaurav Sood"],"pdf_url":"https://arxiv.org/pdf/2303.06823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06821v1","updated":"2023-03-13T02:48:54Z","published":"2023-03-13T02:48:54Z","title":"SDF-3DGAN: A 3D Object Generative Method Based on Implicit Signed\n  Distance Function","summary":"  In this paper, we develop a new method, termed SDF-3DGAN, for 3D object\ngeneration and 3D-Aware image synthesis tasks, which introduce implicit Signed\nDistance Function (SDF) as the 3D object representation method in the\ngenerative field. We apply SDF for higher quality representation of 3D object\nin space and design a new SDF neural renderer, which has higher efficiency and\nhigher accuracy. To train only on 2D images, we first generate the objects,\nwhich are represented by SDF, from Gaussian distribution. Then we render them\nto 2D images and use them to apply GAN training method together with 2D images\nin the dataset. In the new rendering method, we relieve all the potential of\nSDF mathematical property to alleviate computation pressure in the previous SDF\nneural renderer. In specific, our new SDF neural renderer can solve the problem\nof sampling ambiguity when the number of sampling point is not enough, \\ie use\nthe less points to finish higher quality sampling task in the rendering\npipeline. And in this rendering pipeline, we can locate the surface easily.\nTherefore, we apply normal loss on it to control the smoothness of generated\nobject surface, which can make our method enjoy the much higher generation\nquality. Quantitative and qualitative experiments conducted on public\nbenchmarks demonstrate favorable performance against the state-of-the-art\nmethods in 3D object generation task and 3D-Aware image synthesis task. Our\ncodes will be released at https://github.com/lutao2021/SDF-3DGAN.\n","authors":["Lutao Jiang","Ruyi Ji","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05933v2","updated":"2023-03-13T02:45:40Z","published":"2023-03-10T14:11:09Z","title":"Boosting Open-Set Domain Adaptation with Threshold Self-Tuning and\n  Cross-Domain Mixup","summary":"  Open-set domain adaptation (OSDA) aims to not only recognize target samples\nbelonging to common classes shared by source and target domains but also\nperceive unknown class samples. Existing OSDA methods suffer from two\nobstacles. Firstly, a tedious process of manually tuning a hyperparameter\n$threshold$ is required for most OSDA approaches to separate common and unknown\nclasses. It is difficult to determine a proper threshold when the target domain\ndata is unlabeled. Secondly, most OSDA methods rely only on confidence values\nto distinguish between common and unknown classes, using limited source and\ntarget samples to train models, leading to unsatisfactory performance when the\ntarget domain has mostly unknown classes. Our studies demonstrate that\nexploiting multiple criteria within a more continuous latent space is\nbeneficial for the model's performance. In this paper, we design a novel\nthreshold self-tuning and cross-domain mixup (TSCM) method to overcome the two\ndrawbacks. TSCM can automatically tune a proper threshold utilizing unlabeled\ntarget samples rather than manually setting an empirical hyperparameter. Our\nmethod considers multiple criteria instead of only the confidence and uses the\nthreshold generated by itself to separate common and unknown classes in the\ntarget domain. Moreover, we introduce a cross-domain mixup method designed for\nOSDA scenarios to learn domain-invariant features in a more continuous latent\nspace. Comprehensive experiments illustrate that our method consistently\nachieves superior performance on different benchmarks compared with various\nstate-of-the-art methods.\n","authors":["Xinghong Liu","Yi Zhou","Tao Zhou","Jie Qin","Shengcai Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06820v1","updated":"2023-03-13T02:33:34Z","published":"2023-03-13T02:33:34Z","title":"Continuous sign language recognition based on cross-resolution knowledge\n  distillation","summary":"  The goal of continuous sign language recognition(CSLR) research is to apply\nCSLR models as a communication tool in real life, and the real-time requirement\nof the models is important. In this paper, we address the model real-time\nproblem through cross-resolution knowledge distillation. In our study, we found\nthat keeping the frame-level feature scales consistent between the output of\nthe student network and the teacher network is better than recovering the\nframe-level feature sizes for feature distillation. Based on this finding, we\npropose a new frame-level feature extractor that keeps the output frame-level\nfeatures at the same scale as the output of by the teacher network. We further\ncombined with the TSCM+2D hybrid convolution proposed in our previous study to\nform a new lightweight end-to-end CSLR network-Low resolution input\nnet(LRINet). It is then used to combine cross-resolution knowledge distillation\nand traditional knowledge distillation methods to form a CSLR model based on\ncross-resolution knowledge distillation (CRKD). The CRKD uses high-resolution\nframes as input to the teacher network for training, locks the weights after\ntraining, and then uses low-resolution frames as input to the student network\nLRINet to perform knowledge distillation on frame-level features and\nclassification features respectively. Experiments on two large-scale continuous\nsign language datasets have proved the effectiveness of CRKD. Compared with the\nmodel with high-resolution data as input, the calculation amount, parameter\namount and inference time of the model have been significantly reduced under\nthe same experimental conditions, while ensuring the accuracy of the model, and\nhas achieved very competitive results in comparison with other advanced\nmethods.\n","authors":["Qidan Zhu","Jing Li","Fei Yuan","Quan Gan"],"pdf_url":"https://arxiv.org/pdf/2303.06820v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.06819v1","updated":"2023-03-13T02:27:45Z","published":"2023-03-13T02:27:45Z","title":"TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning\n  with Structure-Trajectory Prompted Reconstruction for Person\n  Re-Identification","summary":"  Person re-identification (re-ID) via 3D skeleton data is an emerging topic\nwith prominent advantages. Existing methods usually design skeleton descriptors\nwith raw body joints or perform skeleton sequence representation learning.\nHowever, they typically cannot concurrently model different body-component\nrelations, and rarely explore useful semantics from fine-grained\nrepresentations of body joints. In this paper, we propose a generic\nTransformer-based Skeleton Graph prototype contrastive learning (TranSG)\napproach with structure-trajectory prompted reconstruction to fully capture\nskeletal relations and valuable spatial-temporal semantics from skeleton graphs\nfor person re-ID. Specifically, we first devise the Skeleton Graph Transformer\n(SGT) to simultaneously learn body and motion relations within skeleton graphs,\nso as to aggregate key correlative node features into graph representations.\nThen, we propose the Graph Prototype Contrastive learning (GPC) to mine the\nmost typical graph features (graph prototypes) of each identity, and contrast\nthe inherent similarity between graph representations and different prototypes\nfrom both skeleton and sequence levels to learn discriminative graph\nrepresentations. Last, a graph Structure-Trajectory Prompted Reconstruction\n(STPR) mechanism is proposed to exploit the spatial and temporal contexts of\ngraph nodes to prompt skeleton graph reconstruction, which facilitates\ncapturing more valuable patterns and graph semantics for person re-ID.\nEmpirical evaluations demonstrate that TranSG significantly outperforms\nexisting state-of-the-art methods. We further show its generality under\ndifferent graph modeling, RGB-estimated skeletons, and unsupervised scenarios.\n","authors":["Haocong Rao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2303.06819v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at\n  https://github.com/Kali-Hac/TranSG. Supplemental material is included in the\n  conference proceedings"},{"id":"http://arxiv.org/abs/2303.06817v1","updated":"2023-03-13T02:21:38Z","published":"2023-03-13T02:21:38Z","title":"Transformation-Invariant Network for Few-Shot Object Detection in Remote\n  Sensing Images","summary":"  Object detection in remote sensing images relies on a large amount of labeled\ndata for training. The growing new categories and class imbalance render\nexhaustive annotation non-scalable. Few-shot object detection~(FSOD) tackles\nthis issue by meta-learning on seen base classes and then fine-tuning on novel\nclasses with few labeled samples. However, the object's scale and orientation\nvariations are particularly large in remote sensing images, thus posing\nchallenges to existing few-shot object detection methods. To tackle these\nchallenges, we first propose to integrate a feature pyramid network and use\nprototype features to highlight query features to improve upon existing FSOD\nmethods. We refer to the modified FSOD as a Strong Baseline which is\ndemonstrated to perform significantly better than the original baselines. To\nimprove the robustness of orientation variation, we further propose a\ntransformation-invariant network (TINet) to allow the network to be invariant\nto geometric transformations. Extensive experiments on three widely used remote\nsensing object detection datasets, i.e., NWPU VHR-10.v2, DIOR, and HRRSD\ndemonstrated the effectiveness of the proposed method. Finally, we reproduced\nmultiple FSOD methods for remote sensing images to create an extensive\nbenchmark for follow-up works.\n","authors":["Nanqing Liu","Xun Xu","Turgay Celik","Zongxin Gan","Heng-Chao Li"],"pdf_url":"https://arxiv.org/pdf/2303.06817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14074v3","updated":"2023-03-13T02:00:41Z","published":"2022-09-28T13:15:03Z","title":"Recipro-CAM: Fast gradient-free visual explanations for convolutional\n  neural networks","summary":"  The Convolutional Neural Network (CNN) is a widely used deep learning\narchitecture for computer vision. However, its black box nature makes it\ndifficult to interpret the behavior of the model. To mitigate this issue, AI\npractitioners have explored explainable AI methods like Class Activation Map\n(CAM) and Grad-CAM. Although these methods have shown promise, they are limited\nby architectural constraints or the burden of gradient computing. To overcome\nthis issue, Score-CAM and Ablation-CAM have been proposed as gradient-free\nmethods, but they have longer execution times compared to CAM or Grad-CAM based\nmethods, making them unsuitable for real-world solution though they resolved\ngradient related issues and enabled inference mode XAI. To address this\nchallenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.\nOur approach involves spatially masking the extracted feature maps to exploit\nthe correlation between activation maps and network predictions for target\nclasses. Our proposed method has yielded promising results, outperforming\ncurrent state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)\nmetric by $1.78 \\%$ to $3.72 \\%$, excluding VGG-16 backbone. Moreover,\nRecipro-CAM generates saliency maps at a similar rate to Grad-CAM and is\napproximately $148$ times faster than Score-CAM. The source code for\nRecipro-CAM is available in our data analysis framework.\n","authors":["Seok-Yong Byun","Wonju Lee"],"pdf_url":"https://arxiv.org/pdf/2209.14074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06810v1","updated":"2023-03-13T01:56:53Z","published":"2023-03-13T01:56:53Z","title":"Dynamic Clustering and Cluster Contrastive Learning for Unsupervised\n  Person Re-identification","summary":"  Unsupervised Re-ID methods aim at learning robust and discriminative features\nfrom unlabeled data. However, existing methods often ignore the relationship\nbetween module parameters of Re-ID framework and feature distributions, which\nmay lead to feature misalignment and hinder the model performance. To address\nthis problem, we propose a dynamic clustering and cluster contrastive learning\n(DCCC) method. Specifically, we first design a dynamic clustering parameters\nscheduler (DCPS) which adjust the hyper-parameter of clustering to fit the\nvariation of intra- and inter-class distances. Then, a dynamic cluster\ncontrastive learning (DyCL) method is designed to match the cluster\nrepresentation vectors' weights with the local feature association. Finally, a\nlabel smoothing soft contrastive loss ($L_{ss}$) is built to keep the balance\nbetween cluster contrastive learning and self-supervised learning with low\ncomputational consumption and high computational efficiency. Experiments on\nseveral widely used public datasets validate the effectiveness of our proposed\nDCCC which outperforms previous state-of-the-art methods by achieving the best\nperformance.\n","authors":["Ziqi He","Mengjia Xue","Yunhao Du","Zhicheng Zhao","Fei Su"],"pdf_url":"https://arxiv.org/pdf/2303.06810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06807v1","updated":"2023-03-13T01:42:29Z","published":"2023-03-13T01:42:29Z","title":"Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual\n  Constraints","summary":"  Optical Coherence Tomography Angiography (OCTA) has become increasingly vital\nin the clinical screening of fundus diseases due to its ability to capture\naccurate 3D imaging of blood vessels in a non-contact scanning manner. However,\nthe acquisition of OCTA images remains challenging due to the requirement of\nexclusive sensors and expensive devices. In this paper, we propose a novel\nframework, TransPro, that translates 3D Optical Coherence Tomography (OCT)\nimages into exclusive 3D OCTA images using an image translation pattern. Our\nmain objective is to address two issues in existing image translation\nbaselines, namely, the aimlessness in the translation process and\nincompleteness of the translated object. The former refers to the overall\nquality of the translated OCTA images being satisfactory, but the retinal\nvascular quality being low. The latter refers to incomplete objects in\ntranslated OCTA images due to the lack of global contexts. TransPro merges a 2D\nretinal vascular segmentation model and a 2D OCTA image translation model into\na 3D image translation baseline for the 2D projection map projected by the\ntranslated OCTA images. The 2D retinal vascular segmentation model can improve\nattention to the retinal vascular, while the 2D OCTA image translation model\nintroduces beneficial heuristic contextual information. Extensive experimental\nresults on two challenging datasets demonstrate that TransPro can consistently\noutperform existing approaches with minimal computational overhead during\ntraining and none during testing.\n","authors":["Shuhan Li","Dong Zhang","Xiaomeng Li","Chubin Ou","Lin An","Yanwu Xu","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.06807v1.pdf","comment":"Code is available at: https://github.com/ustlsh/TransPro"},{"id":"http://arxiv.org/abs/2303.06800v1","updated":"2023-03-13T01:10:50Z","published":"2023-03-13T01:10:50Z","title":"Object-Centric Multi-Task Learning for Human Instances","summary":"  Human is one of the most essential classes in visual recognition tasks such\nas detection, segmentation, and pose estimation. Although much effort has been\nput into individual tasks, multi-task learning for these three tasks has been\nrarely studied. In this paper, we explore a compact multi-task network\narchitecture that maximally shares the parameters of the multiple tasks via\nobject-centric learning. To this end, we propose a novel query design to encode\nthe human instance information effectively, called human-centric query (HCQ).\nHCQ enables for the query to learn explicit and structural information of human\nas well such as keypoints. Besides, we utilize HCQ in prediction heads of the\ntarget tasks directly and also interweave HCQ with the deformable attention in\nTransformer decoders to exploit a well-learned object-centric representation.\nExperimental results show that the proposed multi-task network achieves\ncomparable accuracy to state-of-the-art task-specific models in human\ndetection, segmentation, and pose estimation task, while it consumes less\ncomputational costs.\n","authors":["Hyeongseok Son","Sangil Jung","Solae Lee","Seongeun Kim","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2303.06800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06797v1","updated":"2023-03-13T01:07:32Z","published":"2023-03-13T01:07:32Z","title":"Orthogonal Transform Domain Approaches for the Convolutional Layer","summary":"  In this paper, we propose a set of transform-based neural network layers as\nan alternative to the $3\\times3$ Conv2D layers in Convolutional Neural Networks\n(CNNs). The proposed layers can be implemented based on orthogonal transforms\nsuch as Discrete Cosine Transform (DCT) and Hadamard transform (HT), and the\nbiorthogonal Block Wavelet Transform (BWT). Convolutional filtering operations\nare performed in the transform domain using element-wise multiplications by\ntaking advantage of the convolution theorems. Trainable soft-thresholding\nlayers that remove noise in the transform domain bring nonlinearity to the\ntransform domain layers. Compared to the Conv2D layer which is spatial-agnostic\nand channel-specific, the proposed layers are location-specific and\nchannel-specific. The proposed layers reduce the number of parameters and\nmultiplications significantly while improving the accuracy results of regular\nResNets on the ImageNet-1K classification task. Furthermore, the proposed\nlayers can be inserted with a batch normalization layer before the global\naverage pooling layer in the conventional ResNets as an additional layer to\nimprove classification accuracy with a negligible increase in the number of\nparameters and computational cost.\n","authors":["Hongyi Pan","Xin Zhu","Salih Atici","Ahmet Enis Cetin"],"pdf_url":"https://arxiv.org/pdf/2303.06797v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2211.08577"},{"id":"http://arxiv.org/abs/2303.06796v1","updated":"2023-03-13T01:02:28Z","published":"2023-03-13T01:02:28Z","title":"Ins-ATP: Deep Estimation of ATP for Organoid Based on High Throughput\n  Microscopic Images","summary":"  Adenosine triphosphate (ATP) is a high-energy phosphate compound and the most\ndirect energy source in organisms. ATP is an essential biomarker for evaluating\ncell viability in biology. Researchers often use ATP bioluminescence to measure\nthe ATP of organoid after drug to evaluate the drug efficacy. However, ATP\nbioluminescence has some limitations, leading to unreliable drug screening\nresults. Performing ATP bioluminescence causes cell lysis of organoids, so it\nis impossible to observe organoids' long-term viability changes after\nmedication continually. To overcome the disadvantages of ATP bioluminescence,\nwe propose Ins-ATP, a non-invasive strategy, the first organoid ATP estimation\nmodel based on the high-throughput microscopic image. Ins-ATP directly\nestimates the ATP of organoids from high-throughput microscopic images, so that\nit does not influence the drug reactions of organoids. Therefore, the ATP\nchange of organoids can be observed for a long time to obtain more stable\nresults. Experimental results show that the ATP estimation by Ins-ATP is in\ngood agreement with those determined by ATP bioluminescence. Specifically, the\npredictions of Ins-ATP are consistent with the results measured by ATP\nbioluminescence in the efficacy evaluation experiments of different drugs.\n","authors":["Xuesheng Bian","Cheng Wang","Shuting Chen","Weiquan Liu","Sen Xu","Jinxin Zhu","Rugang Wang","Zexin Chen","Min Huang","Gang Li"],"pdf_url":"https://arxiv.org/pdf/2303.06796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06015v2","updated":"2023-03-13T00:32:29Z","published":"2023-03-10T16:19:34Z","title":"Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation","summary":"  Despite the success of deep learning methods on instance segmentation, these\nmodels still suffer from catastrophic forgetting in continual learning\nscenarios. In this paper, our contributions for continual instance segmentation\nare threefold. First, we propose the Y-knowledge distillation (Y-KD), a\nknowledge distillation strategy that shares a common feature extractor between\nthe teacher and student networks. As the teacher is also updated with new data\nin Y-KD, the increased plasticity results in new modules that are specialized\non new classes. Second, our Y-KD approach is supported by a dynamic\narchitecture method that grows new modules for each task and uses all of them\nfor inference with a unique instance segmentation head, which significantly\nreduces forgetting. Third, we complete our approach by leveraging checkpoint\naveraging as a simple method to manually balance the trade-off between the\nperformance on the various sets of classes, thus increasing the control over\nthe model's behavior without any additional cost. These contributions are\nunited in our model that we name the Dynamic Y-KD network.\n  We perform extensive experiments on several single-step and multi-steps\nscenarios on Pascal-VOC, and we show that our approach outperforms previous\nmethods both on past and new classes. For instance, compared to recent work,\nour method obtains +2.1% mAP on old classes in 15-1, +7.6% mAP on new classes\nin 19-1 and reaches 91.5% of the mAP obtained by joint-training on all classes\nin 15-5.\n","authors":["Mathieu Pagé-Fortin","Brahim Chaib-draa"],"pdf_url":"https://arxiv.org/pdf/2303.06015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07530v1","updated":"2023-03-13T23:35:14Z","published":"2023-03-13T23:35:14Z","title":"Towards Unsupervised Learning based Denoising of Cyber Physical System\n  Data to Mitigate Security Concerns","summary":"  A dataset, collected under an industrial setting, often contains a\nsignificant portion of noises. In many cases, using trivial filters is not\nenough to retrieve useful information i.e., accurate value without the noise.\nOne such data is time-series sensor readings collected from moving vehicles\ncontaining fuel information. Due to the noisy dynamics and mobile environment,\nthe sensor readings can be very noisy. Denoising such a dataset is a\nprerequisite for any useful application and security issues. Security is a\nprimitive concern in present vehicular schemes. The server side for retrieving\nthe fuel information can be easily hacked. Providing the accurate and noise\nfree fuel information via vehicular networks become crutial. Therefore, it has\nled us to develop a system that can remove noise and keep the original value.\nThe system is also helpful for vehicle industry, fuel station, and power-plant\nstation that require fuel. In this work, we have only considered the value of\nfuel level, and we have come up with a unique solution to filter out the noise\nof high magnitudes using several algorithms such as interpolation,\nextrapolation, spectral clustering, agglomerative clustering, wavelet analysis,\nand median filtering. We have also employed peak detection and peak validation\nalgorithms to detect fuel refill and consumption in charge-discharge cycles. We\nhave used the R-squared metric to evaluate our model, and it is 98 percent In\nmost cases, the difference between detected value and real value remains within\nthe range of 1L.\n","authors":["Mst Shapna Akter","Hossain Shahriar"],"pdf_url":"https://arxiv.org/pdf/2303.07530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07527v1","updated":"2023-03-13T23:30:48Z","published":"2023-03-13T23:30:48Z","title":"Domain Generalization via Nuclear Norm Regularization","summary":"  The ability to generalize to unseen domains is crucial for machine learning\nsystems deployed in the real world, especially when we only have data from\nlimited training domains. In this paper, we propose a simple and effective\nregularization method based on the nuclear norm of the learned features for\ndomain generalization. Intuitively, the proposed regularizer mitigates the\nimpacts of environmental features and encourages learning domain-invariant\nfeatures. Theoretically, we provide insights into why nuclear norm\nregularization is more effective compared to ERM and alternative regularization\nmethods. Empirically, we conduct extensive experiments on both synthetic and\nreal datasets. We show that nuclear norm regularization achieves strong\nperformance compared to baselines in a wide range of domain generalization\ntasks. Moreover, our regularizer is broadly applicable with various methods\nsuch as ERM and SWAD with consistently improved performance, e.g., 1.7% and\n0.9% test accuracy improvements respectively on the DomainBed benchmark.\n","authors":["Zhenmei Shi","Yifei Ming","Ying Fan","Frederic Sala","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2303.07527v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.07525v1","updated":"2023-03-13T23:27:42Z","published":"2023-03-13T23:27:42Z","title":"Automated Vulnerability Detection in Source Code Using Quantum Natural\n  Language Processing","summary":"  One of the most important challenges in the field of software code audit is\nthe presence of vulnerabilities in software source code. These flaws are highly\nlikely ex-ploited and lead to system compromise, data leakage, or denial of\nser-vice. C and C++ open source code are now available in order to create a\nlarge-scale, classical machine-learning and quantum machine-learning system for\nfunction-level vulnerability identification. We assembled a siz-able dataset of\nmillions of open-source functions that point to poten-tial exploits. We created\nan efficient and scalable vulnerability detection method based on a deep neural\nnetwork model Long Short Term Memory (LSTM), and quantum machine learning model\nLong Short Term Memory (QLSTM), that can learn features extracted from the\nsource codes. The source code is first converted into a minimal intermediate\nrepresentation to remove the pointless components and shorten the de-pendency.\nTherefore, We keep the semantic and syntactic information using state of the\nart word embedding algorithms such as Glove and fastText. The embedded vectors\nare subsequently fed into the classical and quantum convolutional neural\nnetworks to classify the possible vulnerabilities. To measure the performance,\nwe used evaluation metrics such as F1 score, precision, re-call, accuracy, and\ntotal execution time. We made a comparison between the results derived from the\nclassical LSTM and quantum LSTM using basic feature representation as well as\nsemantic and syntactic represen-tation. We found that the QLSTM with semantic\nand syntactic features detects significantly accurate vulnerability and runs\nfaster than its classical counterpart.\n","authors":["Mst Shapna Akter","Hossain Shahriar","Zakirul Alam Bhuiya"],"pdf_url":"https://arxiv.org/pdf/2303.07525v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.07001v1","updated":"2023-03-13T10:59:38Z","published":"2023-03-13T10:59:38Z","title":"Neural Group Recommendation Based on a Probabilistic Semantic\n  Aggregation","summary":"  Recommendation to groups of users is a challenging subfield of recommendation\nsystems. Its key concept is how and where to make the aggregation of each set\nof user information into an individual entity, such as a ranked recommendation\nlist, a virtual user, or a multi-hot input vector encoding. This paper proposes\nan innovative strategy where aggregation is made in the multi-hot vector that\nfeeds the neural network model. The aggregation provides a probabilistic\nsemantic, and the resulting input vectors feed a model that is able to\nconveniently generalize the group recommendation from the individual\npredictions. Furthermore, using the proposed architecture, group\nrecommendations can be obtained by simply feedforwarding the pre-trained model\nwith individual ratings; that is, without the need to obtain datasets\ncontaining group of user information, and without the need of running two\nseparate trainings (individual and group). This approach also avoids\nmaintaining two different models to support both individual and group learning.\nExperiments have tested the proposed architecture using three representative\ncollaborative filtering datasets and a series of baselines; results show\nsuitable accuracy improvements compared to the state-of-the-art.\n","authors":["Jorge Dueñas-Lerín","Raúl Lara-Cabrera","Fernando Ortega","Jesús Bobadilla"],"pdf_url":"https://arxiv.org/pdf/2303.07001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12230v2","updated":"2023-03-13T18:13:44Z","published":"2022-12-23T10:01:14Z","title":"Recommending on graphs: a comprehensive review from a data perspective","summary":"  Recent advances in graph-based learning approaches have demonstrated their\neffectiveness in modelling users' preferences and items' characteristics for\nRecommender Systems (RSS). Most of the data in RSS can be organized into graphs\nwhere various objects (e.g., users, items, and attributes) are explicitly or\nimplicitly connected and influence each other via various relations. Such a\ngraph-based organization brings benefits to exploiting potential properties in\ngraph learning (e.g., random walk and network embedding) techniques to enrich\nthe representations of the user and item nodes, which is an essential factor\nfor successful recommendations. In this paper, we provide a comprehensive\nsurvey of Graph Learning-based Recommender Systems (GLRSs). Specifically, we\nstart from a data-driven perspective to systematically categorize various\ngraphs in GLRSs and analyze their characteristics. Then, we discuss the\nstate-of-the-art frameworks with a focus on the graph learning module and how\nthey address practical recommendation challenges such as scalability, fairness,\ndiversity, explainability and so on. Finally, we share some potential research\ndirections in this rapidly growing area.\n","authors":["Lemei Zhang","Peng Liu","Jon Atle Gulla"],"pdf_url":"https://arxiv.org/pdf/2212.12230v2.pdf","comment":"Accepted by UMUAI"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2206.00395v2","updated":"2023-03-13T17:59:40Z","published":"2022-06-01T11:02:06Z","title":"Optimization with access to auxiliary information","summary":"  We investigate the fundamental optimization question of minimizing a target\nfunction $f(x)$ whose gradients are expensive to compute or have limited\navailability, given access to some auxiliary side function $h(x)$ whose\ngradients are cheap or more available. This formulation captures many settings\nof practical relevance such as i) re-using batches in SGD, ii) transfer\nlearning, iii) federated learning, iv) training with compressed models/dropout,\netc. We propose two generic new algorithms which are applicable in all these\nsettings and prove using only an assumption on the Hessian similarity between\nthe target and side information that we can benefit from this framework.\n","authors":["El Mahdi Chayti","Sai Praneeth Karimireddy"],"pdf_url":"https://arxiv.org/pdf/2206.00395v2.pdf","comment":"We corrected a mistake that we had in Lemma 9 in the previous version\n  of the paper"},{"id":"http://arxiv.org/abs/2303.07338v1","updated":"2023-03-13T17:59:02Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred. In this work, we revisit CIL with\nPTMs and argue that the core factors in CIL are adaptivity for model updating\nand generalizability for knowledge transferring. 1) We first reveal that frozen\nPTM can already provide generalizable embeddings for CIL. Surprisingly, a\nsimple baseline (SimpleCIL) which continually sets the classifiers of PTM to\nprototype features can beat state-of-the-art even without training on the\ndownstream task. 2) Due to the distribution gap between pre-trained and\ndownstream datasets, PTM can be further cultivated with adaptivity via model\nadapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of\nPTM and adapted models for classifier construction. ADAM is a general framework\nthat can be orthogonally combined with any parameter-efficient tuning method,\nwhich holds the advantages of PTM's generalizability and adapted model's\nadaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the\nera of PTM due to data overlapping and propose four new benchmarks for\nassessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive\nexperiments validate the effectiveness of ADAM with a unified and concise\nframework.\n","authors":["Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v1.pdf","comment":"Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2202.06854v3","updated":"2023-03-13T17:53:07Z","published":"2022-02-14T16:40:24Z","title":"Random Laplacian Features for Learning with Hyperbolic Space","summary":"  Due to its geometric properties, hyperbolic space can support high-fidelity\nembeddings of tree- and graph-structured data, upon which various hyperbolic\nnetworks have been developed. Existing hyperbolic networks encode geometric\npriors not only for the input, but also at every layer of the network. This\napproach involves repeatedly mapping to and from hyperbolic space, which makes\nthese networks complicated to implement, computationally expensive to scale,\nand numerically unstable to train. In this paper, we propose a simpler\napproach: learn a hyperbolic embedding of the input, then map once from it to\nEuclidean space using a mapping that encodes geometric priors by respecting the\nisometries of hyperbolic space, and finish with a standard Euclidean network.\nThe key insight is to use a random feature mapping via the eigenfunctions of\nthe Laplace operator, which we show can approximate any isometry-invariant\nkernel on hyperbolic space. Our method can be used together with any graph\nneural networks: using even a linear graph model yields significant\nimprovements in both efficiency and performance over other hyperbolic baselines\nin both transductive and inductive tasks.\n","authors":["Tao Yu","Christopher De Sa"],"pdf_url":"https://arxiv.org/pdf/2202.06854v3.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2212.03241v2","updated":"2023-03-13T17:43:16Z","published":"2022-12-06T18:59:58Z","title":"PØDA: Prompt-driven Zero-shot Domain Adaptation","summary":"  Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a single general textual description of the target domain, i.e., a\nprompt. First, we leverage a pretrained contrastive vision-language model\n(CLIP) to optimize affine transformations of source features, steering them\ntowards target text embeddings, while preserving their content and semantics.\nSecond, we show that augmented features can be used to perform zero-shot domain\nadaptation for semantic segmentation. Experiments demonstrate that our method\nsignificantly outperforms CLIP-based style transfer baselines on several\ndatasets for the downstream task at hand. Our prompt-driven approach even\noutperforms one-shot unsupervised domain adaptation on some datasets, and gives\ncomparable results on others. Our code is available at\nhttps://github.com/astra-vision/PODA.\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Pérez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2212.03241v2.pdf","comment":"Project page: https://astra-vision.github.io/PODA/"},{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07321v1","updated":"2023-03-13T17:42:11Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07320v1","updated":"2023-03-13T17:41:57Z","published":"2023-03-13T17:41:57Z","title":"Model-tuning Via Prompts Makes NLP Models Adversarially Robust","summary":"  In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations, such\nas word-level synonym substitutions. In this work, we demonstrate surprising\ngains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an\nalternative method of adapting to downstream tasks. Rather than modifying the\nmodel (by appending an MLP head), MVP instead modifies the input (by appending\na prompt template). Across three classification datasets, MVP improves\nperformance against adversarial word-level synonym substitutions by an average\nof 8% over standard methods and even outperforms adversarial training-based\nstate-of-art defenses by 3.5%. By combining MVP with adversarial training, we\nachieve further improvements in robust accuracy while maintaining clean\naccuracy. Finally, we conduct ablations to investigate the mechanism underlying\nthese gains. Notably, we find that the main causes of vulnerability of MLP can\nbe attributed to the misalignment between pre-training and fine-tuning tasks,\nand the randomly initialized MLP parameters. Code is available at\nhttps://github.com/acmi-lab/mvp\n","authors":["Mrigank Raman","Pratyush Maini","J. Zico Kolter","Zachary C. Lipton","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2303.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07310v1","updated":"2023-03-13T17:32:46Z","published":"2023-03-13T17:32:46Z","title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph\n  Neural Networks","summary":"  Reduced-order models based on physics are a popular choice in cardiovascular\nmodeling due to their efficiency, but they may experience reduced accuracy when\nworking with anatomies that contain numerous junctions or pathological\nconditions. We develop one-dimensional reduced-order models that simulate blood\nflow dynamics using a graph neural network trained on three-dimensional\nhemodynamic simulation data. Given the initial condition of the system, the\nnetwork iteratively predicts the pressure and flow rate at the vessel\ncenterline nodes. Our numerical results demonstrate the accuracy and\ngeneralizability of our method in physiological geometries comprising a variety\nof anatomies and boundary conditions. Our findings demonstrate that our\napproach can achieve errors below 2% and 3% for pressure and flow rate,\nrespectively, provided there is adequate training data. As a result, our method\nexhibits superior performance compared to physics-based one-dimensional models,\nwhile maintaining high efficiency at inference time.\n","authors":["Luca Pegolotti","Martin R. Pfaller","Natalia L. Rubio","Ke Ding","Rita Brugarolas Brufau","Eric Darve","Alison L. Marsden"],"pdf_url":"https://arxiv.org/pdf/2303.07310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07308v1","updated":"2023-03-13T17:30:43Z","published":"2023-03-13T17:30:43Z","title":"NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial\n  Understanding with Objects","summary":"  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and\nillustrate how it supports object SLAM for consistent spatial understanding\nwith long-term scene changes. NeuSE is a set of latent object embeddings\ncreated from partial object observations. It serves as a compact point cloud\nsurrogate for complete object models, encoding full shape information while\ntransforming SE(3)-equivariantly in tandem with the object in the physical\nworld. With NeuSE, relative frame transforms can be directly derived from\ninferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape\nand pose characterization, can operate independently or in conjunction with\ntypical SLAM systems. It directly infers SE(3) camera pose constraints that are\ncompatible with general SLAM pose graph optimization, while also maintaining a\nlightweight object-centric map that adapts to real-world changes. Our approach\nis evaluated on synthetic and real-world sequences featuring changed objects\nand shows improved localization accuracy and change-aware mapping capability,\nwhen working either standalone or jointly with a common SLAM pipeline.\n","authors":["Jiahui Fu","Yilun Du","Kurran Singh","Joshua B. Tenenbaum","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2303.07308v1.pdf","comment":"Project webpage: https://neuse-slam.github.io/neuse/"},{"id":"http://arxiv.org/abs/2303.07305v1","updated":"2023-03-13T17:30:04Z","published":"2023-03-13T17:30:04Z","title":"Transformer Models for Acute Brain Dysfunction Prediction","summary":"  Acute brain dysfunctions (ABD), which include coma and delirium, are\nprevalent in the ICU, especially among older patients. The current approach in\nmanual assessment of ABD by care providers may be sporadic and subjective.\nHence, there exists a need for a data-driven robust system automating the\nassessment and prediction of ABD. In this work, we develop a machine learning\nsystem for real-time prediction of ADB using Electronic Health Record (HER)\ndata. Our data processing pipeline enables integration of static and temporal\ndata, and extraction of features relevant to ABD. We train several\nstate-of-the-art transformer models and baseline machine learning models\nincluding CatBoost and XGB on the data that was collected from patients\nadmitted to the ICU at UF Shands Hospital. We demonstrate the efficacy of our\nsystem for tasks related to acute brain dysfunction including binary\nclassification of brain acuity and multi-class classification (i.e., coma,\ndelirium, death, or normal), achieving a mean AUROC of 0.953 on our Long-former\nimplementation. Our system can then be deployed for real-time prediction of ADB\nin ICUs to reduce the number of incidents caused by ABD. Moreover, the\nreal-time system has the potential to reduce costs, duration of patients stays\nin the ICU, and mortality among those afflicted.\n","authors":["Brandon Silva","Miguel Contreras","Tezcan Ozrazgat Baslanti","Yuanfang Ren","Guan Ziyuan","Kia Khezeli","Azra Bihorac","Parisa Rashidi"],"pdf_url":"https://arxiv.org/pdf/2303.07305v1.pdf","comment":"15 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2202.04110v3","updated":"2023-03-13T17:20:47Z","published":"2022-02-08T19:27:48Z","title":"PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and\n  Loopy Belief Propagation in JAX","summary":"  PGMax is an open-source Python package for (a) easily specifying discrete\nProbabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically\nrunning efficient and scalable loopy belief propagation (LBP) in JAX. PGMax\nsupports general factor graphs with tractable factors, and leverages modern\naccelerators like GPUs for inference. Compared with existing alternatives,\nPGMax obtains higher-quality inference results with up to three\norders-of-magnitude inference time speedups. PGMax additionally interacts\nseamlessly with the rapidly growing JAX ecosystem, opening up new research\npossibilities. Our source code, examples and documentation are available at\nhttps://github.com/deepmind/PGMax.\n","authors":["Guangyao Zhou","Antoine Dedieu","Nishanth Kumar","Miguel Lázaro-Gredilla","Shrinu Kushagra","Dileep George"],"pdf_url":"https://arxiv.org/pdf/2202.04110v3.pdf","comment":"Update authors list"},{"id":"http://arxiv.org/abs/2303.07295v1","updated":"2023-03-13T17:17:11Z","published":"2023-03-13T17:17:11Z","title":"Meet in the Middle: A New Pre-training Paradigm","summary":"  Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.\n","authors":["Anh Nguyen","Nikos Karampatziakis","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07295v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.02886v2","updated":"2023-03-13T17:16:37Z","published":"2023-01-07T16:17:48Z","title":"Perceptual-Neural-Physical Sound Matching","summary":"  Sound matching algorithms seek to approximate a target waveform by parametric\naudio synthesis. Deep neural networks have achieved promising results in\nmatching sustained harmonic tones. However, the task is more challenging when\ntargets are nonstationary and inharmonic, e.g., percussion. We attribute this\nproblem to the inadequacy of loss function. On one hand, mean square error in\nthe parametric domain, known as \"P-loss\", is simple and fast but fails to\naccommodate the differing perceptual significance of each parameter. On the\nother hand, mean square error in the spectrotemporal domain, known as \"spectral\nloss\", is perceptually motivated and serves in differentiable digital signal\nprocessing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals\nand its gradient may be computationally expensive; hence a slow convergence.\nAgainst this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP\nis the optimal quadratic approximation of spectral loss while being as fast as\nP-loss during training. We instantiate PNP with physical modeling synthesis as\ndecoder and joint time-frequency scattering transform (JTFS) as spectral\nrepresentation. We demonstrate its potential on matching synthetic drum sounds\nin comparison with other loss functions.\n","authors":["Han Han","Vincent Lostanlen","Mathieu Lagrange"],"pdf_url":"https://arxiv.org/pdf/2301.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12276v2","updated":"2023-03-13T17:13:04Z","published":"2022-06-16T23:08:27Z","title":"Multi-Frequency Joint Community Detection and Phase Synchronization","summary":"  This paper studies the joint community detection and phase synchronization\nproblem on the stochastic block model with relative phase, where each node is\nassociated with an unknown phase angle. This problem, with a variety of\nreal-world applications, aims to recover the cluster structure and associated\nphase angles simultaneously. We show this problem exhibits a\n``multi-frequency'' structure by closely examining its maximum likelihood\nestimation (MLE) formulation, whereas existing methods are not originated from\nthis perspective. To this end, two simple yet efficient algorithms that\nleverage the MLE formulation and benefit from the information across multiple\nfrequencies are proposed. The former is a spectral method based on the novel\nmulti-frequency column-pivoted QR factorization. The factorization applied to\nthe top eigenvectors of the observation matrix provides key information about\nthe cluster structure and associated phase angles. The second approach is an\niterative multi-frequency generalized power method, where each iteration\nupdates the estimation in a matrix-multiplication-then-projection manner.\nNumerical experiments show that our proposed algorithms significantly improve\nthe ability of exactly recovering the cluster structure and the accuracy of the\nestimated phase angles, compared to state-of-the-art algorithms.\n","authors":["Lingda Wang","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2206.12276v2.pdf","comment":"Accepted by IEEE Transactions on Signal and Information Processing\n  over Networks"},{"id":"http://arxiv.org/abs/2303.07287v1","updated":"2023-03-13T17:03:19Z","published":"2023-03-13T17:03:19Z","title":"Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm","summary":"  In non-asymptotic statistical inferences, variance-type parameters of\nsub-Gaussian distributions play a crucial role. However, direct estimation of\nthese parameters based on the empirical moment generating function (MGF) is\ninfeasible. To this end, we recommend using a sub-Gaussian intrinsic moment\nnorm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series\nof normalized moments. Importantly, the recommended norm can not only recover\nthe exponential moment bounds for the corresponding MGFs, but also lead to\ntighter Hoeffding's sub-Gaussian concentration inequalities. In practice,\n{\\color{black} we propose an intuitive way of checking sub-Gaussian data with a\nfinite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be\nrobustly estimated via a simple plug-in approach. Our theoretical results are\napplied to non-asymptotic analysis, including the multi-armed bandit.\n","authors":["Huiming Zhang","Haoyu Wei","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.07287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07280v1","updated":"2023-03-13T16:54:11Z","published":"2023-03-13T16:54:11Z","title":"Vision-Language Models as Success Detectors","summary":"  Detecting successful behaviour is crucial for training intelligent agents. As\nsuch, generalisable reward models are a prerequisite for agents that can learn\nto generalise their behaviour. In this work we focus on developing robust\nsuccess detectors that leverage large, pretrained vision-language models\n(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we\ntreat success detection as a visual question answering (VQA) problem, denoted\nSuccessVQA. We study success detection across three vastly different domains:\n(i) interactive language-conditioned agents in a simulated household, (ii) real\nworld robotic manipulation, and (iii) \"in-the-wild\" human egocentric videos. We\ninvestigate the generalisation properties of a Flamingo-based success detection\nmodel across unseen language and visual changes in the first two domains, and\nfind that the proposed method is able to outperform bespoke reward models in\nout-of-distribution test scenarios with either variation. In the last domain of\n\"in-the-wild\" human videos, we show that success detection on unseen real\nvideos presents an even more challenging generalisation task warranting future\nwork. We hope our initial results encourage further work in real world success\ndetection and reward modelling.\n","authors":["Yuqing Du","Ksenia Konyushkova","Misha Denil","Akhil Raju","Jessica Landon","Felix Hill","Nando de Freitas","Serkan Cabi"],"pdf_url":"https://arxiv.org/pdf/2303.07280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10999v2","updated":"2023-03-13T16:51:03Z","published":"2022-11-20T15:27:55Z","title":"LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders","summary":"  Audio-visual speech enhancement aims to extract clean speech from a noisy\nenvironment by leveraging not only the audio itself but also the target\nspeaker's lip movements. This approach has been shown to yield improvements\nover audio-only speech enhancement, particularly for the removal of interfering\nspeech. Despite recent advances in speech synthesis, most audio-visual\napproaches continue to use spectral mapping/masking to reproduce the clean\naudio, often resulting in visual backbones added to existing speech enhancement\narchitectures. In this work, we propose LA-VocE, a new two-stage approach that\npredicts mel-spectrograms from noisy audio-visual speech via a\ntransformer-based architecture, and then converts them into waveform audio\nusing a neural vocoder (HiFi-GAN). We train and evaluate our framework on\nthousands of speakers and 11+ different languages, and study our model's\nability to adapt to different levels of background noise and speech\ninterference. Our experiments show that LA-VocE outperforms existing methods\naccording to multiple metrics, particularly under very noisy scenarios.\n","authors":["Rodrigo Mira","Buye Xu","Jacob Donley","Anurag Kumar","Stavros Petridis","Vamsi Krishna Ithapu","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2211.10999v2.pdf","comment":"accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07275v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"A Survey of Graph Prompting Methods: Techniques, Applications, and\n  Challenges","summary":"  While deep learning has achieved great success on various tasks, the\ntask-specific model training notoriously relies on a large volume of labeled\ndata. Recently, a new training paradigm of ``pre-train, prompt, predict'' has\nbeen proposed to improve model generalization ability with limited labeled\ndata. The main idea is that, based on a pre-trained model, the prompting\nfunction uses a template to augment input samples with indicative context and\nreformalizes the target task to one of the pre-training tasks. In this survey,\nwe provide a unique review of prompting methods from the graph perspective.\nGraph data has served as structured knowledge repositories in various systems\nby explicitly modeling the interaction between entities. Compared with\ntraditional methods, graph prompting functions could induce task-related\ncontext and apply templates with structured knowledge. The pre-trained model is\nthen adaptively generalized for future samples. In particular, we introduce the\nbasic concepts of graph prompt learning, organize the existing work of\ndesigning graph prompting functions, and describe their applications and\nchallenges to a variety of machine learning problems. This survey attempts to\nbridge the gap between structured graphs and prompt design to facilitate future\nmethodology development.\n","authors":["Xuansheng Wu","Kaixiong Zhou","Mingchen Sun","Xin Wang","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07275v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.07264v1","updated":"2023-03-13T16:44:15Z","published":"2023-03-13T16:44:15Z","title":"A Surface-normal Based Neural Framework for Colonoscopy Reconstruction","summary":"  Reconstructing a 3D surface from colonoscopy video is challenging due to\nillumination and reflectivity variation in the video frame that can cause\ndefective shape predictions. Aiming to overcome this challenge, we utilize the\ncharacteristics of surface normal vectors and develop a two-step neural\nframework that significantly improves the colonoscopy reconstruction quality.\nThe normal-based depth initialization network trained with self-supervised\nnormal consistency loss provides depth map initialization to the normal-depth\nrefinement module, which utilizes the relationship between illumination and\nsurface normals to refine the frame-wise normal and depth predictions\nrecursively. Our framework's depth accuracy performance on phantom colonoscopy\ndata demonstrates the value of exploiting the surface normals in colonoscopy\nreconstruction, especially on en face views. Due to its low depth error, the\nprediction result from our framework will require limited post-processing to be\nclinically applicable for real-time colonoscopy reconstruction.\n","authors":["Shuxian Wang","Yubo Zhang","Sarah K. McGill","Julian G. Rosenman","Jan-Michael Frahm","Soumyadip Sengupta","Stephen M. Pizer"],"pdf_url":"https://arxiv.org/pdf/2303.07264v1.pdf","comment":"Accepted at IPMI 2023; first two authors contributed equally"},{"id":"http://arxiv.org/abs/2201.09635v4","updated":"2023-03-13T16:36:11Z","published":"2022-01-24T12:30:38Z","title":"State-Conditioned Adversarial Subgoal Generation","summary":"  Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks\nby performing decision-making and control at successively higher levels of\ntemporal abstraction. However, off-policy HRL often suffers from the problem of\na non-stationary high-level policy since the low-level policy is constantly\nchanging. In this paper, we propose a novel HRL approach for mitigating the\nnon-stationarity by adversarially enforcing the high-level policy to generate\nsubgoals compatible with the current instantiation of the low-level policy. In\npractice, the adversarial learning is implemented by training a simple\nstate-conditioned discriminator network concurrently with the high-level policy\nwhich determines the compatibility level of subgoals. Comparison to\nstate-of-the-art algorithms shows that our approach improves both learning\nefficiency and performance in challenging continuous control tasks.\n","authors":["Vivienne Huiling Wang","Joni Pajarinen","Tinghuai Wang","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2201.09635v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07248v1","updated":"2023-03-13T16:22:16Z","published":"2023-03-13T16:22:16Z","title":"Channel Estimation for Underwater Visible Light Communication: A Sparse\n  Learning Perspective","summary":"  The underwater propagation environment for visible light signals is affected\nby complex factors such as absorption, shadowing, and reflection, making it\nvery challengeable to achieve effective underwater visible light communication\n(UVLC) channel estimation. It is difficult for the UVLC channel to be sparse\nrepresented in the time and frequency domains, which limits the chance of using\nsparse signal processing techniques to achieve better performance of channel\nestimation. To this end, a compressed sensing (CS) based framework is\nestablished in this paper by fully exploiting the sparsity of the underwater\nvisible light channel in the distance domain of the propagation links. In order\nto solve the sparse recovery problem and achieve more accurate UVLC channel\nestimation, a sparse learning based underwater visible light channel estimation\n(SL-UVCE) scheme is proposed. Specifically, a deep-unfolding neural network\nmimicking the classical iterative sparse recovery algorithm of approximate\nmessage passing (AMP) is employed, which decomposes the iterations of AMP into\na series of layers with different learnable parameters. Compared with the\nexisting non-CS-based and CS-based schemes, the proposed scheme shows better\nperformance of accuracy in channel estimation, especially in severe conditions\nsuch as insufficient measurement pilots and large number of multipath\ncomponents.\n","authors":["Younan Mou","Sicong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07248v1.pdf","comment":"This paper has been accepted by and is to appear in Proc. 2023 IEEE\n  International Conference on Communications (ICC)"},{"id":"http://arxiv.org/abs/2210.16192v2","updated":"2023-03-13T16:21:28Z","published":"2022-10-27T12:59:00Z","title":"Learning Audio Features with Metadata and Contrastive Learning","summary":"  Methods based on supervised learning using annotations in an end-to-end\nfashion have been the state-of-the-art for classification problems. However,\nthey may be limited in their generalization capability, especially in the low\ndata regime. In this study, we address this issue using supervised contrastive\nlearning combined with available metadata to solve multiple pretext tasks that\nlearn a good representation of data. We apply our approach on ICBHI, a\nrespiratory sound classification dataset suited for this setting. We show that\nlearning representations using only metadata, without class labels, obtains\nsimilar performance as using cross entropy with those labels only. In addition,\nwe obtain state-of-the-art score when combining class labels with metadata\nusing multiple supervised contrastive learning. This work suggests the\npotential of using multiple metadata sources in supervised contrastive\nsettings, in particular in settings with class imbalance and few data. Our code\nis released at https://github.com/ilyassmoummad/scl_icbhi2017\n","authors":["Ilyass Moummad","Nicolas Farrugia"],"pdf_url":"https://arxiv.org/pdf/2210.16192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05737v2","updated":"2023-03-13T16:19:43Z","published":"2023-03-10T06:46:23Z","title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition\n  Performance in Clinical Settings","summary":"  Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n","authors":["Joel Shor","Ruyue Agnes Bi","Subhashini Venugopalan","Steven Ibara","Roman Goldenberg","Ehud Rivlin"],"pdf_url":"https://arxiv.org/pdf/2303.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09435v4","updated":"2023-03-13T16:15:46Z","published":"2022-05-19T09:50:25Z","title":"Adversarial random forests for density estimation and generative\n  modeling","summary":"  We propose methods for density estimation and data synthesis using a novel\nform of unsupervised random forests. Inspired by generative adversarial\nnetworks, we implement a recursive procedure in which trees gradually learn\nstructural properties of the data through alternating rounds of generation and\ndiscrimination. The method is provably consistent under minimal assumptions.\nUnlike classic tree-based alternatives, our approach provides smooth\n(un)conditional densities and allows for fully synthetic data generation. We\nachieve comparable or superior performance to state-of-the-art probabilistic\ncircuits and deep learning models on various tabular data benchmarks while\nexecuting about two orders of magnitude faster on average. An accompanying\n$\\texttt{R}$ package, $\\texttt{arf}$, is available on $\\texttt{CRAN}$.\n","authors":["David S. Watson","Kristin Blesch","Jan Kapar","Marvin N. Wright"],"pdf_url":"https://arxiv.org/pdf/2205.09435v4.pdf","comment":"Camera ready version (AISTATS 2023)"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.10488v4","updated":"2023-03-13T16:05:11Z","published":"2022-10-19T11:53:13Z","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective","summary":"  Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2210.10488v4.pdf","comment":"Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023"},{"id":"http://arxiv.org/abs/2211.02641v2","updated":"2023-03-13T15:57:19Z","published":"2022-10-25T04:48:11Z","title":"Graph Neural Networks on SPD Manifolds for Motor Imagery Classification:\n  A Perspective from the Time-Frequency Analysis","summary":"  The classification of motor imagery (MI) is a highly sought-after research\ntopic in the field of Electroencephalography (EEG)-based brain-computer\ninterfaces (BCIs), with immense commercial value. Over the past two decades,\nthere has been a fundamental shift in the trend of MI-EEG classifiers,\nresulting in a gradual increase in their performance. The emergence of\nTensor-CSPNet, the first geometric deep learning (GDL) framework in BCI\nresearch, is attributed to the imperative of characterizing the non-Euclidean\nnature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based\nclassifier that capitalizes on the second-order statistics of EEGs. In contrast\nto the conventional approach of utilizing first-order statistics for EEG\nsignals, the utilization of these second-order statistics represents the\nclassical treatment. These statistics provide adequate discriminative\ninformation, rendering them suitable for MI-EEG classification. In this study,\nwe introduce another GDL classifier, called Graph-CSPNet, for MI-EEG\nclassification. Graph-CSPNet utilizes graph-based techniques to characterize\nEEG signals in both the time and frequency domains, realizing the fundamental\nperspective of time-frequency analysis. The architecture of Graph-CSPNet is\nfurther simplified, offering greater flexibility to cope with variable\ntime-frequency resolution for signal segmentation and capturing localized\nfluctuations. In contrast to Tensor-CSPNet, this approach enables Graph-CSPNet\nto achieve better results in MI-EEG classification. To evaluate the efficacy of\nGraph-CSPNet, we utilize five commonly-used publicly available MI-EEG datasets,\nand it produces near-optimal classification accuracies, winning nine out of\neleven subject-specific scenarios. The Python implementation of Graph-CSPNet is\navailable on a GitHub repository\nhttps://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet.\n","authors":["Ce Ju","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2211.02641v2.pdf","comment":"17 pages, 5 figures, 11 Tables; This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2208.14698v5","updated":"2023-03-13T15:57:00Z","published":"2022-08-31T08:47:02Z","title":"Bayesian Optimization-based Combinatorial Assignment","summary":"  We study the combinatorial assignment domain, which includes combinatorial\nauctions and course allocation. The main challenge in this domain is that the\nbundle space grows exponentially in the number of items. To address this,\nseveral papers have recently proposed machine learning-based preference\nelicitation algorithms that aim to elicit only the most important information\nfrom agents. However, the main shortcoming of this prior work is that it does\nnot model a mechanism's uncertainty over values for not yet elicited bundles.\nIn this paper, we address this shortcoming by presenting a Bayesian\noptimization-based combinatorial assignment (BOCA) mechanism. Our key technical\ncontribution is to integrate a method for capturing model uncertainty into an\niterative combinatorial auction mechanism. Concretely, we design a new method\nfor estimating an upper uncertainty bound that can be used to define an\nacquisition function to determine the next query to the agents. This enables\nthe mechanism to properly explore (and not just exploit) the bundle space\nduring its preference elicitation phase. We run computational experiments in\nseveral spectrum auction domains to evaluate BOCA's performance. Our results\nshow that BOCA achieves higher allocative efficiency than state-of-the-art\napproaches.\n","authors":["Jakob Weissteiner","Jakob Heiss","Julien Siems","Sven Seuken"],"pdf_url":"https://arxiv.org/pdf/2208.14698v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13939v2","updated":"2023-03-13T15:50:44Z","published":"2022-04-29T08:32:02Z","title":"Short-Term Density Forecasting of Low-Voltage Load using\n  Bernstein-Polynomial Normalizing Flows","summary":"  The transition to a fully renewable energy grid requires better forecasting\nof demand at the low-voltage level to increase efficiency and ensure reliable\ncontrol. However, high fluctuations and increasing electrification cause huge\nforecast variability, not reflected in traditional point estimates.\nProbabilistic load forecasts take future uncertainties into account and thus\nallow more informed decision-making for the planning and operation of\nlow-carbon energy systems. We propose an approach for flexible conditional\ndensity forecasting of short-term load based on Bernstein polynomial\nnormalizing flows, where a neural network controls the parameters of the flow.\nIn an empirical study with 363 smart meter customers, our density predictions\ncompare favorably against Gaussian and Gaussian mixture densities. Also, they\noutperform a non-parametric approach based on the pinball loss for 24h-ahead\nload forecasting for two different neural network architectures.\n","authors":["Marcel Arpogaus","Marcus Voss","Beate Sick","Mark Nigge-Uricher","Oliver Dürr"],"pdf_url":"https://arxiv.org/pdf/2204.13939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00047v2","updated":"2023-03-13T15:41:00Z","published":"2023-01-31T19:28:00Z","title":"Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture\n  Models","summary":"  This letter presents a continuous probabilistic modeling methodology for\nspatial point cloud data using finite Gaussian Mixture Models (GMMs) where the\nnumber of components are adapted based on the scene complexity. Few\nhierarchical and adaptive methods have been proposed to address the challenge\nof balancing model fidelity with size. Instead, state-of-the-art mapping\napproaches require tuning parameters for specific use cases, but do not\ngeneralize across diverse environments. To address this gap, we utilize a\nself-organizing principle from information-theoretic learning to automatically\nadapt the complexity of the GMM model based on the relevant information in the\nsensor data. The approach is evaluated against existing point cloud modeling\ntechniques on real-world data with varying degrees of scene complexity.\n","authors":["Kshitij Goel","Nathan Michael","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2302.00047v2.pdf","comment":"8 pages, 6 figures, to appear in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2302.14853v2","updated":"2023-03-13T15:37:44Z","published":"2023-02-28T18:51:55Z","title":"An Efficient Tester-Learner for Halfspaces","summary":"  We give the first efficient algorithm for learning halfspaces in the testable\nlearning model recently defined by Rubinfeld and Vasilyan (2023). In this\nmodel, a learner certifies that the accuracy of its output hypothesis is near\noptimal whenever the training set passes an associated test, and training sets\ndrawn from some target distribution -- e.g., the Gaussian -- must pass the\ntest. This model is more challenging than distribution-specific agnostic or\nMassart noise models where the learner is allowed to fail arbitrarily if the\ndistributional assumption does not hold.\n  We consider the setting where the target distribution is Gaussian (or more\ngenerally any strongly log-concave distribution) in $d$ dimensions and the\nnoise model is either Massart or adversarial (agnostic). For Massart noise, our\ntester-learner runs in polynomial time and outputs a hypothesis with\n(information-theoretically optimal) error $\\mathsf{opt} + \\epsilon$ for any\nstrongly log-concave target distribution. For adversarial noise, our\ntester-learner obtains error $O(\\mathsf{opt}) + \\epsilon$ in polynomial time\nwhen the target distribution is Gaussian; for strongly log-concave\ndistributions, we obtain $\\tilde{O}(\\mathsf{opt}) + \\epsilon$ in\nquasipolynomial time.\n  Prior work on testable learning ignores the labels in the training set and\nchecks that the empirical moments of the covariates are close to the moments of\nthe base distribution. Here we develop new tests of independent interest that\nmake critical use of the labels and combine them with the moment-matching\napproach of Gollakota et al. (2023). This enables us to simulate a variant of\nthe algorithm of Diakonikolas et al. (2020) for learning noisy halfspaces using\nnonconvex SGD but in the testable learning setting.\n","authors":["Aravind Gollakota","Adam R. Klivans","Konstantinos Stavropoulos","Arsen Vasilyan"],"pdf_url":"https://arxiv.org/pdf/2302.14853v2.pdf","comment":"26 pages, 3 figures, Version v2: strengthened the agnostic guarantee"},{"id":"http://arxiv.org/abs/2303.07189v1","updated":"2023-03-13T15:30:28Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death\nworldwide, yet early detection and treatment can prevent the progression of the\ndisease. In contrast to the conventional method of detecting COPD with\nspirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a\nmeasure of morphological changes in the lung. It has been shown that automated\ndetection of COPD can be performed with deep learning models. However, the\npotential of incorporating optimal window setting selection, typically carried\nout by clinicians during examination of CT scans for COPD, is generally\noverlooked in deep learning approaches. We aim to optimize the binary\nclassification of COPD with densely connected convolutional neural networks\n(DenseNets) through implementation of manual and automated Window-Setting\nOptimization (WSO) steps. Our dataset consisted of 78 CT scans from the\nKlinikum rechts der Isar research hospital. Repeated inference on the test set\nshowed that without WSO, the plain DenseNet resulted in a mean slice-level AUC\nof 0.80$\\pm$0.05. With input images manually adjusted to the emphysema window\nsetting, the plain DenseNet model predicted COPD with a mean AUC of\n0.86$\\pm$0.04. By automating the WSO through addition of a customized layer to\nthe DenseNet, an optimal window setting in the proximity of the emphysema\nwindow setting was learned and a mean AUC of 0.82$\\pm$0.04 was achieved.\nDetection of COPD with DenseNet models was optimized by WSO of CT data to the\nemphysema window setting range, demonstrating the importance of implementing\noptimal window setting selection in the deep learning pipeline.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Johannes Thalhammer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07184v1","updated":"2023-03-13T15:27:07Z","published":"2023-03-13T15:27:07Z","title":"Traffic Prediction with Transfer Learning: A Mutual Information-based\n  Approach","summary":"  In modern traffic management, one of the most essential yet challenging tasks\nis accurately and timely predicting traffic. It has been well investigated and\nexamined that deep learning-based Spatio-temporal models have an edge when\nexploiting Spatio-temporal relationships in traffic data. Typically,\ndata-driven models require vast volumes of data, but gathering data in small\ncities can be difficult owing to constraints such as equipment deployment and\nmaintenance costs. To resolve this problem, we propose TrafficTL, a cross-city\ntraffic prediction approach that uses big data from other cities to aid\ndata-scarce cities in traffic prediction. Utilizing a periodicity-based\ntransfer paradigm, it identifies data similarity and reduces negative transfer\ncaused by the disparity between two data distributions from distant cities. In\naddition, the suggested method employs graph reconstruction techniques to\nrectify defects in data from small data cities. TrafficTL is evaluated by\ncomprehensive case studies on three real-world datasets and outperforms the\nstate-of-the-art baseline by around 8 to 25 percent.\n","authors":["Yunjie Huang","Xiaozhuang Song","Yuanshao Zhu","Shiyao Zhang","James J. Q. Yu"],"pdf_url":"https://arxiv.org/pdf/2303.07184v1.pdf","comment":"submited to T-ITS, 16 pages, 13 figures in color"},{"id":"http://arxiv.org/abs/2303.07180v1","updated":"2023-03-13T15:22:50Z","published":"2023-03-13T15:22:50Z","title":"Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-\n  and Category-Aware Transformers","summary":"  As we all know, multi-view data is more expressive than single-view data and\nmulti-label annotation enjoys richer supervision information than single-label,\nwhich makes multi-view multi-label learning widely applicable for various\npattern recognition tasks. In this complex representation learning problem,\nthree main challenges can be characterized as follows: i) How to learn\nconsistent representations of samples across all views? ii) How to exploit and\nutilize category correlations of multi-label to guide inference? iii) How to\navoid the negative impact resulting from the incompleteness of views or labels?\nTo cope with these problems, we propose a general multi-view multi-label\nlearning framework named label-guided masked view- and category-aware\ntransformers in this paper. First, we design two transformer-style based\nmodules for cross-view features aggregation and multi-label classification,\nrespectively. The former aggregates information from different views in the\nprocess of extracting view-specific features, and the latter learns subcategory\nembedding to improve classification performance. Second, considering the\nimbalance of expressive power among views, an adaptively weighted view fusion\nmodule is proposed to obtain view-consistent embedding features. Third, we\nimpose a label manifold constraint in sample-level representation learning to\nmaximize the utilization of supervised information. Last but not least, all the\nmodules are designed under the premise of incomplete views and labels, which\nmakes our method adaptable to arbitrary multi-view and multi-label data.\nExtensive experiments on five datasets confirm that our method has clear\nadvantages over other state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Xiaoling Luo","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07180v1.pdf","comment":"Accepted to AAAI-23"},{"id":"http://arxiv.org/abs/2303.07172v1","updated":"2023-03-13T15:14:26Z","published":"2023-03-13T15:14:26Z","title":"Evaluating Visual Number Discrimination in Deep Neural Networks","summary":"  The ability to discriminate between large and small quantities is a core\naspect of basic numerical competence in both humans and animals. In this work,\nwe examine the extent to which the state-of-the-art neural networks designed\nfor vision exhibit this basic ability. Motivated by studies in animal and\ninfant numerical cognition, we use the numerical bisection procedure to test\nnumber discrimination in different families of neural architectures. Our\nresults suggest that vision-specific inductive biases are helpful in numerosity\ndiscrimination, as models with such biases have lowest test errors on the task,\nand often have psychometric curves that qualitatively resemble those of humans\nand animals performing the task. However, even the strongest models, as\nmeasured on standard metrics of performance, fail to discriminate quantities in\ntransfer experiments with differing training and testing conditions, indicating\nthat such inductive biases might not be sufficient.\n","authors":["Ivana Kajić","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2303.07172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03984v2","updated":"2023-03-13T15:11:48Z","published":"2023-03-07T15:33:12Z","title":"Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax\n  Optimization","summary":"  In the paper, we study a class of nonconvex nonconcave minimax optimization\nproblems (i.e., $\\min_x\\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in\n$x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition\nin $y$. Moreover, we propose a class of enhanced momentum-based gradient\ndescent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic\nNonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use\nvarious adaptive learning rates in updating the variables $x$ and $y$ without\nrelying on any global and coordinate-wise adaptive learning rates.\nTheoretically, we present an effective convergence analysis framework for our\nmethods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the\nbest known sample (gradient) complexity of $O(\\epsilon^{-3})$ only requiring\none sample at each loop in finding an $\\epsilon$-stationary solution (i.e.,\n$\\mathbb{E}\\|\\nabla F(x)\\|\\leq \\epsilon$, where $F(x)=\\max_y f(x,y)$). This\nmanuscript commemorates the mathematician Boris Polyak (1935-2023).\n","authors":["Feihu Huang"],"pdf_url":"https://arxiv.org/pdf/2303.03984v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2303.07166v1","updated":"2023-03-13T15:09:52Z","published":"2023-03-13T15:09:52Z","title":"Improved Tree Search for Automatic Program Synthesis","summary":"  In the task of automatic program synthesis, one obtains pairs of matching\ninputs and outputs and generates a computer program, in a particular\ndomain-specific language (DSL), which given each sample input returns the\nmatching output. A key element is being able to perform an efficient search in\nthe space of valid programs. Here, we suggest a variant of MCTS that leads to\nstate of the art results on two vastly different DSLs. The exploration method\nwe propose includes multiple contributions: a modified visit count, a\npreprocessing procedure for the training dataset, and encoding the part of the\nprogram that was already executed.\n","authors":["Aran Carmon","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2303.07166v1.pdf","comment":"Proceedings of the 2nd Exploration in Reinforcement Learning Workshop\n  at the 36th International Conference on Machine Learning, 2019"},{"id":"http://arxiv.org/abs/2303.07160v1","updated":"2023-03-13T14:35:55Z","published":"2023-03-13T14:35:55Z","title":"Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond","summary":"  We study convergence lower bounds of without-replacement stochastic gradient\ndescent (SGD) for solving smooth (strongly-)convex finite-sum minimization\nproblems. Unlike most existing results focusing on final iterate lower bounds\nin terms of the number of components $n$ and the number of epochs $K$, we seek\nbounds for arbitrary weighted average iterates that are tight in all factors\nincluding the condition number $\\kappa$. For SGD with Random Reshuffling, we\npresent lower bounds that have tighter $\\kappa$ dependencies than existing\nbounds. Our results are the first to perfectly close the gap between lower and\nupper bounds for weighted average iterates in both strongly-convex and convex\ncases. We also prove weighted average iterate lower bounds for arbitrary\npermutation-based SGD, which apply to all variants that carefully choose the\nbest permutation. Our bounds improve the existing bounds in factors of $n$ and\n$\\kappa$ and thereby match the upper bounds shown for a recently proposed\nalgorithm called GraB.\n","authors":["Jaeyoung Cha","Jaewook Lee","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2303.07160v1.pdf","comment":"62 pages"},{"id":"http://arxiv.org/abs/2303.07154v1","updated":"2023-03-13T14:28:21Z","published":"2023-03-13T14:28:21Z","title":"Differential Good Arm Identification","summary":"  This paper targets a variant of the stochastic multi-armed bandit problem\ncalled good arm identification (GAI). GAI is a pure-exploration bandit problem\nwith the goal to output as many good arms using as few samples as possible,\nwhere a good arm is defined as an arm whose expected reward is greater than a\ngiven threshold. In this work, we propose DGAI - a differentiable good arm\nidentification algorithm to improve the sample complexity of the\nstate-of-the-art HDoC algorithm in a data-driven fashion. We also showed that\nthe DGAI can further boost the performance of a general multi-arm bandit (MAB)\nproblem given a threshold as a prior knowledge to the arm set. Extensive\nexperiments confirm that our algorithm outperform the baseline algorithms\nsignificantly in both synthetic and real world datasets for both GAI and MAB\ntasks.\n","authors":["Yun-Da Tsai","Tzu-Hsien Tsai","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2303.07154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07153v1","updated":"2023-03-13T14:27:34Z","published":"2023-03-13T14:27:34Z","title":"SA-CNN: Application to text categorization issues using simulated\n  annealing-based convolutional neural network optimization","summary":"  Convolutional neural networks (CNNs) are a representative class of deep\nlearning algorithms including convolutional computation that perform\ntranslation-invariant classification of input data based on their hierarchical\narchitecture. However, classical convolutional neural network learning methods\nuse the steepest descent algorithm for training, and the learning performance\nis greatly influenced by the initial weight settings of the convolutional and\nfully connected layers, requiring re-tuning to achieve better performance under\ndifferent model structures and data. Combining the strengths of the simulated\nannealing algorithm in global search, we propose applying it to the\nhyperparameter search process in order to increase the effectiveness of\nconvolutional neural networks (CNNs). In this paper, we introduce SA-CNN neural\nnetworks for text classification tasks based on Text-CNN neural networks and\nimplement the simulated annealing algorithm for hyperparameter search.\nExperiments demonstrate that we can achieve greater classification accuracy\nthan earlier models with manual tuning, and the improvement in time and space\nfor exploration relative to human tuning is substantial.\n","authors":["Zihao Guo","Yueying Cao"],"pdf_url":"https://arxiv.org/pdf/2303.07153v1.pdf","comment":"ACM EITCE-2022"},{"id":"http://arxiv.org/abs/2303.07152v1","updated":"2023-03-13T14:26:27Z","published":"2023-03-13T14:26:27Z","title":"Score Attack: A Lower Bound Technique for Optimal Differentially Private\n  Learning","summary":"  Achieving optimal statistical performance while ensuring the privacy of\npersonal data is a challenging yet crucial objective in modern data analysis.\nHowever, characterizing the optimality, particularly the minimax lower bound,\nunder privacy constraints is technically difficult.\n  To address this issue, we propose a novel approach called the score attack,\nwhich provides a lower bound on the differential-privacy-constrained minimax\nrisk of parameter estimation. The score attack method is based on the tracing\nattack concept in differential privacy and can be applied to any statistical\nmodel with a well-defined score statistic. It can optimally lower bound the\nminimax risk of estimating unknown model parameters, up to a logarithmic\nfactor, while ensuring differential privacy for a range of statistical\nproblems. We demonstrate the effectiveness and optimality of this general\nmethod in various examples, such as the generalized linear model in both\nclassical and high-dimensional sparse settings, the Bradley-Terry-Luce model\nfor pairwise comparisons, and nonparametric regression over the Sobolev class.\n","authors":["T. Tony Cai","Yichen Wang","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07152v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2011.03900"},{"id":"http://arxiv.org/abs/2303.07150v1","updated":"2023-03-13T14:23:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07143v1","updated":"2023-03-13T14:11:34Z","published":"2023-03-13T14:11:34Z","title":"Multi-Microphone Speaker Separation by Spatial Regions","summary":"  We consider the task of region-based source separation of reverberant\nmulti-microphone recordings. We assume pre-defined spatial regions with a\nsingle active source per region. The objective is to estimate the signals from\nthe individual spatial regions as captured by a reference microphone while\nretaining a correspondence between signals and spatial regions. We propose a\ndata-driven approach using a modified version of a state-of-the-art network,\nwhere different layers model spatial and spectro-temporal information. The\nnetwork is trained to enforce a fixed mapping of regions to network outputs.\nUsing speech from LibriMix, we construct a data set specifically designed to\ncontain the region information. Additionally, we train the network with\npermutation invariant training. We show that both training methods result in a\nfixed mapping of regions to network outputs, achieve comparable performance,\nand that the networks exploit spatial information. The proposed network\noutperforms a baseline network by 1.5 dB in scale-invariant\nsignal-to-distortion ratio.\n","authors":["Julian Wechsler","Srikanth Raj Chetupalli","Wolfgang Mack","Emanuël A. P. Habets"],"pdf_url":"https://arxiv.org/pdf/2303.07143v1.pdf","comment":"Submitted to the 2023 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing"},{"id":"http://arxiv.org/abs/2204.03471v4","updated":"2023-03-13T14:05:43Z","published":"2022-04-07T14:39:38Z","title":"DynLight: Realize dynamic phase duration with multi-level traffic signal\n  control","summary":"  We would like to withdraw this article for the following reasons: 1 this\narticle is not satisfactory for limited language and theoretical description; 2\nwe have enriched and revised this article with the help of other authors; 3 we\nmust update the author contribution information.\n","authors":["Liang Zhang","Shubin Xie","Jianming Deng"],"pdf_url":"https://arxiv.org/pdf/2204.03471v4.pdf","comment":"We would like to withdraw this article for the following reasons: 1\n  this article is not satisfactory for limited language and theoretical\n  description; 2 we have enriched and revised this article with the help of\n  other authors; 3 we must update the author contribution information. PLease\n  see: arXiv:2211.01025"},{"id":"http://arxiv.org/abs/2303.07139v1","updated":"2023-03-13T14:05:19Z","published":"2023-03-13T14:05:19Z","title":"Comparing statistical and machine learning methods for time series\n  forecasting in data-driven logistics -- A simulation study","summary":"  Many planning and decision activities in logistics and supply chain\nmanagement are based on forecasts of multiple time dependent factors.\nTherefore, the quality of planning depends on the quality of the forecasts. We\ncompare various forecasting methods in terms of out of the box forecasting\nperformance on a broad set of simulated time series. We simulate various linear\nand non-linear time series and look at the one step forecast performance of\nstatistical learning methods.\n","authors":["Lena Schmid","Moritz Roidl","Markus Pauly"],"pdf_url":"https://arxiv.org/pdf/2303.07139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07138v1","updated":"2023-03-13T14:05:18Z","published":"2023-03-13T14:05:18Z","title":"Transferable Deep Learning Power System Short-Term Voltage Stability\n  Assessment with Physics-Informed Topological Feature Engineering","summary":"  Deep learning (DL) algorithms have been widely applied to short-term voltage\nstability (STVS) assessment in power systems. However, transferring the\nknowledge learned in one power grid to other power grids with topology changes\nis still a challenging task. This paper proposed a transferable DL-based model\nfor STVS assessment by constructing the topology-aware voltage dynamic features\nfrom raw PMU data. Since the reactive power flow and grid topology are\nessential to voltage stability, the topology-aware and physics-informed voltage\ndynamic features are utilized to effectively represent the topological and\ntemporal patterns from post-disturbance system dynamic trajectories. The\nproposed DL-based STVS assessment model is tested under random operating\nconditions on the New England 39-bus system. It has 99.99\\% classification\naccuracy of the short-term voltage stability status using the topology-aware\nand physics-informed voltage dynamic features. In addition to high accuracy,\nthe experiments show good adaptability to PMU errors. Moreover, The proposed\nSTVS assessment method has outstanding performance on new grid topologies after\nfine-tuning. In particular, the highest accuracy reaches 99.68\\% in evaluation,\nwhich demonstrates a good knowledge transfer ability of the proposed model for\npower grid topology change.\n","authors":["Zijian Feng","Xin Chen","Zijian Lv","Peiyuan Sun","Kai Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07138v1.pdf","comment":"This work has been submitted to the IEEE Transactions on Power\n  Systems for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2303.07131v1","updated":"2023-03-13T14:01:37Z","published":"2023-03-13T14:01:37Z","title":"Evolutionary quantum feature selection","summary":"  Effective feature selection is essential for enhancing the performance of\nartificial intelligence models. It involves identifying feature combinations\nthat optimize a given metric, but this is a challenging task due to the\nproblem's exponential time complexity. In this study, we present an innovative\nheuristic called Evolutionary Quantum Feature Selection (EQFS) that employs the\nQuantum Circuit Evolution (QCE) algorithm. Our approach harnesses the unique\ncapabilities of QCE, which utilizes shallow depth circuits to generate sparse\nprobability distributions. Our computational experiments demonstrate that EQFS\ncan identify good feature combinations with quadratic scaling in the number of\nfeatures. To evaluate EQFS's performance, we counted the number of times a\ngiven classical model assesses the cost function for a specific metric, as a\nfunction of the number of generations.\n","authors":["Anton S. Albino","Otto M. Pires","Mauro Q. Nooblath","Erick G. S. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2303.07131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07130v1","updated":"2023-03-13T13:59:47Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) of COVID-19 can be categorized based on the extent of\nlung involvement observed on a CT scan. This paper proposes a domain\nknowledge-based pipeline to extract the infection regions using diverse\nimage-processing algorithms and a pre-trained UNET model. An ensemble of three\nmachine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),\nand Support Vector Machine (SVM), is employed to classify the CT scans into\ndifferent severity classes. The proposed system achieved a macro F1 score of\n57.47% on the validation dataset in the AI-Enabled Medical Image Analysis\nWorkshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07129v1","updated":"2023-03-13T13:59:20Z","published":"2023-03-13T13:59:20Z","title":"AdaptiveNet: Post-deployment Neural Architecture Adaptation for Diverse\n  Edge Environments","summary":"  Deep learning models are increasingly deployed to edge devices for real-time\napplications. To ensure stable service quality across diverse edge\nenvironments, it is highly desirable to generate tailored model architectures\nfor different conditions. However, conventional pre-deployment model generation\napproaches are not satisfactory due to the difficulty of handling the diversity\nof edge environments and the demand for edge information. In this paper, we\npropose to adapt the model architecture after deployment in the target\nenvironment, where the model quality can be precisely measured and private edge\ndata can be retained. To achieve efficient and effective edge model generation,\nwe introduce a pretraining-assisted on-cloud model elastification method and an\nedge-friendly on-device architecture search method. Model elastification\ngenerates a high-quality search space of model architectures with the guidance\nof a developer-specified oracle model. Each subnet in the space is a valid\nmodel with different environment affinity, and each device efficiently finds\nand maintains the most suitable subnet based on a series of edge-tailored\noptimizations. Extensive experiments on various edge devices demonstrate that\nour approach is able to achieve significantly better accuracy-latency tradeoffs\n(e.g. 46.74\\% higher on average accuracy with a 60\\% latency budget) than\nstrong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes\non the edge server).\n","authors":["Hao Wen","Yuanchun Li","Zunshuai Zhang","Shiqi Jiang","Xiaozhou Ye","Ye Ouyang","Ya-Qin Zhang","Yunxin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07127v1","updated":"2023-03-13T13:58:03Z","published":"2023-03-13T13:58:03Z","title":"Improving physics-informed neural networks with meta-learned\n  optimization","summary":"  We show that the error achievable using physics-informed neural networks for\nsolving systems of differential equations can be substantially reduced when\nthese networks are trained using meta-learned optimization methods rather than\nto using fixed, hand-crafted optimizers as traditionally done. We choose a\nlearnable optimization method based on a shallow multi-layer perceptron that is\nmeta-trained for specific classes of differential equations. We illustrate\nmeta-trained optimizers for several equations of practical relevance in\nmathematical physics, including the linear advection equation, Poisson's\nequation, the Korteweg--de Vries equation and Burgers' equation. We also\nillustrate that meta-learned optimizers exhibit transfer learning abilities, in\nthat a meta-trained optimizer on one differential equation can also be\nsuccessfully deployed on another differential equation.\n","authors":["Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2303.07127v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2210.04087v2","updated":"2023-03-13T13:56:31Z","published":"2022-10-08T18:49:58Z","title":"Symmetry Defense Against CNN Adversarial Perturbation Attacks","summary":"  Convolutional neural network classifiers (CNNs) are susceptible to\nadversarial attacks that perturb original samples to fool classifiers such as\nan autonomous vehicle's road sign image classifier. CNNs also lack invariance\nin the classification of symmetric samples because CNNs can classify symmetric\nsamples differently. Considered together, the CNN lack of adversarial\nrobustness and the CNN lack of invariance mean that the classification of\nsymmetric adversarial samples can differ from their incorrect classification.\nCould symmetric adversarial samples revert to their correct classification?\nThis paper answers this question by designing a symmetry defense that inverts\nor horizontally flips adversarial samples before classification against\nadversaries unaware of the defense. Against adversaries aware of the defense,\nthe defense devises a Klein four symmetry subgroup that includes the horizontal\nflip and pixel inversion symmetries. The symmetry defense uses the subgroup\nsymmetries in accuracy evaluation and the subgroup closure property to confine\nthe transformations that an adaptive adversary can apply before or after\ngenerating the adversarial sample. Without changing the preprocessing,\nparameters, or model, the proposed symmetry defense counters the Projected\nGradient Descent (PGD) and AutoAttack attacks with near-default accuracies for\nImageNet. Without using attack knowledge or adversarial samples, the proposed\ndefense exceeds the current best defense, which trains on adversarial samples.\nThe defense maintains and even improves the classification accuracy of\nnon-adversarial samples.\n","authors":["Blerta Lindqvist"],"pdf_url":"https://arxiv.org/pdf/2210.04087v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07125v1","updated":"2023-03-13T13:56:20Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\n\\url{https://github.com/ai-med/PANIC}.\n","authors":["Tom Nuno Wolf","Sebastian Pölster","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07123v1","updated":"2023-03-13T13:56:11Z","published":"2023-03-13T13:56:11Z","title":"Modality-Agnostic Debiasing for Single Domain Generalization","summary":"  Deep neural networks (DNNs) usually fail to generalize well to outside of\ndistribution (OOD) data, especially in the extreme case of single domain\ngeneralization (single-DG) that transfers DNNs from single domain to multiple\nunseen domains. Existing single-DG techniques commonly devise various\ndata-augmentation algorithms, and remould the multi-source domain\ngeneralization methodology to learn domain-generalized (semantic) features.\nNevertheless, these methods are typically modality-specific, thereby being only\napplicable to one single modality (e.g., image). In contrast, we target a\nversatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that\nenables generalization for different modalities. Technically, MAD introduces a\nnovel two-branch classifier: a biased-branch encourages the classifier to\nidentify the domain-specific (superficial) features, and a general-branch\ncaptures domain-generalized features based on the knowledge from biased-branch.\nOur MAD is appealing in view that it is pluggable to most single-DG models. We\nvalidate the superiority of our MAD in a variety of single-DG scenarios with\ndifferent modalities, including recognition on 1D texts, 2D images, 3D point\nclouds, and semantic segmentation on 2D images. More remarkably, for\nrecognition on 3D point clouds and semantic segmentation on 2D images, MAD\nimproves DSU by 2.82\\% and 1.5\\% in accuracy and mIOU.\n","authors":["Sanqing Qu","Yingwei Pan","Guang Chen","Ting Yao","Changjun Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2303.07123v1.pdf","comment":"To appear in CVPR-2023"},{"id":"http://arxiv.org/abs/2303.07110v1","updated":"2023-03-13T13:44:04Z","published":"2023-03-13T13:44:04Z","title":"Upcycling Models under Domain and Category Shift","summary":"  Deep neural networks (DNNs) often perform poorly in the presence of domain\nshift and category shift. How to upcycle DNNs and adapt them to the target task\nremains an important open problem. Unsupervised Domain Adaptation (UDA),\nespecially recently proposed Source-free Domain Adaptation (SFDA), has become a\npromising technology to address this issue. Nevertheless, existing SFDA methods\nrequire that the source domain and target domain share the same label space,\nconsequently being only applicable to the vanilla closed-set setting. In this\npaper, we take one step further and explore the Source-free Universal Domain\nAdaptation (SF-UniDA). The goal is to identify \"known\" data samples under both\ndomain and category shift, and reject those \"unknown\" data samples (not present\nin source classes), with only the knowledge from standard pre-trained source\nmodel. To this end, we introduce an innovative global and local clustering\nlearning technique (GLC). Specifically, we design a novel, adaptive one-vs-all\nglobal clustering algorithm to achieve the distinction across different target\nclasses and introduce a local k-NN clustering strategy to alleviate negative\ntransfer. We examine the superiority of our GLC on multiple benchmarks with\ndifferent category shift scenarios, including partial-set, open-set, and\nopen-partial-set DA. Remarkably, in the most challenging open-partial-set DA\nscenario, GLC outperforms UMAD by 14.8\\% on the VisDA benchmark. The code is\navailable at https://github.com/ispc-lab/GLC.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Roehrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07110v1.pdf","comment":"To appear in CVPR 2023. The code has been made public"},{"id":"http://arxiv.org/abs/2303.07109v1","updated":"2023-03-13T13:43:59Z","published":"2023-03-13T13:43:59Z","title":"Transformer-based World Models Are Happy With 100k Interactions","summary":"  Deep neural networks have been successful in many reinforcement learning\nsettings. However, compared to human learners they are overly data hungry. To\nbuild a sample-efficient world model, we apply a transformer to real-world\nepisodes in an autoregressive manner: not only the compact latent states and\nthe taken actions but also the experienced or predicted rewards are fed into\nthe transformer, so that it can attend flexibly to all three modalities at\ndifferent time steps. The transformer allows our world model to access previous\nstates directly, instead of viewing them through a compressed recurrent state.\nBy utilizing the Transformer-XL architecture, it is able to learn long-term\ndependencies while staying computationally efficient. Our transformer-based\nworld model (TWM) generates meaningful, new experience, which is used to train\na policy that outperforms previous model-free and model-based reinforcement\nlearning algorithms on the Atari 100k benchmark.\n","authors":["Jan Robine","Marc Höftmann","Tobias Uelwer","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2303.07109v1.pdf","comment":"Published as a conference paper at ICLR 2023. Code is available at\n  https://github.com/jrobine/twm"},{"id":"http://arxiv.org/abs/2303.03634v2","updated":"2023-03-13T13:33:15Z","published":"2023-03-07T03:46:53Z","title":"PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation","summary":"  Fall accidents are critical issues in an aging and aged society. Recently,\nmany researchers developed pre-impact fall detection systems using deep\nlearning to support wearable-based fall protection systems for preventing\nsevere injuries. However, most works only employed simple neural network models\ninstead of complex models considering the usability in resource-constrained\nmobile devices and strict latency requirements. In this work, we propose a\nnovel pre-impact fall detection via CNN-ViT knowledge distillation, namely\nPreFallKD, to strike a balance between detection performance and computational\ncomplexity. The proposed PreFallKD transfers the detection knowledge from the\npre-trained teacher model (vision transformer) to the student model\n(lightweight convolutional neural networks). Additionally, we apply data\naugmentation techniques to tackle issues of data imbalance. We conduct the\nexperiment on the KFall public dataset and compare PreFallKD with other\nstate-of-the-art models. The experiment results show that PreFallKD could boost\nthe student model during the testing phase and achieves reliable F1-score\n(92.66%) and lead time (551.3 ms).\n","authors":["Tin-Han Chi","Kai-Chun Liu","Chia-Yeh Hsieh","Yu Tsao","Chia-Tai Chan"],"pdf_url":"https://arxiv.org/pdf/2303.03634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07182v6","updated":"2023-03-13T13:27:02Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk Pflüger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v6.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2209.11173v3","updated":"2023-03-13T13:13:10Z","published":"2022-09-19T15:56:08Z","title":"U-Sleep's resilience to AASM guidelines","summary":"  AASM guidelines are the result of decades of efforts aiming at standardizing\nsleep scoring procedure, with the final goal of sharing a worldwide common\nmethodology. The guidelines cover several aspects from the technical/digital\nspecifications,e.g., recommended EEG derivations, to detailed sleep scoring\nrules accordingly to age. Automated sleep scoring systems have always largely\nexploited the standards as fundamental guidelines. In this context, deep\nlearning has demonstrated better performance compared to classical machine\nlearning. Our present work shows that a deep learning based sleep scoring\nalgorithm may not need to fully exploit the clinical knowledge or to strictly\nadhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a\nstate-of-the-art sleep scoring algorithm, can be strong enough to solve the\nscoring task even using clinically non-recommended or non-conventional\nderivations, and with no need to exploit information about the chronological\nage of the subjects. We finally strengthen a well-known finding that using data\nfrom multiple data centers always results in a better performing model compared\nwith training on a single cohort. Indeed, we show that this latter statement is\nstill valid even by increasing the size and the heterogeneity of the single\ndata cohort. In all our experiments we used 28528 polysomnography studies from\n13 different clinical studies.\n","authors":["Luigi Fiorillo","Giuliana Monachino","Julia van der Meer","Marco Pesce","Jan D. Warncke","Markus H. Schmidt","Claudio L. A. Bassetti","Athina Tzovara","Paolo Favaro","Francesca D. Faraci"],"pdf_url":"https://arxiv.org/pdf/2209.11173v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00815v3","updated":"2023-03-13T12:48:19Z","published":"2023-01-01T12:48:12Z","title":"A attention way in Explainable methods for infant brain","summary":"  Deploying reliable deep learning techniques in interdisciplinary applications\nneeds learned models to output accurate and ({even more importantly})\nexplainable predictions. Existing approaches typically explicate network\noutputs in a post-hoc fashion, under an implicit assumption that faithful\nexplanations come from accurate predictions/classifications. We have an\nopposite claim that explanations boost (or even determine) classification. That\nis, end-to-end learning of explanation factors to augment discriminative\nrepresentation extraction could be a more intuitive strategy to inversely\nassure fine-grained explainability, e.g., in those neuroimaging and\nneuroscience studies with high-dimensional data containing noisy, redundant,\nand task-irrelevant information. In this paper, we propose such an explainable\ngeometric deep network dubbed.\n","authors":["Chenyu Xue"],"pdf_url":"https://arxiv.org/pdf/2301.00815v3.pdf","comment":"Some parts of the thesis are still being revised"},{"id":"http://arxiv.org/abs/2303.07068v1","updated":"2023-03-13T12:44:32Z","published":"2023-03-13T12:44:32Z","title":"n-Step Temporal Difference Learning with Optimal n","summary":"  We consider the problem of finding the optimal value of n in the n-step\ntemporal difference (TD) algorithm. We find the optimal n by resorting to the\nmodel-free optimization technique of simultaneous perturbation stochastic\napproximation (SPSA). We adopt a one-simulation SPSA procedure that is\noriginally for continuous optimization to the discrete optimization framework\nbut incorporates a cyclic perturbation sequence. We prove the convergence of\nour proposed algorithm, SDPSA, and show that it finds the optimal value of n in\nn-step TD. Through experiments, we show that the optimal value of n is achieved\nwith SDPSA for any arbitrary initial value of the same.\n","authors":["Lakshmi Mandal","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2303.07068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07067v1","updated":"2023-03-13T12:42:02Z","published":"2023-03-13T12:42:02Z","title":"Cross-device Federated Learning for Mobile Health Diagnostics: A First\n  Study on COVID-19 Detection","summary":"  Federated learning (FL) aided health diagnostic models can incorporate data\nfrom a large number of personal edge devices (e.g., mobile phones) while\nkeeping the data local to the originating devices, largely ensuring privacy.\nHowever, such a cross-device FL approach for health diagnostics still imposes\nmany challenges due to both local data imbalance (as extreme as local data\nconsists of a single disease class) and global data imbalance (the disease\nprevalence is generally low in a population). Since the federated server has no\naccess to data distribution information, it is not trivial to solve the\nimbalance issue towards an unbiased model. In this paper, we propose FedLoss, a\nnovel cross-device FL framework for health diagnostics. Here the federated\nserver averages the models trained on edge devices according to the predictive\nloss on the local data, rather than using only the number of samples as\nweights. As the predictive loss better quantifies the data distribution at a\ndevice, FedLoss alleviates the impact of data imbalance. Through a real-world\ndataset on respiratory sound and symptom-based COVID-$19$ detection task, we\nvalidate the superiority of FedLoss. It achieves competitive COVID-$19$\ndetection performance compared to a centralised model with an AUC-ROC of\n$79\\%$. It also outperforms the state-of-the-art FL baselines in sensitivity\nand convergence speed. Our work not only demonstrates the promise of federated\nCOVID-$19$ detection but also paves the way to a plethora of mobile health\nmodel development in a privacy-preserving fashion.\n","authors":["Tong Xia","Jing Han","Abhirup Ghosh","Cecilia Mascolo"],"pdf_url":"https://arxiv.org/pdf/2303.07067v1.pdf","comment":"This paper has been accepted by IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07062v1","updated":"2023-03-13T12:34:17Z","published":"2023-03-13T12:34:17Z","title":"Quantile Online Learning for Semiconductor Failure Analysis","summary":"  With high device integration density and evolving sophisticated device\nstructures in semiconductor chips, detecting defects becomes elusive and\ncomplex. Conventionally, machine learning (ML)-guided failure analysis is\nperformed with offline batch mode training. However, the occurrence of new\ntypes of failures or changes in the data distribution demands retraining the\nmodel. During the manufacturing process, detecting defects in a single-pass\nonline fashion is more challenging and favoured. This paper focuses on novel\nquantile online learning for semiconductor failure analysis. The proposed\nmethod is applied to semiconductor device-level defects: FinFET bridge defect,\nGAA-FET bridge defect, GAA-FET dislocation defect, and a public database:\nSECOM. From the obtained results, we observed that the proposed method is able\nto perform better than the existing methods. Our proposed method achieved an\noverall accuracy of 86.66% and compared with the second-best existing method it\nimproves 15.50% on the GAA-FET dislocation defect dataset.\n","authors":["Bangjian Zhou","Pan Jieming","Maheswari Sivan","Aaron Voon-Yew Thean","J. Senthilnath"],"pdf_url":"https://arxiv.org/pdf/2303.07062v1.pdf","comment":"5 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2007.14462v4","updated":"2023-03-13T12:23:59Z","published":"2020-07-20T16:39:15Z","title":"Anomaly Awareness","summary":"  We present a new algorithm for anomaly detection called Anomaly Awareness.\nThe algorithm learns about normal events while being made aware of the\nanomalies through a modification of the cost function. We show how this method\nworks in different Particle Physics situations and in standard Computer Vision\ntasks. For example, we apply the method to images from a Fat Jet topology\ngenerated by Standard Model Top and QCD events, and test it against an array of\nnew physics scenarios, including Higgs production with EFT effects and\nresonances decaying into two, three or four subjets. We find that the algorithm\nis effective identifying anomalies not seen before, and becomes robust as we\nmake it aware of a varied-enough set of anomalies.\n","authors":["Charanjit K. Khosa","Veronica Sanz"],"pdf_url":"https://arxiv.org/pdf/2007.14462v4.pdf","comment":"12 pages, 17 figures"},{"id":"http://arxiv.org/abs/2303.07053v1","updated":"2023-03-13T12:22:38Z","published":"2023-03-13T12:22:38Z","title":"Bandit-supported care planning for older people with complex health and\n  care needs","summary":"  Long-term care service for old people is in great demand in most of the aging\nsocieties. The number of nursing homes residents is increasing while the number\nof care providers is limited. Due to the care worker shortage, care to\nvulnerable older residents cannot be fully tailored to the unique needs and\npreference of each individual. This may bring negative impacts on health\noutcomes and quality of life among institutionalized older people. To improve\ncare quality through personalized care planning and delivery with limited care\nworkforce, we propose a new care planning model assisted by artificial\nintelligence. We apply bandit algorithms which optimize the clinical decision\nfor care planning by adapting to the sequential feedback from the past\ndecisions. We evaluate the proposed model on empirical data acquired from the\nSystems for Person-centered Elder Care (SPEC) study, a ICT-enhanced care\nmanagement program.\n","authors":["Gi-Soo Kim","Young Suh Hong","Tae Hoon Lee","Myunghee Cho Paik","Hongsoo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08888v2","updated":"2023-03-13T12:14:55Z","published":"2023-02-17T14:17:44Z","title":"Multimodal Federated Learning via Contrastive Representation Ensemble","summary":"  With the increasing amount of multimedia data on modern mobile systems and\nIoT infrastructures, harnessing these rich multimodal data without breaching\nuser privacy becomes a critical issue. Federated learning (FL) serves as a\nprivacy-conscious alternative to centralized machine learning. However,\nexisting FL methods extended to multimodal data all rely on model aggregation\non single modality level, which restrains the server and clients to have\nidentical model architecture for each modality. This limits the global model in\nterms of both model complexity and data capacity, not to mention task\ndiversity. In this work, we propose Contrastive Representation Ensemble and\nAggregation for Multimodal FL (CreamFL), a multimodal federated learning\nframework that enables training larger server models from clients with\nheterogeneous model architectures and data modalities, while only communicating\nknowledge on public dataset. To achieve better multimodal representation\nfusion, we design a global-local cross-modal ensemble strategy to aggregate\nclient representations. To mitigate local model drift caused by two\nunprecedented heterogeneous factors stemming from multimodal discrepancy\n(modality gap and task gap), we further propose two inter-modal and intra-modal\ncontrasts to regularize local training, which complements information of the\nabsent modality for uni-modal clients and regularizes local clients to head\ntowards global consensus. Thorough evaluations and ablation studies on\nimage-text retrieval and visual question answering tasks showcase the\nsuperiority of CreamFL over state-of-the-art FL methods and its practical\nvalue.\n","authors":["Qiying Yu","Yang Liu","Yimu Wang","Ke Xu","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2302.08888v2.pdf","comment":"ICLR 2023. Code is available at https://github.com/FLAIR-THU/CreamFL"},{"id":"http://arxiv.org/abs/2303.07048v1","updated":"2023-03-13T12:13:28Z","published":"2023-03-13T12:13:28Z","title":"Hybrid Variational Autoencoder for Time Series Forecasting","summary":"  Variational autoencoders (VAE) are powerful generative models that learn the\nlatent representations of input data as random variables. Recent studies show\nthat VAE can flexibly learn the complex temporal dynamics of time series and\nachieve more promising forecasting results than deterministic models. However,\na major limitation of existing works is that they fail to jointly learn the\nlocal patterns (e.g., seasonality and trend) and temporal dynamics of time\nseries for forecasting. Accordingly, we propose a novel hybrid variational\nautoencoder (HyVAE) to integrate the learning of local patterns and temporal\ndynamics by variational inference for time series forecasting. Experimental\nresults on four real-world datasets show that the proposed HyVAE achieves\nbetter forecasting results than various counterpart methods, as well as two\nHyVAE variants that only learn the local patterns or temporal dynamics of time\nseries, respectively.\n","authors":["Borui Cai","Shuiqiao Yang","Longxiang Gao","Yong Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.07048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07046v1","updated":"2023-03-13T12:13:16Z","published":"2023-03-13T12:13:16Z","title":"Deploying Offline Reinforcement Learning with Human Feedback","summary":"  Reinforcement learning (RL) has shown promise for decision-making tasks in\nreal-world applications. One practical framework involves training\nparameterized policy models from an offline dataset and subsequently deploying\nthem in an online environment. However, this approach can be risky since the\noffline training may not be perfect, leading to poor performance of the RL\nmodels that may take dangerous actions. To address this issue, we propose an\nalternative framework that involves a human supervising the RL models and\nproviding additional feedback in the online deployment phase. We formalize this\nonline deployment problem and develop two approaches. The first approach uses\nmodel selection and the upper confidence bound algorithm to adaptively select a\nmodel to deploy from a candidate set of trained offline RL models. The second\napproach involves fine-tuning the model in the online deployment phase when a\nsupervision signal arrives. We demonstrate the effectiveness of these\napproaches for robot locomotion control and traffic light control tasks through\nempirical validation.\n","authors":["Ziniu Li","Ke Xu","Liu Liu","Lanqing Li","Deheng Ye","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05608v3","updated":"2023-03-13T11:57:06Z","published":"2022-06-11T20:16:24Z","title":"Gradient Boosting Performs Gaussian Process Inference","summary":"  This paper shows that gradient boosting based on symmetric decision trees can\nbe equivalently reformulated as a kernel method that converges to the solution\nof a certain Kernel Ridge Regression problem. Thus, we obtain the convergence\nto a Gaussian Process' posterior mean, which, in turn, allows us to easily\ntransform gradient boosting into a sampler from the posterior to provide better\nknowledge uncertainty estimates through Monte-Carlo estimation of the posterior\nvariance. We show that the proposed sampler allows for better knowledge\nuncertainty estimates leading to improved out-of-domain detection.\n","authors":["Aleksei Ustimenko","Artem Beliakov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2206.05608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07030v1","updated":"2023-03-13T11:45:48Z","published":"2023-03-13T11:45:48Z","title":"$\\nabla$SD: Differentiable Programming for Sparse Tensors","summary":"  Sparse tensors are prevalent in many data-intensive applications, yet\nexisting differentiable programming frameworks are tailored towards dense\ntensors. This presents a significant challenge for efficiently computing\ngradients through sparse tensor operations, as their irregular sparsity\npatterns can result in substantial memory and computational overheads. In this\nwork, we introduce a novel framework that enables the efficient and automatic\ndifferentiation of sparse tensors, addressing this fundamental issue. Our\nexperiments demonstrate the effectiveness of the proposed framework in terms of\nperformance and scalability, outperforming state-of-the-art frameworks across a\nrange of synthetic and real-world datasets. Our approach offers a promising\ndirection for enabling efficient and scalable differentiable programming with\nsparse tensors, which has significant implications for numerous applications in\nmachine learning, natural language processing, and scientific computing.\n","authors":["Amir Shaikhha","Mathieu Huot","Shideh Hashemian"],"pdf_url":"https://arxiv.org/pdf/2303.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02678v2","updated":"2023-03-13T11:43:55Z","published":"2022-10-27T14:24:48Z","title":"Efficient ECG-based Atrial Fibrillation Detection via Parameterised\n  Hypercomplex Neural Networks","summary":"  Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated\nwith a high risk for serious conditions like stroke. The use of wearable\ndevices embedded with automatic and timely AF assessment from\nelectrocardiograms (ECGs) has shown to be promising in preventing\nlife-threatening situations. Although deep neural networks have demonstrated\nsuperiority in model performance, their use on wearable devices is limited by\nthe trade-off between model performance and complexity. In this work, we\npropose to use lightweight convolutional neural networks (CNNs) with\nparameterised hypercomplex (PH) layers for AF detection based on ECGs. The\nproposed approach trains small-scale CNNs, thus overcoming the limited\ncomputing resources on wearable devices. We show comparable performance to\ncorresponding real-valued CNNs on two publicly available ECG datasets using\nsignificantly fewer model parameters. PH models are more flexible than other\nhypercomplex neural networks and can operate on any number of input ECG leads.\n","authors":["Leonie Basso","Zhao Ren","Wolfgang Nejdl"],"pdf_url":"https://arxiv.org/pdf/2211.02678v2.pdf","comment":"Revised paper organisation. Further experiments to emphasise flexible\n  model compression and comparison with other baselines"},{"id":"http://arxiv.org/abs/2211.01704v2","updated":"2023-03-13T11:41:49Z","published":"2022-11-03T10:56:17Z","title":"Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and\n  Envelope-based Features for Machinery Fault Detection","summary":"  Acoustic-based fault detection has a high potential to monitor the health\ncondition of mechanical parts. However, the background noise of an industrial\nenvironment may negatively influence the performance of fault detection.\nLimited attention has been paid to improving the robustness of fault detection\nagainst industrial environmental noise. Therefore, we present the Lenze\nproduction background-noise (LPBN) real-world dataset and an automated and\nnoise-robust auditory inspection (ARAI) system for the end-of-line inspection\nof geared motors. An acoustic array is used to acquire data from motors with a\nminor fault, major fault, or which are healthy. A benchmark is provided to\ncompare the psychoacoustic features with different types of envelope features\nbased on expert knowledge of the gearbox. To the best of our knowledge, we are\nthe first to apply time-varying psychoacoustic features for fault detection. We\ntrain a state-of-the-art one-class-classifier, on samples from healthy motors\nand separate the faulty ones for fault detection using a threshold. The\nbest-performing approaches achieve an area under curve of 0.87 (logarithm\nenvelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).\n","authors":["Peter Wißbrock","Yvonne Richter","David Pelkmann","Zhao Ren","Gregory Palmer"],"pdf_url":"https://arxiv.org/pdf/2211.01704v2.pdf","comment":"the final published version at ICASSP 2023 include small additional\n  content as well as some minor revisions"},{"id":"http://arxiv.org/abs/2303.07009v1","updated":"2023-03-13T11:07:17Z","published":"2023-03-13T11:07:17Z","title":"Symbolic Regression for PDEs using Pruned Differentiable Programs","summary":"  Physics-informed Neural Networks (PINNs) have been widely used to obtain\naccurate neural surrogates for a system of Partial Differential Equations\n(PDE). One of the major limitations of PINNs is that the neural solutions are\nchallenging to interpret, and are often treated as black-box solvers. While\nSymbolic Regression (SR) has been studied extensively, very few works exist\nwhich generate analytical expressions to directly perform SR for a system of\nPDEs. In this work, we introduce an end-to-end framework for obtaining\nmathematical expressions for solutions of PDEs. We use a trained PINN to\ngenerate a dataset, upon which we perform SR. We use a Differentiable Program\nArchitecture (DPA) defined using context-free grammar to describe the space of\nsymbolic expressions. We improve the interpretability by pruning the DPA in a\ndepth-first manner using the magnitude of weights as our heuristic. On average,\nwe observe a 95.3% reduction in parameters of DPA while maintaining accuracy at\npar with PINNs. Furthermore, on an average, pruning improves the accuracy of\nDPA by 7.81% . We demonstrate our framework outperforms the existing\nstate-of-the-art SR solvers on systems of complex PDEs like Navier-Stokes:\nKovasznay flow and Taylor-Green Vortex flow. Furthermore, we produce analytical\nexpressions for a complex industrial use-case of an Air-Preheater, without\nsuffering from performance loss viz-a-viz PINNs.\n","authors":["Ritam Majumdar","Vishal Jadhav","Anirudh Deodhar","Shirish Karande","Lovekesh Vig","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2303.07009v1.pdf","comment":"Publication accepted at International Conference for Learning\n  Representations 2023: Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2303.07000v1","updated":"2023-03-13T10:57:35Z","published":"2023-03-13T10:57:35Z","title":"Predicting Density of States via Multi-modal Transformer","summary":"  The density of states (DOS) is a spectral property of materials, which\nprovides fundamental insights on various characteristics of materials. In this\npaper, we propose a model to predict the DOS by reflecting the nature of DOS:\nDOS determines the general distribution of states as a function of energy.\nSpecifically, we integrate the heterogeneous information obtained from the\ncrystal structure and the energies via multi-modal transformer, thereby\nmodeling the complex relationships between the atoms in the crystal structure,\nand various energy levels. Extensive experiments on two types of DOS, i.e.,\nPhonon DOS and Electron DOS, with various real-world scenarios demonstrate the\nsuperiority of DOSTransformer. The source code for DOSTransformer is available\nat https://github.com/HeewoongNoh/DOSTransformer.\n","authors":["Namkyeong Lee","Heewoong Noh","Sungwon Kim","Dongmin Hyun","Gyoung S. Na","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2303.07000v1.pdf","comment":"ICLR 2023 Workshop on Machine Learning for Materials (ML4Materials)"},{"id":"http://arxiv.org/abs/2303.06999v1","updated":"2023-03-13T10:54:52Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., when considering 200\nproposals from our method, we detect label errors with a precision for a) of up\nto 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06992v1","updated":"2023-03-13T10:47:24Z","published":"2023-03-13T10:47:24Z","title":"Improving Mutual Information Estimation with Annealed and Energy-Based\n  Bounds","summary":"  Mutual information (MI) is a fundamental quantity in information theory and\nmachine learning. However, direct estimation of MI is intractable, even if the\ntrue joint probability density for the variables of interest is known, as it\ninvolves estimating a potentially high-dimensional log partition function. In\nthis work, we present a unifying view of existing MI bounds from the\nperspective of importance sampling, and propose three novel bounds based on\nthis approach. Since accurate estimation of MI without density information\nrequires a sample size exponential in the true MI, we assume either a single\nmarginal or the full joint density information is known. In settings where the\nfull joint density is available, we propose Multi-Sample Annealed Importance\nSampling (AIS) bounds on MI, which we demonstrate can tightly estimate large\nvalues of MI in our experiments. In settings where only a single marginal\ndistribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds.\nOur GIWAE bound unifies variational and contrastive bounds in a single\nframework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our\nMINE-AIS method improves upon existing energy-based methods such as MINE-DV and\nMINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC\nsampling to estimate gradients for training and Multi-Sample AIS for evaluating\nthe bound. Our methods are particularly suitable for evaluating MI in deep\ngenerative models, since explicit forms of the marginal or joint densities are\noften available. We evaluate our bounds on estimating the MI of VAEs and GANs\ntrained on the MNIST and CIFAR datasets, and showcase significant gains over\nexisting bounds in these challenging settings with high ground truth MI.\n","authors":["Rob Brekelmans","Sicong Huang","Marzyeh Ghassemi","Greg Ver Steeg","Roger Grosse","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2303.06992v1.pdf","comment":"A shorter version appeared in the International Conference on\n  Learning Representations (ICLR) 2022"},{"id":"http://arxiv.org/abs/2303.06980v1","updated":"2023-03-13T10:30:02Z","published":"2023-03-13T10:30:02Z","title":"Self-supervised based general laboratory progress pretrained model for\n  cardiovascular event detection","summary":"  Regular surveillance is an indispensable aspect of managing cardiovascular\ndisorders. Patient recruitment for rare or specific diseases is often limited\ndue to their small patient size and episodic observations, whereas prevalent\ncases accumulate longitudinal data easily due to regular follow-ups. These\ndata, however, are notorious for their irregularity, temporality, sparsity, and\nabsenteeism. In this study, we leveraged self-supervised learning (SSL) and\ntransfer learning to overcome the above-mentioned barriers, transferring\npatient progress trends in cardiovascular laboratory parameters from prevalent\ncases to rare or specific cardiovascular events detection. We pretrained a\ngeneral laboratory progress (GLP) pretrain model using hypertension patients\n(who were yet to be diabetic), and transferred their laboratory progress trend\nto assist in detecting target vessel revascularization (TVR) in percutaneous\ncoronary intervention patients. GLP adopted a two-stage training process that\nutilized interpolated data, enhancing the performance of SSL. After pretraining\nGLP, we fine-tuned it for TVR prediction. The proposed two-stage training\nprocess outperformed SSL. Upon processing by GLP, the classification\ndemonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged\naccuracy. All metrics were significantly superior (p < 0.01) to the performance\nof prior GLP processing. The representation displayed distinct separability\nindependent of algorithmic mechanisms, and diverse data distribution trend. Our\napproach effectively transferred the progression trends of cardiovascular\nlaboratory parameters from prevalent cases to small-numbered cases, thereby\ndemonstrating its efficacy in aiding the risk assessment of cardiovascular\nevents without limiting to episodic observation. The potential for extending\nthis approach to other laboratory tests and diseases is promising.\n","authors":["Li-Chin Chen","Kuo-Hsuan Hung","Yi-Ju Tseng","Hsin-Yao Wang","Tse-Min Lu","Wei-Chieh Huang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2303.06980v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/1904.03445v3","updated":"2023-03-13T10:28:26Z","published":"2019-04-06T13:47:48Z","title":"Feature-Based Interpolation and Geodesics in the Latent Spaces of\n  Generative Models","summary":"  Interpolating between points is a problem connected simultaneously with\nfinding geodesics and study of generative models. In the case of geodesics, we\nsearch for the curves with the shortest length, while in the case of generative\nmodels we typically apply linear interpolation in the latent space. However,\nthis interpolation uses implicitly the fact that Gaussian is unimodal. Thus the\nproblem of interpolating in the case when the latent density is non-Gaussian is\nan open problem.\n  In this paper, we present a general and unified approach to interpolation,\nwhich simultaneously allows us to search for geodesics and interpolating curves\nin latent space in the case of arbitrary density. Our results have a strong\ntheoretical background based on the introduced quality measure of an\ninterpolating curve. In particular, we show that maximising the quality measure\nof the curve can be equivalently understood as a search of geodesic for a\ncertain redefinition of the Riemannian metric on the space.\n  We provide examples in three important cases. First, we show that our\napproach can be easily applied to finding geodesics on manifolds. Next, we\nfocus our attention in finding interpolations in pre-trained generative models.\nWe show that our model effectively works in the case of arbitrary density.\nMoreover, we can interpolate in the subset of the space consisting of data\npossessing a given feature. The last case is focused on finding interpolation\nin the space of chemical compounds.\n","authors":["Łukasz Struski","Michał Sadowski","Tomasz Danel","Jacek Tabor","Igor T. Podolak"],"pdf_url":"https://arxiv.org/pdf/1904.03445v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.03880v3","updated":"2023-03-13T10:20:38Z","published":"2022-09-08T15:35:42Z","title":"Learning Sparse Graphon Mean Field Games","summary":"  Although the field of multi-agent reinforcement learning (MARL) has made\nconsiderable progress in the last years, solving systems with a large number of\nagents remains a hard challenge. Graphon mean field games (GMFGs) enable the\nscalable analysis of MARL problems that are otherwise intractable. By the\nmathematical structure of graphons, this approach is limited to dense graphs\nwhich are insufficient to describe many real-world networks such as power law\ngraphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs,\nwhich leverages the graph theoretical concept of $L^p$ graphons and provides a\nmachine learning tool to efficiently and accurately approximate solutions for\nsparse network problems. This especially includes power law networks which are\nempirically observed in various application areas and cannot be captured by\nstandard graphons. We derive theoretical existence and convergence guarantees\nand give empirical examples that demonstrate the accuracy of our learning\napproach for systems with many agents. Furthermore, we extend the Online Mirror\nDescent (OMD) learning algorithm to our setup to accelerate learning speed,\nempirically show its capabilities, and conduct a theoretical analysis using the\nnovel concept of smoothed step graphons. In general, we provide a scalable,\nmathematically well-founded machine learning approach to a large class of\notherwise intractable problems of great relevance in numerous research fields.\n","authors":["Christian Fabian","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2209.03880v3.pdf","comment":"accepted for publication at the International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2023; code available at:\n  https://github.com/ChrFabian/Learning_sparse_GMFGs"},{"id":"http://arxiv.org/abs/2303.06972v1","updated":"2023-03-13T10:16:19Z","published":"2023-03-13T10:16:19Z","title":"Leveraging Neural Koopman Operators to Learn Continuous Representations\n  of Dynamical Systems from Scarce Data","summary":"  Over the last few years, several works have proposed deep learning\narchitectures to learn dynamical systems from observation data with no or\nlittle knowledge of the underlying physics. A line of work relies on learning\nrepresentations where the dynamics of the underlying phenomenon can be\ndescribed by a linear operator, based on the Koopman operator theory. However,\ndespite being able to provide reliable long-term predictions for some dynamical\nsystems in ideal situations, the methods proposed so far have limitations, such\nas requiring to discretize intrinsically continuous dynamical systems, leading\nto data loss, especially when handling incomplete or sparsely sampled data.\nHere, we propose a new deep Koopman framework that represents dynamics in an\nintrinsically continuous way, leading to better performance on limited training\ndata, as exemplified on several datasets arising from dynamical systems.\n","authors":["Anthony Frion","Lucas Drumetz","Mauro Dalla Mura","Guillaume Tochon","Abdeldjalil Aissa El Bey"],"pdf_url":"https://arxiv.org/pdf/2303.06972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04788v2","updated":"2023-03-13T10:13:00Z","published":"2022-04-10T22:58:02Z","title":"Representation Learning by Detecting Incorrect Location Embeddings","summary":"  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2204.04788v2.pdf","comment":"accepted at AAAI2023, https://github.com/Separius/DILEMMA"},{"id":"http://arxiv.org/abs/2303.06965v1","updated":"2023-03-13T10:06:41Z","published":"2023-03-13T10:06:41Z","title":"Uni-RXN: An Unified Framework that Bridge the Gap between Chemical\n  Reaction Pretraining and Conditional Molecule Generation","summary":"  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. Machine learning for chemistry is a rapidly\nadvancing field with numerous applications. In recent years, there has been a\ngrowing need for a large-scale deep-learning framework that can efficiently\ncapture the basic rules of chemical reactions. In this paper, we have proposed\na unified framework that addresses both the reaction representation learning\nand molecule generation tasks, which allows for a more holistic approach.\nInspired by the organic chemistry mechanism, we develop a novel pretraining\nframework that enables us to incorporate inductive biases into the model. Our\nframework achieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, this framework can be applied to reaction-based\ngenerative models, overcoming the limitations of current molecule generation\nmodels that rely on a small number of reaction templates. In the extensive\nexperiments, our model generates synthesizable drug-like structures of high\nquality. Overall, our work presents a significant step toward a large-scale\ndeep-learning framework for a variety of reaction-based applications.\n","authors":["Bo Qiang","Yiran Zhou","Yuheng Ding","Ningfeng Liu","Liangren Zhang","Zhenming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04788v2","updated":"2023-03-13T10:04:28Z","published":"2023-03-08T18:39:43Z","title":"Enabling Non-Linear Quantum Operations through Variational Quantum\n  Splines","summary":"  The postulates of quantum mechanics impose only unitary transformations on\nquantum states, which is a severe limitation for quantum machine learning\nalgorithms. Quantum Splines (QSplines) have recently been proposed to\napproximate quantum activation functions to introduce non-linearity in quantum\nalgorithms. However, QSplines make use of the HHL as a subroutine and require a\nfault-tolerant quantum computer to be correctly implemented. This work proposes\nthe Generalised QSplines (GQSplines), a novel method for approximating\nnon-linear quantum activation functions using hybrid quantum-classical\ncomputation. The GQSplines overcome the highly demanding requirements of the\noriginal QSplines in terms of quantum hardware and can be implemented using\nnear-term quantum computers. Furthermore, the proposed method relies on a\nflexible problem representation for non-linear approximation and it is suitable\nto be embedded in existing quantum neural network architectures. In addition,\nwe provide a practical implementation of GQSplines using Pennylane and show\nthat our model outperforms the original QSplines in terms of quality of\nfitting.\n","authors":["Matteo Antonio Inajetovic","Filippo Orazi","Antonio Macaluso","Stefano Lodi","Claudio Sartori"],"pdf_url":"https://arxiv.org/pdf/2303.04788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03393v2","updated":"2023-03-13T09:57:05Z","published":"2021-06-07T07:43:28Z","title":"Adversarially Regularized Graph Attention Networks for Inductive\n  Learning on Partially Labeled Graphs","summary":"  The high cost of data labeling often results in node label shortage in real\napplications. To improve node classification accuracy, graph-based\nsemi-supervised learning leverages the ample unlabeled nodes to train together\nwith the scarce available labeled nodes. However, most existing methods require\nthe information of all nodes, including those to be predicted, during model\ntraining, which is not practical for dynamic graphs with newly added nodes. To\naddress this issue, an adversarially regularized graph attention model is\nproposed to classify newly added nodes in a partially labeled graph. An\nattention-based aggregator is designed to generate the representation of a node\nby aggregating information from its neighboring nodes, thus naturally\ngeneralizing to previously unseen nodes. In addition, adversarial training is\nemployed to improve the model's robustness and generalization ability by\nenforcing node representations to match a prior distribution. Experiments on\nreal-world datasets demonstrate the effectiveness of the proposed method in\ncomparison with the state-of-the-art methods. The code is available at\nhttps://github.com/JiarenX/AGAIN.\n","authors":["Jiaren Xiao","Quanyu Dai","Xiaochen Xie","James Lam","Ka-Wai Kwok"],"pdf_url":"https://arxiv.org/pdf/2106.03393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02731v2","updated":"2023-03-13T09:42:22Z","published":"2022-10-06T07:44:51Z","title":"PSVRF: Learning to restore Pitch-Shifted Voice without reference","summary":"  Pitch scaling algorithms have a significant impact on the security of\nAutomatic Speaker Verification (ASV) systems. Although numerous anti-spoofing\nalgorithms have been proposed to identify the pitch-shifted voice and even\nrestore it to the original version, they either have poor performance or\nrequire the original voice as a reference, limiting the prospects of\napplications. In this paper, we propose a no-reference approach termed\nPSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on\nAISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised\nby various pitch-scaling techniques, which obviously enhances the robustness of\nASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF\neven surpasses that of the state-of-the-art reference-based approach.\n","authors":["Yangfu Li","Xiaodan Lin","Jiaxin Yang"],"pdf_url":"https://arxiv.org/pdf/2210.02731v2.pdf","comment":"Have some errors"},{"id":"http://arxiv.org/abs/2303.06947v1","updated":"2023-03-13T09:31:20Z","published":"2023-03-13T09:31:20Z","title":"A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X\n  Communications in Dynamic Environments","summary":"  Digital Twins (DTs) for physical wireless environments have been recently\nproposed as accurate virtual representations of the propagation environment\nthat can enable multi-layer decisions at the physical communication equipment.\nAt high frequency bands, DTs can help to overcome the challenges emerging in\nthe high mobility conditions featuring vehicular environments. In this paper,\nwe propose a novel data-driven workflow for the creation of the DT of a\nVehicle-to-Everything (V2X) communication scenario and a multi-modal simulation\nframework for the generation of realistic sensor data and accurate\nmmWave/sub-THz wireless channels. The proposed method leverages an automotive\nsimulation and testing framework based on the Unreal Engine game engine and an\naccurate ray-tracing channel simulator. Simulations over an urban scenario show\nthe achievable realistic sensor and channel modelling both at the\ninfrastructure and at an ego-vehicle.\n","authors":["Lorenzo Cazzella","Francesco Linsalata","Maurizio Magarini","Matteo Matteucci","Umberto Spagnolini"],"pdf_url":"https://arxiv.org/pdf/2303.06947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06946v1","updated":"2023-03-13T09:27:52Z","published":"2023-03-13T09:27:52Z","title":"Context-Aware Selective Label Smoothing for Calibrating Sequence\n  Recognition Model","summary":"  Despite the success of deep neural network (DNN) on sequential data (i.e.,\nscene text and speech) recognition, it suffers from the over-confidence problem\nmainly due to overfitting in training with the cross-entropy loss, which may\nmake the decision-making less reliable. Confidence calibration has been\nrecently proposed as one effective solution to this problem. Nevertheless, the\nmajority of existing confidence calibration methods aims at non-sequential\ndata, which is limited if directly applied to sequential data since the\nintrinsic contextual dependency in sequences or the class-specific statistical\nprior is seldom exploited. To the end, we propose a Context-Aware Selective\nLabel Smoothing (CASLS) method for calibrating sequential data. The proposed\nCASLS fully leverages the contextual dependency in sequences to construct\nconfusion matrices of contextual prediction statistics over different classes.\nClass-specific error rates are then used to adjust the weights of smoothing\nstrength in order to achieve adaptive calibration. Experimental results on\nsequence recognition tasks, including scene text recognition and speech\nrecognition, demonstrate that our method can achieve the state-of-the-art\nperformance.\n","authors":["Shuangping Huang","Yu Luo","Zhenzhou Zhuang","Jin-Gang Yu","Mengchao He","Yongpan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06945v1","updated":"2023-03-13T09:27:34Z","published":"2023-03-13T09:27:34Z","title":"CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for\n  Protein-Protein Interaction Site Prediction","summary":"  Protein-protein interactions are essential in biochemical processes. Accurate\nprediction of the protein-protein interaction sites (PPIs) deepens our\nunderstanding of biological mechanism and is crucial for new drug design.\nHowever, conventional experimental methods for PPIs prediction are costly and\ntime-consuming so that many computational approaches, especially ML-based\nmethods, have been developed recently. Although these approaches have achieved\ngratifying results, there are still two limitations: (1) Most models have\nexcavated some useful input features, but failed to take coevolutionary\nfeatures into account, which could provide clues for inter-residue\nrelationships; (2) The attention-based models only allocate attention weights\nfor neighboring residues, instead of doing it globally, neglecting that some\nresidues being far away from the target residues might also matter.\n  We propose a coevolution-enhanced global attention neural network, a\nsequence-based deep learning model for PPIs prediction, called CoGANPPIS. It\nutilizes three layers in parallel for feature extraction: (1) Local-level\nrepresentation aggregation layer, which aggregates the neighboring residues'\nfeatures; (2) Global-level representation learning layer, which employs a novel\ncoevolution-enhanced global attention mechanism to allocate attention weights\nto all the residues on the same protein sequences; (3) Coevolutionary\ninformation learning layer, which applies CNN & pooling to coevolutionary\ninformation to obtain the coevolutionary profile representation. Then, the\nthree outputs are concatenated and passed into several fully connected layers\nfor the final prediction. Application on two benchmark datasets demonstrated a\nstate-of-the-art performance of our model. The source code is publicly\navailable at https://github.com/Slam1423/CoGANPPIS_source_code.\n","authors":["Jiaxing Guo","Xuening Zhu","Zixin Hu","Xiaoxi Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07160v2","updated":"2023-03-13T09:18:52Z","published":"2023-02-14T16:14:39Z","title":"Learning a model is paramount for sample efficiency in reinforcement\n  learning control of PDEs","summary":"  The goal of this paper is to make a strong point for the usage of dynamical\nmodels when using reinforcement learning (RL) for feedback control of dynamical\nsystems governed by partial differential equations (PDEs). To breach the gap\nbetween the immense promises we see in RL and the applicability in complex\nengineering systems, the main challenges are the massive requirements in terms\nof the training data, as well as the lack of performance guarantees. We present\na solution for the first issue using a data-driven surrogate model in the form\nof a convolutional LSTM with actuation. We demonstrate that learning an\nactuated model in parallel to training the RL agent significantly reduces the\ntotal amount of required data sampled from the real system. Furthermore, we\nshow that iteratively updating the model is of major importance to avoid biases\nin the RL training. Detailed ablation studies reveal the most important\ningredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky\nequation do demonstarte our findings.\n","authors":["Stefan Werner","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2302.07160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06937v1","updated":"2023-03-13T09:11:54Z","published":"2023-03-13T09:11:54Z","title":"Addressing Catastrophic Forgetting in Federated Class-Continual Learning","summary":"  This paper focuses on an under-explored yet important problem: Federated\nClass-Continual Learning (FCCL), where new classes are dynamically added in\nfederated learning. Existing FCCL works suffer from various limitations, such\nas requiring additional datasets or storing the private data from previous\ntasks. In response, we first demonstrate that non-IID data exacerbates\ncatastrophic forgetting issue in FL. Then we propose a novel method called\nTARGET (federat\\textbf{T}ed cl\\textbf{A}ss-continual lea\\textbf{R}nin\\textbf{G}\nvia \\textbf{E}xemplar-free dis\\textbf{T}illation), which alleviates\ncatastrophic forgetting in FCCL while preserving client data privacy. Our\nproposed method leverages the previously trained global model to transfer\nknowledge of old tasks to the current task at the model level. Moreover, a\ngenerator is trained to produce synthetic data to simulate the global\ndistribution of data on each client at the data level. Compared to previous\nFCCL methods, TARGET does not require any additional datasets or storing real\ndata from previous tasks, which makes it ideal for data-sensitive scenarios.\n","authors":["Jie Zhang","Chen Chen","Weiming Zhuang","Lingjuan Lv"],"pdf_url":"https://arxiv.org/pdf/2303.06937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12538v2","updated":"2023-03-13T09:04:19Z","published":"2023-02-24T09:49:43Z","title":"UnbiasedNets: A Dataset Diversification Framework for Robustness Bias\n  Alleviation in Neural Networks","summary":"  Performance of trained neural network (NN) models, in terms of testing\naccuracy, has improved remarkably over the past several years, especially with\nthe advent of deep learning. However, even the most accurate NNs can be biased\ntoward a specific output classification due to the inherent bias in the\navailable training datasets, which may propagate to the real-world\nimplementations. This paper deals with the robustness bias, i.e., the bias\nexhibited by the trained NN by having a significantly large robustness to noise\nfor a certain output class, as compared to the remaining output classes. The\nbias is shown to result from imbalanced datasets, i.e., the datasets where all\noutput classes are not equally represented. Towards this, we propose the\nUnbiasedNets framework, which leverages K-means clustering and the NN's noise\ntolerance to diversify the given training dataset, even from relatively smaller\ndatasets. This generates balanced datasets and reduces the bias within the\ndatasets themselves. To the best of our knowledge, this is the first framework\ncatering to the robustness bias problem in NNs. We use real-world datasets to\ndemonstrate the efficacy of the UnbiasedNets for data diversification, in case\nof both binary and multi-label classifiers. The results are compared to\nwell-known tools aimed at generating balanced datasets, and illustrate how\nexisting works have limited success while addressing the robustness bias. In\ncontrast, UnbiasedNets provides a notable improvement over existing works,\nwhile even reducing the robustness bias significantly in some cases, as\nobserved by comparing the NNs trained on the diversified and original datasets.\n","authors":["Mahum Naseer","Bharath Srinivas Prabakaran","Osman Hasan","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2302.12538v2.pdf","comment":"Springer Machine Learning 2023"},{"id":"http://arxiv.org/abs/2303.06931v1","updated":"2023-03-13T08:55:10Z","published":"2023-03-13T08:55:10Z","title":"DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability\n  Assessment","summary":"  Deep Neural Networks (DNNs) and their accelerators are being deployed ever\nmore frequently in safety-critical applications leading to increasing\nreliability concerns. A traditional and accurate method for assessing DNNs'\nreliability has been resorting to fault injection, which, however, suffers from\nprohibitive time complexity. While analytical and hybrid fault\ninjection-/analytical-based methods have been proposed, they are either\ninaccurate or specific to particular accelerator architectures. In this work,\nwe propose a novel accurate, fine-grain, metric-oriented, and\naccelerator-agnostic method called DeepVigor that provides vulnerability value\nranges for DNN neurons' outputs. An outcome of DeepVigor is an analytical model\nrepresenting vulnerable and non-vulnerable ranges for each neuron that can be\nexploited to develop different techniques for improving DNNs' reliability.\nMoreover, DeepVigor provides reliability assessment metrics based on\nvulnerability factors for bits, neurons, and layers using the vulnerability\nranges. The proposed method is not only faster than fault injection but also\nprovides extensive and accurate information about the reliability of DNNs,\nindependent from the accelerator. The experimental evaluations in the paper\nindicate that the proposed vulnerability ranges are 99.9% to 100% accurate even\nwhen evaluated on previously unseen test data. Also, it is shown that the\nobtained vulnerability factors represent the criticality of bits, neurons, and\nlayers proficiently. DeepVigor is implemented in the PyTorch framework and\nvalidated on complex DNN benchmarks.\n","authors":["Mohammad Hasan Ahmadilivani","Mahdi Taheri","Jaan Raik","Masoud Daneshtalab","Maksim Jenihhin"],"pdf_url":"https://arxiv.org/pdf/2303.06931v1.pdf","comment":"6 pages, 6 figures, 2 tables, accepted at ETS 2023\n  (cas.polito.it/ETS23/#/program-conference#tab-accepted)"},{"id":"http://arxiv.org/abs/2210.13867v2","updated":"2023-03-13T08:41:43Z","published":"2022-10-25T09:43:36Z","title":"A Dynamical System View of Langevin-Based Non-Convex Sampling","summary":"  Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees.\n","authors":["Mohammad Reza Karimi","Ya-Ping Hsieh","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2210.13867v2.pdf","comment":"typos corrected, references added"},{"id":"http://arxiv.org/abs/2205.12706v2","updated":"2023-03-13T08:33:02Z","published":"2022-05-25T12:02:59Z","title":"Maximum Mean Discrepancy on Exponential Windows for Online Change\n  Detection","summary":"  Detecting changes is of fundamental importance when analyzing data streams\nand has many applications, e.g., predictive maintenance, fraud detection, or\nmedicine. A principled approach to detect changes is to compare the\ndistributions of observations within the stream to each other via hypothesis\ntesting. Maximum mean discrepancy (MMD; also called energy distance) is a\nwell-known (semi-)metric on the space of probability distributions. MMD gives\nrise to powerful non-parametric two-sample tests on kernel-enriched domains\nunder mild conditions, which makes its deployment for change detection\ndesirable. However, the classic MMD estimators suffer quadratic complexity,\nwhich prohibits their application in the online change detection setting. We\npropose a general-purpose change detection algorithm, Maximum Mean Discrepancy\non Exponential Windows (MMDEW), which leverages the MMD two-sample test,\nfacilitates its efficient online computation on any kernel-enriched domain, and\nis able to detect any disparity between distributions. Our experiments and\nanalysis show that (1) MMDEW achieves better detection quality than\nstate-of-the-art competitors and that (2) the algorithm has polylogarithmic\nruntime and logarithmic memory requirements, which allow its deployment to the\nstreaming setting.\n","authors":["Florian Kalinke","Marco Heyden","Edouard Fouché","Klemens Böhm"],"pdf_url":"https://arxiv.org/pdf/2205.12706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.03386v2","updated":"2023-03-13T08:23:37Z","published":"2020-12-06T22:24:28Z","title":"SoK: Training Machine Learning Models over Multiple Sources with Privacy\n  Preservation","summary":"  Nowadays, gathering high-quality training data from multiple data sources\nwith privacy preservation is a crucial challenge to training high-performance\nmachine learning models. The potential solutions could break the barriers among\nisolated data corpus, and consequently enlarge the range of data available for\nprocessing. To this end, both academic researchers and industrial vendors are\nrecently strongly motivated to propose two main-stream folders of solutions\nmainly based on software constructions: 1) Secure Multi-party Learning (MPL for\nshort); and 2) Federated Learning (FL for short). The above two technical\nfolders have their advantages and limitations when we evaluate them according\nto the following five criteria: security, efficiency, data distribution, the\naccuracy of trained models, and application scenarios.\n  Motivated to demonstrate the research progress and discuss the insights on\nthe future directions, we thoroughly investigate these protocols and frameworks\nof both MPL and FL. At first, we define the problem of Training machine\nlearning Models over Multiple data sources with Privacy Preservation (TMMPP for\nshort). Then, we compare the recent studies of TMMPP from the aspects of the\ntechnical routes, the number of parties supported, data partitioning, threat\nmodel, and machine learning models supported, to show their advantages and\nlimitations. Next, we investigate and evaluate five popular FL platforms.\nFinally, we discuss the potential directions to resolve the problem of TMMPP in\nthe future.\n","authors":["Lushan Song","Guopeng Lin","Jiaxuan Wang","Haoqi Wu","Wenqiang Ruan","Weili Han"],"pdf_url":"https://arxiv.org/pdf/2012.03386v2.pdf","comment":"19pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.03179v3","updated":"2023-03-13T08:23:37Z","published":"2022-03-07T07:23:18Z","title":"Detecting data-driven robust statistical arbitrage strategies with deep\n  neural networks","summary":"  We present an approach, based on deep neural networks, that allows\nidentifying robust statistical arbitrage strategies in financial markets.\nRobust statistical arbitrage strategies refer to trading strategies that enable\nprofitable trading under model ambiguity. The presented novel methodology\nallows to consider a large amount of underlying securities simultaneously and\ndoes not depend on the identification of cointegrated pairs of assets, hence it\nis applicable on high-dimensional financial markets or in markets where\nclassical pairs trading approaches fail. Moreover, we provide a method to build\nan ambiguity set of admissible probability measures that can be derived from\nobserved market data. Thus, the approach can be considered as being model-free\nand entirely data-driven. We showcase the applicability of our method by\nproviding empirical investigations with highly profitable trading performances\neven in 50 dimensions, during financial crises, and when the cointegration\nrelationship between asset pairs stops to persist.\n","authors":["Ariel Neufeld","Julian Sester","Daiying Yin"],"pdf_url":"https://arxiv.org/pdf/2203.03179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07892v2","updated":"2023-03-13T08:19:14Z","published":"2022-06-16T02:46:26Z","title":"Max-Margin Works while Large Margin Fails: Generalization without\n  Uniform Convergence","summary":"  A major challenge in modern machine learning is theoretically understanding\nthe generalization properties of overparameterized models. Many existing tools\nrely on uniform convergence (UC), a property that, when it holds, guarantees\nthat the test loss will be close to the training loss, uniformly over a class\nof candidate models. Nagarajan and Kolter (2019) show that in certain simple\nlinear and neural-network settings, any uniform convergence bound will be\nvacuous, leaving open the question of how to prove generalization in settings\nwhere UC fails. Our main contribution is proving novel generalization bounds in\ntwo such settings, one linear, and one non-linear. We study the linear\nclassification setting of Nagarajan and Kolter, and a quadratic ground truth\nfunction learned via a two-layer neural network in the non-linear regime. We\nprove a new type of margin bound showing that above a certain signal-to-noise\nthreshold, any near-max-margin classifier will achieve almost no test loss in\nthese two settings. Our results show that near-max-margin is important: while\nany model that achieves at least a $(1 - \\epsilon)$-fraction of the max-margin\ngeneralizes well, a classifier achieving half of the max-margin may fail\nterribly. Building on the impossibility results of Nagarajan and Kolter, under\nslightly stronger assumptions, we show that one-sided UC bounds and classical\nmargin bounds will fail on near-max-margin classifiers. Our analysis provides\ninsight on why memorization can coexist with generalization: we show that in\nthis challenging regime where generalization occurs but UC fails,\nnear-max-margin classifiers simultaneously contain some generalizable\ncomponents and some overfitting components that memorize the data. The presence\nof the overfitting components is enough to preclude UC, but the near-extremal\nmargin guarantees that sufficient generalizable components are present.\n","authors":["Margalit Glasgow","Colin Wei","Mary Wootters","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2206.07892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05205v2","updated":"2023-03-13T08:06:17Z","published":"2023-03-09T12:19:20Z","title":"Real-time scheduling of renewable power systems through planning-based\n  reinforcement learning","summary":"  The growing renewable energy sources have posed significant challenges to\ntraditional power scheduling. It is difficult for operators to obtain accurate\nday-ahead forecasts of renewable generation, thereby requiring the future\nscheduling system to make real-time scheduling decisions aligning with\nultra-short-term forecasts. Restricted by the computation speed, traditional\noptimization-based methods can not solve this problem. Recent developments in\nreinforcement learning (RL) have demonstrated the potential to solve this\nchallenge. However, the existing RL methods are inadequate in terms of\nconstraint complexity, algorithm performance, and environment fidelity. We are\nthe first to propose a systematic solution based on the state-of-the-art\nreinforcement learning algorithm and the real power grid environment. The\nproposed approach enables planning and finer time resolution adjustments of\npower generators, including unit commitment and economic dispatch, thus\nincreasing the grid's ability to admit more renewable energy. The well-trained\nscheduling agent significantly reduces renewable curtailment and load shedding,\nwhich are issues arising from traditional scheduling's reliance on inaccurate\nday-ahead forecasts. High-frequency control decisions exploit the existing\nunits' flexibility, reducing the power grid's dependence on hardware\ntransformations and saving investment and operating costs, as demonstrated in\nexperimental results. This research exhibits the potential of reinforcement\nlearning in promoting low-carbon and intelligent power systems and represents a\nsolid step toward sustainable electricity generation.\n","authors":["Shaohuai Liu","Jinbo Liu","Weirui Ye","Nan Yang","Guanglun Zhang","Haiwang Zhong","Chongqing Kang","Qirong Jiang","Xuri Song","Fangchun Di","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2303.05205v2.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2204.00497v3","updated":"2023-03-13T07:50:11Z","published":"2022-04-01T14:57:52Z","title":"Separate and conquer heuristic allows robust mining of contrast sets in\n  classification, regression, and survival data","summary":"  Identifying differences between groups is one of the most important knowledge\ndiscovery problems. The procedure, also known as contrast sets mining, is\napplied in a wide range of areas like medicine, industry, or economics.\n  In the paper we present RuleKit-CS, an algorithm for contrast set mining\nbased on separate and conquer - a well established heuristic for decision rule\ninduction. Multiple passes accompanied with an attribute penalization scheme\nprovide contrast sets describing same examples with different attributes,\ndistinguishing presented approach from the standard separate and conquer. The\nalgorithm was also generalized for regression and survival data allowing\nidentification of contrast sets whose label attribute/survival prognosis is\nconsistent with the label/prognosis for the predefined contrast groups. This\nfeature, not provided by the existing approaches, further extends the usability\nof RuleKit-CS.\n  Experiments on over 130 data sets from various areas and detailed analysis of\nselected cases confirmed RuleKit-CS to be a useful tool for discovering\ndifferences between defined groups. The algorithm was implemented as a part of\nthe RuleKit suite available at GitHub under GNU AGPL 3 licence\n(https://github.com/adaa-polsl/RuleKit).\n  Keywords: contrast sets, separate and conquer, regression, survival\n","authors":["Adam Gudyś","Marek Sikora","Łukasz Wróbel"],"pdf_url":"https://arxiv.org/pdf/2204.00497v3.pdf","comment":"36 pages, 6 figures, 3 tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2303.02045v2","updated":"2023-03-13T07:41:15Z","published":"2023-03-03T16:12:59Z","title":"Uncertainty Estimation by Fisher Information-based Evidential Deep\n  Learning","summary":"  Uncertainty estimation is a key factor that makes deep learning reliable in\npractical applications. Recently proposed evidential neural networks explicitly\naccount for different uncertainties by treating the network's outputs as\nevidence to parameterize the Dirichlet distribution, and achieve impressive\nperformance in uncertainty estimation. However, for high data uncertainty\nsamples but annotated with the one-hot label, the evidence-learning process for\nthose mislabeled classes is over-penalized and remains hindered. To address\nthis problem, we propose a novel method, Fisher Information-based Evidential\nDeep Learning ($\\mathcal{I}$-EDL). In particular, we introduce Fisher\nInformation Matrix (FIM) to measure the informativeness of evidence carried by\neach sample, according to which we can dynamically reweight the objective loss\nterms to make the network more focused on the representation learning of\nuncertain classes. The generalization ability of our network is further\nimproved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our\nproposed method consistently outperforms traditional EDL-related algorithms in\nmultiple uncertainty estimation tasks, especially in the more challenging\nfew-shot classification settings.\n","authors":["Danruo Deng","Guangyong Chen","Yang Yu","Furui Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.02045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05492v2","updated":"2023-03-13T07:40:08Z","published":"2022-12-11T12:37:31Z","title":"Client Selection for Federated Bayesian Learning","summary":"  Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric\ndistributed learning framework for federated Bayesian learning, where multiple\nclients jointly train a machine learning model by communicating a number of\nnon-random and interacting particles with the server. Since communication\nresources are limited, selecting the clients with most informative local\nlearning updates can improve the model convergence and communication\nefficiency. In this paper, we propose two selection schemes for DSVGD based on\nKernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive\nthe upper bound on the decrease of the global free energy per iteration for\nboth schemes, which is then minimized to speed up the model convergence. We\nevaluate and compare our schemes with conventional schemes in terms of model\naccuracy, convergence speed, and stability using various learning tasks and\ndatasets.\n","authors":["Jiarong Yang","Yuan Liu","Rahif Kassab"],"pdf_url":"https://arxiv.org/pdf/2212.05492v2.pdf","comment":"To appear in IEEE Journal on Selected Areas in Communications Special\n  Issue on Communication-Efficient Distributed Learning over Networks"},{"id":"http://arxiv.org/abs/2208.12506v3","updated":"2023-03-13T07:35:42Z","published":"2022-08-26T08:56:33Z","title":"EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning","summary":"  The standard diagnostic procedures for targeted therapies in lung cancer\ntreatment involve histological subtyping and subsequent detection of key driver\nmutations, such as EGFR. Even though molecular profiling can uncover the driver\nmutation, the process is often expensive and time-consuming. Deep\nlearning-oriented image analysis offers a more economical alternative for\ndiscovering driver mutations directly from whole slide images (WSIs). In this\nwork, we used customized deep learning pipelines with weak supervision to\nidentify the morphological correlates of EGFR mutation from hematoxylin and\neosin-stained WSIs, in addition to detecting tumor and histologically subtyping\nit. We demonstrate the effectiveness of our pipeline by conducting rigorous\nexperiments and ablation studies on two lung cancer datasets - TCGA and a\nprivate dataset from India. With our pipeline, we achieved an average area\nunder the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological\nsubtyping between adenocarcinoma and squamous cell carcinoma on the TCGA\ndataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA\ndataset and 0.783 on the dataset from India. Our key learning points include\nthe following. Firstly, there is no particular advantage of using a feature\nextractor layers trained on histology, if one is going to fine-tune the feature\nextractor on the target dataset. Secondly, selecting patches with high\ncellularity, presumably capturing tumor regions, is not always helpful, as the\nsign of a disease class may be present in the tumor-adjacent stroma.\n","authors":["Ravi Kant Gupta","Shivani Nandgaonkar","Nikhil Cherian Kurian","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2208.12506v3.pdf","comment":"We need to improve"},{"id":"http://arxiv.org/abs/2303.06902v1","updated":"2023-03-13T07:32:37Z","published":"2023-03-13T07:32:37Z","title":"Molecular Property Prediction by Semantic-invariant Contrastive Learning","summary":"  Contrastive learning have been widely used as pretext tasks for\nself-supervised pre-trained molecular representation learning models in\nAI-aided drug design and discovery. However, exiting methods that generate\nmolecular views by noise-adding operations for contrastive learning may face\nthe semantic inconsistency problem, which leads to false positive pairs and\nconsequently poor prediction performance. To address this problem, in this\npaper we first propose a semantic-invariant view generation method by properly\nbreaking molecular graphs into fragment pairs. Then, we develop a\nFragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on\nthis view generation method for molecular property prediction. The FraSICL\nmodel consists of two branches to generate representations of views for\ncontrastive learning, meanwhile a multi-view fusion and an auxiliary similarity\nloss are introduced to make better use of the information contained in\ndifferent fragment-pair views. Extensive experiments on various benchmark\ndatasets show that with the least number of pre-training samples, FraSICL can\nachieve state-of-the-art performance, compared with major existing counterpart\nmodels.\n","authors":["Ziqiao Zhang","Ailin Xie","Jihong Guan","Shuigeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00395v2","updated":"2023-03-13T07:15:18Z","published":"2022-02-01T13:22:26Z","title":"Is the Performance of My Deep Network Too Good to Be True? A Direct\n  Approach to Estimating the Bayes Error in Binary Classification","summary":"  There is a fundamental limitation in the prediction performance that a\nmachine learning model can achieve due to the inevitable uncertainty of the\nprediction target. In classification problems, this can be characterized by the\nBayes error, which is the best achievable error with any classifier. The Bayes\nerror can be used as a criterion to evaluate classifiers with state-of-the-art\nperformance and can be used to detect test set overfitting. We propose a simple\nand direct Bayes error estimator, where we just take the mean of the labels\nthat show \\emph{uncertainty} of the class assignments. Our flexible approach\nenables us to perform Bayes error estimation even for weakly supervised data.\nIn contrast to others, our method is model-free and even instance-free.\nMoreover, it has no hyperparameters and gives a more accurate estimate of the\nBayes error than several baselines empirically. Experiments using our method\nsuggest that recently proposed deep networks such as the Vision Transformer may\nhave reached, or is about to reach, the Bayes error for benchmark datasets.\nFinally, we discuss how we can study the inherent difficulty of the\nacceptance/rejection decision for scientific articles, by estimating the Bayes\nerror of the ICLR papers from 2017 to 2023.\n","authors":["Takashi Ishida","Ikko Yamane","Nontawat Charoenphakdee","Gang Niu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2202.00395v2.pdf","comment":"ICLR 2023 (notable-top-5%)"},{"id":"http://arxiv.org/abs/2210.00215v4","updated":"2023-03-13T07:09:20Z","published":"2022-10-01T07:36:51Z","title":"Differentiable Parsing and Visual Grounding of Natural Language\n  Instructions for Object Placement","summary":"  We present a new method, PARsing And visual GrOuNding (ParaGon), for\ngrounding natural language in object placement tasks. Natural language\ngenerally describes objects and spatial relations with compositionality and\nambiguity, two major obstacles to effective language grounding. For\ncompositionality, ParaGon parses a language instruction into an object-centric\ngraph representation to ground objects individually. For ambiguity, ParaGon\nuses a novel particle-based graph neural network to reason about object\nplacements with uncertainty. Essentially, ParaGon integrates a parsing\nalgorithm into a probabilistic, data-driven learning framework. It is fully\ndifferentiable and trained end-to-end from data for robustness against complex,\nambiguous language input.\n","authors":["Zirui Zhao","Wee Sun Lee","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2210.00215v4.pdf","comment":"To appear in ICRA 2023"},{"id":"http://arxiv.org/abs/2110.10423v4","updated":"2023-03-13T06:18:52Z","published":"2021-10-20T08:18:16Z","title":"ProxyBO: Accelerating Neural Architecture Search via Bayesian\n  Optimization with Zero-cost Proxies","summary":"  Designing neural architectures requires immense manual efforts. This has\npromoted the development of neural architecture search (NAS) to automate the\ndesign. While previous NAS methods achieve promising results but run slowly,\nzero-cost proxies run extremely fast but are less promising. Therefore, it is\nof great potential to accelerate NAS via those zero-cost proxies. The existing\nmethod has two limitations, which are unforeseeable reliability and one-shot\nusage. To address the limitations, we present ProxyBO, an efficient Bayesian\noptimization (BO) framework that utilizes the zero-cost proxies to accelerate\nneural architecture search. We apply the generalization ability measurement to\nestimate the fitness of proxies on the task during each iteration and design a\nnovel acquisition function to combine BO with zero-cost proxies based on their\ndynamic influence. Extensive empirical studies show that ProxyBO consistently\noutperforms competitive baselines on five tasks from three public benchmarks.\nConcretely, ProxyBO achieves up to 5.41x and 3.86x speedups over the\nstate-of-the-art approaches REA and BRP-NAS.\n","authors":["Yu Shen","Yang Li","Jian Zheng","Wentao Zhang","Peng Yao","Jixiang Li","Sen Yang","Ji Liu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2110.10423v4.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.06879v1","updated":"2023-03-13T05:54:01Z","published":"2023-03-13T05:54:01Z","title":"Spacecraft Anomaly Detection with Attention Temporal Convolution Network","summary":"  Spacecraft faces various situations when carrying out exploration missions in\ncomplex space, thus monitoring the anomaly status of spacecraft is crucial to\nthe development of \\textcolor{blue}{the} aerospace industry. The time series\ntelemetry data generated by on-orbit spacecraft \\textcolor{blue}{contains}\nimportant information about the status of spacecraft. However, traditional\ndomain knowledge-based spacecraft anomaly detection methods are not effective\ndue to high dimensionality and complex correlation among variables. In this\nwork, we propose an anomaly detection framework for spacecraft multivariate\ntime-series data based on temporal convolution networks (TCNs). First, we\nemploy dynamic graph attention to model the complex correlation among variables\nand time series. Second, temporal convolution networks with parallel processing\nability are used to extract multidimensional \\textcolor{blue}{features} for\n\\textcolor{blue}{the} downstream prediction task. Finally, many potential\nanomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL\nspacecraft datasets show the superiority of our proposed model with respect to\nstate-of-the-art methods.\n","authors":["Liang Liu","Ling Tian","Zhao Kang","Tianqi Wan"],"pdf_url":"https://arxiv.org/pdf/2303.06879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03452v2","updated":"2023-03-13T05:52:05Z","published":"2021-12-07T02:28:12Z","title":"Location Leakage in Federated Signal Maps","summary":"  We consider the problem of predicting cellular network performance (signal\nmaps) from measurements collected by several mobile devices. We formulate the\nproblem within the online federated learning framework: (i) federated learning\n(FL) enables users to collaboratively train a model, while keeping their\ntraining data on their devices; (ii) measurements are collected as users move\naround over time and are used for local training in an online fashion. We\nconsider an honest-but-curious server, who observes the updates from target\nusers participating in FL and infers their location using a deep leakage from\ngradients (DLG) type of attack, originally developed to reconstruct training\ndata of DNN image classifiers. We make the key observation that a DLG attack,\napplied to our setting, infers the average location of a batch of local data,\nand can thus be used to reconstruct the target users' trajectory at a coarse\ngranularity. We build on this observation to protect location privacy, in our\nsetting, by revisiting and designing mechanisms within the federated learning\nframework including: tuning the FL parameters for averaging, curating local\nbatches so as to mislead the DLG attacker, and aggregating across multiple\nusers with different trajectories. We evaluate the performance of our\nalgorithms through both analysis and simulation based on real-world mobile\ndatasets, and we show that they achieve a good privacy-utility tradeoff.\n","authors":["Evita Bakopoulou","Jiang Zhang","Mengwei Yang","Konstantinos Psounis","Athina Markopoulou"],"pdf_url":"https://arxiv.org/pdf/2112.03452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06871v1","updated":"2023-03-13T05:42:58Z","published":"2023-03-13T05:42:58Z","title":"Physics-driven machine learning models coupling PyTorch and Firedrake","summary":"  Partial differential equations (PDEs) are central to describing and modelling\ncomplex physical systems that arise in many disciplines across science and\nengineering. However, in many realistic applications PDE modelling provides an\nincomplete description of the physics of interest. PDE-based machine learning\ntechniques are designed to address this limitation. In this approach, the PDE\nis used as an inductive bias enabling the coupled model to rely on fundamental\nphysical laws while requiring less training data. The deployment of\nhigh-performance simulations coupling PDEs and machine learning to complex\nproblems necessitates the composition of capabilities provided by machine\nlearning and PDE-based frameworks. We present a simple yet effective coupling\nbetween the machine learning framework PyTorch and the PDE system Firedrake\nthat provides researchers, engineers and domain specialists with a high\nproductive way of specifying coupled models while only requiring trivial\nchanges to existing code.\n","authors":["Nacime Bouziani","David A. Ham"],"pdf_url":"https://arxiv.org/pdf/2303.06871v1.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2303.06870v1","updated":"2023-03-13T05:37:46Z","published":"2023-03-13T05:37:46Z","title":"Three Guidelines You Should Know for Universally Slimmable\n  Self-Supervised Learning","summary":"  We propose universally slimmable self-supervised learning (dubbed as US3L) to\nachieve better accuracy-efficiency trade-offs for deploying self-supervised\nmodels across different devices. We observe that direct adaptation of\nself-supervised learning (SSL) to universally slimmable networks misbehaves as\nthe training process frequently collapses. We then discover that temporal\nconsistent guidance is the key to the success of SSL for universally slimmable\nnetworks, and we propose three guidelines for the loss design to ensure this\ntemporal consistency from a unified gradient perspective. Moreover, we propose\ndynamic sampling and group regularization strategies to simultaneously improve\ntraining efficiency and accuracy. Our US3L method has been empirically\nvalidated on both convolutional neural networks and vision transformers. With\nonly once training and one copy of weights, our method outperforms various\nstate-of-the-art methods (individually trained or not) on benchmarks including\nrecognition, object detection and instance segmentation. Our code is available\nat https://github.com/megvii-research/US3L-CVPR2023.\n","authors":["Yun-Hao Cao","Peiqin Sun","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06870v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06865v1","updated":"2023-03-13T05:19:28Z","published":"2023-03-13T05:19:28Z","title":"High-throughput Generative Inference of Large Language Models with a\n  Single GPU","summary":"  The high computational and memory requirements of large language model (LLM)\ninference traditionally make it feasible only with multiple high-end\naccelerators. Motivated by the emerging demand for latency-insensitive tasks\nwith batched processing, this paper initiates the study of high-throughput LLM\ninference using limited resources, such as a single commodity GPU. We present\nFlexGen, a high-throughput generation engine for running LLMs with limited GPU\nmemory. FlexGen can be flexibly configured under various hardware resource\nconstraints by aggregating memory and computation from the GPU, CPU, and disk.\nThrough a linear programming optimizer, it searches for efficient patterns to\nstore and access tensors. FlexGen further compresses these weights and the\nattention cache to 4 bits with negligible accuracy loss. These techniques\nenable FlexGen to have a larger space of batch size choices and thus\nsignificantly increase maximum throughput. As a result, when running OPT-175B\non a single 16GB GPU, FlexGen achieves significantly higher throughput compared\nto state-of-the-art offloading systems, reaching a generation throughput of 1\ntoken/s for the first time with an effective batch size of 144. On the HELM\nbenchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7\nrepresentative sub-scenarios in 21 hours. The code is available at\nhttps://github.com/FMInference/FlexGen\n","authors":["Ying Sheng","Lianmin Zheng","Binhang Yuan","Zhuohan Li","Max Ryabinin","Daniel Y. Fu","Zhiqiang Xie","Beidi Chen","Clark Barrett","Joseph E. Gonzalez","Percy Liang","Christopher Ré","Ion Stoica","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06856v1","updated":"2023-03-13T05:01:50Z","published":"2023-03-13T05:01:50Z","title":"Dynamic Neural Network for Multi-Task Learning Searching across Diverse\n  Network Topologies","summary":"  In this paper, we present a new MTL framework that searches for structures\noptimized for multiple tasks with diverse graph topologies and shares features\namong tasks. We design a restricted DAG-based central network with\nread-in/read-out layers to build topologically diverse task-adaptive structures\nwhile limiting search space and time. We search for a single optimized network\nthat serves as multiple task adaptive sub-networks using our three-stage\ntraining process. To make the network compact and discretized, we propose a\nflow-based reduction algorithm and a squeeze loss used in the training process.\nWe evaluate our optimized network on various public MTL datasets and show ours\nachieves state-of-the-art performance. An extensive ablation study\nexperimentally validates the effectiveness of the sub-module and schemes in our\nframework.\n","authors":["Wonhyeok Choi","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2303.06856v1.pdf","comment":"Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06851v1","updated":"2023-03-13T04:46:08Z","published":"2023-03-13T04:46:08Z","title":"On the Regret of Online Edge Service Hosting","summary":"  We consider the problem of service hosting where a service provider can\ndynamically rent edge resources via short term contracts to ensure better\nquality of service to its customers. The service can also be partially hosted\nat the edge, in which case, customers' requests can be partially served at the\nedge. The total cost incurred by the system is modeled as a combination of the\nrent cost, the service cost incurred due to latency in serving customers, and\nthe fetch cost incurred as a result of the bandwidth used to fetch the\ncode/databases of the service from the cloud servers to host the service at the\nedge. In this paper, we compare multiple hosting policies with regret as a\nmetric, defined as the difference in the cost incurred by the policy and the\noptimal policy over some time horizon $T$. In particular we consider the Retro\nRenting (RR) and Follow The Perturbed Leader (FTPL) policies proposed in the\nliterature and provide performance guarantees on the regret of these policies.\nWe show that under i.i.d stochastic arrivals, RR policy has linear regret while\nFTPL policy has constant regret. Next, we propose a variant of FTPL, namely\nWait then FTPL (W-FTPL), which also has constant regret while demonstrating\nmuch better dependence on the fetch cost. We also show that under adversarial\narrivals, RR policy has linear regret while both FTPL and W-FTPL have regret\n$\\mathrm{O}(\\sqrt{T})$ which is order-optimal.\n","authors":["R Sri Prakash","Nikhil Karamchandani","Sharayu Moharir"],"pdf_url":"https://arxiv.org/pdf/2303.06851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01970v5","updated":"2023-03-13T04:40:22Z","published":"2022-05-04T09:37:16Z","title":"Non-Stationary Bandit Learning via Predictive Sampling","summary":"  Thompson sampling has proven effective across a wide range of stationary\nbandit environments. However, as we demonstrate in this paper, it can perform\npoorly when applied to non-stationary environments. We show that such failures\nare attributed to the fact that, when exploring, the algorithm does not\ndifferentiate actions based on how quickly the information acquired loses its\nusefulness due to non-stationarity. Building upon this insight, we propose\npredictive sampling, an algorithm that deprioritizes acquiring information that\nquickly loses usefulness. Theoretical guarantee on the performance of\npredictive sampling is established through a Bayesian regret bound. We provide\nversions of predictive sampling for which computations tractably scale to\ncomplex bandit environments of practical interest. Through numerical\nsimulations, we demonstrate that predictive sampling outperforms Thompson\nsampling in all non-stationary environments examined.\n","authors":["Yueyang Liu","Benjamin Van Roy","Kuang Xu"],"pdf_url":"https://arxiv.org/pdf/2205.01970v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.02144v2","updated":"2023-03-13T04:38:12Z","published":"2020-08-05T14:05:37Z","title":"FRMDN: Flow-based Recurrent Mixture Density Network","summary":"  The class of recurrent mixture density networks is an important class of\nprobabilistic models used extensively in sequence modeling and\nsequence-to-sequence mapping applications. In this class of models, the density\nof a target sequence in each time-step is modeled by a Gaussian mixture model\nwith the parameters given by a recurrent neural network. In this paper, we\ngeneralize recurrent mixture density networks by defining a Gaussian mixture\nmodel on a non-linearly transformed target sequence in each time-step. The\nnon-linearly transformed space is created by normalizing flow. We observed that\nthis model significantly improves the fit to image sequences measured by the\nlog-likelihood. We also applied the proposed model on some speech and image\ndata, and observed that the model has significant modeling power outperforming\nother state-of-the-art methods in terms of the log-likelihood.\n","authors":["Seyedeh Fatemeh Razavi","Reshad Hosseini","Tina Behzad"],"pdf_url":"https://arxiv.org/pdf/2008.02144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06847v1","updated":"2023-03-13T04:31:35Z","published":"2023-03-13T04:31:35Z","title":"Label Distribution Learning from Logical Label","summary":"  Label distribution learning (LDL) is an effective method to predict the label\ndescription degree (a.k.a. label distribution) of a sample. However, annotating\nlabel distribution (LD) for training samples is extremely costly. So recent\nstudies often first use label enhancement (LE) to generate the estimated label\ndistribution from the logical label and then apply external LDL algorithms on\nthe recovered label distribution to predict the label distribution for unseen\nsamples. But this step-wise manner overlooks the possible connections between\nLE and LDL. Moreover, the existing LE approaches may assign some description\ndegrees to invalid labels. To solve the above problems, we propose a novel\nmethod to learn an LDL model directly from the logical label, which unifies LE\nand LDL into a joint model, and avoids the drawbacks of the previous LE\nmethods. Extensive experiments on various datasets prove that the proposed\napproach can construct a reliable LDL model directly from the logical label,\nand produce more accurate label distribution than the state-of-the-art LE\nmethods.\n","authors":["Yuheng Jia","Jiawei Tang","Jiahao Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.06847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.01198v4","updated":"2023-03-13T04:29:53Z","published":"2022-09-02T17:49:34Z","title":"Estimation of Correlation Matrices from Limited time series Data using\n  Machine Learning","summary":"  Correlation matrices contain a wide variety of spatio-temporal information\nabout a dynamical system. Predicting correlation matrices from partial time\nseries information of a few nodes characterizes the spatio-temporal dynamics of\nthe entire underlying system. This information can help to predict the\nunderlying network structure, e.g., inferring neuronal connections from spiking\ndata, deducing causal dependencies between genes from expression data, and\ndiscovering long spatial range influences in climate variations. Traditional\nmethods of predicting correlation matrices utilize time series data of all the\nnodes of the underlying networks. Here, we use a supervised machine learning\ntechnique to predict the correlation matrix of entire systems from finite time\nseries information of a few randomly selected nodes. The accuracy of the\nprediction validates that only a limited time series of a subset of the entire\nsystem is enough to make good correlation matrix predictions. Furthermore,\nusing an unsupervised learning algorithm, we furnish insights into the success\nof the predictions from our model. Finally, we employ the machine learning\nmodel developed here to real-world data sets.\n","authors":["Nikhil Easaw","Woo Seok Lee","Prashant Singh Lohiya","Sarika Jalan","Priodyuti Pradhan"],"pdf_url":"https://arxiv.org/pdf/2209.01198v4.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.06845v1","updated":"2023-03-13T04:21:33Z","published":"2023-03-13T04:21:33Z","title":"Transformer Encoder with Multiscale Deep Learning for Pain\n  Classification Using Physiological Signals","summary":"  Pain is a serious worldwide health problem that affects a vast proportion of\nthe population. For efficient pain management and treatment, accurate\nclassification and evaluation of pain severity are necessary. However, this can\nbe challenging as pain is a subjective sensation-driven experience. Traditional\ntechniques for measuring pain intensity, e.g. self-report scales, are\nsusceptible to bias and unreliable in some instances. Consequently, there is a\nneed for more objective and automatic pain intensity assessment strategies. In\nthis research, we develop PainAttnNet (PAN), a novel transfomer-encoder\ndeep-learning framework for classifying pain intensities with physiological\nsignals as input. The proposed approach is comprised of three feature\nextraction architectures: multiscale convolutional networks (MSCN), a\nsqueeze-and-excitation residual network (SEResNet), and a transformer encoder\nblock. On the basis of pain stimuli, MSCN extracts short- and long-window\ninformation as well as sequential features. SEResNet highlights relevant\nextracted features by mapping the interdependencies among features. The third\narchitecture employs a transformer encoder consisting of three temporal\nconvolutional networks (TCN) with three multi-head attention (MHA) layers to\nextract temporal dependencies from the features. Using the publicly available\nBioVid pain dataset, we test the proposed PainAttnNet model and demonstrate\nthat our outcomes outperform state-of-the-art models. These results confirm\nthat our approach can be utilized for automated classification of pain\nintensity using physiological signals to improve pain management and treatment.\n","authors":["Zhenyuan Lu","Burcu Ozek","Sagar Kamarthi"],"pdf_url":"https://arxiv.org/pdf/2303.06845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06836v1","updated":"2023-03-13T03:46:37Z","published":"2023-03-13T03:46:37Z","title":"Label Information Bottleneck for Label Enhancement","summary":"  In this work, we focus on the challenging problem of Label Enhancement (LE),\nwhich aims to exactly recover label distributions from logical labels, and\npresent a novel Label Information Bottleneck (LIB) method for LE. For the\nrecovery process of label distributions, the label irrelevant information\ncontained in the dataset may lead to unsatisfactory recovery performance. To\naddress this limitation, we make efforts to excavate the essential label\nrelevant information to improve the recovery performance. Our method formulates\nthe LE problem as the following two joint processes: 1) learning the\nrepresentation with the essential label relevant information, 2) recovering\nlabel distributions based on the learned representation. The label relevant\ninformation can be excavated based on the \"bottleneck\" formed by the learned\nrepresentation. Significantly, both the label relevant information about the\nlabel assignments and the label relevant information about the label gaps can\nbe explored in our method. Evaluation experiments conducted on several\nbenchmark label distribution learning datasets verify the effectiveness and\ncompetitiveness of LIB. Our source codes are available\n\"https://github.com/qinghai-zheng/LIBLE\"\n","authors":["Qinghai Zheng","Jihua Zhu","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06836v1.pdf","comment":"Accepted by CVPR 2023, our source codes are available at\n  \"https://github.com/qinghai-zheng/LIBLE\""},{"id":"http://arxiv.org/abs/2303.06833v1","updated":"2023-03-13T03:29:58Z","published":"2023-03-13T03:29:58Z","title":"Transformer-based Planning for Symbolic Regression","summary":"  Symbolic regression (SR) is a challenging task in machine learning that\ninvolves finding a mathematical expression for a function based on its values.\nRecent advancements in SR have demonstrated the efficacy of pretrained\ntransformer-based models for generating equations as sequences, which benefit\nfrom large-scale pretraining on synthetic datasets and offer considerable\nadvantages over GP-based methods in terms of inference time. However, these\nmodels focus on supervised pretraining goals borrowed from text generation and\nignore equation-specific objectives like accuracy and complexity. To address\nthis, we propose TPSR, a Transformer-based Planning strategy for Symbolic\nRegression that incorporates Monte Carlo Tree Search into the transformer\ndecoding process. TPSR, as opposed to conventional decoding strategies, allows\nfor the integration of non-differentiable feedback, such as fitting accuracy\nand complexity, as external sources of knowledge into the equation generation\nprocess. Extensive experiments on various datasets show that our approach\noutperforms state-of-the-art methods, enhancing the model's fitting-complexity\ntrade-off, extrapolation abilities, and robustness to noise. We also\ndemonstrate that the utilization of various caching mechanisms can further\nenhance the efficiency of TPSR.\n","authors":["Parshin Shojaee","Kazem Meidani","Amir Barati Farimani","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2303.06833v1.pdf","comment":"Parshin Shojaee and Kazem Meidani contributed equally to this work"},{"id":"http://arxiv.org/abs/2303.06832v1","updated":"2023-03-13T03:28:36Z","published":"2023-03-13T03:28:36Z","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","summary":"  ODIN is an innovative approach that addresses the problem of dataset\nconstraints by integrating generative AI models. Traditional zero-shot learning\nmethods are constrained by the training dataset. To fundamentally overcome this\nlimitation, ODIN attempts to mitigate the dataset constraints by generating\non-demand datasets based on user requirements. ODIN consists of three main\nmodules: a prompt generator, a text-to-image generator, and an image\npost-processor. To generate high-quality prompts and images, we adopted a large\nlanguage model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,\nStable Diffusion), respectively. We evaluated ODIN on various datasets in terms\nof model accuracy and data diversity to demonstrate its potential, and\nconducted post-experiments for further investigation. Overall, ODIN is a\nfeasible approach that enables Al to learn unseen knowledge beyond the training\ndataset.\n","authors":[" Spchoi","Jihoon Lee","HyeongSeok Ahn","Sanghee Jung","Bumsoo Kang"],"pdf_url":"https://arxiv.org/pdf/2303.06832v1.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2301.01217v3","updated":"2023-03-13T03:27:49Z","published":"2022-12-31T04:26:25Z","title":"Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples","summary":"  There is a growing interest in developing unlearnable examples (UEs) against\nvisual privacy leaks on the Internet. UEs are training samples added with\ninvisible but unlearnable noise, which have been found can prevent unauthorized\ntraining of machine learning models. UEs typically are generated via a bilevel\noptimization framework with a surrogate model to remove (minimize) errors from\nthe original samples, and then applied to protect the data against unknown\ntarget models. However, existing UE generation methods all rely on an ideal\nassumption called label-consistency, where the hackers and protectors are\nassumed to hold the same label for a given sample. In this work, we propose and\npromote a more practical label-agnostic setting, where the hackers may exploit\nthe protected data quite differently from the protectors. E.g., a m-class\nunlearnable dataset held by the protector may be exploited by the hacker as a\nn-class dataset. Existing UE generation methods are rendered ineffective in\nthis challenging setting. To tackle this challenge, we present a novel\ntechnique called Unlearnable Clusters (UCs) to generate label-agnostic\nunlearnable examples with cluster-wise perturbations. Furthermore, we propose\nto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the\nsurrogate model to improve the transferability of the crafted UCs to diverse\ndomains. We empirically verify the effectiveness of our proposed approach under\na variety of settings with different datasets, target models, and even\ncommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available\nat \\url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.\n","authors":["Jiaming Zhang","Xingjun Ma","Qi Yi","Jitao Sang","Yugang Jiang","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2301.01217v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2201.06837v3","updated":"2023-03-13T03:00:12Z","published":"2022-01-18T09:27:13Z","title":"Landslide Susceptibility Modeling by Interpretable Neural Network","summary":"  Landslides are notoriously difficult to predict because numerous spatially\nand temporally varying factors contribute to slope stability. Artificial neural\nnetworks (ANN) have been shown to improve prediction accuracy but are largely\nuninterpretable. Here we introduce an additive ANN optimization framework to\nassess landslide susceptibility, as well as dataset division and outcome\ninterpretation techniques. We refer to our approach, which features full\ninterpretability, high accuracy, high generalizability and low model\ncomplexity, as superposable neural network (SNN) optimization. We validate our\napproach by training models on landslide inventory from three different\neasternmost Himalaya regions. Our SNN outperformed physically-based and\nstatistical models and achieved similar performance to state-of-the-art deep\nneural networks. The SNN models found the product of slope and precipitation\nand hillslope aspect to be important primary contributors to high landslide\nsusceptibility, which highlights the importance of strong slope-climate\ncouplings, along with microclimates, on landslide occurrences.\n","authors":["Khaled Youssef","Kevin Shao","Seulgi Moon","Louis-Serge Bouchard"],"pdf_url":"https://arxiv.org/pdf/2201.06837v3.pdf","comment":"79 pages (including SI section); 8 main figures; 12 supplementary\n  figures; 9 supplementary tables"},{"id":"http://arxiv.org/abs/2303.06827v1","updated":"2023-03-13T03:00:03Z","published":"2023-03-13T03:00:03Z","title":"Kernel Density Bayesian Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning~(IRL) is a powerful framework to infer an\nagent's reward function by observing its behavior, but IRL algorithms that\nlearn point estimates of the reward function can be misleading because there\nmay be several functions that describe an agent's behavior equally well. A\nBayesian approach to IRL models a distribution over candidate reward functions,\nalleviating the shortcomings of learning a point estimate. However, several\nBayesian IRL algorithms use a $Q$-value function in place of the likelihood\nfunction. The resulting posterior is computationally intensive to calculate,\nhas few theoretical guarantees, and the $Q$-value function is often a poor\napproximation for the likelihood. We introduce kernel density Bayesian IRL\n(KD-BIRL), which uses conditional kernel density estimation to directly\napproximate the likelihood, providing an efficient framework that, with a\nmodified reward function parameterization, is applicable to environments with\ncomplex and infinite state spaces. We demonstrate KD-BIRL's benefits through a\nseries of experiments in Gridworld environments and a simulated sepsis\ntreatment task.\n","authors":["Aishwarya Mandyam","Didong Li","Diana Cai","Andrew Jones","Barbara E. Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2303.06827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06825v1","updated":"2023-03-13T02:50:59Z","published":"2023-03-13T02:50:59Z","title":"Best-of-three-worlds Analysis for Linear Bandits with\n  Follow-the-regularized-leader Algorithm","summary":"  The linear bandit problem has been studied for many years in both stochastic\nand adversarial settings. Designing an algorithm that can optimize the\nenvironment without knowing the loss type attracts lots of interest.\n\\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and\nthen switches between different algorithms specially designed for different\nsettings. However, such an approach requires meticulous designs to perform well\nin all settings. Follow-the-regularized-leader (FTRL) is another popular\nalgorithm type that can adapt to different environments. This algorithm is of\nsimple design and the regret bounds are shown to be optimal in traditional\nmulti-armed bandit problems compared with the detect-switch type algorithms.\nDesigning an FTRL-type algorithm for linear bandits is an important question\nthat has been open for a long time. In this paper, we prove that the FTRL-type\nalgorithm with a negative entropy regularizer can achieve the\nbest-of-three-world results for the linear bandit problem with the tacit\ncooperation between the choice of the learning rate and the specially designed\nself-bounding inequality.\n","authors":["Fang Kong","Canzhe Zhao","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2303.06825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00233v3","updated":"2023-03-13T02:49:38Z","published":"2022-06-01T04:57:50Z","title":"DM$^2$: Decentralized Multi-Agent Reinforcement Learning for\n  Distribution Matching","summary":"  Current approaches to multi-agent cooperation rely heavily on centralized\nmechanisms or explicit communication protocols to ensure convergence. This\npaper studies the problem of distributed multi-agent learning without resorting\nto centralized components or explicit communication. It examines the use of\ndistribution matching to facilitate the coordination of independent agents. In\nthe proposed scheme, each agent independently minimizes the distribution\nmismatch to the corresponding component of a target visitation distribution.\nThe theoretical analysis shows that under certain conditions, each agent\nminimizing its individual distribution mismatch allows the convergence to the\njoint policy that generated the target distribution. Further, if the target\ndistribution is from a joint policy that optimizes a cooperative task, the\noptimal policy for a combination of this task reward and the distribution\nmatching reward is the same joint policy. This insight is used to formulate a\npractical algorithm (DM$^2$), in which each individual agent matches a target\ndistribution derived from concurrently sampled trajectories from a joint expert\npolicy. Experimental validation on the StarCraft domain shows that combining\n(1) a task reward, and (2) a distribution matching reward for expert\ndemonstrations for the same task, allows agents to outperform a naive\ndistributed baseline. Additional experiments probe the conditions under which\nexpert demonstrations need to be sampled to obtain the learning benefits.\n","authors":["Caroline Wang","Ishan Durugkar","Elad Liebman","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2206.00233v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06818v1","updated":"2023-03-13T02:25:59Z","published":"2023-03-13T02:25:59Z","title":"Backdoor Defense via Deconfounded Representation Learning","summary":"  Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor\nattacks, where attackers embed hidden backdoors in the DNN model by injecting a\nfew poisoned examples into the training dataset. While extensive efforts have\nbeen made to detect and remove backdoors from backdoored DNNs, it is still not\nclear whether a backdoor-free clean model can be directly obtained from\npoisoned datasets. In this paper, we first construct a causal graph to model\nthe generation process of poisoned data and find that the backdoor attack acts\nas the confounder, which brings spurious associations between the input images\nand target labels, making the model predictions less reliable. Inspired by the\ncausal understanding, we propose the Causality-inspired Backdoor Defense (CBD),\nto learn deconfounded representations for reliable classification.\nSpecifically, a backdoored model is intentionally trained to capture the\nconfounding effects. The other clean model dedicates to capturing the desired\ncausal effects by minimizing the mutual information with the confounding\nrepresentations from the backdoored model and employing a sample-wise\nre-weighting scheme. Extensive experiments on multiple benchmark datasets\nagainst 6 state-of-the-art attacks verify that our proposed defense method is\neffective in reducing backdoor threats while maintaining high accuracy in\npredicting benign samples. Further analysis shows that CBD can also resist\npotential adaptive attacks. The code is available at\n\\url{https://github.com/zaixizhang/CBD}.\n","authors":["Zaixi Zhang","Qi Liu","Zhicai Wang","Zepu Lu","Qingyong Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06818v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06815v1","updated":"2023-03-13T02:14:42Z","published":"2023-03-13T02:14:42Z","title":"Provable Convergence of Tensor Decomposition-Based Neural Network\n  Training","summary":"  Advanced tensor decomposition, such as tensor train (TT), has been widely\nstudied for tensor decomposition-based neural network (NN) training, which is\none of the most common model compression methods. However, training NN with\ntensor decomposition always suffers significant accuracy loss and convergence\nissues. In this paper, a holistic framework is proposed for tensor\ndecomposition-based NN training by formulating TT decomposition-based NN\ntraining as a nonconvex optimization problem. This problem can be solved by the\nproposed tensor block coordinate descent (tenBCD) method, which is a\ngradient-free algorithm. The global convergence of tenBCD to a critical point\nat a rate of O(1/k) is established with the Kurdyka {\\L}ojasiewicz (K{\\L})\nproperty, where k is the number of iterations. The theoretical results can be\nextended to the popular residual neural networks (ResNets). The effectiveness\nand efficiency of our proposed framework are verified through an image\nclassification dataset, where our proposed method can converge efficiently in\ntraining and prevent overfitting.\n","authors":["Chenyang Li","Bo Shen"],"pdf_url":"https://arxiv.org/pdf/2303.06815v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2107.12422 by other authors"},{"id":"http://arxiv.org/abs/2209.14074v3","updated":"2023-03-13T02:00:41Z","published":"2022-09-28T13:15:03Z","title":"Recipro-CAM: Fast gradient-free visual explanations for convolutional\n  neural networks","summary":"  The Convolutional Neural Network (CNN) is a widely used deep learning\narchitecture for computer vision. However, its black box nature makes it\ndifficult to interpret the behavior of the model. To mitigate this issue, AI\npractitioners have explored explainable AI methods like Class Activation Map\n(CAM) and Grad-CAM. Although these methods have shown promise, they are limited\nby architectural constraints or the burden of gradient computing. To overcome\nthis issue, Score-CAM and Ablation-CAM have been proposed as gradient-free\nmethods, but they have longer execution times compared to CAM or Grad-CAM based\nmethods, making them unsuitable for real-world solution though they resolved\ngradient related issues and enabled inference mode XAI. To address this\nchallenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.\nOur approach involves spatially masking the extracted feature maps to exploit\nthe correlation between activation maps and network predictions for target\nclasses. Our proposed method has yielded promising results, outperforming\ncurrent state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)\nmetric by $1.78 \\%$ to $3.72 \\%$, excluding VGG-16 backbone. Moreover,\nRecipro-CAM generates saliency maps at a similar rate to Grad-CAM and is\napproximately $148$ times faster than Score-CAM. The source code for\nRecipro-CAM is available in our data analysis framework.\n","authors":["Seok-Yong Byun","Wonju Lee"],"pdf_url":"https://arxiv.org/pdf/2209.14074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14013v3","updated":"2023-03-13T01:32:16Z","published":"2023-02-27T18:12:56Z","title":"Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular\n  Data","summary":"  Recent progress in semi- and self-supervised learning has caused a rift in\nthe long-held belief about the need for an enormous amount of labeled data for\nmachine learning and the irrelevancy of unlabeled data. Although it has been\nsuccessful in various data, there is no dominant semi- and self-supervised\nlearning method that can be generalized for tabular data (i.e. most of the\nexisting methods require appropriate tabular datasets and architectures). In\nthis paper, we revisit self-training which can be applied to any kind of\nalgorithm including the most widely used architecture, gradient boosting\ndecision tree, and introduce curriculum pseudo-labeling (a state-of-the-art\npseudo-labeling technique in image) for a tabular domain. Furthermore, existing\npseudo-labeling techniques do not assure the cluster assumption when computing\nconfidence scores of pseudo-labels generated from unlabeled data. To overcome\nthis issue, we propose a novel pseudo-labeling approach that regularizes the\nconfidence scores based on the likelihoods of the pseudo-labels so that more\nreliable pseudo-labels which lie in high density regions can be obtained. We\nexhaustively validate the superiority of our approaches using various models\nand tabular datasets.\n","authors":["Minwook Kim","Juseong Kim","Giltae Song"],"pdf_url":"https://arxiv.org/pdf/2302.14013v3.pdf","comment":"10 pages for the main part and 8 extra pages for the appendix. 2\n  figures and 3 tables for the main part"},{"id":"http://arxiv.org/abs/2209.12307v2","updated":"2023-03-13T01:15:29Z","published":"2022-09-25T19:26:30Z","title":"On the Stability Analysis of Open Federated Learning Systems","summary":"  We consider the open federated learning (FL) systems, where clients may join\nand/or leave the system during the FL process. Given the variability of the\nnumber of present clients, convergence to a fixed model cannot be guaranteed in\nopen systems. Instead, we resort to a new performance metric that we term the\nstability of open FL systems, which quantifies the magnitude of the learned\nmodel in open systems. Under the assumption that local clients' functions are\nstrongly convex and smooth, we theoretically quantify the radius of stability\nfor two FL algorithms, namely local SGD and local Adam. We observe that this\nradius relies on several key parameters, including the function condition\nnumber as well as the variance of the stochastic gradient. Our theoretical\nresults are further verified by numerical simulations on both synthetic and\nreal-world benchmark data-sets.\n","authors":["Youbang Sun","Heshan Fernando","Tianyi Chen","Shahin Shahrampour"],"pdf_url":"https://arxiv.org/pdf/2209.12307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06799v1","updated":"2023-03-13T01:09:37Z","published":"2023-03-13T01:09:37Z","title":"Gaussian Process on the Product of Directional Manifolds","summary":"  We present a principled study on establishing Gaussian processes over\nvariables on the product of directional manifolds. As a basic functional\ncomponent, a manifold-adaptive kernel is presented based on the von Mises\ndistribution for Gaussian process regression on unit circles. Afterward, a\nnovel hypertoroidal von Mises kernel is introduced to enable topology-aware\nGaussian processes on hypertori with consideration of correlational circular\ncomponents. Based thereon, we enable multi-output regression for learning\nvector-valued functions on hypertori using intrinsic coregionalization model\nand provide analytical derivatives in hyperparameter optimization. The proposed\nmulti-output hypertoroidal Gaussian process is further embedded to a\ndata-driven recursive estimation scheme for learning unknown range sensing\nmodels of angle-of-arrival inputs. Evaluations on range-based localization show\nthat the proposed scheme enables superior tracking accuracy over parametric\nmodeling and common Gaussian processes.\n","authors":["Ziyu Cao","Kailai Li"],"pdf_url":"https://arxiv.org/pdf/2303.06799v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2303.07538v1","updated":"2023-03-13T23:49:09Z","published":"2023-03-13T23:49:09Z","title":"HiSSNet: Sound Event Detection and Speaker Identification via\n  Hierarchical Prototypical Networks for Low-Resource Headphones","summary":"  Modern noise-cancelling headphones have significantly improved users'\nauditory experiences by removing unwanted background noise, but they can also\nblock out sounds that matter to users. Machine learning (ML) models for sound\nevent detection (SED) and speaker identification (SID) can enable headphones to\nselectively pass through important sounds; however, implementing these models\nfor a user-centric experience presents several unique challenges. First, most\npeople spend limited time customizing their headphones, so the sound detection\nshould work reasonably well out of the box. Second, the models should be able\nto learn over time the specific sounds that are important to users based on\ntheir implicit and explicit interactions. Finally, such models should have a\nsmall memory footprint to run on low-power headphones with limited on-chip\nmemory. In this paper, we propose addressing these challenges using HiSSNet\n(Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model that\nuses a hierarchical prototypical network to detect both general and specific\nsounds of interest and characterize both alarm-like and speech sounds. We show\nthat HiSSNet outperforms an SEID model trained using non-hierarchical\nprototypical networks by 6.9 - 8.6 percent. When compared to state-of-the-art\n(SOTA) models trained specifically for SED or SID alone, HiSSNet achieves\nsimilar or better performance while reducing the memory footprint required to\nsupport multiple capabilities on-device.\n","authors":["N Shashaank","Berker Banar","Mohammad Rasool Izadi","Jeremy Kemmerer","Shuo Zhang"," Chuan-Che"," Huang"],"pdf_url":"https://arxiv.org/pdf/2303.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07537v1","updated":"2023-03-13T23:47:46Z","published":"2023-03-13T23:47:46Z","title":"Fractional dynamics foster deep learning of COPD stage prediction","summary":"  Chronic obstructive pulmonary disease (COPD) is one of the leading causes of\ndeath worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable\nbecause the test depends on an adequate effort from the tester and testee.\nMoreover, the early diagnosis of COPD is challenging. We address COPD detection\nby constructing two novel physiological signals datasets (4432 records from 54\npatients in the WestRo COPD dataset and 13824 medical records from 534 patients\nin the WestRo Porti COPD dataset). The authors demonstrate their complex\ncoupled fractal dynamical characteristics and perform a fractional-order\ndynamics deep learning analysis to diagnose COPD. The authors found that the\nfractional-order dynamical modeling can extract distinguishing signatures from\nthe physiological signals across patients with all COPD stages from stage 0\n(healthy) to stage 4 (very severe). They use the fractional signatures to\ndevelop and train a deep neural network that predicts COPD stages based on the\ninput features (such as thorax breathing effort, respiratory rate, or oxygen\nsaturation). The authors show that the fractional dynamic deep learning model\n(FDDLM) achieves a COPD prediction accuracy of 98.66% and can serve as a robust\nalternative to spirometry. The FDDLM also has high accuracy when validated on a\ndataset with different physiological signals.\n","authors":["Chenzhong Yin","Mihai Udrescu","Gaurav Gupta","Mingxi Cheng","Andrei Lihu","Lucretia Udrescu","Paul Bogdan","David M Mannino","Stefan Mihaicuta"],"pdf_url":"https://arxiv.org/pdf/2303.07537v1.pdf","comment":"Published on Advanced Science"},{"id":"http://arxiv.org/abs/2303.07535v1","updated":"2023-03-13T23:44:40Z","published":"2023-03-13T23:44:40Z","title":"Path Planning using Reinforcement Learning: A Policy Iteration Approach","summary":"  With the impact of real-time processing being realized in the recent past,\nthe need for efficient implementations of reinforcement learning algorithms has\nbeen on the rise. Albeit the numerous advantages of Bellman equations utilized\nin RL algorithms, they are not without the large search space of design\nparameters.\n  This research aims to shed light on the design space exploration associated\nwith reinforcement learning parameters, specifically that of Policy Iteration.\nGiven the large computational expenses of fine-tuning the parameters of\nreinforcement learning algorithms, we propose an auto-tuner-based ordinal\nregression approach to accelerate the process of exploring these parameters\nand, in return, accelerate convergence towards an optimal policy. Our approach\nprovides 1.82x peak speedup with an average of 1.48x speedup over the previous\nstate-of-the-art.\n","authors":["Saumil Shivdikar","Jagannath Nirmal"],"pdf_url":"https://arxiv.org/pdf/2303.07535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09127v3","updated":"2023-03-13T23:43:41Z","published":"2022-06-18T06:10:13Z","title":"Nonparametric Multi-shape Modeling with Uncertainty Quantification","summary":"  The modeling and uncertainty quantification of closed curves is an important\nproblem in the field of shape analysis, and can have significant ramifications\nfor subsequent statistical tasks. Many of these tasks involve collections of\nclosed curves, which often exhibit structural similarities at multiple levels.\nModeling multiple closed curves in a way that efficiently incorporates such\nbetween-curve dependence remains a challenging problem. In this work, we\npropose and investigate a multiple-output (a.k.a. multi-output),\nmulti-dimensional Gaussian process modeling framework. We illustrate the\nproposed methodological advances, and demonstrate the utility of meaningful\nuncertainty quantification, on several curve and shape-related tasks. This\nmodel-based approach not only addresses the problem of inference on closed\ncurves (and their shapes) with kernel constructions, but also opens doors to\nnonparametric modeling of multi-level dependence for functional objects in\ngeneral.\n","authors":["Hengrui Luo","Justin D. Strait"],"pdf_url":"https://arxiv.org/pdf/2206.09127v3.pdf","comment":"66 pages, 20 figures"},{"id":"http://arxiv.org/abs/2303.07527v1","updated":"2023-03-13T23:30:48Z","published":"2023-03-13T23:30:48Z","title":"Domain Generalization via Nuclear Norm Regularization","summary":"  The ability to generalize to unseen domains is crucial for machine learning\nsystems deployed in the real world, especially when we only have data from\nlimited training domains. In this paper, we propose a simple and effective\nregularization method based on the nuclear norm of the learned features for\ndomain generalization. Intuitively, the proposed regularizer mitigates the\nimpacts of environmental features and encourages learning domain-invariant\nfeatures. Theoretically, we provide insights into why nuclear norm\nregularization is more effective compared to ERM and alternative regularization\nmethods. Empirically, we conduct extensive experiments on both synthetic and\nreal datasets. We show that nuclear norm regularization achieves strong\nperformance compared to baselines in a wide range of domain generalization\ntasks. Moreover, our regularizer is broadly applicable with various methods\nsuch as ERM and SWAD with consistently improved performance, e.g., 1.7% and\n0.9% test accuracy improvements respectively on the DomainBed benchmark.\n","authors":["Zhenmei Shi","Yifei Ming","Ying Fan","Frederic Sala","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2303.07527v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.07525v1","updated":"2023-03-13T23:27:42Z","published":"2023-03-13T23:27:42Z","title":"Automated Vulnerability Detection in Source Code Using Quantum Natural\n  Language Processing","summary":"  One of the most important challenges in the field of software code audit is\nthe presence of vulnerabilities in software source code. These flaws are highly\nlikely ex-ploited and lead to system compromise, data leakage, or denial of\nser-vice. C and C++ open source code are now available in order to create a\nlarge-scale, classical machine-learning and quantum machine-learning system for\nfunction-level vulnerability identification. We assembled a siz-able dataset of\nmillions of open-source functions that point to poten-tial exploits. We created\nan efficient and scalable vulnerability detection method based on a deep neural\nnetwork model Long Short Term Memory (LSTM), and quantum machine learning model\nLong Short Term Memory (QLSTM), that can learn features extracted from the\nsource codes. The source code is first converted into a minimal intermediate\nrepresentation to remove the pointless components and shorten the de-pendency.\nTherefore, We keep the semantic and syntactic information using state of the\nart word embedding algorithms such as Glove and fastText. The embedded vectors\nare subsequently fed into the classical and quantum convolutional neural\nnetworks to classify the possible vulnerabilities. To measure the performance,\nwe used evaluation metrics such as F1 score, precision, re-call, accuracy, and\ntotal execution time. We made a comparison between the results derived from the\nclassical LSTM and quantum LSTM using basic feature representation as well as\nsemantic and syntactic represen-tation. We found that the QLSTM with semantic\nand syntactic features detects significantly accurate vulnerability and runs\nfaster than its classical counterpart.\n","authors":["Mst Shapna Akter","Hossain Shahriar","Zakirul Alam Bhuiya"],"pdf_url":"https://arxiv.org/pdf/2303.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07522v1","updated":"2023-03-13T23:17:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v1.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.07519v1","updated":"2023-03-13T23:11:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100\\%\nrate. Accuracy shows great improvement when scaling the models, with the\nlargest model (GPT-J) yielding impressive accuracy ranging between 25% to over\n80% for different prompt categories. We open source the finetuned Architext\nmodels and our synthetic dataset, hoping to inspire experimentation in this\nexciting area of design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.07347v1","updated":"2023-03-13T17:59:59Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v1.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2202.03861v4","updated":"2023-03-13T07:29:12Z","published":"2022-02-08T13:46:42Z","title":"Towards Making a Trojan-horse Attack on Text-to-Image Retrieval","summary":"  While deep learning based image retrieval is reported to be vulnerable to\nadversarial attacks, existing works are mainly on image-to-image retrieval with\ntheir attacks performed at the front end via query modification. By contrast,\nwe present in this paper the first study about a threat that occurs at the back\nend of a text-to-image retrieval (T2IR) system. Our study is motivated by the\nfact that the image collection indexed by the system will be regularly updated\ndue to the arrival of new images from various sources such as web crawlers and\nadvertisers. With malicious images indexed, it is possible for an attacker to\nindirectly interfere with the retrieval process, letting users see certain\nimages that are completely irrelevant w.r.t. their queries. We put this thought\ninto practice by proposing a novel Trojan-horse attack (THA). In particular, we\nconstruct a set of Trojan-horse images by first embedding word-specific\nadversarial information into a QR code and then putting the code on benign\nadvertising images. A proof-of-concept evaluation, conducted on two popular\nT2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed\nTHA in a white-box mode.\n","authors":["Fan Hu","Aozhu Chen","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2202.03861v4.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2206.07331v2","updated":"2023-03-13T06:52:04Z","published":"2022-06-15T07:26:27Z","title":"ETMA: Efficient Transformer Based Multilevel Attention framework for\n  Multimodal Fake News Detection","summary":"  In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.\n","authors":["Ashima Yadav","Shivani Gaba","Haneef Khan","Ishan Budhiraja","Akansha Singh","Krishan Kant Singh"],"pdf_url":"https://arxiv.org/pdf/2206.07331v2.pdf","comment":"Accepted for publication in IEEE Transactions on Computational Social\n  Systems"},{"id":"http://arxiv.org/abs/2303.06859v1","updated":"2023-03-13T05:04:18Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Casual-IRDIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07523v1","updated":"2023-03-13T23:18:17Z","published":"2023-03-13T23:18:17Z","title":"Investigating the Characteristics and Performance of Augmented Reality\n  Applications on Head-Mounted Displays: A Study of the Hololens Application\n  Store","summary":"  Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained\nsignificant traction over the recent years. Nevertheless, it remains unclear\nwhat AR HMD-based applications have been developed over the years and what\ntheir system performance is when they are run on HMDs. In this paper, we aim to\nshed light into this direction. Our study focuses on the applications available\non the Microsoft Hololens application store given the wide use of the Hololens\nheadset. Our study has two major parts: (i) we collect metadata about the\napplications available on the Microsoft Hololens application store to\nunderstand their characteristics (e.g., categories, pricing, permissions\nrequested, hardware and software compatibility); and (ii) we interact with\nthese applications while running on a Hololens 2 headset and collect data about\nsystems-related metrics (e.g., memory and storage usage, time spent on CPU and\nGPU related operations) to investigate the systems performance of applications.\nOur study has resulted in several interesting findings, which we share with the\nresearch community.\n","authors":["Pubudu Wijesooriya","Sheikh Muhammad Farjad","Nikolaos Stergiou","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2303.07523v1.pdf","comment":"This paper has been accepted for publication by IEEE ICC workshops\n  2023"}]},"2023-03-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.08127v1","updated":"2023-03-14T17:57:06Z","published":"2023-03-14T17:57:06Z","title":"CB2: Collaborative Natural Language Interaction Research Platform","summary":"  CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n","authors":["Jacob Sharf","Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2303.08127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08117v1","updated":"2023-03-14T17:49:50Z","published":"2023-03-14T17:49:50Z","title":"Do Transformers Parse while Predicting the Masked Word?","summary":"  Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.\n","authors":["Haoyu Zhao","Abhishek Panigrahi","Rong Ge","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2303.08117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08114v1","updated":"2023-03-14T17:47:25Z","published":"2023-03-14T17:47:25Z","title":"Simfluence: Modeling the Influence of Individual Training Examples by\n  Simulating Training Runs","summary":"  Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.\n","authors":["Kelvin Guu","Albert Webson","Ellie Pavlick","Lucas Dixon","Ian Tenney","Tolga Bolukbasi"],"pdf_url":"https://arxiv.org/pdf/2303.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07247v2","updated":"2023-03-14T17:40:21Z","published":"2023-03-13T16:20:33Z","title":"Are Models Trained on Indian Legal Data Fair?","summary":"  Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.\n","authors":["Sahil Girhepuje","Anmol Goel","Gokul S Krishnan","Shreya Goyal","Satyendra Pandey","Ponnurangam Kumaraguru","Balaraman Ravindran"],"pdf_url":"https://arxiv.org/pdf/2303.07247v2.pdf","comment":"Presented at the Symposium on AI and Law (SAIL) 2023"},{"id":"http://arxiv.org/abs/2208.11175v2","updated":"2023-03-14T17:06:58Z","published":"2022-08-23T20:03:27Z","title":"Ordinal analysis of lexical patterns","summary":"  Words are fundamental linguistic units that connect thoughts and things\nthrough meaning. However, words do not appear independently in a text sequence.\nThe existence of syntactic rules induces correlations among neighboring words.\nUsing an ordinal pattern approach, we present an analysis of lexical\nstatistical connections for 11 major languages. We find that the diverse\nmanners that languages utilize to express word relations give rise to unique\npattern structural distributions. Furthermore, fluctuations of these pattern\ndistributions for a given language can allow us to determine both the\nhistorical period when the text was written and its author. Taken together, our\nresults emphasize the relevance of ordinal time series analysis in linguistic\ntypology, historical linguistics and stylometry.\n","authors":["David Sanchez","Luciano Zunino","Juan De Gregorio","Raul Toral","Claudio Mirasso"],"pdf_url":"https://arxiv.org/pdf/2208.11175v2.pdf","comment":"9 pages, 12 figures, 2 tables; v2: the section on universality has\n  been removed because previous results were affected by spurious correlations.\n  Published version"},{"id":"http://arxiv.org/abs/2303.07142v2","updated":"2023-03-14T17:01:59Z","published":"2023-03-13T14:09:53Z","title":"Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification","summary":"  This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n","authors":["Benjamin Clavié","Alexandru Ciceu","Frederick Naylor","Guillaume Soulié","Thomas Brightwell"],"pdf_url":"https://arxiv.org/pdf/2303.07142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08044v1","updated":"2023-03-14T16:23:23Z","published":"2023-03-14T16:23:23Z","title":"Happy-GLL: modular, reusable and complete top-down parsers for\n  parameterized nonterminals","summary":"  Parser generators and parser combinator libraries are the most popular tools\nfor producing parsers. Parser combinators use the host language to provide\nreusable components in the form of higher-order functions with parsers as\nparameters. Very few parser generators support this kind of reuse through\nabstraction and even fewer generate parsers that are as modular and reusable as\nthe parts of the grammar for which they are produced. This paper presents a\nstrategy for generating modular, reusable and complete top-down parsers from\nsyntax descriptions with parameterized nonterminals, based on the FUN-GLL\nvariant of the GLL algorithm.\n  The strategy is discussed and demonstrated as a novel back-end for the Happy\nparser generator. Happy grammars can contain `parameterized nonterminals' in\nwhich parameters abstract over grammar symbols, granting an abstraction\nmechanism to define reusable grammar operators. However, the existing Happy\nback-ends do not deliver on the full potential of parameterized nonterminals as\nparameterized nonterminals cannot be reused across grammars. Moreover, the\nparser generation process may fail to terminate or may result in exponentially\nlarge parsers generated in an exponential amount of time.\n  The GLL back-end presented in this paper implements parameterized\nnonterminals successfully by generating higher-order functions that resemble\nparser combinators, inheriting all the advantages of top-down parsing. The\nback-end is capable of generating parsers for the full class of context-free\ngrammars, generates parsers in linear time and generates parsers that find all\nderivations of the input string. To our knowledge, the presented GLL back-end\nmakes Happy the first parser generator that combines all these features.\n  This paper describes the translation procedure of the GLL back-end and\ncompares it to the LALR and GLR back-ends of Happy in several experiments.\n","authors":["L. Thomas van Binsbergen","Damian Frolich"],"pdf_url":"https://arxiv.org/pdf/2303.08044v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.08038v1","updated":"2023-03-14T16:17:55Z","published":"2023-03-14T16:17:55Z","title":"Progress Note Understanding -- Assessment and Plan Reasoning: Overview\n  of the 2022 N2C2 Track 3 Shared Task","summary":"  Daily progress notes are common types in the electronic health record (EHR)\nwhere healthcare providers document the patient's daily progress and treatment\nplans. The EHR is designed to document all the care provided to patients, but\nit also enables note bloat with extraneous information that distracts from the\ndiagnoses and treatment plans. Applications of natural language processing\n(NLP) in the EHR is a growing field with the majority of methods in information\nextraction. Few tasks use NLP methods for downstream diagnostic decision\nsupport. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:\nProgress Note Understanding - Assessment and Plan Reasoning as one step towards\na new suite of tasks. The Assessment and Plan Reasoning task focuses on the\nmost critical components of progress notes, Assessment and Plan subsections\nwhere health problems and diagnoses are contained. The goal of the task was to\ndevelop and evaluate NLP systems that automatically predict causal relations\nbetween the overall status of the patient contained in the Assessment section\nand its relation to each component of the Plan section which contains the\ndiagnoses and treatment plans. The goal of the task was to identify and\nprioritize diagnoses as the first steps in diagnostic decision support to find\nthe most relevant information in long documents like daily progress notes. We\npresent the results of 2022 n2c2 Track 3 and provide a description of the data,\nevaluation, participation and system performance.\n","authors":["Yanjun Gao","Dmitriy Dligach","Timothy Miller","Matthew M Churpek","Ozlem Uzuner","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2303.08038v1.pdf","comment":"To appear in Journal of Biomedical Informatics"},{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08032v1","updated":"2023-03-14T16:11:47Z","published":"2023-03-14T16:11:47Z","title":"BODEGA: Benchmark for Adversarial Example Generation in Credibility\n  Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we introduce BODEGA: a benchmark for testing both victim models\nand attack methods on four misinformation detection tasks in an evaluation\nframework designed to simulate real use-cases of content moderation. We also\nsystematically test the robustness of popular text classifiers against\navailable attacking techniques and discover that, indeed, in some cases barely\nsignificant changes in input text can mislead the models. We openly share the\nBODEGA code and data in hope of enhancing the comparability and replicability\nof further research in this area.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08021v1","updated":"2023-03-14T16:04:13Z","published":"2023-03-14T16:04:13Z","title":"Optimizing Deep Learning Model Parameters with the Bees Algorithm for\n  Improved Medical Text Classification","summary":"  This paper introduces a novel mechanism to obtain the optimal parameters of a\ndeep learning model using the Bees Algorithm, which is a recent promising swarm\nintelligence algorithm. The optimization problem is to maximize the accuracy of\nclassifying ailments based on medical text given the initial hyper-parameters\nto be adjusted throughout a definite number of iterations. Experiments included\ntwo different datasets: English and Arabic. The highest accuracy achieved is\n99.63% on the English dataset using Long Short-Term Memory (LSTM) along with\nthe Bees Algorithm, and 88% on the Arabic dataset using AraBERT.\n","authors":["Mai A. Shaaban","Mariam Kashkash","Maryam Alghfeli","Adham Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2303.08021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07992v1","updated":"2023-03-14T15:46:28Z","published":"2023-03-14T15:46:28Z","title":"Evaluation of ChatGPT as a Question Answering System for Answering\n  Complex Questions","summary":"  ChatGPT is a powerful large language model (LLM) that has made remarkable\nprogress in natural language understanding. Nevertheless, the performance and\nlimitations of the model still need to be extensively evaluated. As ChatGPT\ncovers resources such as Wikipedia and supports natural language question\nanswering, it has garnered attention as a potential replacement for traditional\nknowledge based question answering (KBQA) models. Complex question answering is\na challenge task of KBQA, which comprehensively tests the ability of models in\nsemantic parsing and reasoning. To assess the performance of ChatGPT as a\nquestion answering system (QAS) using its own knowledge, we present a framework\nthat evaluates its ability to answer complex questions. Our approach involves\ncategorizing the potential features of complex questions and describing each\ntest question with multiple labels to identify combinatorial reasoning.\nFollowing the black-box testing specifications of CheckList proposed by Ribeiro\net.al, we develop an evaluation method to measure the functionality and\nreliability of ChatGPT in reasoning for answering complex questions. We use the\nproposed framework to evaluate the performance of ChatGPT in question answering\non 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual\ndatasets, with a total of approximately 190,000 test cases. We compare the\nevaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common\nlong-term problems in LLMs. The dataset and code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.\n","authors":["Yiming Tan","Dehai Min","Yu Li","Wenbo Li","Nan Hu","Yongrui Chen","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07991v1","updated":"2023-03-14T15:45:35Z","published":"2023-03-14T15:45:35Z","title":"Finding the Needle in a Haystack: Unsupervised Rationale Extraction from\n  Long Text Classifiers","summary":"  Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.\n","authors":["Kamil Bujel","Andrew Caines","Helen Yannakoudakis","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2303.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12278v2","updated":"2023-03-14T15:27:08Z","published":"2022-09-25T17:54:59Z","title":"Neural inhibition during speech planning contributes to contrastive\n  hyperarticulation","summary":"  Previous work has demonstrated that words are hyperarticulated on dimensions\nof speech that differentiate them from a minimal pair competitor. This\nphenomenon has been termed contrastive hyperarticulation (CH). We present a\ndynamic neural field (DNF) model of voice onset time (VOT) planning that\nderives CH from an inhibitory influence of the minimal pair competitor during\nplanning. We test some predictions of the model with a novel experiment\ninvestigating CH of voiceless stop consonant VOT in pseudowords. The results\ndemonstrate a CH effect in pseudowords, consistent with a basis for the effect\nin the real-time planning and production of speech. The scope and magnitude of\nCH in pseudowords was reduced compared to CH in real words, consistent with a\nrole for interactive activation between lexical and phonological levels of\nplanning. We discuss the potential of our model to unify an apparently\ndisparate set of phenomena, from CH to phonological neighborhood effects to\nphonetic trace effects in speech errors.\n","authors":["Michael C. Stern","Jason A. Shaw"],"pdf_url":"https://arxiv.org/pdf/2209.12278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07971v1","updated":"2023-03-14T15:24:05Z","published":"2023-03-14T15:24:05Z","title":"A Theory of Emergent In-Context Learning as Implicit Structure Induction","summary":"  Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.\n","authors":["Michael Hahn","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2303.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07271v4","updated":"2023-03-14T14:53:37Z","published":"2022-06-15T03:18:56Z","title":"Human heuristics for AI-generated language are flawed","summary":"  Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems suggest words, complete\nsentences, or produce entire conversations. AI-generated language is often not\nidentified as such but presented as language written by humans, raising\nconcerns about novel forms of deception and manipulation. Here, we study how\nhumans discern whether verbal self-presentations, one of the most personal and\nconsequential forms of language, were generated by AI. In six experiments,\nparticipants (N = 4,600) were unable to detect self-presentations generated by\nstate-of-the-art AI language models in professional, hospitality, and dating\ncontexts. A computational analysis of language features shows that human\njudgments of AI-generated language are hindered by intuitive but flawed\nheuristics such as associating first-person pronouns, use of contractions, or\nfamily topics with human-written language. We experimentally demonstrate that\nthese heuristics make human judgment of AI-generated language predictable and\nmanipulable, allowing AI systems to produce text perceived as \"more human than\nhuman.\" We discuss solutions, such as AI accents, to reduce the deceptive\npotential of language generated by AI, limiting the subversion of human\nintuition.\n","authors":["Maurice Jakesch","Jeffrey Hancock","Mor Naaman"],"pdf_url":"https://arxiv.org/pdf/2206.07271v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03053v2","updated":"2023-03-14T14:31:05Z","published":"2022-11-06T07:53:19Z","title":"Suffix Retrieval-Augmented Language Modeling","summary":"  Causal language modeling (LM) uses word history to predict the next word.\nBERT, on the other hand, makes use of bi-directional word information in a\nsentence to predict words at masked positions. While BERT is effective in\nsequence encoding, it is non-causal by nature and is not designed for sequence\ngeneration. In this paper, we propose a novel language model, SUffix\nREtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual\neffect in an autoregressive manner. SUREALM employs an embedding retriever to\nsearch for training sentences in a data store that share similar word history\nduring sequence generation. In particular, the suffix portions of the retrieved\nsentences mimick the \"future\" context. We evaluated our proposed model on the\nDSTC9 spoken dialogue corpus and showed promising word perplexity reduction on\nthe validation and test set compared to competitive baselines.\n","authors":["Zecheng Wang","Yik-Cheung Tam"],"pdf_url":"https://arxiv.org/pdf/2211.03053v2.pdf","comment":"5 pages, 1 figure. Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2110.08895v4","updated":"2023-03-14T14:29:58Z","published":"2021-10-17T19:03:51Z","title":"DECAR: Deep Clustering for learning general-purpose Audio\n  Representations","summary":"  We introduce DECAR, a self-supervised pre-training approach for learning\ngeneral-purpose audio representations. Our system is based on clustering: it\nutilizes an offline clustering step to provide target labels that act as\npseudo-labels for solving a prediction task. We develop on top of recent\nadvances in self-supervised learning for computer vision and design a\nlightweight, easy-to-use self-supervised pre-training scheme. We pre-train\nDECAR embeddings on a balanced subset of the large-scale Audioset dataset and\ntransfer those representations to 9 downstream classification tasks, including\nspeech, music, animal sounds, and acoustic scenes. Furthermore, we conduct\nablation studies identifying key design choices and also make all our code and\npre-trained models publicly available.\n","authors":["Sreyan Ghosh","Sandesh V Katta","Ashish Seth","S. Umesh"],"pdf_url":"https://arxiv.org/pdf/2110.08895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07924v1","updated":"2023-03-14T14:10:16Z","published":"2023-03-14T14:10:16Z","title":"Improving Accented Speech Recognition with Multi-Domain Training","summary":"  Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.\n","authors":["Lucas Maison","Yannick Estève"],"pdf_url":"https://arxiv.org/pdf/2303.07924v1.pdf","comment":"5 pages, 2 figures. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07914v1","updated":"2023-03-14T13:56:36Z","published":"2023-03-14T13:56:36Z","title":"Adapting Offline Speech Translation Models for Streaming with\n  Future-Aware Distillation and Inference","summary":"  A popular approach to streaming speech translation is to employ a single\noffline model with a \\textit{wait-$k$} policy to support different latency\nrequirements, which is simpler than training multiple online models with\ndifferent latency constraints. However, there is a mismatch problem in using a\nmodel trained with complete utterances for streaming inference with partial\ninput. We demonstrate that speech representations extracted at the end of a\nstreaming input are significantly different from those extracted from a\ncomplete utterance. To address this issue, we propose a new approach called\nFuture-Aware Streaming Translation (FAST) that adapts an offline ST model for\nstreaming input. FAST includes a Future-Aware Inference (FAI) strategy that\nincorporates future context through a trainable masked embedding, and a\nFuture-Aware Distillation (FAD) framework that transfers future context from an\napproximation of full speech to streaming input. Our experiments on the MuST-C\nEnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs\nbetween translation quality and latency than strong baselines. Extensive\nanalyses suggest that our methods effectively alleviate the aforementioned\nmismatch problem between offline training and online inference.\n","authors":["Biao Fu","Kai Fan","Minpeng Liao","Zhongqiang Huang","Boxing Chen","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2303.07914v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2302.08399v5","updated":"2023-03-14T13:47:26Z","published":"2023-02-16T16:18:03Z","title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks","summary":"  Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n","authors":["Tomer Ullman"],"pdf_url":"https://arxiv.org/pdf/2302.08399v5.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07895v1","updated":"2023-03-14T13:28:39Z","published":"2023-03-14T13:28:39Z","title":"The Learnability of In-Context Learning","summary":"  In-context learning is a surprising and important phenomenon that emerged\nwhen modern language models were scaled to billions of learned parameters.\nWithout modifying a large language model's weights, it can be tuned to perform\nvarious downstream natural language tasks simply by including concatenated\ntraining examples of these tasks in its input. Though disruptive for many\npractical applications of large language models, this emergent learning\nparadigm is not well understood from a theoretical perspective. In this paper,\nwe propose a first-of-its-kind PAC based framework for in-context learnability,\nand use it to provide the first finite sample complexity results for the\nin-context learning setup. Our framework includes an initial pretraining phase,\nwhich fits a function to the pretraining distribution, and then a second\nin-context learning phase, which keeps this function constant and concatenates\ntraining examples of the downstream task in its input. We use our framework in\norder to prove that, under mild assumptions, when the pretraining distribution\nis a mixture of latent tasks (a model often considered for natural language\npretraining), these tasks can be efficiently learned via in-context learning,\neven though the model's weights are unchanged and the input significantly\ndiverges from the pretraining distribution. Our theoretical analysis reveals\nthat in this setting, in-context learning is more about identifying the task\nthan about learning it, a result which is in line with a series of recent\nempirical findings. We hope that the in-context learnability framework\npresented in this paper will facilitate future progress towards a deeper\nunderstanding of this important new learning paradigm.\n","authors":["Noam Wies","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2303.07895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07865v1","updated":"2023-03-14T12:56:47Z","published":"2023-03-14T12:56:47Z","title":"Geolocation Predicting of Tweets Using BERT-Based Models","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context.\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v1.pdf","comment":"27 pages, 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2301.00656v2","updated":"2023-03-14T12:23:33Z","published":"2022-12-12T05:55:07Z","title":"TriNet: stabilizing self-supervised learning from complete or slow\n  collapse on ASR","summary":"  Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n","authors":["Lixin Cao","Jun Wang","Ben Yang","Dan Su","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2301.00656v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07833v1","updated":"2023-03-14T12:15:52Z","published":"2023-03-14T12:15:52Z","title":"X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue\n  Generation","summary":"  In multi-turn dialogue generation, responses are not only related to the\ntopic and background of the context but also related to words and phrases in\nthe sentences of the context. However, currently widely used hierarchical\ndialog models solely rely on context representations from the utterance-level\nencoder, ignoring the sentence representations output by the word-level\nencoder. This inevitably results in a loss of information while decoding and\ngenerating. In this paper, we propose a new dialog model X-ReCoSa to tackle\nthis problem which aggregates multi-scale context information for hierarchical\ndialog models. Specifically, we divide the generation decoder into upper and\nlower parts, namely the intention part and the generation part. Firstly, the\nintention part takes context representations as input to generate the intention\nof the response. Then the generation part generates words depending on sentence\nrepresentations. Therefore, the hierarchical information has been fused into\nresponse generation. we conduct experiments on the English dataset DailyDialog.\nExperimental results exhibit that our method outperforms baseline models on\nboth automatic metric-based and human-based evaluations.\n","authors":["Danqin Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12529v2","updated":"2023-03-14T11:09:19Z","published":"2023-02-24T09:29:40Z","title":"Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph\n  Question Answering","summary":"  Knowledge graphs (KGs) have received increasing attention due to its wide\napplications on natural language processing. However, its use case on temporal\nquestion answering (QA) has not been well-explored. Most of existing methods\nare developed based on pre-trained language models, which might not be capable\nto learn \\emph{temporal-specific} presentations of entities in terms of\ntemporal KGQA task. To alleviate this problem, we propose a novel\n\\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion\nnetwork. Inspired by the step-by-step reasoning behavior of humans. For each\ngiven question, TMA first extracts the relevant concepts from the KG, and then\nfeeds them into a multiway adaptive module to produce a\n\\emph{temporal-specific} representation of the question. This representation\ncan be incorporated with the pre-trained KG embedding to generate the final\nprediction. Empirical results verify that the proposed model achieves better\nperformance than the state-of-the-art models in the benchmark dataset. Notably,\nthe Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex\nquestions are absolutely improved by 24\\% and 10\\% compared to the\nbest-performing baseline. Furthermore, we also show that TMA employing an\nadaptive fusion mechanism can provide interpretability by analyzing the\nproportion of information in question representations.\n","authors":["Yonghao Liu","Di Liang","Fang Fang","Sirui Wang","Wei Wu","Rui Jiang"],"pdf_url":"https://arxiv.org/pdf/2302.12529v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.16264v2","updated":"2023-03-14T11:08:18Z","published":"2022-10-28T16:52:48Z","title":"Efficient Speech Translation with Dynamic Latent Perceivers","summary":"  Transformers have been the dominant architecture for Speech Translation in\nrecent years, achieving significant improvements in translation quality. Since\nspeech signals are longer than their textual counterparts, and due to the\nquadratic complexity of the Transformer, a down-sampling step is essential for\nits adoption in Speech Translation. Instead, in this research, we propose to\nease the complexity by using a Perceiver encoder to map the speech inputs to a\nfixed-length latent representation. Furthermore, we introduce a novel way of\ntraining Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent\nspaces without any additional computational overhead. Speech-to-Text Perceivers\nwith DLA can match the performance of Transformer baselines across three\nlanguage pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to\nDLA at inference, and can be flexibly deployed with various computational\nbudgets, without significant drops in translation quality.\n","authors":["Ioannis Tsiamas","Gerard I. Gállego","José A. R. Fonollosa","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2210.16264v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2302.12530v2","updated":"2023-03-14T10:51:55Z","published":"2023-02-24T09:29:55Z","title":"Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts","summary":"  Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.\n","authors":["Chao Xue","Di Liang","Sirui Wang","Wei Wu","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.12530v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2110.03501v3","updated":"2023-03-14T10:30:51Z","published":"2021-10-07T14:37:06Z","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!","summary":"  Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.\n","authors":["Kimia Noorbakhsh","Modar Sulaiman","Mahdi Sharifi","Kallol Roy","Pooyan Jamshidi"],"pdf_url":"https://arxiv.org/pdf/2110.03501v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.07740v1","updated":"2023-03-14T09:36:42Z","published":"2023-03-14T09:36:42Z","title":"Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening","summary":"  Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.\n","authors":["Min Cao","Yang Bai","Jingyao Wang","Ziqiang Cao","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07740v1.pdf","comment":"11 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.07726v1","updated":"2023-03-14T09:15:51Z","published":"2023-03-14T09:15:51Z","title":"Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme\n  Conversion","summary":"  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.\n","authors":["Jungjun Kim","Changjin Han","Gyuhyeon Nam","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07726v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07711v1","updated":"2023-03-14T08:52:58Z","published":"2023-03-14T08:52:58Z","title":"Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised\n  Style Extractor and Hierarchical Modeling in Speech Synthesis","summary":"  Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesized speech of a target speaker's timbre. In most\nprevious methods, the synthesized fine-grained prosody features often represent\nthe source speaker's average style, similar to the one-to-many problem(i.e.,\nmultiple prosody variations correspond to the same text). In response to this\nproblem, a strength-controlled semi-supervised style extractor is proposed to\ndisentangle the style from content and timbre, improving the representation and\ninterpretability of the global style embedding, which can alleviate the\none-to-many mapping and data imbalance problems in prosody prediction. A\nhierarchical prosody predictor is proposed to improve prosody modeling. We find\nthat better style transfer can be achieved by using the source speaker's\nprosody features that are easily predicted. Additionally, a\nspeaker-transfer-wise cycle consistency loss is proposed to assist the model in\nlearning unseen style-timbre combinations during the training phase.\nExperimental results show that the method outperforms the baseline. We provide\na website with audio samples.\n","authors":["Chunyu Qiang","Peng Yang","Hao Che","Ying Zhang","Xiaorui Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07711v1.pdf","comment":"Accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2211.14769v3","updated":"2023-03-14T08:21:31Z","published":"2022-11-27T09:01:31Z","title":"Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied\n  Agents under Federated Learning","summary":"  Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n","authors":["Yunchao Zhang","Zonglin Di","Kaiwen Zhou","Cihang Xie","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.14769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12462v2","updated":"2023-03-14T08:11:26Z","published":"2022-05-25T03:21:27Z","title":"Improving CTC-based ASR Models with Gated Interlayer Collaboration","summary":"  The CTC-based automatic speech recognition (ASR) models without the external\nlanguage model usually lack the capacity to model conditional dependencies and\ntextual interactions. In this paper, we present a Gated Interlayer\nCollaboration (GIC) mechanism to improve the performance of CTC-based models,\nwhich introduces textual information into the model and thus relaxes the\nconditional independence assumption of CTC-based models. Specifically, we\nconsider the weighted sum of token embeddings as the textual representation for\neach position, where the position-specific weights are the softmax probability\ndistribution constructed via inter-layer auxiliary CTC losses. The textual\nrepresentations are then fused with acoustic features by developing a gate\nunit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the\nproposed method outperforms several strong baselines.\n","authors":["Yuting Yang","Yuke Li","Binbin Du"],"pdf_url":"https://arxiv.org/pdf/2205.12462v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07689v1","updated":"2023-03-14T08:04:38Z","published":"2023-03-14T08:04:38Z","title":"Dual-Attention Model for Aspect-Level Sentiment Classification","summary":"  I propose a novel dual-attention model(DAM) for aspect-level sentiment\nclassification. Many methods have been proposed, such as support vector\nmachines for artificial design features, long short-term memory networks based\non attention mechanisms, and graph neural networks based on dependency parsing.\nWhile these methods all have decent performance, I think they all miss one\nimportant piece of syntactic information: dependency labels. Based on this\nidea, this paper proposes a model using dependency labels for the attention\nmechanism to do this task. We evaluate the proposed approach on three datasets:\nlaptop and restaurant are from SemEval 2014, and the last one is a twitter\ndataset. Experimental results show that the dual attention model has good\nperformance on all three datasets.\n","authors":["Mengfei Ye"],"pdf_url":"https://arxiv.org/pdf/2303.07689v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07687v1","updated":"2023-03-14T08:01:21Z","published":"2023-03-14T08:01:21Z","title":"Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy","summary":"  Because of predicting all the target tokens in parallel, the\nnon-autoregressive models greatly improve the decoding efficiency of speech\nrecognition compared with traditional autoregressive models. In this work, we\npresent dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross\nEntropy (AXE), finding the monotonic alignment that minimizes the cross-entropy\nloss through dynamic programming, (2) Dynamic Rectification, creating new\ntraining samples by replacing some masks with model predicted tokens. The AXE\nignores the absolute position alignment between prediction and ground truth\nsentence and focuses on tokens matching in relative order. The dynamic\nrectification method makes the model capable of simulating the non-mask but\npossible wrong tokens, even if they have high confidence. Our experiments on\nWSJ dataset demonstrated that not only AXE loss but also the rectification\nmethod could improve the WER performance of Mask CTC.\n","authors":["Xulong Zhang","Haobin Tang","Jianzong Wang","Ning Cheng","Jian Luo","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.07687v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07682v1","updated":"2023-03-14T07:53:19Z","published":"2023-03-14T07:53:19Z","title":"QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis","summary":"  Recent expressive text to speech (TTS) models focus on synthesizing emotional\nspeech, but some fine-grained styles such as intonation are neglected. In this\npaper, we propose QI-TTS which aims to better transfer and control intonation\nto further deliver the speaker's questioning intention while transferring\nemotion from reference speech. We propose a multi-style extractor to extract\nstyle embedding from two different levels. While the sentence level represents\nemotion, the final syllable level represents intonation. For fine-grained\nintonation control, we use relative attributes to represent intonation\nintensity at the syllable level.Experiments have validated the effectiveness of\nQI-TTS for improving intonation expressiveness in emotional speech synthesis.\n","authors":["Haobin Tang","Xulong Zhang","Jianzong Wang","Ning Cheng","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.07682v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07678v1","updated":"2023-03-14T07:27:30Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2211.04215v2","updated":"2023-03-14T07:15:08Z","published":"2022-11-08T13:01:17Z","title":"Active Relation Discovery: Towards General and Label-aware Open Relation\n  Extraction","summary":"  Open Relation Extraction (OpenRE) aims to discover novel relations from open\ndomains. Previous OpenRE methods mainly suffer from two problems: (1)\nInsufficient capacity to discriminate between known and novel relations. When\nextending conventional test settings to a more general setting where test data\nmight also come from seen classes, existing approaches have a significant\nperformance decline. (2) Secondary labeling must be performed before practical\napplication. Existing methods cannot label human-readable and meaningful types\nfor novel relations, which is urgently required by the downstream tasks. To\naddress these issues, we propose the Active Relation Discovery (ARD) framework,\nwhich utilizes relational outlier detection for discriminating known and novel\nrelations and involves active learning for labeling novel relations. Extensive\nexperiments on three real-world datasets show that ARD significantly\noutperforms previous state-of-the-art methods on both conventional and our\nproposed general OpenRE settings. The source code and datasets will be\navailable for reproducibility.\n","authors":["Yangning Li","Yinghui Li","Xi Chen","Hai-Tao Zheng","Ying Shen","Hong-Gee Kim"],"pdf_url":"https://arxiv.org/pdf/2211.04215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07665v1","updated":"2023-03-14T07:10:03Z","published":"2023-03-14T07:10:03Z","title":"RenewNAT: Renewing Potential Translation for Non-Autoregressive\n  Transformer","summary":"  Non-autoregressive neural machine translation (NAT) models are proposed to\naccelerate the inference process while maintaining relatively high performance.\nHowever, existing NAT models are difficult to achieve the desired\nefficiency-quality trade-off. For one thing, fully NAT models with efficient\ninference perform inferior to their autoregressive counterparts. For another,\niterative NAT models can, though, achieve comparable performance while\ndiminishing the advantage of speed. In this paper, we propose RenewNAT, a\nflexible framework with high efficiency and effectiveness, to incorporate the\nmerits of fully and iterative NAT models. RenewNAT first generates the\npotential translation results and then renews them in a single pass. It can\nachieve significant performance improvements at the same expense as traditional\nNAT models (without introducing additional model parameters and decoding\nlatency). Experimental results on various translation benchmarks (e.g.,\n\\textbf{4} WMT) show that our framework consistently improves the performance\nof strong fully NAT methods (e.g., GLAT and DSLP) without additional speed\noverhead.\n","authors":["Pei Guo","Yisheng Xiao","Juntao Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07665v1.pdf","comment":"Accepted by AAAI23"},{"id":"http://arxiv.org/abs/2303.07650v1","updated":"2023-03-14T06:34:18Z","published":"2023-03-14T06:34:18Z","title":"Cross-lingual Alzheimer's Disease detection based on paralinguistic and\n  pre-trained features","summary":"  We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,\nwhich aims to investigate which acoustic features can be generalized and\ntransferred across languages for Alzheimer's Disease (AD) prediction. The\nchallenge consists of two tasks: one is to classify the speech of AD patients\nand healthy individuals, and the other is to infer Mini Mental State\nExamination (MMSE) score based on speech only. The difficulty is mainly\nembodied in the mismatch of the dataset, in which the training set is in\nEnglish while the test set is in Greek. We extract paralinguistic features\nusing openSmile toolkit and acoustic features using XLSR-53. In addition, we\nextract linguistic features after transcribing the speech into text. These\nfeatures are used as indicators for AD detection in our method. Our method\nachieves an accuracy of 69.6% on the classification task and a root mean\nsquared error (RMSE) of 4.788 on the regression task. The results show that our\nproposed method is expected to achieve automatic multilingual Alzheimer's\nDisease detection through spontaneous speech.\n","authors":["Xuchu Chen","Yu Pu","Jinpeng Li","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07650v1.pdf","comment":"accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07624v1","updated":"2023-03-14T04:47:00Z","published":"2023-03-14T04:47:00Z","title":"I3D: Transformer architectures with input-dependent dynamic depth for\n  speech recognition","summary":"  Transformer-based end-to-end speech recognition has achieved great success.\nHowever, the large footprint and computational overhead make it difficult to\ndeploy these models in some real-world applications. Model compression\ntechniques can reduce the model size and speed up inference, but the compressed\nmodel has a fixed architecture which might be suboptimal. We propose a novel\nTransformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong\nperformance-efficiency trade-offs. With a similar number of layers at inference\ntime, I3D-based models outperform the vanilla Transformer and the static pruned\nmodel via iterative layer pruning. We also present interesting analysis on the\ngate probabilities and the input-dependency, which helps us better understand\ndeep encoders.\n","authors":["Yifan Peng","Jaesong Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2303.07624v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2101.06397v2","updated":"2023-03-14T04:18:42Z","published":"2021-01-16T08:12:03Z","title":"To Understand Representation of Layer-aware Sequence Encoders as\n  Multi-order-graph","summary":"  In this paper, we propose an explanation of representation for self-attention\nnetwork (SAN) based neural sequence encoders, which regards the information\ncaptured by the model and the encoding of the model as graph structure and the\ngeneration of these graph structures respectively. The proposed explanation\napplies to existing works on SAN-based models and can explain the relationship\namong the ability to capture the structural or linguistic information, depth of\nmodel, and length of sentence, and can also be extended to other models such as\nrecurrent neural network based models. We also propose a revisited multigraph\ncalled Multi-order-Graph (MoG) based on our explanation to model the graph\nstructures in the SAN-based model as subgraphs in MoG and convert the encoding\nof SAN-based model to the generation of MoG. Based on our explanation, we\nfurther introduce a Graph-Transformer by enhancing the ability to capture\nmultiple subgraphs of different orders and focusing on subgraphs of high\norders. Experimental results on multiple neural machine translation tasks show\nthat the Graph-Transformer can yield effective performance improvement.\n","authors":["Sufeng Duan","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2101.06397v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2009.07489"},{"id":"http://arxiv.org/abs/2303.07616v1","updated":"2023-03-14T03:49:22Z","published":"2023-03-14T03:49:22Z","title":"The Life Cycle of Knowledge in Big Language Models: A Survey","summary":"  Knowledge plays a critical role in artificial intelligence. Recently, the\nextensive success of pre-trained language models (PLMs) has raised significant\nattention about how knowledge can be acquired, maintained, updated and used by\nlanguage models. Despite the enormous amount of related studies, there still\nlacks a unified view of how knowledge circulates within language models\nthroughout the learning, tuning, and application processes, which may prevent\nus from further understanding the connections between current progress or\nrealizing existing limitations. In this survey, we revisit PLMs as\nknowledge-based systems by dividing the life circle of knowledge in PLMs into\nfive critical periods, and investigating how knowledge circulates when it is\nbuilt, maintained and used. To this end, we systematically review existing\nstudies of each period of the knowledge life cycle, summarize the main\nchallenges and current limitations, and discuss future directions.\n","authors":["Boxi Cao","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2303.07616v1.pdf","comment":"paperlist: https://github.com/c-box/KnowledgeLifecycle"},{"id":"http://arxiv.org/abs/2303.07610v1","updated":"2023-03-14T03:13:02Z","published":"2023-03-14T03:13:02Z","title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on\n  Consistency with Human Preferences","summary":"  As a natural language assistant, ChatGPT is capable of performing various\ntasks, including but not limited to article generation, code completion, and\ndata analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\nlevel of accuracy and reliability in terms of content evaluation, exhibiting\nthe capability of mimicking human preferences. To further explore ChatGPT's\npotential in this regard, a study is conducted to assess its ability to rank\ncontent. In order to do so, a test set consisting of prompts is created,\ncovering a wide range of use cases, and five models are utilized to generate\ncorresponding responses. ChatGPT is then instructed to rank the responses\ngenerated by these models. The results on the test set show that ChatGPT's\nranking preferences are consistent with human to a certain extent. This\npreliminary experimental finding implies that ChatGPT's zero-shot ranking\ncapability could be used to reduce annotation pressure in a number of ranking\ntasks.\n","authors":["Yunjie Ji","Yan Gong","Yiping Peng","Chao Ni","Peiyan Sun","Dongyu Pan","Baochang Ma","Xiangang Li"],"pdf_url":"https://arxiv.org/pdf/2303.07610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07585v1","updated":"2023-03-14T02:11:24Z","published":"2023-03-14T02:11:24Z","title":"Input-length-shortening and text generation via attention values","summary":"  Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.\n","authors":["Neşet Özkan Tan","Alex Yuxuan Peng","Joshua Bensemann","Qiming Bao","Tim Hartill","Mark Gahegan","Michael Witbrock"],"pdf_url":"https://arxiv.org/pdf/2303.07585v1.pdf","comment":"7 pages, 4 figures. AAAI23-EMC2"},{"id":"http://arxiv.org/abs/2303.07576v1","updated":"2023-03-14T01:53:49Z","published":"2023-03-14T01:53:49Z","title":"Diffusion Models in NLP: A Survey","summary":"  Diffusion models have become a powerful family of deep generative models,\nwith record-breaking performance in many applications. This paper first gives\nan overview and derivation of the basic theory of diffusion models, then\nreviews the research results of diffusion models in the field of natural\nlanguage processing, from text generation, text-driven image generation and\nother four aspects, and analyzes and summarizes the relevant literature\nmaterials sorted out, and finally records the experience and feelings of this\ntopic literature review research.\n","authors":["Yuansong Zhu","Yu Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.04831v2","updated":"2023-03-14T01:53:07Z","published":"2022-01-13T08:25:53Z","title":"Knowledge Graph Augmented Network Towards Multiview Representation\n  Learning for Aspect-based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment\nanalysis. To better comprehend long complicated sentences and obtain accurate\naspect-specific information, linguistic and commonsense knowledge are generally\nrequired in this task. However, most current methods employ complicated and\ninefficient approaches to incorporate external knowledge, e.g., directly\nsearching the graph nodes. Additionally, the complementarity between external\nknowledge and linguistic information has not been thoroughly studied. To this\nend, we propose a knowledge graph augmented network KGAN, which aims to\neffectively incorporate external knowledge with explicitly syntactic and\ncontextual information. In particular, KGAN captures the sentiment feature\nrepresentations from multiple different perspectives, i.e., context-, syntax-\nand knowledge-based. First, KGAN learns the contextual and syntactic\nrepresentations in parallel to fully extract the semantic features. Then, KGAN\nintegrates the knowledge graphs into the embedding space, based on which the\naspect-specific knowledge representations are further obtained via an attention\nmechanism. Last, we propose a hierarchical fusion module to complement these\nmulti-view representations in a local-to-global manner. Extensive experiments\non five popular ABSA benchmarks demonstrate the effectiveness and robustness of\nour KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN\nachieves a new record of state-of-the-art performance among all datasets.\n","authors":["Qihuang Zhong","Liang Ding","Juhua Liu","Bo Du","Hua Jin","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2201.04831v2.pdf","comment":"Accepted by IEEE TKDE 2023"},{"id":"http://arxiv.org/abs/2208.02131v2","updated":"2023-03-14T23:51:53Z","published":"2022-08-03T15:11:01Z","title":"Masked Vision and Language Modeling for Multi-modal Representation\n  Learning","summary":"  In this paper, we study how to use masked signal modeling in vision and\nlanguage (V+L) representation learning. Instead of developing masked language\nmodeling (MLM) and masked image modeling (MIM) independently, we propose to\nbuild joint masked vision and language modeling, where the masked signal of one\nmodality is reconstructed with the help from another modality. This is\nmotivated by the nature of image-text paired data that both of the image and\nthe text convey almost the same information but in different formats. The\nmasked signal reconstruction of one modality conditioned on another modality\ncan also implicitly learn cross-modal alignment between language tokens and\nimage patches. Our experiments on various V+L tasks show that the proposed\nmethod, along with common V+L alignment losses, achieves state-of-the-art\nperformance in the regime of millions of pre-training data. Also, we\noutperforms the other competitors by a significant margin in limited data\nscenarios.\n","authors":["Gukyeong Kwon","Zhaowei Cai","Avinash Ravichandran","Erhan Bas","Rahul Bhotika","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2208.02131v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2023"},{"id":"http://arxiv.org/abs/2303.08268v1","updated":"2023-03-14T23:01:27Z","published":"2023-03-14T23:01:27Z","title":"Chat with the Environment: Interactive Multimodal Perception using Large\n  Language Models","summary":"  Programming robot behaviour in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in zero-shot robotic planning. However, it remains challenging to\nground LLMs in multimodal sensory input and continuous action output, while\nenabling a robot to interact with its environment and acquire novel information\nas its policies unfold. We develop a robot interaction scenario with a\npartially observable state, which necessitates a robot to decide on a range of\nepistemic actions in order to sample sensory information among multiple\nmodalities, before being able to execute the task correctly. An interactive\nperception framework is therefore proposed with an LLM as its backbone, whose\nability is exploited to instruct epistemic actions and to reason over the\nresulting multimodal sensations (vision, sound, haptics, proprioception), as\nwell as to plan an entire task execution based on the interactively acquired\ninformation. Our study demonstrates that LLMs can provide high-level planning\nand reasoning skills and control interactive robot behaviour in a multimodal\nenvironment, while multimodal modules with the context of the environmental\nstate help ground the LLMs and extend their processing ability.\n","authors":["Xufeng Zhao","Mengdi Li","Cornelius Weber","Muhammad Burhan Hafez","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2303.08268v1.pdf","comment":"See website at https://xf-zhao.github.io/projects/Matcha/"},{"id":"http://arxiv.org/abs/2210.16554v2","updated":"2023-03-14T22:57:12Z","published":"2022-10-29T10:03:56Z","title":"End-to-end Spoken Language Understanding with Tree-constrained Pointer\n  Generator","summary":"  End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.\n","authors":["Guangzhi Sun","Chao Zhang","Philip C. Woodland"],"pdf_url":"https://arxiv.org/pdf/2210.16554v2.pdf","comment":"5 pages, to appear in ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08262v1","updated":"2023-03-14T22:37:31Z","published":"2023-03-14T22:37:31Z","title":"Clinical Concept and Relation Extraction Using Prompt-based Machine\n  Reading Comprehension","summary":"  Objective: To develop a natural language processing system that solves both\nclinical concept extraction and relation extraction in a unified prompt-based\nmachine reading comprehension (MRC) architecture with good generalizability for\ncross-institution applications.\n  Methods: We formulate both clinical concept extraction and relation\nextraction using a unified prompt-based MRC architecture and explore\nstate-of-the-art transformer models. We compare our MRC models with existing\ndeep learning models for concept extraction and end-to-end relation extraction\nusing two benchmark datasets developed by the 2018 National NLP Clinical\nChallenges (n2c2) challenge (medications and adverse drug events) and the 2022\nn2c2 challenge (relations of social determinants of health [SDoH]). We also\nevaluate the transfer learning ability of the proposed MRC models in a\ncross-institution setting. We perform error analyses and examine how different\nprompting strategies affect the performance of MRC models.\n  Results and Conclusion: The proposed MRC models achieve state-of-the-art\nperformance for clinical concept and relation extraction on the two benchmark\ndatasets, outperforming previous non-MRC transformer models. GatorTron-MRC\nachieves the best strict and lenient F1-scores for concept extraction,\noutperforming previous deep learning models on the two datasets by 1%~3% and\n0.7%~1.3%, respectively. For end-to-end relation extraction, GatorTron-MRC and\nBERT-MIMIC-MRC achieve the best F1-scores, outperforming previous deep learning\nmodels by 0.9%~2.4% and 10%-11%, respectively. For cross-institution\nevaluation, GatorTron-MRC outperforms traditional GatorTron by 6.4% and 16% for\nthe two datasets, respectively. The proposed method is better at handling\nnested/overlapped concepts, extracting relations, and has good portability for\ncross-institute applications.\n","authors":["Cheng Peng","Xi Yang","Zehao Yu","Jiang Bian","William R. Hogan","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2303.08262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08259v1","updated":"2023-03-14T22:22:28Z","published":"2023-03-14T22:22:28Z","title":"Contextualized Medication Information Extraction Using Transformer-based\n  Deep Learning Architectures","summary":"  Objective: To develop a natural language processing (NLP) system to extract\nmedications and contextual information that help understand drug changes. This\nproject is part of the 2022 n2c2 challenge.\n  Materials and methods: We developed NLP systems for medication mention\nextraction, event classification (indicating medication changes discussed or\nnot), and context classification to classify medication changes context into 5\northogonal dimensions related to drug changes. We explored 6 state-of-the-art\npretrained transformer models for the three subtasks, including GatorTron, a\nlarge language model pretrained using >90 billion words of text (including >80\nbillion words from >290 million clinical notes identified at the University of\nFlorida Health). We evaluated our NLP systems using annotated data and\nevaluation scripts provided by the 2022 n2c2 organizers.\n  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for\nmedication extraction (ranked 3rd), 0.9379 for event classification (ranked\n2nd), and the best micro-average accuracy of 0.9126 for context classification.\nGatorTron outperformed existing transformer models pretrained using smaller\ngeneral English text and clinical text corpora, indicating the advantage of\nlarge language models.\n  Conclusion: This study demonstrated the advantage of using large transformer\nmodels for contextual medication information extraction from clinical\nnarratives.\n","authors":["Aokun Chen","Zehao Yu","Xi Yang","Yi Guo","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2303.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13627v2","updated":"2023-03-14T21:47:37Z","published":"2022-09-27T18:44:41Z","title":"How GPT-3 responds to different publics on climate change and Black\n  Lives Matter: A critical appraisal of equity in conversational AI","summary":"  Autoregressive language models, which use deep learning to produce human-like\ntexts, have become increasingly widespread. Such models are powering popular\nvirtual assistants in areas like smart health, finance, and autonomous driving.\nWhile the parameters of these large language models are improving, concerns\npersist that these models might not work equally for all subgroups in society.\nDespite growing discussions of AI fairness across disciplines, there lacks\nsystemic metrics to assess what equity means in dialogue systems and how to\nengage different populations in the assessment loop. Grounded in theories of\ndeliberative democracy and science and technology studies, this paper proposes\nan analytical framework for unpacking the meaning of equity in human-AI\ndialogues. Using this framework, we conducted an auditing study to examine how\nGPT-3 responded to different sub-populations on crucial science and social\ntopics: climate change and the Black Lives Matter (BLM) movement. Our corpus\nconsists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals\nwho vary in gender, race and ethnicity, education level, English as a first\nlanguage, and opinions toward the issues. We found a substantively worse user\nexperience with GPT-3 among the opinion and the education minority\nsubpopulations; however, these two groups achieved the largest knowledge gain,\nchanging attitudes toward supporting BLM and climate change efforts after the\nchat. We traced these user experience divides to conversational differences and\nfound that GPT-3 used more negative expressions when it responded to the\neducation and opinion minority groups, compared to its responses to the\nmajority groups. We discuss the implications of our findings for a deliberative\nconversational AI system that centralizes diversity, equity, and inclusion.\n","authors":["Kaiping Chen","Anqi Shao","Jirayu Burapacheep","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2209.13627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07274v2","updated":"2023-03-14T21:30:06Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08233v1","updated":"2023-03-14T20:59:04Z","published":"2023-03-14T20:59:04Z","title":"NL4Opt Competition: Formulating Optimization Problems Based on Their\n  Natural Language Descriptions","summary":"  The Natural Language for Optimization (NL4Opt) Competition was created to\ninvestigate methods of extracting the meaning and formulation of an\noptimization problem based on its text description. Specifically, the goal of\nthe competition is to increase the accessibility and usability of optimization\nsolvers by allowing non-experts to interface with them using natural language.\nWe separate this challenging goal into two sub-tasks: (1) recognize and label\nthe semantic entities that correspond to the components of the optimization\nproblem; (2) generate a meaning representation (i.e., a logical form) of the\nproblem from its detected problem entities. The first task aims to reduce\nambiguity by detecting and tagging the entities of the optimization problems.\nThe second task creates an intermediate representation of the linear\nprogramming (LP) problem that is converted into a format that can be used by\ncommercial solvers. In this report, we present the LP word problem dataset and\nshared tasks for the NeurIPS 2022 competition. Furthermore, we present the\nwinning solutions. Through this competition, we hope to bring interest towards\nthe development of novel machine learning applications and datasets for\noptimization modeling.\n","authors":["Rindranirina Ramamonjison","Timothy T. Yu","Raymond Li","Haley Li","Giuseppe Carenini","Bissan Ghaddar","Shiqi He","Mahdi Mostajabdaveh","Amin Banitalebi-Dehkordi","Zirui Zhou","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07322v2","updated":"2023-03-14T19:35:05Z","published":"2023-02-14T20:07:31Z","title":"TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language\n  Experiments","summary":"  The evidence is growing that machine and deep learning methods can learn the\nsubtle differences between the language produced by people with various forms\nof cognitive impairment such as dementia and cognitively healthy individuals.\nValuable public data repositories such as TalkBank have made it possible for\nresearchers in the computational community to join forces and learn from each\nother to make significant advances in this area. However, due to variability in\napproaches and data selection strategies used by various researchers, results\nobtained by different groups have been difficult to compare directly. In this\npaper, we present TRESTLE (\\textbf{T}oolkit for \\textbf{R}eproducible\n\\textbf{E}xecution of \\textbf{S}peech \\textbf{T}ext and \\textbf{L}anguage\n\\textbf{E}xperiments), an open source platform that focuses on two datasets\nfrom the TalkBank repository with dementia detection as an illustrative domain.\nSuccessfully deployed in the hackallenge (Hackathon/Challenge) of the\nInternational Workshop on Health Intelligence at AAAI 2022, TRESTLE provides a\nprecise digital blueprint of the data pre-processing and selection strategies\nthat can be reused via TRESTLE by other researchers seeking comparable results\nwith their peers and current state-of-the-art (SOTA) approaches.\n","authors":["Changye Li","Weizhe Xu","Trevor Cohen","Martin Michalowski","Serguei Pakhomov"],"pdf_url":"https://arxiv.org/pdf/2302.07322v2.pdf","comment":"Accepted at AMIA Informatics Summit"},{"id":"http://arxiv.org/abs/2210.16043v2","updated":"2023-03-14T19:31:10Z","published":"2022-10-28T10:26:46Z","title":"Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised\n  Speech Models","summary":"  Given the strong results of self-supervised models on various tasks, there\nhave been surprisingly few studies exploring self-supervised representations\nfor acoustic word embeddings (AWE), fixed-dimensional vectors representing\nvariable-length spoken word segments. In this work, we study several\npre-trained models and pooling methods for constructing AWEs with\nself-supervised representations. Owing to the contextualized nature of\nself-supervised representations, we hypothesize that simple pooling methods,\nsuch as averaging, might already be useful for constructing AWEs. When\nevaluating on a standard word discrimination task, we find that HuBERT\nrepresentations with mean-pooling rival the state of the art on English AWEs.\nMore surprisingly, despite being trained only on English, HuBERT\nrepresentations evaluated on Xitsonga, Mandarin, and French consistently\noutperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on\nEnglish).\n","authors":["Ramon Sanabria","Hao Tang","Sharon Goldwater"],"pdf_url":"https://arxiv.org/pdf/2210.16043v2.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08179v1","updated":"2023-03-14T18:58:08Z","published":"2023-03-14T18:58:08Z","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain","summary":"  This paper presents medBERT.de, a pre-trained German BERT model specifically\ndesigned for the German medical domain. The model has been trained on a large\ncorpus of 4.7 Million German medical documents and has been shown to achieve\nnew state-of-the-art performance on eight different medical benchmarks covering\na wide range of disciplines and medical document types. In addition to\nevaluating the overall performance of the model, this paper also conducts a\nmore in-depth analysis of its capabilities. We investigate the impact of data\ndeduplication on the model's performance, as well as the potential benefits of\nusing more efficient tokenization methods. Our results indicate that\ndomain-specific models such as medBERT.de are particularly useful for longer\ntexts, and that deduplication of training data does not necessarily lead to\nimproved performance. Furthermore, we found that efficient tokenization plays\nonly a minor role in improving model performance, and attribute most of the\nimproved performance to the large amount of training data. To encourage further\nresearch, the pre-trained model weights and new benchmarks based on\nradiological data are made publicly available for use by the scientific\ncommunity.\n","authors":["Keno K. Bressem","Jens-Michalis Papaioannou","Paul Grundmann","Florian Borchert","Lisa C. Adams","Leonhard Liu","Felix Busch","Lina Xu","Jan P. Loyen","Stefan M. Niehues","Moritz Augustin","Lennart Grosser","Marcus R. Makowski","Hugo JWL. Aerts","Alexander Löser"],"pdf_url":"https://arxiv.org/pdf/2303.08179v1.pdf","comment":"Keno K. Bressem and Jens-Michalis Papaioannou and Paul Grundmann\n  contributed equally"},{"id":"http://arxiv.org/abs/2302.02083v3","updated":"2023-03-14T18:49:26Z","published":"2023-02-04T03:50:01Z","title":"Theory of Mind May Have Spontaneously Emerged in Large Language Models","summary":"  Theory of mind (ToM), or the ability to impute unobservable mental states to\nothers, is central to human social interactions, communication, empathy,\nself-consciousness, and morality. We tested several language models using 40\nclassic false-belief tasks widely used to test ToM in humans. The models\npublished before 2020 showed virtually no ability to solve ToM tasks. Yet, the\nfirst version of GPT-3 (\"davinci-001\"), published in May 2020, solved about 40%\nof false-belief tasks-performance comparable with 3.5-year-old children. Its\nsecond version (\"davinci-002\"; January 2022) solved 70% of false-belief tasks,\nperformance comparable with six-year-olds. Its most recent version, GPT-3.5\n(\"davinci-003\"; November 2022), solved 90% of false-belief tasks, at the level\nof seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks\n(95%). These findings suggest that ToM-like ability (thus far considered to be\nuniquely human) may have spontaneously emerged as a byproduct of language\nmodels' improving language skills.\n","authors":["Michal Kosinski"],"pdf_url":"https://arxiv.org/pdf/2302.02083v3.pdf","comment":"TRY RUNNING ToM EXPERIMENTS ON YOUR OWN: The code and tasks used in\n  this study are available at Colab\n  (https://colab.research.google.com/drive/1zQKSDEhqEFcLCf5LuW--A-TGcAhF19hT).\n  Don't worry if you are not an expert coder, you should be able to run this\n  code with no-to-minimum Python skills. Or copy-paste the tasks to ChatGPT's\n  web interface"},{"id":"http://arxiv.org/abs/2303.08572v1","updated":"2023-03-14T13:54:11Z","published":"2023-03-14T13:54:11Z","title":"Distinguishing Cause from Effect on Categorical Data: The Uniform\n  Channel Model","summary":"  Distinguishing cause from effect using observations of a pair of random\nvariables is a core problem in causal discovery. Most approaches proposed for\nthis task, namely additive noise models (ANM), are only adequate for\nquantitative data. We propose a criterion to address the cause-effect problem\nwith categorical variables (living in sets with no meaningful order), inspired\nby seeing a conditional probability mass function (pmf) as a discrete\nmemoryless channel. We select as the most likely causal direction the one in\nwhich the conditional pmf is closer to a uniform channel (UC). The rationale is\nthat, in a UC, as in an ANM, the conditional entropy (of the effect given the\ncause) is independent of the cause distribution, in agreement with the\nprinciple of independence of cause and mechanism. Our approach, which we call\nthe uniform channel model (UCM), thus extends the ANM rationale to categorical\nvariables. To assess how close a conditional pmf (estimated from data) is to a\nUC, we use statistical testing, supported by a closed-form estimate of a UC\nchannel. On the theoretical front, we prove identifiability of the UCM and show\nits equivalence with a structural causal model with a low-cardinality exogenous\nvariable. Finally, the proposed method compares favorably with recent\nstate-of-the-art alternatives in experiments on synthetic, benchmark, and real\ndata.\n","authors":["Mário A. T. Figueiredo","Catarina A. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2303.08572v1.pdf","comment":"20 pages, 2 appendices"},{"id":"http://arxiv.org/abs/2302.12530v2","updated":"2023-03-14T10:51:55Z","published":"2023-02-24T09:29:55Z","title":"Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts","summary":"  Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.\n","authors":["Chao Xue","Di Liang","Sirui Wang","Wei Wu","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.12530v2.pdf","comment":"ICASSP 2023. arXiv admin note: text overlap with arXiv:2210.03454"},{"id":"http://arxiv.org/abs/2303.09455v1","updated":"2023-03-14T17:05:08Z","published":"2023-03-14T17:05:08Z","title":"Learning Cross-lingual Visual Speech Representations","summary":"  Cross-lingual self-supervised learning has been a growing research topic in\nthe last few years. However, current works only explored the use of audio\nsignals to create representations. In this work, we study cross-lingual\nself-supervised visual representation learning. We use the recently-proposed\nRaw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual\nmodel with unlabelled multilingual data, and then fine-tune the visual model on\nlabelled transcriptions. Our experiments show that: (1) multi-lingual models\nwith more data outperform monolingual ones, but, when keeping the amount of\ndata fixed, monolingual models tend to reach better performance; (2)\nmulti-lingual outperforms English-only pre-training; (3) using languages which\nare more similar yields better results; and (4) fine-tuning on unseen languages\nis competitive to using the target language in the pre-training set. We hope\nour study inspires future research on non-English-only speech representation\nlearning.\n","authors":["Andreas Zinonos","Alexandros Haliassos","Pingchuan Ma","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2303.09455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09446v1","updated":"2023-03-14T09:47:23Z","published":"2023-03-14T09:47:23Z","title":"Controlling High-Dimensional Data With Sparse Input","summary":"  We address the problem of human-in-the-loop control for generating\nhighly-structured data. This task is challenging because existing generative\nmodels lack an efficient interface through which users can modify the output.\nUsers have the option to either manually explore a non-interpretable latent\nspace, or to laboriously annotate the data with conditioning labels. To solve\nthis, we introduce a novel framework whereby an encoder maps a sparse, human\ninterpretable control space onto the latent space of a generative model. We\napply this framework to the task of controlling prosody in text-to-speech\nsynthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is\nspecifically designed to encode sparse prosodic features and output complete\nwaveforms. We show empirically that MICVAE displays desirable qualities of a\nsparse human-in-the-loop control mechanism: efficiency, robustness, and\nfaithfulness. With even a very small number of input values (~4), MICVAE\nenables users to improve the quality of the output significantly, in terms of\nlistener preference (4:1).\n","authors":["Dan Andrei Iliescu","Devang Savita Ram Mohan","Tian Huey Teh","Zack Hodari"],"pdf_url":"https://arxiv.org/pdf/2303.09446v1.pdf","comment":"11 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.08138v1","updated":"2023-03-14T17:59:59Z","published":"2023-03-14T17:59:59Z","title":"Diversity-Aware Meta Visual Prompting","summary":"  We present Diversity-Aware Meta Visual Prompting~(DAM-VP), an efficient and\neffective prompting method for transferring pre-trained models to downstream\ntasks with frozen backbone. A challenging issue in visual prompting is that\nimage datasets sometimes have a large data diversity whereas a per-dataset\ngeneric prompt can hardly handle the complex distribution shift toward the\noriginal pretraining data distribution properly. To address this issue, we\npropose a dataset Diversity-Aware prompting strategy whose initialization is\nrealized by a Meta-prompt. Specifically, we cluster the downstream dataset into\nsmall homogeneity subsets in a diversity-adaptive way, with each subset has its\nown prompt optimized separately. Such a divide-and-conquer design reduces the\noptimization difficulty greatly and significantly boosts the prompting\nperformance. Furthermore, all the prompts are initialized with a meta-prompt,\nwhich is learned across several datasets. It is a bootstrapped paradigm, with\nthe key observation that the prompting knowledge learned from previous datasets\ncould help the prompt to converge faster and perform better on a new dataset.\nDuring inference, we dynamically select a proper prompt for each input, based\non the feature distance between the input and each subset. Through extensive\nexperiments, our DAM-VP demonstrates superior efficiency and effectiveness,\nclearly surpassing previous prompting methods in a series of downstream\ndatasets for different pretraining models. Our code is available at:\n\\url{https://github.com/shikiw/DAM-VP}.\n","authors":["Qidong Huang","Xiaoyi Dong","Dongdong Chen","Weiming Zhang","Feifei Wang","Gang Hua","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2303.08138v1.pdf","comment":"CVPR2023, code is available at https://github.com/shikiw/DAM-VP"},{"id":"http://arxiv.org/abs/2303.08137v1","updated":"2023-03-14T17:59:47Z","published":"2023-03-14T17:59:47Z","title":"LayoutDM: Discrete Diffusion Model for Controllable Layout Generation","summary":"  Controllable layout generation aims at synthesizing plausible arrangement of\nelement bounding boxes with optional constraints, such as type or position of a\nspecific element. In this work, we try to solve a broad range of layout\ngeneration tasks in a single model that is based on discrete state-space\ndiffusion models. Our model, named LayoutDM, naturally handles the structured\nlayout data in the discrete representation and learns to progressively infer a\nnoiseless layout from the initial input, where we model the layout corruption\nprocess by modality-wise discrete diffusion. For conditional generation, we\npropose to inject layout constraints in the form of masking or logit adjustment\nduring inference. We show in the experiments that our LayoutDM successfully\ngenerates high-quality layouts and outperforms both task-specific and\ntask-agnostic baselines on several layout tasks.\n","authors":["Naoto Inoue","Kotaro Kikuchi","Edgar Simo-Serra","Mayu Otani","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.08137v1.pdf","comment":"To be published in CVPR2023, project page:\n  https://cyberagentailab.github.io/layout-dm/"},{"id":"http://arxiv.org/abs/2303.08134v1","updated":"2023-03-14T17:59:02Z","published":"2023-03-14T17:59:02Z","title":"Parameter is Not All You Need: Starting from Non-Parametric Networks for\n  3D Point Cloud Analysis","summary":"  We present a Non-parametric Network for 3D point cloud analysis, Point-NN,\nwhich consists of purely non-learnable components: farthest point sampling\n(FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric\nfunctions. Surprisingly, it performs well on various 3D tasks, requiring no\nparameters or training, and even surpasses existing fully trained models.\nStarting from this basic non-parametric model, we propose two extensions.\nFirst, Point-NN can serve as a base architectural framework to construct\nParametric Networks by simply inserting linear layers on top. Given the\nsuperior non-parametric foundation, the derived Point-PN exhibits a high\nperformance-efficiency trade-off with only a few learnable parameters. Second,\nPoint-NN can be regarded as a plug-and-play module for the already trained 3D\nmodels during inference. Point-NN captures the complementary geometric\nknowledge and enhances existing methods for different 3D benchmarks without\nre-training. We hope our work may cast a light on the community for\nunderstanding 3D point clouds with non-parametric methods. Code is available at\nhttps://github.com/ZrrSkywalker/Point-NN.\n","authors":["Renrui Zhang","Liuhui Wang","Yali Wang","Peng Gao","Hongsheng Li","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2303.08134v1.pdf","comment":"Accepted by CVPR 2023. Code is available at\n  https://github.com/ZrrSkywalker/Point-NN"},{"id":"http://arxiv.org/abs/2303.08133v1","updated":"2023-03-14T17:59:01Z","published":"2023-03-14T17:59:01Z","title":"MeshDiffusion: Score-based Generative 3D Mesh Modeling","summary":"  We consider the task of generating realistic 3D shapes, which is useful for a\nvariety of applications such as automatic scene generation and physical\nsimulation. Compared to other 3D representations like voxels and point clouds,\nmeshes are more desirable in practice, because (1) they enable easy and\narbitrary manipulation of shapes for relighting and simulation, and (2) they\ncan fully leverage the power of modern graphics pipelines which are mostly\noptimized for meshes. Previous scalable methods for generating meshes typically\nrely on sub-optimal post-processing, and they tend to produce overly-smooth or\nnoisy surfaces without fine-grained geometric details. To overcome these\nshortcomings, we take advantage of the graph structure of meshes and use a\nsimple yet very effective generative modeling method to generate 3D meshes.\nSpecifically, we represent meshes with deformable tetrahedral grids, and then\ntrain a diffusion model on this direct parametrization. We demonstrate the\neffectiveness of our model on multiple generative tasks.\n","authors":["Zhen Liu","Yao Feng","Michael J. Black","Derek Nowrouzezahrai","Liam Paull","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08133v1.pdf","comment":"Published in ICLR 2023 (Spotlight, Notable-top-25%)"},{"id":"http://arxiv.org/abs/2303.08132v1","updated":"2023-03-14T17:58:44Z","published":"2023-03-14T17:58:44Z","title":"InstMove: Instance Motion for Object-centric Video Segmentation","summary":"  Despite significant efforts, cutting-edge video segmentation methods still\nremain sensitive to occlusion and rapid movement, due to their reliance on the\nappearance of objects in the form of object embeddings, which are vulnerable to\nthese disturbances. A common solution is to use optical flow to provide motion\ninformation, but essentially it only considers pixel-level motion, which still\nrelies on appearance similarity and hence is often inaccurate under occlusion\nand fast movement. In this work, we study the instance-level motion and present\nInstMove, which stands for Instance Motion for Object-centric Video\nSegmentation. In comparison to pixel-wise motion, InstMove mainly relies on\ninstance-level motion information that is free from image feature embeddings,\nand features physical interpretations, making it more accurate and robust\ntoward occlusion and fast-moving objects. To better fit in with the video\nsegmentation tasks, InstMove uses instance masks to model the physical presence\nof an object and learns the dynamic model through a memory network to predict\nits position and shape in the next frame. With only a few lines of code,\nInstMove can be integrated into current SOTA methods for three different video\nsegmentation tasks and boost their performance. Specifically, we improve the\nprevious arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and\n4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects.\nThese results suggest that instance-level motion is robust and accurate, and\nhence serving as a powerful solution in complex scenarios for object-centric\nvideo segmentation.\n","authors":["Qihao Liu","Junfeng Wu","Yi Jiang","Xiang Bai","Alan Yuille","Song Bai"],"pdf_url":"https://arxiv.org/pdf/2303.08132v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08131v1","updated":"2023-03-14T17:58:34Z","published":"2023-03-14T17:58:34Z","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","summary":"  We present \\ourmodel{}, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, \\ourmodel{} beats the\nstate-of-the-art method for open-vocabulary instance and panoptic segmentation\nacross 5 datasets, and outperforms previous work for open-vocabulary detection\non LVIS and ODinW under similar settings. When transferred to specific tasks,\nour model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and\ninstance segmentation on ADE20K and Cityscapes.\n  Finally, we note that \\ourmodel{} is the first to explore the potential of\njoint training on segmentation and detection, and hope it can be received as a\nstrong baseline for developing a single model for both tasks in open world.\n","authors":["Hao Zhang","Feng Li","Xueyan Zou","Shilong Liu","Chunyuan Li","Jianfeng Gao","Jianwei Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08131v1.pdf","comment":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"id":"http://arxiv.org/abs/2303.08129v1","updated":"2023-03-14T17:58:03Z","published":"2023-03-14T17:58:03Z","title":"PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D\n  Object Detection","summary":"  Masked Autoencoders learn strong visual representations and achieve\nstate-of-the-art results in several independent modalities, yet very few works\nhave addressed their capabilities in multi-modality settings. In this work, we\nfocus on point cloud and RGB image data, two modalities that are often\npresented together in the real world, and explore their meaningful\ninteractions. To improve upon the cross-modal synergy in existing works, we\npropose PiMAE, a self-supervised pre-training framework that promotes 3D and 2D\ninteraction through three aspects. Specifically, we first notice the importance\nof masking strategies between the two sources and utilize a projection module\nto complementarily align the mask and visible tokens of the two modalities.\nThen, we utilize a well-crafted two-branch MAE pipeline with a novel shared\ndecoder to promote cross-modality interaction in the mask tokens. Finally, we\ndesign a unique cross-modal reconstruction module to enhance representation\nlearning for both modalities. Through extensive experiments performed on\nlarge-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we\ndiscover it is nontrivial to interactively learn point-image features, where we\ngreatly improve multiple 3D detectors, 2D detectors, and few-shot classifiers\nby 2.9%, 6.7%, and 2.4%, respectively. Code is available at\nhttps://github.com/BLVLab/PiMAE.\n","authors":["Anthony Chen","Kevin Zhang","Renrui Zhang","Zihan Wang","Yuheng Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08129v1.pdf","comment":"Accepted by CVPR2023. Code is available at\n  https://github.com/BLVLab/PiMAE"},{"id":"http://arxiv.org/abs/2303.08128v1","updated":"2023-03-14T17:57:47Z","published":"2023-03-14T17:57:47Z","title":"ViperGPT: Visual Inference via Python Execution for Reasoning","summary":"  Answering visual queries is a complex task that requires both visual\nprocessing and reasoning. End-to-end models, the dominant approach for this\ntask, do not explicitly differentiate between the two, limiting\ninterpretability and generalization. Learning modular programs presents a\npromising alternative, but has proven challenging due to the difficulty of\nlearning both the programs and modules simultaneously. We introduce ViperGPT, a\nframework that leverages code-generation models to compose vision-and-language\nmodels into subroutines to produce a result for any query. ViperGPT utilizes a\nprovided API to access the available modules, and composes them by generating\nPython code that is later executed. This simple approach requires no further\ntraining, and achieves state-of-the-art results across various complex visual\ntasks.\n","authors":["Dídac Surís","Sachit Menon","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2303.08128v1.pdf","comment":"Website: https://viper.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2303.08120v1","updated":"2023-03-14T17:52:29Z","published":"2023-03-14T17:52:29Z","title":"Blind Video Deflickering by Neural Filtering with a Flawed Atlas","summary":"  Many videos contain flickering artifacts. Common causes of flicker include\nvideo processing algorithms, video generation algorithms, and capturing videos\nunder specific situations. Prior work usually requires specific guidance such\nas the flickering frequency, manual annotations, or extra consistent videos to\nremove the flicker. In this work, we propose a general flicker removal\nframework that only receives a single flickering video as input without\nadditional guidance. Since it is blind to a specific flickering type or\nguidance, we name this \"blind deflickering.\" The core of our approach is\nutilizing the neural atlas in cooperation with a neural filtering strategy. The\nneural atlas is a unified representation for all frames in a video that\nprovides temporal consistency guidance but is flawed in many cases. To this\nend, a neural network is trained to mimic a filter to learn the consistent\nfeatures (e.g., color, brightness) and avoid introducing the artifacts in the\natlas. To validate our method, we construct a dataset that contains diverse\nreal-world flickering videos. Extensive experiments show that our method\nachieves satisfying deflickering performance and even outperforms baselines\nthat use extra guidance on a public benchmark.\n","authors":["Chenyang Lei","Xuanchi Ren","Zhaoxiang Zhang","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.08120v1.pdf","comment":"To appear in CVPR2023. Code:\n  github.com/ChenyangLEI/All-In-One-Deflicker Website:\n  chenyanglei.github.io/deflicker"},{"id":"http://arxiv.org/abs/2303.08113v1","updated":"2023-03-14T17:47:18Z","published":"2023-03-14T17:47:18Z","title":"Homeomorphic Image Registration via Conformal-Invariant Hyperelastic\n  Regularisation","summary":"  Deformable image registration is a fundamental task in medical image analysis\nand plays a crucial role in a wide range of clinical applications. Recently,\ndeep learning-based approaches have been widely studied for deformable medical\nimage registration and achieved promising results. However, existing deep\nlearning image registration techniques do not theoretically guarantee\ntopology-preserving transformations. This is a key property to preserve\nanatomical structures and achieve plausible transformations that can be used in\nreal clinical settings. We propose a novel framework for deformable image\nregistration. Firstly, we introduce a novel regulariser based on\nconformal-invariant properties in a nonlinear elasticity setting. Our\nregulariser enforces the deformation field to be smooth, invertible and\norientation-preserving. More importantly, we strictly guarantee topology\npreservation yielding to a clinical meaningful registration. Secondly, we boost\nthe performance of our regulariser through coordinate MLPs, where one can view\nthe to-be-registered images as continuously differentiable entities. We\ndemonstrate, through numerical and visual experiments, that our framework is\nable to outperform current techniques for image registration.\n","authors":["Jing Zou","Noémie Debroux","Lihao Liu","Jing Qin","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2303.08113v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.08096v1","updated":"2023-03-14T17:33:39Z","published":"2023-03-14T17:33:39Z","title":"MELON: NeRF with Unposed Images Using Equivalence Class Estimation","summary":"  Neural radiance fields enable novel-view synthesis and scene reconstruction\nwith photorealistic quality from a few images, but require known and accurate\ncamera poses. Conventional pose estimation algorithms fail on smooth or\nself-similar scenes, while methods performing inverse rendering from unposed\nviews require a rough initialization of the camera orientations. The main\ndifficulty of pose estimation lies in real-life objects being almost invariant\nunder certain transformations, making the photometric distance between rendered\nviews non-convex with respect to the camera parameters. Using an equivalence\nrelation that matches the distribution of local minima in camera space, we\nreduce this space to its quotient set, in which pose estimation becomes a more\nconvex problem. Using a neural-network to regularize pose estimation, we\ndemonstrate that our method - MELON - can reconstruct a neural radiance field\nfrom unposed images with state-of-the-art accuracy while requiring ten times\nfewer views than adversarial approaches.\n","authors":["Axel Levy","Mark Matthews","Matan Sela","Gordon Wetzstein","Dmitry Lagun"],"pdf_url":"https://arxiv.org/pdf/2303.08096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08085v1","updated":"2023-03-14T17:16:16Z","published":"2023-03-14T17:16:16Z","title":"Alias-Free Convnets: Fractional Shift Invariance via Polynomial\n  Activations","summary":"  Although CNNs are believed to be invariant to translations, recent works have\nshown this is not the case, due to aliasing effects that stem from downsampling\nlayers. The existing architectural solutions to prevent aliasing are partial\nsince they do not solve these effects, that originate in non-linearities. We\npropose an extended anti-aliasing method that tackles both downsampling and\nnon-linear layers, thus creating truly alias-free, shift-invariant CNNs. We\nshow that the presented model is invariant to integer as well as fractional\n(i.e., sub-pixel) translations, thus outperforming other shift-invariant\nmethods in terms of robustness to adversarial translations.\n","authors":["Hagay Michaeli","Tomer Michaeli","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2303.08085v1.pdf","comment":"https://github.com/hmichaeli/alias_free_convnets"},{"id":"http://arxiv.org/abs/2303.08084v1","updated":"2023-03-14T17:14:21Z","published":"2023-03-14T17:14:21Z","title":"Editing Implicit Assumptions in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models often make implicit assumptions about the\nworld when generating images. While some assumptions are useful (e.g., the sky\nis blue), they can also be outdated, incorrect, or reflective of social biases\npresent in the training data. Thus, there is a need to control these\nassumptions without requiring explicit user input or costly re-training. In\nthis work, we aim to edit a given implicit assumption in a pre-trained\ndiffusion model. Our Text-to-Image Model Editing method, TIME for short,\nreceives a pair of inputs: a \"source\" under-specified prompt for which the\nmodel makes an implicit assumption (e.g., \"a pack of roses\"), and a\n\"destination\" prompt that describes the same setting, but with a specified\ndesired attribute (e.g., \"a pack of blue roses\"). TIME then updates the model's\ncross-attention layers, as these layers assign visual meaning to textual\ntokens. We edit the projection matrices in these layers such that the source\nprompt is projected close to the destination prompt. Our method is highly\nefficient, as it modifies a mere 2.2% of the model's parameters in under one\nsecond. To evaluate model editing approaches, we introduce TIMED (TIME\nDataset), containing 147 source and destination prompt pairs from various\ndomains. Our experiments (using Stable Diffusion) show that TIME is successful\nin model editing, generalizes well for related prompts unseen during editing,\nand imposes minimal effect on unrelated generations.\n","authors":["Hadas Orgad","Bahjat Kawar","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2303.08084v1.pdf","comment":"Project page: https://time-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2303.03052v2","updated":"2023-03-14T17:04:49Z","published":"2023-03-06T11:51:28Z","title":"Masked Images Are Counterfactual Samples for Robust Fine-tuning","summary":"  Deep learning models are challenged by the distribution shift between the\ntraining data and test data. Recently, the large models pre-trained on diverse\ndata demonstrate unprecedented robustness to various distribution shifts.\nHowever, fine-tuning on these models can lead to a trade-off between\nin-distribution (ID) performance and out-of-distribution (OOD) robustness.\nExisting methods for tackling this trade-off do not explicitly address the OOD\nrobustness problem. In this paper, based on causal analysis on the\naforementioned problems, we propose a novel fine-tuning method, which use\nmasked images as counterfactual samples that help improving the robustness of\nthe fine-tuning model. Specifically, we mask either the semantics-related or\nsemantics-unrelated patches of the images based on class activation map to\nbreak the spurious correlation, and refill the masked patches with patches from\nother images. The resulting counterfactual samples are used in feature-based\ndistillation with the pre-trained model. Extensive experiments verify that\nregularizing the fine-tuning with the proposed masked images can achieve a\nbetter trade-off between ID and OOD performance, surpassing previous methods on\nthe OOD performance. Our code will be publicly available.\n","authors":["Yao Xiao","Ziyi Tang","Pengxu Wei","Cong Liu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.03052v2.pdf","comment":"Accepted by CVPR 2023 (v2: improve the clarity)"},{"id":"http://arxiv.org/abs/2210.10272v2","updated":"2023-03-14T17:02:34Z","published":"2022-10-19T03:29:58Z","title":"Training set cleansing of backdoor poisoning by self-supervised\n  representation learning","summary":"  A backdoor or Trojan attack is an important type of data poisoning attack\nagainst deep neural network (DNN) classifiers, wherein the training dataset is\npoisoned with a small number of samples that each possess the backdoor pattern\n(usually a pattern that is either imperceptible or innocuous) and which are\nmislabeled to the attacker's target class. When trained on a backdoor-poisoned\ndataset, a DNN behaves normally on most benign test samples but makes incorrect\npredictions to the target class when the test sample has the backdoor pattern\nincorporated (i.e., contains a backdoor trigger). Here we focus on image\nclassification tasks and show that supervised training may build stronger\nassociation between the backdoor pattern and the associated target class than\nthat between normal features and the true class of origin. By contrast,\nself-supervised representation learning ignores the labels of samples and\nlearns a feature embedding based on images' semantic content. %We thus propose\nto use unsupervised representation learning to avoid emphasising\nbackdoor-poisoned training samples and learn a similar feature embedding for\nsamples of the same class. Using a feature embedding found by self-supervised\nrepresentation learning, a data cleansing method, which combines sample\nfiltering and re-labeling, is developed. Experiments on CIFAR-10 benchmark\ndatasets show that our method achieves state-of-the-art performance in\nmitigating backdoor attacks.\n","authors":["H. Wang","S. Karami","O. Dia","H. Ritter","E. Emamjomeh-Zadeh","J. Chen","Z. Xiang","D. J. Miller","G. Kesidis"],"pdf_url":"https://arxiv.org/pdf/2210.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03992v3","updated":"2023-03-14T16:59:18Z","published":"2023-02-08T11:01:19Z","title":"Convolutional Neural Networks Trained to Identify Words Provide a\n  Surprisingly Good Account of Visual Form Priming Effects","summary":"  A wide variety of orthographic coding schemes and models of visual word\nidentification have been developed to account for masked priming data that\nprovide a measure of orthographic similarity between letter strings. These\nmodels tend to include hand-coded orthographic representations with single unit\ncoding for specific forms of knowledge (e.g., units coding for a letter in a\ngiven position). Here we assess how well a range of these coding schemes and\nmodels account for the pattern of form priming effects taken from the Form\nPriming Project and compare these findings to results observed with 11 standard\ndeep neural network models (DNNs) developed in computer science. We find that\ndeep convolutional networks (CNNs) perform as well or better than the coding\nschemes and word recognition models, whereas transformer networks did less\nwell. The success of CNNs is remarkable as their architectures were not\ndeveloped to support word recognition (they were designed to perform well on\nobject recognition), they classify pixel images of words (rather than\nartificial encodings of letter strings), and their training was highly\nsimplified (not respecting many key aspects of human experience). In addition\nto these form priming effects, we find that the DNNs can account for visual\nsimilarity effects on priming that are beyond all current psychological models\nof priming. The findings add to the recent work of (Hannagan et al., 2021) and\nsuggest that CNNs should be given more attention in psychology as models of\nhuman visual word recognition.\n","authors":["Dong Yin","Valerio Biscione","Jeffrey Bowers"],"pdf_url":"https://arxiv.org/pdf/2302.03992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08063v1","updated":"2023-03-14T16:58:11Z","published":"2023-03-14T16:58:11Z","title":"Interpretable ODE-style Generative Diffusion Model via Force Field\n  Construction","summary":"  For a considerable time, researchers have focused on developing a method that\nestablishes a deep connection between the generative diffusion model and\nmathematical physics. Despite previous efforts, progress has been limited to\nthe pursuit of a single specialized method. In order to advance the\ninterpretability of diffusion models and explore new research directions, it is\nessential to establish a unified ODE-style generative diffusion model. Such a\nmodel should draw inspiration from physical models and possess a clear\ngeometric meaning. This paper aims to identify various physical models that are\nsuitable for constructing ODE-style generative diffusion models accurately from\na mathematical perspective. We then summarize these models into a unified\nmethod. Additionally, we perform a case study where we use the theoretical\nmodel identified by our method to develop a range of new diffusion model\nmethods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the\neffectiveness of our approach. We have constructed a computational framework\nthat attains highly proficient results with regards to image generation speed,\nalongside an additional model that demonstrates exceptional performance in both\nInception score and FID score. These results underscore the significance of our\nmethod in advancing the field of diffusion models.\n","authors":["Weiyang Jin","Yongpei Zhu","Yuxi Peng"],"pdf_url":"https://arxiv.org/pdf/2303.08063v1.pdf","comment":"16pages, 13figures, 2tables"},{"id":"http://arxiv.org/abs/2303.04115v2","updated":"2023-03-14T16:57:30Z","published":"2023-03-07T18:28:39Z","title":"Predicted Embedding Power Regression for Large-Scale Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) inputs can compromise the performance and safety of\nreal world machine learning systems. While many methods exist for OOD detection\nand work well on small scale datasets with lower resolution and few classes,\nfew methods have been developed for large-scale OOD detection. Existing\nlarge-scale methods generally depend on maximum classification probability,\nsuch as the state-of-the-art grouped softmax method. In this work, we develop a\nnovel approach that calculates the probability of the predicted class label\nbased on label distributions learned during the training process. Our method\nperforms better than current state-of-the-art methods with only a negligible\nincrease in compute cost. We evaluate our method against contemporary methods\nacross $14$ datasets and achieve a statistically significant improvement with\nrespect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).\n","authors":["Hong Yang","William Gebhardt","Alexander G. Ororbia","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2303.04115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.11741v2","updated":"2023-03-14T16:57:14Z","published":"2022-09-21T21:17:56Z","title":"Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking\n  Neural Networks with Learnable Neuronal Dynamics","summary":"  Event-based cameras have recently shown great potential for high-speed motion\nestimation owing to their ability to capture temporally rich information\nasynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired\nevent-driven processing can efficiently handle such asynchronous data, while\nneuron models such as the leaky-integrate and fire (LIF) can keep track of the\nquintessential timing information contained in the inputs. SNNs achieve this by\nmaintaining a dynamic state in the neuron memory, retaining important\ninformation while forgetting redundant data over time. Thus, we posit that SNNs\nwould allow for better performance on sequential regression tasks compared to\nsimilarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult\nto train due to vanishing spikes at later layers. To that effect, we propose an\nadaptive fully-spiking framework with learnable neuronal dynamics to alleviate\nthe spike vanishing problem. We utilize surrogate gradient-based\nbackpropagation through time (BPTT) to train our deep SNNs from scratch. We\nvalidate our approach for the task of optical flow estimation on the\nMulti-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.\nOur experiments on these datasets show an average reduction of 13% in average\nendpoint error (AEE) compared to state-of-the-art ANNs. We also explore several\ndown-scaled models and observe that our SNN models consistently outperform\nsimilarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the\nimportance of SNNs for smaller models and their suitability at the edge. In\nterms of efficiency, our SNNs offer substantial savings in network parameters\n(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE\ncompared to the state-of-the-art ANN implementations.\n","authors":["Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2209.11741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08061v1","updated":"2023-03-14T16:54:59Z","published":"2023-03-14T16:54:59Z","title":"Point Cloud Diffusion Models for Automatic Implant Generation","summary":"  Advances in 3D printing of biocompatible materials make patient-specific\nimplants increasingly popular. The design of these implants is, however, still\na tedious and largely manual process. Existing approaches to automate implant\ngeneration are mainly based on 3D U-Net architectures on downsampled or\npatch-wise data, which can result in a loss of detail or contextual\ninformation. Following the recent success of Diffusion Probabilistic Models, we\npropose a novel approach for implant generation based on a combination of 3D\npoint cloud diffusion models and voxelization networks. Due to the stochastic\nsampling process in our diffusion model, we can propose an ensemble of\ndifferent implants per defect, from which the physicians can choose the most\nsuitable one. We evaluate our method on the SkullBreak and SkullFix datasets,\ngenerating high-quality implants and achieving competitive evaluation scores.\n","authors":["Paul Friedrich","Julia Wolleb","Florentin Bieder","Florian M. Thieringer","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2303.08061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02390v2","updated":"2023-03-14T16:54:14Z","published":"2022-10-05T17:05:56Z","title":"Bayesian Prompt Learning for Image-Language Model Generalization","summary":"  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\n","authors":["Mohammad Mahdi Derakhshani","Enrique Sanchez","Adrian Bulat","Victor Guilherme Turrisi da Costa","Cees G. M. Snoek","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2210.02390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08050v1","updated":"2023-03-14T16:32:24Z","published":"2023-03-14T16:32:24Z","title":"Subjective and Objective Quality Assessment for in-the-Wild Computer\n  Graphics Images","summary":"  Computer graphics images (CGIs) are artificially generated by means of\ncomputer programs and are widely perceived under various scenarios, such as\ngames, streaming media, etc. In practical, the quality of CGIs consistently\nsuffers from poor rendering during the production and inevitable compression\nartifacts during the transmission of multimedia applications. However, few\nworks have been dedicated to dealing with the challenge of computer graphics\nimages quality assessment (CGIQA). Most image quality assessment (IQA) metrics\nare developed for natural scene images (NSIs) and validated on the databases\nconsisting of NSIs with synthetic distortions, which are not suitable for\nin-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and\nCGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000\nCGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled\nlaboratory environment to obtain the accurate perceptual ratings of the CGIs.\nThen, we propose an effective deep learning-based no-reference (NR) IQA model\nby utilizing multi-stage feature fusion strategy and multi-stage channel\nattention mechanism. The major motivation of the proposed model is to make full\nuse of inter-channel information from low-level to high-level since CGIs have\napparent patterns as well as rich interactive semantic content. Experimental\nresults show that the proposed method outperforms all other state-of-the-art NR\nIQA methods on the constructed CGIQA-6k database and other CGIQA-related\ndatabases. The database along with the code will be released to facilitate\nfurther research.\n","authors":["Zicheng Zhang","Wei Sun","Tao Wang","Wei Lu","Quan Zhou","Jun he","Qiyuan Wang","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.08050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09667v2","updated":"2023-03-14T16:26:17Z","published":"2023-01-23T19:09:36Z","title":"Improving Performance of Object Detection using the Mechanisms of Visual\n  Recognition in Humans","summary":"  Object recognition systems are usually trained and evaluated on high\nresolution images. However, in real world applications, it is common that the\nimages have low resolutions or have small sizes. In this study, we first track\nthe performance of the state-of-the-art deep object recognition network,\nFaster- RCNN, as a function of image resolution. The results reveals negative\neffects of low resolution images on recognition performance. They also show\nthat different spatial frequencies convey different information about the\nobjects in recognition process. It means multi-resolution recognition system\ncan provides better insight into optimal selection of features that results in\nbetter recognition of objects. This is similar to the mechanisms of the human\nvisual systems that are able to implement multi-scale representation of a\nvisual scene simultaneously. Then, we propose a multi-resolution object\nrecognition framework rather than a single-resolution network. The proposed\nframework is evaluated on the PASCAL VOC2007 database. The experimental results\nshow the performance of our adapted multi-resolution Faster-RCNN framework\noutperforms the single-resolution Faster-RCNN on input images with various\nresolutions with an increase in the mean Average Precision (mAP) of 9.14%\nacross all resolutions and 1.2% on the full-spectrum images. Furthermore, the\nproposed model yields robustness of the performance over a wide range of\nspatial frequencies.\n","authors":["Amir Ghasemi","Nasrin Bayat","Fatemeh Mottaghian","Akram Bayat"],"pdf_url":"https://arxiv.org/pdf/2301.09667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08035v1","updated":"2023-03-14T16:15:28Z","published":"2023-03-14T16:15:28Z","title":"ISimDL: Importance Sampling-Driven Acceleration of Fault Injection\n  Simulations for Evaluating the Robustness of Deep Learning","summary":"  Deep Learning (DL) systems have proliferated in many applications, requiring\nspecialized hardware accelerators and chips. In the nano-era, devices have\nbecome increasingly more susceptible to permanent and transient faults.\nTherefore, we need an efficient methodology for analyzing the resilience of\nadvanced DL systems against such faults, and understand how the faults in\nneural accelerator chips manifest as errors at the DL application level, where\nfaults can lead to undetectable and unrecoverable errors. Using fault\ninjection, we can perform resilience investigations of the DL system by\nmodifying neuron weights and outputs at the software-level, as if the hardware\nhad been affected by a transient fault. Existing fault models reduce the search\nspace, allowing faster analysis, but requiring a-priori knowledge on the model,\nand not allowing further analysis of the filtered-out search space. Therefore,\nwe propose ISimDL, a novel methodology that employs neuron sensitivity to\ngenerate importance sampling-based fault-scenarios. Without any a-priori\nknowledge of the model-under-test, ISimDL provides an equivalent reduction of\nthe search space as existing works, while allowing long simulations to cover\nall the possible faults, improving on existing model requirements. Our\nexperiments show that the importance sampling provides up to 15x higher\nprecision in selecting critical faults than the random uniform sampling,\nreaching such precision in less than 100 faults. Additionally, we showcase\nanother practical use-case for importance sampling for reliable DNN design,\nnamely Fault Aware Training (FAT). By using ISimDL to select the faults leading\nto errors, we can insert the faults during the DNN training process to harden\nthe DNN against such faults. Using importance sampling in FAT reduces the\noverhead required for finding faults that lead to a predetermined drop in\naccuracy by more than 12x.\n","authors":["Alessio Colucci","Andreas Steininger","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.08035v1.pdf","comment":"Under review at IJCNN2023"},{"id":"http://arxiv.org/abs/2209.07383v2","updated":"2023-03-14T16:15:21Z","published":"2022-09-15T15:47:31Z","title":"Visual Recognition with Deep Nearest Centroids","summary":"  We devise deep nearest centroids (DNC), a conceptually elegant yet\nsurprisingly effective network for large-scale visual recognition, by\nrevisiting Nearest Centroids, one of the most classic and simple classifiers.\nCurrent deep models learn the classifier in a fully parametric manner, ignoring\nthe latent data structure and lacking simplicity and explainability. DNC\ninstead conducts nonparametric, case-based reasoning; it utilizes sub-centroids\nof training samples to describe class distributions and clearly explains the\nclassification as the proximity of test data and the class sub-centroids in the\nfeature space. Due to the distance-based nature, the network output\ndimensionality is flexible, and all the learnable parameters are only for data\nembedding. That means all the knowledge learnt for ImageNet classification can\nbe completely transferred for pixel recognition learning, under the\n\"pre-training and fine-tuning\" paradigm. Apart from its nested simplicity and\nintuitive decision-making mechanism, DNC can even possess ad-hoc explainability\nwhen the sub-centroids are selected as actual training images that humans can\nview and inspect. Compared with parametric counterparts, DNC performs better on\nimage classification (CIFAR-10, ImageNet) and greatly boots pixel recognition\n(ADE20K, Cityscapes), with improved transparency and fewer learnable\nparameters, using various network architectures (ResNet, Swin) and segmentation\nmodels (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights\ninto related fields.\n","authors":["Wenguan Wang","Cheng Han","Tianfei Zhou","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2209.07383v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v3","updated":"2023-03-14T16:11:35Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v3.pdf","comment":"ACCV 2022 (Best Paper Award)\n  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html"},{"id":"http://arxiv.org/abs/2303.08029v1","updated":"2023-03-14T16:10:36Z","published":"2023-03-14T16:10:36Z","title":"Class-level Multiple Distributions Representation are Necessary for\n  Semantic Segmentation","summary":"  Existing approaches focus on using class-level features to improve semantic\nsegmentation performance. How to characterize the relationships of intra-class\npixels and inter-class pixels is the key to extract the discriminative\nrepresentative class-level features. In this paper, we introduce for the first\ntime to describe intra-class variations by multiple distributions. Then,\nmultiple distributions representation learning(\\textbf{MDRL}) is proposed to\naugment the pixel representations for semantic segmentation. Meanwhile, we\ndesign a class multiple distributions consistency strategy to construct\ndiscriminative multiple distribution representations of embedded pixels.\nMoreover, we put forward a multiple distribution semantic aggregation module to\naggregate multiple distributions of the corresponding class to enhance pixel\nsemantic information. Our approach can be seamlessly integrated into popular\nsegmentation frameworks FCN/PSPNet/CCNet and achieve 5.61\\%/1.75\\%/0.75\\% mIoU\nimprovements on ADE20K. Extensive experiments on the Cityscapes, ADE20K\ndatasets have proved that our method can bring significant performance\nimprovement.\n","authors":["Jianjian Yin","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.08029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08010v1","updated":"2023-03-14T15:57:54Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07997v1","updated":"2023-03-14T15:48:47Z","published":"2023-03-14T15:48:47Z","title":"FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction\n  from Visuo-tactile Feedback","summary":"  In this paper, we address the problem of using visuo-tactile feedback for\n6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose\nFingerSLAM, a closed-loop factor graph-based pose estimator that combines local\ntactile sensing at finger-tip and global vision sensing from a wrist-mount\ncamera. FingerSLAM is constructed with two constituent pose estimators: a\nmulti-pass refined tactile-based pose estimator that captures movements from\ndetailed local textures, and a single-pass vision-based pose estimator that\npredicts from a global view of the object. We also design a loop closure\nmechanism that actively matches current vision and tactile images to previously\nstored key-frames to reduce accumulated error. FingerSLAM incorporates the two\nsensing modalities of tactile and vision, as well as the loop closure mechanism\nwith a factor graph-based optimization framework. Such a framework produces an\noptimized pose estimation solution that is more accurate than the standalone\nestimators. The estimated poses are then used to reconstruct the shape of the\nunknown object incrementally by stitching the local point clouds recovered from\ntactile images. We train our system on real-world data collected with 20\nobjects. We demonstrate reliable visuo-tactile pose estimation and shape\nreconstruction through quantitative and qualitative real-world evaluations on 6\nobjects that are unseen during training.\n","authors":["Jialiang Zhao","Maria Bauza","Edward H. Adelson"],"pdf_url":"https://arxiv.org/pdf/2303.07997v1.pdf","comment":"Submitted and accepted to 2023 IEEE International Conference on\n  Robotics and Automation (ICRA 2023)"},{"id":"http://arxiv.org/abs/2303.07989v1","updated":"2023-03-14T15:44:45Z","published":"2023-03-14T15:44:45Z","title":"A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing","summary":"  Air-writing refers to virtually writing linguistic characters through hand\ngestures in three-dimensional space with six degrees of freedom. This paper\nproposes a generic video camera-aided convolutional neural network (CNN) based\nair-writing framework. Gestures are performed using a marker of fixed color in\nfront of a generic video camera, followed by color-based segmentation to\nidentify the marker and track the trajectory of the marker tip. A pre-trained\nCNN is then used to classify the gesture. The recognition accuracy is further\nimproved using transfer learning with the newly acquired data. The performance\nof the system varies significantly on the illumination condition due to\ncolor-based segmentation. In a less fluctuating illumination condition, the\nsystem is able to recognize isolated unistroke numerals of multiple languages.\nThe proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in\nperson independent evaluations on English, Bengali and Devanagari numerals,\nrespectively.\n","authors":["Prasun Roy","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2303.07989v1.pdf","comment":"Accepted in The International Conference on Frontiers of Handwriting\n  Recognition (ICFHR) 2018"},{"id":"http://arxiv.org/abs/2010.12669v2","updated":"2023-03-14T15:20:15Z","published":"2020-10-23T21:07:40Z","title":"Position and Rotation Invariant Sign Language Recognition from 3D Kinect\n  Data with Recurrent Neural Networks","summary":"  Sign language is a gesture-based symbolic communication medium among speech\nand hearing impaired people. It also serves as a communication bridge between\nnon-impaired and impaired populations. Unfortunately, in most situations, a\nnon-impaired person is not well conversant in such symbolic languages\nrestricting the natural information flow between these two categories.\nTherefore, an automated translation mechanism that seamlessly translates sign\nlanguage into natural language can be highly advantageous. In this paper, we\nattempt to perform recognition of 30 basic Indian sign gestures. Gestures are\nrepresented as temporal sequences of 3D maps (RGB + depth), each consisting of\n3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent\nneural network (RNN) is employed as the classifier. To improve the classifier's\nperformance, we use geometric transformation for the alignment correction of\ndepth frames. In our experiments, the model achieves 84.81% accuracy.\n","authors":["Prasun Roy","Saumik Bhattacharya","Partha Pratim Roy","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2010.12669v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.07963v1","updated":"2023-03-14T15:07:51Z","published":"2023-03-14T15:07:51Z","title":"RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning","summary":"  This paper introduces a new method for 3D point cloud registration based on\ndeep learning. The architecture is composed of three distinct blocs: (i) an\nencoder composed of a convolutional graph-based descriptor that encodes the\nimmediate neighbourhood of each point and an attention mechanism that encodes\nthe variations of the surface normals. Such descriptors are refined by\nhighlighting attention between the points of the same set and then between the\npoints of the two sets. (ii) a matching process that estimates a matrix of\ncorrespondences using the Sinkhorn algorithm. (iii) Finally, the rigid\ntransformation between the two point clouds is calculated by RANSAC using the\nKc best scores from the correspondence matrix. We conduct experiments on the\nModelNet40 dataset, and our proposed architecture shows very promising results,\noutperforming state-of-the-art methods in most of the simulated configurations,\nincluding partial overlap and data augmentation with Gaussian noise.\n","authors":["Karim Slimani","Brahim Tamadazte","Catherine Achard"],"pdf_url":"https://arxiv.org/pdf/2303.07963v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2206.04636v3","updated":"2023-03-14T15:07:16Z","published":"2022-06-09T17:34:39Z","title":"Spatial Entropy as an Inductive Bias for Vision Transformers","summary":"  Recent work on Vision Transformers (VTs) showed that introducing a local\ninductive bias in the VT architecture helps reducing the number of samples\nnecessary for training. However, the architecture modifications lead to a loss\nof generality of the Transformer backbone, partially contradicting the push\ntowards the development of uniform architectures, shared, e.g., by both the\nComputer Vision and the Natural Language Processing areas. In this work, we\npropose a different and complementary direction, in which a local bias is\nintroduced using an auxiliary self-supervised task, performed jointly with\nstandard supervised training. Specifically, we exploit the observation that the\nattention maps of VTs, when trained with self-supervision, can contain a\nsemantic segmentation structure which does not spontaneously emerge when\ntraining is supervised. Thus, we explicitly encourage the emergence of this\nspatial clustering as a form of training regularization. In more detail, we\nexploit the assumption that, in a given image, objects usually correspond to\nfew connected regions, and we propose a spatial formulation of the information\nentropy to quantify this object-based inductive bias. By minimizing the\nproposed spatial entropy, we include an additional self-supervised signal\nduring training. Using extensive experiments, we show that the proposed\nregularization leads to equivalent or better results than other VT proposals\nwhich include a local bias by changing the basic Transformer architecture, and\nit can drastically boost the VT final accuracy when using small-medium training\nsets. The code is available at https://github.com/helia95/SAR.\n","authors":["Elia Peruzzo","Enver Sangineto","Yahui Liu","Marco De Nadai","Wei Bi","Bruno Lepri","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2206.04636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00841v3","updated":"2023-03-14T15:04:33Z","published":"2022-10-03T11:57:30Z","title":"Smooth image-to-image translations with latent space interpolations","summary":"  Multi-domain image-to-image (I2I) translations can transform a source image\naccording to the style of a target domain. One important, desired\ncharacteristic of these transformations, is their graduality, which corresponds\nto a smooth change between the source and the target image when their\nrespective latent-space representations are linearly interpolated. However,\nstate-of-the-art methods usually perform poorly when evaluated using\ninter-domain interpolations, often producing abrupt changes in the appearance\nor non-realistic intermediate images. In this paper, we argue that one of the\nmain reasons behind this problem is the lack of sufficient inter-domain\ntraining data and we propose two different regularization methods to alleviate\nthis issue: a new shrinkage loss, which compacts the latent space, and a Mixup\ndata-augmentation strategy, which flattens the style representations between\ndomains. We also propose a new metric to quantitatively evaluate the degree of\nthe interpolation smoothness, an aspect which is not sufficiently covered by\nthe existing I2I translation metrics. Using both our proposed metric and\nstandard evaluation protocols, we show that our regularization techniques can\nimprove the state-of-the-art multi-domain I2I translations by a large margin.\nOur code will be made publicly available upon the acceptance of this article.\n","authors":["Yahui Liu","Enver Sangineto","Yajing Chen","Linchao Bao","Haoxian Zhang","Nicu Sebe","Bruno Lepri","Marco De Nadai"],"pdf_url":"https://arxiv.org/pdf/2210.00841v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06274v2","updated":"2023-03-14T14:53:19Z","published":"2023-03-11T01:21:13Z","title":"CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,\n  Segmentation, Classification and Counting","summary":"  Nuclear detection, segmentation and morphometric profiling are essential in\nhelping us further understand the relationship between histology and patient\noutcome. To drive innovation in this area, we setup a community-wide challenge\nusing the largest available dataset of its kind to assess nuclear segmentation\nand cellular composition. Our challenge, named CoNIC, stimulated the\ndevelopment of reproducible algorithms for cellular recognition with real-time\nresult inspection on public leaderboards. We conducted an extensive\npost-challenge analysis based on the top-performing models using 1,658\nwhole-slide images of colon tissue. With around 700 million detected nuclei per\nmodel, associated features were used for dysplasia grading and survival\nanalysis, where we demonstrated that the challenge's improvement over the\nprevious state-of-the-art led to significant boosts in downstream performance.\nOur findings also suggest that eosinophils and neutrophils play an important\nrole in the tumour microevironment. We release challenge models and WSI-level\nresults to foster the development of further methods for biomarker discovery.\n","authors":["Simon Graham","Quoc Dang Vu","Mostafa Jahanifar","Martin Weigert","Uwe Schmidt","Wenhua Zhang","Jun Zhang","Sen Yang","Jinxi Xiang","Xiyue Wang","Josef Lorenz Rumberger","Elias Baumann","Peter Hirsch","Lihao Liu","Chenyang Hong","Angelica I. Aviles-Rivero","Ayushi Jain","Heeyoung Ahn","Yiyu Hong","Hussam Azzuni","Min Xu","Mohammad Yaqub","Marie-Claire Blache","Benoît Piégu","Bertrand Vernay","Tim Scherr","Moritz Böhland","Katharina Löffler","Jiachen Li","Weiqin Ying","Chixin Wang","Dagmar Kainmueller","Carola-Bibiane Schönlieb","Shuolin Liu","Dhairya Talsania","Yughender Meda","Prakash Mishra","Muhammad Ridzuan","Oliver Neumann","Marcel P. Schilling","Markus Reischl","Ralf Mikut","Banban Huang","Hsiang-Chin Chien","Ching-Ping Wang","Chia-Yen Lee","Hong-Kun Lin","Zaiyi Liu","Xipeng Pan","Chu Han","Jijun Cheng","Muhammad Dawood","Srijay Deshpande","Raja Muhammad Saad Bashir","Adam Shephard","Pedro Costa","João D. Nunes","Aurélio Campilho","Jaime S. Cardoso","Hrishikesh P S","Densen Puthussery","Devika R G","Jiji C V","Ye Zhang","Zijie Fang","Zhifan Lin","Yongbing Zhang","Chunhui Lin","Liukun Zhang","Lijian Mao","Min Wu","Vi Thi-Tuong Vo","Soo-Hyung Kim","Taebum Lee","Satoshi Kondo","Satoshi Kasai","Pranay Dumbhare","Vedant Phuse","Yash Dubey","Ankush Jamthikar","Trinh Thi Le Vuong","Jin Tae Kwak","Dorsa Ziaei","Hyun Jung","Tianyi Miao","David Snead","Shan E Ahmed Raza","Fayyaz Minhas","Nasir M. Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2303.06274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07951v1","updated":"2023-03-14T14:49:52Z","published":"2023-03-14T14:49:52Z","title":"MetaMixer: A Regularization Strategy for Online Knowledge Distillation","summary":"  Online knowledge distillation (KD) has received increasing attention in\nrecent years. However, while most existing online KD methods focus on\ndeveloping complicated model structures and training strategies to improve the\ndistillation of high-level knowledge like probability distribution, the effects\nof the multi-level knowledge in the online KD are greatly overlooked,\nespecially the low-level knowledge. Thus, to provide a novel viewpoint to\nonline KD, we propose MetaMixer, a regularization strategy that can strengthen\nthe distillation by combining the low-level knowledge that impacts the\nlocalization capability of the networks, and high-level knowledge that focuses\non the whole image. Experiments under different conditions show that MetaMixer\ncan achieve significant performance gains over state-of-the-art methods.\n","authors":["Maorong Wang","Ling Xiao","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2303.07951v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.07945v1","updated":"2023-03-14T14:35:59Z","published":"2023-03-14T14:35:59Z","title":"Edit-A-Video: Single Video Editing with Object-Aware Consistency","summary":"  Despite the fact that text-to-video (TTV) model has recently achieved\nremarkable success, there have been few approaches on TTV for its extension to\nvideo editing. Motivated by approaches on TTV models adapting from\ndiffusion-based text-to-image (TTI) models, we suggest the video editing\nframework given only a pretrained TTI model and a single <text, video> pair,\nwhich we term Edit-A-Video. The framework consists of two stages: (1) inflating\nthe 2D model into the 3D model by appending temporal modules and tuning on the\nsource video (2) inverting the source video into the noise and editing with\ntarget text prompt and attention map injection. Each stage enables the temporal\nmodeling and preservation of semantic attributes of the source video. One of\nthe key challenges for video editing include a background inconsistency\nproblem, where the regions not included for the edit suffer from undesirable\nand inconsistent temporal alterations. To mitigate this issue, we also\nintroduce a novel mask blending method, termed as sparse-causal blending (SC\nBlending). We improve previous mask blending methods to reflect the temporal\nconsistency so that the area where the editing is applied exhibits smooth\ntransition while also achieving spatio-temporal consistency of the unedited\nregions. We present extensive experimental results over various types of text\nand videos, and demonstrate the superiority of the proposed method compared to\nbaselines in terms of background consistency, text alignment, and video editing\nquality.\n","authors":["Chaehun Shin","Heeseung Kim","Che Hyun Lee","Sang-gil Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.07945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07944v1","updated":"2023-03-14T14:34:51Z","published":"2023-03-14T14:34:51Z","title":"Non-Contrastive Unsupervised Learning of Physiological Signals from\n  Video","summary":"  Subtle periodic signals such as blood volume pulse and respiration can be\nextracted from RGB video, enabling remote health monitoring at low cost.\nAdvancements in remote pulse estimation -- or remote photoplethysmography\n(rPPG) -- are currently driven by deep learning solutions. However, modern\napproaches are trained and evaluated on benchmark datasets with associated\nground truth from contact-PPG sensors. We present the first non-contrastive\nunsupervised learning framework for signal regression to break free from the\nconstraints of labelled video data. With minimal assumptions of periodicity and\nfinite bandwidth, our approach is capable of discovering the blood volume pulse\ndirectly from unlabelled videos. We find that encouraging sparse power spectra\nwithin normal physiological bandlimits and variance over batches of power\nspectra is sufficient for learning visual features of periodic signals. We\nperform the first experiments utilizing unlabelled video data not specifically\ncreated for rPPG to train robust pulse rate estimators. Given the limited\ninductive biases and impressive empirical results, the approach is\ntheoretically capable of discovering other periodic signals from video,\nenabling multiple physiological measurements without the need for ground truth\nsignals. Codes to fully reproduce the experiments are made available along with\nthe paper.\n","authors":["Jeremy Speth","Nathan Vance","Patrick Flynn","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2303.07944v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07938v1","updated":"2023-03-14T14:25:29Z","published":"2023-03-14T14:25:29Z","title":"Controllable Mesh Generation Through Sparse Latent Point Diffusion\n  Models","summary":"  Mesh generation is of great value in various applications involving computer\ngraphics and virtual content, yet designing generative models for meshes is\nchallenging due to their irregular data structure and inconsistent topology of\nmeshes in the same category. In this work, we design a novel sparse latent\npoint diffusion model for mesh generation. Our key insight is to regard point\nclouds as an intermediate representation of meshes, and model the distribution\nof point clouds instead. While meshes can be generated from point clouds via\ntechniques like Shape as Points (SAP), the challenges of directly generating\nmeshes can be effectively avoided. To boost the efficiency and controllability\nof our mesh generation method, we propose to further encode point clouds to a\nset of sparse latent points with point-wise semantic meaningful features, where\ntwo DDPMs are trained in the space of sparse latent points to respectively\nmodel the distribution of the latent point positions and features at these\nlatent points. We find that sampling in this latent space is faster than\ndirectly sampling dense point clouds. Moreover, the sparse latent points also\nenable us to explicitly control both the overall structures and local details\nof the generated meshes. Extensive experiments are conducted on the ShapeNet\ndataset, where our proposed sparse latent point diffusion model achieves\nsuperior performance in terms of generation quality and controllability when\ncompared to existing methods.\n","authors":["Zhaoyang Lyu","Jinyi Wang","Yuwei An","Ya Zhang","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2303.07938v1.pdf","comment":"Accepted to CVPR 2023. Project page is at https://slide-3d.github.io"},{"id":"http://arxiv.org/abs/2303.02489v2","updated":"2023-03-14T14:25:04Z","published":"2023-03-04T19:53:00Z","title":"CapDet: Unifying Dense Captioning and Open-World Detection Pretraining","summary":"  Benefiting from large-scale vision-language pre-training on image-text pairs,\nopen-world detection methods have shown superior generalization ability under\nthe zero-shot or few-shot detection settings. However, a pre-defined category\nspace is still required during the inference stage of existing methods and only\nthe objects belonging to that space will be predicted. To introduce a \"real\"\nopen-world detector, in this paper, we propose a novel method named CapDet to\neither predict under a given category list or directly generate the category of\npredicted bounding boxes. Specifically, we unify the open-world detection and\ndense caption tasks into a single yet effective framework by introducing an\nadditional dense captioning head to generate the region-grounded captions.\nBesides, adding the captioning task will in turn benefit the generalization of\ndetection performance since the captioning dataset covers more concepts.\nExperiment results show that by unifying the dense caption task, our CapDet has\nobtained significant performance improvements (e.g., +2.1% mAP on LVIS rare\nclasses) over the baseline method on LVIS (1203 classes). Besides, our CapDet\nalso achieves state-of-the-art performance on dense captioning tasks, e.g.,\n15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.\n","authors":["Yanxin Long","Youpeng Wen","Jianhua Han","Hang Xu","Pengzhen Ren","Wei Zhang","Shen Zhao","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2303.02489v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07937v1","updated":"2023-03-14T14:24:31Z","published":"2023-03-14T14:24:31Z","title":"Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D\n  Generation","summary":"  Text-to-3D generation has shown rapid progress in recent days with the advent\nof score distillation, a methodology of using pretrained text-to-2D diffusion\nmodels to optimize neural radiance field (NeRF) in the zero-shot setting.\nHowever, the lack of 3D awareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible 3D scene. To address\nthis issue, we propose \\ours, a novel framework that incorporates 3D awareness\ninto pretrained 2D diffusion models, enhancing the robustness and 3D\nconsistency of score distillation-based methods. We realize this by first\nconstructing a coarse 3D structure of a given text prompt and then utilizing\nprojected, view-specific depth map as a condition for the diffusion model.\nAdditionally, we introduce a training strategy that enables the 2D diffusion\nmodel learns to handle the errors and sparsity within the coarse 3D structure\nfor robust generation, as well as a method for ensuring semantic consistency\nthroughout all viewpoints of the scene. Our framework surpasses the limitations\nof prior arts, and has significant implications for 3D consistent generation of\n2D diffusion models.\n","authors":["Junyoung Seo","Wooseok Jang","Min-Seop Kwak","Jaehoon Ko","Hyeonsu Kim","Junho Kim","Jin-Hwa Kim","Jiyoung Lee","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07937v1.pdf","comment":"Project page https://ku-cvlab.github.io/3DFuse/"},{"id":"http://arxiv.org/abs/2303.07929v1","updated":"2023-03-14T14:12:33Z","published":"2023-03-14T14:12:33Z","title":"DAA: A Delta Age AdaIN operation for age estimation via binary code\n  transformer","summary":"  Naked eye recognition of age is usually based on comparison with the age of\nothers. However, this idea is ignored by computer tasks because it is difficult\nto obtain representative contrast images of each age. Inspired by the transfer\nlearning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature\ndifference with each age, which obtains the style map of each age through the\nlearned values representing the mean and standard deviation. We let the input\nof transfer learning as the binary code of age natural number to obtain\ncontinuous age feature information. The learned two groups of values in Binary\ncode mapping are corresponding to the mean and standard deviation of the\ncomparison ages. In summary, our method consists of four parts: FaceEncoder,\nDAA operation, Binary code mapping, and AgeDecoder modules. After getting the\ndelta age via AgeDecoder, we take the average value of all comparison ages and\ndelta ages as the predicted age. Compared with state-of-the-art methods, our\nmethod achieves better performance with fewer parameters on multiple facial age\ndatasets.\n","authors":["Ping Chen","Xingpeng Zhang","Ye Li","Ju Tao","Bin Xiao","Bing Wang","Zongjie Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07929v1.pdf","comment":"Accepted by CVPR2023; 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.07910v1","updated":"2023-03-14T13:50:31Z","published":"2023-03-14T13:50:31Z","title":"Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm","summary":"  Parameter-Efficient Transfer Learning (PETL) aims at efficiently adapting\nlarge models pre-trained on massive data to downstream tasks with limited\ntask-specific data. In view of the practicality of PETL, previous works focus\non tuning a small set of parameters for each downstream task in an end-to-end\nmanner while rarely considering the task distribution shift issue between the\npre-training task and the downstream task. This paper proposes a novel\ntwo-stage paradigm, where the pre-trained model is first aligned to the target\ndistribution. Then the task-relevant information is leveraged for effective\nadaptation. Specifically, the first stage narrows the task distribution shift\nby tuning the scale and shift in the LayerNorm layers. In the second stage, to\nefficiently learn the task-relevant information, we propose a Taylor\nexpansion-based importance score to identify task-relevant channels for the\ndownstream task and then only tune such a small portion of channels, making the\nadaptation to be parameter-efficient. Overall, we present a promising new\ndirection for PETL, and the proposed paradigm achieves state-of-the-art\nperformance on the average accuracy of 19 downstream tasks.\n","authors":["Hengyuan Zhao","Hao Luo","Yuyang Zhao","Pichao Wang","Fan Wang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.07910v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.07909v1","updated":"2023-03-14T13:49:54Z","published":"2023-03-14T13:49:54Z","title":"Text-to-image Diffusion Model in Generative AI: A Survey","summary":"  This survey reviews text-to-image diffusion models in the context that\ndiffusion models have emerged to be popular for a wide range of generative\ntasks. As a self-contained work, this survey starts with a brief introduction\nof how a basic diffusion model works for image synthesis, followed by how\ncondition or guidance improves learning. Based on that, we present a review of\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\ntext-to-image. We further summarize applications beyond text-to-image\ngeneration: text-guided creative generation and text-guided image editing.\nBeyond the progress made so far, we discuss existing challenges and promising\nfuture directions.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Mengchun Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.07909v1.pdf","comment":"First survey on the recent progress of text-to-image generation based\n  on the diffusion model (under progress)"},{"id":"http://arxiv.org/abs/2303.07898v1","updated":"2023-03-14T13:36:36Z","published":"2023-03-14T13:36:36Z","title":"AutoEnsemble: Automated Ensemble Search Framework for Semantic\n  Segmentation Using Image Labels","summary":"  A key bottleneck of employing state-of-the-art semantic segmentation networks\nin the real world is the availability of training labels. Standard semantic\nsegmentation networks require massive pixel-wise annotated labels to reach\nstate-of-the-art prediction quality. Hence, several works focus on semantic\nsegmentation networks trained with only image-level annotations. However, when\nscrutinizing the state-of-the-art results in more detail, we notice that\nalthough they are very close to each other on average prediction quality,\ndifferent approaches perform better in different classes while providing low\nquality in others. To address this problem, we propose a novel framework,\nAutoEnsemble, which employs an ensemble of the \"pseudo-labels\" for a given set\nof different segmentation techniques on a class-wise level. Pseudo-labels are\nthe pixel-wise predictions of the image-level semantic segmentation frameworks\nused to train the final segmentation model. Our pseudo-labels seamlessly\ncombine the strong points of multiple segmentation techniques approaches to\nreach superior prediction quality. We reach up to 2.4% improvement over\nAutoEnsemble's components. An exhaustive analysis was performed to demonstrate\nAutoEnsemble's effectiveness over state-of-the-art frameworks for image-level\nsemantic segmentation.\n","authors":["Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07898v1.pdf","comment":"This paper is submitted to a IEEE conference for peer review\n  publication"},{"id":"http://arxiv.org/abs/2303.07896v1","updated":"2023-03-14T13:31:05Z","published":"2023-03-14T13:31:05Z","title":"Automated Ensemble Search Framework for Semantic Segmentation Using\n  Medical Imaging Labels","summary":"  Reliable classification and detection of certain medical conditions, in\nimages, with state-of-the-art semantic segmentation networks, require vast\namounts of pixel-wise annotation. However, the public availability of such\ndatasets is minimal. Therefore, semantic segmentation with image-level labels\npresents a promising alternative to this problem. Nevertheless, very few works\nhave focused on evaluating this technique and its applicability to the medical\nsector. Due to their complexity and the small number of training examples in\nmedical datasets, classifier-based weakly supervised networks like class\nactivation maps (CAMs) struggle to extract useful information from them.\nHowever, most state-of-the-art approaches rely on them to achieve their\nimprovements. Therefore, we propose a framework that can still utilize the\nlow-quality CAM predictions of complicated datasets to improve the accuracy of\nour results. Our framework achieves that by first utilizing lower threshold\nCAMs to cover the target object with high certainty; second, by combining\nmultiple low-threshold CAMs that even out their errors while highlighting the\ntarget object. We performed exhaustive experiments on the popular multi-modal\nBRATS and prostate DECATHLON segmentation challenge datasets. Using the\nproposed framework, we have demonstrated an improved dice score of up to 8% on\nBRATS and 6% on DECATHLON datasets compared to the previous state-of-the-art.\n","authors":["Erik Ostrowski","Bharath Srinivas Prabakaran","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07892v1","updated":"2023-03-14T13:25:55Z","published":"2023-03-14T13:25:55Z","title":"Image Label based Semantic Segmentation Framework using Object\n  Perimeters","summary":"  Achieving high-quality semantic segmentation predictions using only\nimage-level labels enables a new level of real-world applicability. Although\nstate-of-the-art networks deliver reliable predictions, the amount of\nhandcrafted pixel-wise annotations to enable these results are not feasible in\nmany real-world applications. Hence, several works have already targeted this\nbottleneck, using classifier-based networks like Class Activation Maps (CAMs)\nas a base. Addressing CAM's weaknesses of fuzzy borders and incomplete\npredictions, state-of-the-art approaches rely only on adding regulations to the\nclassifier loss or using pixel-similarity-based refinement after the fact. We\npropose a framework that introduces an additional module using object\nperimeters for improved saliency. We define object perimeter information as the\nline separating the object and background. Our new PerimeterFit module will be\napplied to pre-refine the CAM predictions before using the\npixel-similarity-based network. In this way, our PerimeterFit increases the\nquality of the CAM prediction while simultaneously improving the false negative\nrate. We investigated a wide range of state-of-the-art unsupervised semantic\nsegmentation networks and edge detection techniques to create useful perimeter\nmaps, which enable our framework to predict object locations with sharper\nperimeters. We achieved up to 1.5\\% improvement over frameworks without our\nPerimeterFit module. We conduct an exhaustive analysis to illustrate that our\nframework enhances existing state-of-the-art frameworks for image-level-based\nsemantic segmentation. The framework is open-source and accessible online at\nhttps://github.com/ErikOstrowski/Perimeter-based-Semantic-Segmentation.\n","authors":["Erik Ostrowski","Bharath Srinivas Prabakaran","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03595v2","updated":"2023-03-14T13:15:04Z","published":"2023-03-07T02:00:34Z","title":"LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global\n  Cross-Modal Fusion","summary":"  LiDAR-camera fusion methods have shown impressive performance in 3D object\ndetection. Recent advanced multi-modal methods mainly perform global fusion,\nwhere image features and point cloud features are fused across the whole scene.\nSuch practice lacks fine-grained region-level information, yielding suboptimal\nfusion performance. In this paper, we present the novel Local-to-Global fusion\nnetwork (LoGoNet), which performs LiDAR-camera fusion at both local and global\nlevels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous\nliterature, while we exclusively use point centroids to more precisely\nrepresent the position of voxel features, thus achieving better cross-modal\nalignment. As to the Local Fusion (LoF), we first divide each proposal into\nuniform grids and then project these grid centers to the images. The image\nfeatures around the projected grid points are sampled to be fused with\nposition-decorated point cloud features, maximally utilizing the rich\ncontextual information around the proposals. The Feature Dynamic Aggregation\n(FDA) module is further proposed to achieve information interaction between\nthese locally and globally fused features, thus producing more informative\nmulti-modal features. Extensive experiments on both Waymo Open Dataset (WOD)\nand KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D\ndetection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection\nleaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy\nthat, for the first time, the detection performance on three classes surpasses\n80 APH (L2) simultaneously. Code will be available at\n\\url{https://github.com/sankin97/LoGoNet}.\n","authors":["Xin Li","Tao Ma","Yuenan Hou","Botian Shi","Yuchen Yang","Youquan Liu","Xingjiao Wu","Qin Chen","Yikang Li","Yu Qiao","Liang He"],"pdf_url":"https://arxiv.org/pdf/2303.03595v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.16762v3","updated":"2023-03-14T13:07:50Z","published":"2022-11-30T06:02:01Z","title":"GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided\n  Distance Representation","summary":"  We present a learning-based method, namely GeoUDF,to tackle the long-standing\nand challenging problem of reconstructing a discrete surface from a sparse\npoint cloud.To be specific, we propose a geometry-guided learning method for\nUDF and its gradient estimation that explicitly formulates the unsigned\ndistance of a query point as the learnable affine averaging of its distances to\nthe tangent planes of neighboring points on the surface. Besides,we model the\nlocal geometric structure of the input point clouds by explicitly learning a\nquadratic polynomial for each point. This not only facilitates upsampling the\ninput sparse point cloud but also naturally induces unoriented normal, which\nfurther augments UDF estimation. Finally, to extract triangle meshes from the\npredicted UDF we propose a customized edge-based marching cube module. We\nconduct extensive experiments and ablation studies to demonstrate the\nsignificant advantages of our method over state-of-the-art methods in terms of\nreconstruction accuracy, efficiency, and generality. The source code is\npublicly available at https://github.com/rsy6318/GeoUDF.\n","authors":["Siyu Ren","Junhui Hou","Xiaodong Chen","Ying He","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2211.16762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03729v2","updated":"2023-03-14T13:07:09Z","published":"2023-03-07T08:37:48Z","title":"Learning Discriminative Representations for Skeleton Based Action\n  Recognition","summary":"  Human action recognition aims at classifying the category of human action\nfrom a segment of a video. Recently, people have dived into designing GCN-based\nmodels to extract features from skeletons for performing this task, because\nskeleton representations are much more efficient and robust than other\nmodalities such as RGB frames. However, when employing the skeleton data, some\nimportant clues like related items are also discarded. It results in some\nambiguous actions that are hard to be distinguished and tend to be\nmisclassified. To alleviate this problem, we propose an auxiliary feature\nrefinement head (FR Head), which consists of spatial-temporal decoupling and\ncontrastive feature refinement, to obtain discriminative representations of\nskeletons. Ambiguous samples are dynamically discovered and calibrated in the\nfeature space. Furthermore, FR Head could be imposed on different stages of\nGCNs to build a multi-level refinement for stronger supervision. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\nOur proposed models obtain competitive results from state-of-the-art methods\nand can help to discriminate those ambiguous samples. Codes are available at\nhttps://github.com/zhysora/FR-Head.\n","authors":["Huanyu Zhou","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03729v2.pdf","comment":"Accepted by CVPR2023. 10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.07868v1","updated":"2023-03-14T13:01:25Z","published":"2023-03-14T13:01:25Z","title":"DynaMask: Dynamic Mask Selection for Instance Segmentation","summary":"  The representative instance segmentation methods mostly segment different\nobject instances with a mask of the fixed resolution, e.g., 28*28 grid.\nHowever, a low-resolution mask loses rich details, while a high-resolution mask\nincurs quadratic computation overhead. It is a challenging task to predict the\noptimal binary mask for each instance. In this paper, we propose to dynamically\nselect suitable masks for different object proposals. First, a dual-level\nFeature Pyramid Network (FPN) with adaptive feature aggregation is developed to\ngradually increase the mask grid resolution, ensuring high-quality segmentation\nof objects. Specifically, an efficient region-level top-down path (r-FPN) is\nintroduced to incorporate complementary contextual and detailed information\nfrom different stages of image-level FPN (i-FPN). Then, to alleviate the\nincrease of computation and memory costs caused by using large masks, we\ndevelop a Mask Switch Module (MSM) with negligible computational cost to select\nthe most suitable mask resolution for each instance, achieving high efficiency\nwhile maintaining high segmentation accuracy. Without bells and whistles, the\nproposed method, namely DynaMask, brings consistent and noticeable performance\nimprovements over other state-of-the-arts at a moderate computation overhead.\nThe source code: https://github.com/lslrh/DynaMask.\n","authors":["Ruihuang Li","Chenhang He","Shuai Li","Yabin Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07868v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.16927v2","updated":"2023-03-14T12:56:57Z","published":"2022-11-30T11:57:45Z","title":"3D GAN Inversion with Facial Symmetry Prior","summary":"  Recently, a surge of high-quality 3D-aware GANs have been proposed, which\nleverage the generative power of neural rendering. It is natural to associate\n3D GANs with GAN inversion methods to project a real image into the generator's\nlatent space, allowing free-view consistent synthesis and editing, referred as\n3D GAN inversion. Although with the facial prior preserved in pre-trained 3D\nGANs, reconstructing a 3D portrait with only one monocular image is still an\nill-pose problem. The straightforward application of 2D GAN inversion methods\nfocuses on texture similarity only while ignoring the correctness of 3D\ngeometry shapes. It may raise geometry collapse effects, especially when\nreconstructing a side face under an extreme pose. Besides, the synthetic\nresults in novel views are prone to be blurry. In this work, we propose a novel\nmethod to promote 3D GAN inversion by introducing facial symmetry prior. We\ndesign a pipeline and constraints to make full use of the pseudo auxiliary view\nobtained via image flipping, which helps obtain a robust and reasonable\ngeometry shape during the inversion process. To enhance texture fidelity in\nunobserved viewpoints, pseudo labels from depth-guided 3D warping can provide\nextra supervision. We design constraints aimed at filtering out conflict areas\nfor optimization in asymmetric situations. Comprehensive quantitative and\nqualitative evaluations on image reconstruction and editing demonstrate the\nsuperiority of our method.\n","authors":["Fei Yin","Yong Zhang","Xuan Wang","Tengfei Wang","Xiaoyu Li","Yuan Gong","Yanbo Fan","Xiaodong Cun","Ying Shan","Cengiz Oztireli","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2211.16927v2.pdf","comment":"Project Page is at https://feiiyin.github.io/SPI/"},{"id":"http://arxiv.org/abs/2303.07863v1","updated":"2023-03-14T12:53:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v1.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2303.07853v1","updated":"2023-03-14T12:46:52Z","published":"2023-03-14T12:46:52Z","title":"BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised\n  Semantic Segmentation of Medical Images","summary":"  Weakly Supervised Semantic Segmentation (WSSS) with only image-level\nsupervision is a promising approach to deal with the need for Segmentation\nnetworks, especially for generating a large number of pixel-wise masks in a\ngiven dataset. However, most state-of-the-art image-level WSSS techniques lack\nan understanding of the geometric features embedded in the images since the\nnetwork cannot derive any object boundary information from just image-level\nlabels. We define a boundary here as the line separating an object and its\nbackground, or two different objects. To address this drawback, we propose our\nnovel BoundaryCAM framework, which deploys state-of-the-art class activation\nmaps combined with various post-processing techniques in order to achieve\nfine-grained higher-accuracy segmentation masks. To achieve this, we\ninvestigate a state-of-the-art unsupervised semantic segmentation network that\ncan be used to construct a boundary map, which enables BoundaryCAM to predict\nobject locations with sharper boundaries. By applying our method to WSSS\npredictions, we were able to achieve up to 10% improvements even to the benefit\nof the current state-of-the-art WSSS methods for medical imaging. The framework\nis open-source and accessible online at\nhttps://github.com/bharathprabakaran/BoundaryCAM.\n","authors":["Bharath Srinivas Prabakaran","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07852v1","updated":"2023-03-14T12:46:48Z","published":"2023-03-14T12:46:48Z","title":"FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network\n  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features","summary":"  Ultrasound imaging is one of the most prominent technologies to evaluate the\ngrowth, progression, and overall health of a fetus during its gestation.\nHowever, the interpretation of the data obtained from such studies is best left\nto expert physicians and technicians who are trained and well-versed in\nanalyzing such images. To improve the clinical workflow and potentially develop\nan at-home ultrasound-based fetal monitoring platform, we present a novel fetus\nphantom ultrasound dataset, FPUS23, which can be used to identify (1) the\ncorrect diagnostic planes for estimating fetal biometric values, (2) fetus\norientation, (3) their anatomical features, and (4) bounding boxes of the fetus\nphantom anatomies at 23 weeks gestation. The entire dataset is composed of\n15,728 images, which are used to train four different Deep Neural Network\nmodels, built upon a ResNet34 backbone, for detecting aforementioned fetus\nfeatures and use-cases. We have also evaluated the models trained using our\nFPUS23 dataset, to show that the information learned by these models can be\nused to substantially increase the accuracy on real-world ultrasound fetus\ndatasets. We make the FPUS23 dataset and the pre-trained models publicly\naccessible at https://github.com/bharathprabakaran/FPUS23, which will further\nfacilitate future research on fetal ultrasound imaging and analysis.\n","authors":["Bharath Srinivas Prabakaran","Paul Hamelmann","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07849v1","updated":"2023-03-14T12:41:56Z","published":"2023-03-14T12:41:56Z","title":"Implicit Stacked Autoregressive Model for Video Prediction","summary":"  Future frame prediction has been approached through two primary methods:\nautoregressive and non-autoregressive. Autoregressive methods rely on the\nMarkov assumption and can achieve high accuracy in the early stages of\nprediction when errors are not yet accumulated. However, their performance\ntends to decline as the number of time steps increases. In contrast,\nnon-autoregressive methods can achieve relatively high performance but lack\ncorrelation between predictions for each time step. In this paper, we propose\nan Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which\nis an implicit video prediction model that applies a stacked autoregressive\nmethod. Like non-autoregressive methods, stacked autoregressive methods use the\nsame observed frame to estimate all future frames. However, they use their own\npredictions as input, similar to autoregressive methods. As the number of time\nsteps increases, predictions are sequentially stacked in the queue. To evaluate\nthe effectiveness of IAM4VP, we conducted experiments on three common future\nframe prediction benchmark datasets and weather\\&climate prediction benchmark\ndatasets. The results demonstrate that our proposed model achieves\nstate-of-the-art performance.\n","authors":["Minseok Seo","Hakjin Lee","Doyi Kim","Junghoon Seo"],"pdf_url":"https://arxiv.org/pdf/2303.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05183v2","updated":"2023-03-14T12:41:01Z","published":"2023-03-09T11:21:59Z","title":"Blind2Sound: Self-Supervised Image Denoising without Residual Noise","summary":"  Self-supervised blind denoising for Poisson-Gaussian noise remains a\nchallenging task. Pseudo-supervised pairs constructed from single noisy images\nre-corrupt the signal and degrade the performance. The visible blindspots solve\nthe information loss in masked inputs. However, without explicitly noise\nsensing, mean square error as an objective function cannot adjust denoising\nintensities for dynamic noise levels, leading to noticeable residual noise. In\nthis paper, we propose Blind2Sound, a simple yet effective approach to overcome\nresidual noise in denoised images. The proposed adaptive re-visible loss senses\nnoise levels and performs personalized denoising without noise residues while\nretaining the signal lossless. The theoretical analysis of intermediate medium\ngradients guarantees stable training, while the Cramer Gaussian loss acts as a\nregularization to facilitate the accurate perception of noise levels and\nimprove the performance of the denoiser. Experiments on synthetic and\nreal-world datasets show the superior performance of our method, especially for\nsingle-channel images.\n","authors":["Zejin Wang","Jiazheng Liu","Hao Zhai","Hua Han"],"pdf_url":"https://arxiv.org/pdf/2303.05183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07840v1","updated":"2023-03-14T12:26:48Z","published":"2023-03-14T12:26:48Z","title":"Precise Facial Landmark Detection by Reference Heatmap Transformer","summary":"  Most facial landmark detection methods predict landmarks by mapping the input\nfacial appearance features to landmark heatmaps and have achieved promising\nresults. However, when the face image is suffering from large poses, heavy\nocclusions and complicated illuminations, they cannot learn discriminative\nfeature representations and effective facial shape constraints, nor can they\naccurately predict the value of each element in the landmark heatmap, limiting\ntheir detection accuracy. To address this problem, we propose a novel Reference\nHeatmap Transformer (RHT) by introducing reference heatmap information for more\nprecise facial landmark detection. The proposed RHT consists of a Soft\nTransformation Module (STM) and a Hard Transformation Module (HTM), which can\ncooperate with each other to encourage the accurate transformation of the\nreference heatmap information and facial shape constraints. Then, a Multi-Scale\nFeature Fusion Module (MSFFM) is proposed to fuse the transformed heatmap\nfeatures and the semantic features learned from the original face images to\nenhance feature representations for producing more accurate target heatmaps. To\nthe best of our knowledge, this is the first study to explore how to enhance\nfacial landmark detection by transforming the reference heatmap information.\nThe experimental results from challenging benchmark datasets demonstrate that\nour proposed method outperforms the state-of-the-art methods in the literature.\n","authors":["Jun Wan","Jun Liu","Jie Zhou","Zhihui Lai","Linlin Shen","Hang Sun","Ping Xiong","Wenwen Min"],"pdf_url":"https://arxiv.org/pdf/2303.07840v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing, March 2023"},{"id":"http://arxiv.org/abs/2302.02314v2","updated":"2023-03-14T12:12:00Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  Most computer vision models are developed based on either convolutional\nneural network (CNN) or transformer, while the former (latter) method captures\nlocal (global) features. To relieve model performance limitations due to the\nlack of global (local) features, we develop a novel classification network CECT\nby controllable ensemble CNN and transformer. CECT is composed of a\nconvolutional encoder block, a transposed-convolutional decoder block, and a\ntransformer classification block. Different from conventional CNN- or\ntransformer-based methods, our CECT can capture features at both multi-local\nand global scales. Besides, the contribution of local features at different\nscales can be controlled with the proposed ensemble coefficients. We evaluate\nCECT on two public COVID-19 datasets and it outperforms existing\nstate-of-the-art methods on all evaluation metrics. With remarkable feature\ncapture ability, we believe CECT can be extended to other medical image\nclassification scenarios as a diagnosis assistant.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.09100v2","updated":"2023-03-14T12:08:11Z","published":"2022-12-18T14:56:22Z","title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images","summary":"  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset will be made public with the code and\nmodels on the project website https://abdullahamdi.com/sparf/ .\n","authors":["Abdullah Hamdi","Bernard Ghanem","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2212.09100v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.07831v1","updated":"2023-03-14T12:07:48Z","published":"2023-03-14T12:07:48Z","title":"Quaternion Orthogonal Transformer for Facial Expression Recognition in\n  the Wild","summary":"  Facial expression recognition (FER) is a challenging topic in artificial\nintelligence. Recently, many researchers have attempted to introduce Vision\nTransformer (ViT) to the FER task. However, ViT cannot fully utilize emotional\nfeatures extracted from raw images and requires a lot of computing resources.\nTo overcome these problems, we propose a quaternion orthogonal transformer\n(QOT) for FER. Firstly, to reduce redundancy among features extracted from\npre-trained ResNet-50, we use the orthogonal loss to decompose and compact\nthese features into three sets of orthogonal sub-features. Secondly, three\northogonal sub-features are integrated into a quaternion matrix, which\nmaintains the correlations between different orthogonal components. Finally, we\ndevelop a quaternion vision transformer (Q-ViT) for feature classification. The\nQ-ViT adopts quaternion operations instead of the original operations in ViT,\nwhich improves the final accuracies with fewer parameters. Experimental results\non three in-the-wild FER datasets show that the proposed QOT outperforms\nseveral state-of-the-art models and reduces the computations.\n","authors":["Yu Zhou","Liyuan Guo","Lianghai Jin"],"pdf_url":"https://arxiv.org/pdf/2303.07831v1.pdf","comment":"This paper has been accepted to ICASSP2023"},{"id":"http://arxiv.org/abs/2303.07820v1","updated":"2023-03-14T11:53:12Z","published":"2023-03-14T11:53:12Z","title":"Adaptive Rotated Convolution for Rotated Object Detection","summary":"  Rotated object detection aims to identify and locate objects in images with\narbitrary orientation. In this scenario, the oriented directions of objects\nvary considerably across different images, while multiple orientations of\nobjects exist within an image. This intrinsic characteristic makes it\nchallenging for standard backbone networks to extract high-quality features of\nthese arbitrarily orientated objects. In this paper, we present Adaptive\nRotated Convolution (ARC) module to handle the aforementioned challenges. In\nour ARC module, the convolution kernels rotate adaptively to extract object\nfeatures with varying orientations in different images, and an efficient\nconditional computation mechanism is introduced to accommodate the large\norientation variations of objects within an image. The two designs work\nseamlessly in rotated object detection problem. Moreover, ARC can conveniently\nserve as a plug-and-play module in various vision backbones to boost their\nrepresentation ability to detect oriented objects accurately. Experiments on\ncommonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our\nproposed ARC module in the backbone network, the performance of multiple\npopular oriented object detectors is significantly improved (e.g. +3.03% mAP on\nRotated RetinaNet and +4.16% on CFA). Combined with the highly competitive\nmethod Oriented R-CNN, the proposed approach achieves state-of-the-art\nperformance on the DOTA dataset with 81.77% mAP.\n","authors":["Yifan Pu","Yiru Wang","Zhuofan Xia","Yizeng Han","Yulin Wang","Weihao Gan","Zidong Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2303.07820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12039v2","updated":"2023-03-14T11:48:41Z","published":"2022-11-22T06:21:31Z","title":"Accelerating Diffusion Sampling with Classifier-based Feature\n  Distillation","summary":"  Although diffusion model has shown great potential for generating higher\nquality images than GANs, slow sampling speed hinders its wide application in\npractice. Progressive distillation is thus proposed for fast sampling by\nprogressively aligning output images of $N$-step teacher sampler with\n$N/2$-step student sampler. In this paper, we argue that this\ndistillation-based accelerating method can be further improved, especially for\nfew-step samplers, with our proposed \\textbf{C}lassifier-based \\textbf{F}eature\n\\textbf{D}istillation (CFD). Instead of aligning output images, we distill\nteacher's sharpened feature distribution into the student with a\ndataset-independent classifier, making the student focus on those important\nfeatures to improve performance. We also introduce a dataset-oriented loss to\nfurther optimize the model. Experiments on CIFAR-10 show the superiority of our\nmethod in achieving high quality and fast sampling. Code is provided at\n\\url{https://github.com/zju-SWJ/RCFD}.\n","authors":["Wujie Sun","Defang Chen","Can Wang","Deshi Ye","Yan Feng","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2211.12039v2.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2303.07815v1","updated":"2023-03-14T11:46:04Z","published":"2023-03-14T11:46:04Z","title":"MobileVOS: Real-Time Video Object Segmentation Contrastive Learning\n  meets Knowledge Distillation","summary":"  This paper tackles the problem of semi-supervised video object segmentation\non resource-constrained devices, such as mobile phones. We formulate this\nproblem as a distillation task, whereby we demonstrate that small\nspace-time-memory networks with finite memory can achieve competitive results\nwith state of the art, but at a fraction of the computational cost (32\nmilliseconds per frame on a Samsung Galaxy S22). Specifically, we provide a\ntheoretically grounded framework that unifies knowledge distillation with\nsupervised contrastive representation learning. These models are able to\njointly benefit from both pixel-wise contrastive learning and distillation from\na pre-trained teacher. We validate this loss by achieving competitive J&F to\nstate of the art on both the standard DAVIS and YouTube benchmarks, despite\nrunning up to 5x faster, and with 32x fewer parameters.\n","authors":["Roy Miles","Mehmet Kerim Yucel","Bruno Manganelli","Albert Saa-Garriga"],"pdf_url":"https://arxiv.org/pdf/2303.07815v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07814v1","updated":"2023-03-14T11:44:58Z","published":"2023-03-14T11:44:58Z","title":"Kinematic Data-Based Action Segmentation for Surgical Applications","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nIn the context of surgical procedures, action segmentation is critical for\nworkflow analysis algorithms. This work presents two contributions related to\naction segmentation on kinematic data. Firstly, we introduce two multi-stage\narchitectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Horizontal-Flip, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieve state-of-the-art\nperformance on all benchmark datasets and establish a strong baseline for the\nBRS dataset.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2110.01303v3","updated":"2023-03-14T11:40:35Z","published":"2021-10-04T10:19:53Z","title":"Incremental Class Learning using Variational Autoencoders with\n  Similarity Learning","summary":"  Catastrophic forgetting in neural networks during incremental learning\nremains a challenging problem. Previous research investigated catastrophic\nforgetting in fully connected networks, with some earlier work exploring\nactivation functions and learning algorithms. Applications of neural networks\nhave been extended to include similarity learning. Understanding how similarity\nlearning loss functions would be affected by catastrophic forgetting is of\nsignificant interest. Our research investigates catastrophic forgetting for\nfour well-known similarity-based loss functions during incremental class\nlearning. The loss functions are Angular, Contrastive, Center, and Triplet\nloss. Our results show that the catastrophic forgetting rate differs across\nloss functions on multiple datasets. The Angular loss was least affected,\nfollowed by Contrastive, Triplet loss, and Center loss with good mining\ntechniques. We implemented three existing incremental learning techniques,\niCaRL, EWC, and EBLL. We further proposed a novel technique using Variational\nAutoencoders (VAEs) to generate representation as exemplars passed through the\nnetwork's intermediate layers. Our method outperformed three existing\nstate-of-the-art techniques. We show that one does not require stored images\n(exemplars) for incremental learning with similarity learning. The generated\nrepresentations from VAEs help preserve regions of the embedding space used by\nprior knowledge so that new knowledge does not ``overwrite'' it.\n","authors":["Jiahao Huo","Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2110.01303v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01970v4","updated":"2023-03-14T11:40:34Z","published":"2023-01-05T09:11:16Z","title":"CAT: LoCalization and IdentificAtion Cascade Detection Transformer for\n  Open-World Object Detection","summary":"  Open-world object detection (OWOD), as a more general and challenging goal,\nrequires the model trained from data on known objects to detect both known and\nunknown objects and incrementally learn to identify these unknown objects. The\nexisting works which employ standard detection framework and fixed\npseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion\nof detecting unknown objects substantially reduces the model's ability to\ndetect known ones. (ii) The PLM does not adequately utilize the priori\nknowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee\nthat the model is trained in the right direction. We observe that humans\nsubconsciously prefer to focus on all foreground objects and then identify each\none in detail, rather than localize and identify a single object\nsimultaneously, for alleviating the confusion. This motivates us to propose a\nnovel solution called CAT: LoCalization and IdentificAtion Cascade Detection\nTransformer which decouples the detection process via the shared decoder in the\ncascade decoding way. In the meanwhile, we propose the self-adaptive\npseudo-labelling mechanism which combines the model-driven with input-driven\nPLM and self-adaptively generates robust pseudo-labels for unknown objects,\nsignificantly improving the ability of CAT to retrieve unknown objects.\nComprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL\nVOC, show that our model outperforms the state-of-the-art in terms of all\nmetrics in the task of OWOD, incremental object detection (IOD) and open-set\ndetection.\n","authors":["Shuailei Ma","Yuefeng Wang","Jiaqi Fan","Ying Wei","Thomas H. Li","Hongli Liu","Fanbing Lv"],"pdf_url":"https://arxiv.org/pdf/2301.01970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/1911.10375v2","updated":"2023-03-14T11:38:46Z","published":"2019-11-23T15:16:36Z","title":"Region Normalization for Image Inpainting","summary":"  Feature Normalization (FN) is an important technique to help neural network\ntraining, which typically normalizes features across spatial dimensions. Most\nprevious image inpainting methods apply FN in their networks without\nconsidering the impact of the corrupted regions of the input image on\nnormalization, e.g. mean and variance shifts. In this work, we show that the\nmean and variance shifts caused by full-spatial FN limit the image inpainting\nnetwork training and we propose a spatial region-wise normalization named\nRegion Normalization (RN) to overcome the limitation. RN divides spatial pixels\ninto different regions according to the input mask, and computes the mean and\nvariance in each region for normalization. We develop two kinds of RN for our\nimage inpainting network: (1) Basic RN (RN-B), which normalizes pixels from the\ncorrupted and uncorrupted regions separately based on the original inpainting\nmask to solve the mean and variance shift problem; (2) Learnable RN (RN-L),\nwhich automatically detects potentially corrupted and uncorrupted regions for\nseparate normalization, and performs global affine transformation to enhance\ntheir fusion. We apply RN-B in the early layers and RN-L in the latter layers\nof the network respectively. Experiments show that our method outperforms\ncurrent state-of-the-art methods quantitatively and qualitatively. We further\ngeneralize RN to other inpainting networks and achieve consistent performance\nimprovements. Our code is available at https://github.com/geekyutao/RN.\n","authors":["Tao Yu","Zongyu Guo","Xin Jin","Shilin Wu","Zhibo Chen","Weiping Li","Zhizheng Zhang","Sen Liu"],"pdf_url":"https://arxiv.org/pdf/1911.10375v2.pdf","comment":"Accepted to AAAI-2020. Code URL:https://github.com/geekyutao/RN"},{"id":"http://arxiv.org/abs/2303.07811v1","updated":"2023-03-14T11:31:45Z","published":"2023-03-14T11:31:45Z","title":"ICICLE: Interpretable Class Incremental Continual Learning","summary":"  Continual learning enables incremental learning of new tasks without\nforgetting those previously learned, resulting in positive knowledge transfer\nthat can enhance performance on both new and old tasks. However, continual\nlearning poses new challenges for interpretability, as the rationale behind\nmodel predictions may change over time, leading to interpretability concept\ndrift. We address this problem by proposing Interpretable Class-InCremental\nLEarning (ICICLE), an exemplar-free approach that adopts a prototypical\npart-based approach. It consists of three crucial novelties: interpretability\nregularization that distills previously learned concepts while preserving\nuser-friendly positive reasoning; proximity-based prototype initialization\nstrategy dedicated to the fine-grained setting; and task-recency bias\ncompensation devoted to prototypical parts. Our experimental results\ndemonstrate that ICICLE reduces the interpretability concept drift and\noutperforms the existing exemplar-free methods of common class-incremental\nlearning when applied to concept-based models. We make the code available.\n","authors":["Dawid Rymarczyk","Joost van de Weijer","Bartosz Zieliński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2303.07811v1.pdf","comment":"Under review, code will be shared after the acceptance"},{"id":"http://arxiv.org/abs/2303.07806v1","updated":"2023-03-14T11:25:02Z","published":"2023-03-14T11:25:02Z","title":"USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised\n  Semantic Segmentation","summary":"  Seed area generation is usually the starting point of weakly supervised\nsemantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a\nmulti-label classification network is the de facto paradigm for seed area\ngeneration, but CAMs generated from Convolutional Neural Networks (CNNs) and\nTransformers are prone to be under- and over-activated, respectively, which\nmakes the strategies to refine CAMs for CNNs usually inappropriate for\nTransformers, and vice versa. In this paper, we propose a Unified optimization\nparadigm for Seed Area GEneration (USAGE) for both types of networks, in which\nthe objective function to be optimized consists of two terms: One is a\ngeneration loss, which controls the shape of seed areas by a temperature\nparameter following a deterministic principle for different types of networks;\nThe other is a regularization loss, which ensures the consistency between the\nseed areas that are generated by self-adaptive network adjustment from\ndifferent views, to overturn false activation in seed areas. Experimental\nresults show that USAGE consistently improves seed area generation for both\nCNNs and Transformers by large margins, e.g., outperforming state-of-the-art\nmethods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated\nseed areas on Transformers, we achieve state-of-the-art WSSS results on both\nPASCAL VOC and MS COCO.\n","authors":["Zelin Peng","Guanchun Wang","Lingxi Xie","Dongsheng Jiang","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.07806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07798v1","updated":"2023-03-14T11:15:37Z","published":"2023-03-14T11:15:37Z","title":"OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav","summary":"  We present a single neural network architecture composed of task-agnostic\ncomponents (ViTs, convolutions, and LSTMs) that achieves state-of-art results\non both the ImageNav (\"go to location in <this picture>\") and ObjectNav (\"find\na chair\") tasks without any task-specific modules like object detection,\nsegmentation, mapping, or planning modules. Such general-purpose methods offer\nadvantages of simplicity in design, positive scaling with available compute,\nand versatile applicability to multiple tasks. Our work builds upon the recent\nsuccess of self-supervised learning (SSL) for pre-training vision transformers\n(ViT). However, while the training recipes for convolutional networks are\nmature and robust, the recipes for ViTs are contingent and brittle, and in the\ncase of ViTs for visual navigation, yet to be fully discovered. Specifically,\nwe find that vanilla ViTs do not outperform ResNets on visual navigation. We\npropose the use of a compression layer operating over ViT patch representations\nto preserve spatial information along with policy training improvements. These\nimprovements allow us to demonstrate positive scaling laws for the first time\nin visual navigation tasks. Consequently, our model advances state-of-the-art\nperformance on ImageNav from 54.2% to 82.0% success and performs competitively\nagainst concurrent state-of-art on ObjectNav with success rate of 64.0% vs.\n65.0%. Overall, this work does not present a fundamentally new approach, but\nrather recommendations for training a general-purpose architecture that\nachieves state-of-art performance today and could serve as a strong baseline\nfor future methods.\n","authors":["Karmesh Yadav","Arjun Majumdar","Ram Ramrakhya","Naoki Yokoyama","Alexei Baevski","Zsolt Kira","Oleksandr Maksymets","Dhruv Batra"],"pdf_url":"https://arxiv.org/pdf/2303.07798v1.pdf","comment":"15 pages, 7 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.07790v1","updated":"2023-03-14T11:04:50Z","published":"2023-03-14T11:04:50Z","title":"Object Detection During Newborn Resuscitation Activities","summary":"  Birth asphyxia is a major newborn mortality problem in low-resource\ncountries. International guideline provides treatment recommendations; however,\nthe importance and effect of the different treatments are not fully explored.\nThe available data is collected in Tanzania, during newborn resuscitation, for\nanalysis of the resuscitation activities and the response of the newborn. An\nimportant step in the analysis is to create activity timelines of the episodes,\nwhere activities include ventilation, suction, stimulation etc. Methods: The\navailable recordings are noisy real-world videos with large variations. We\npropose a two-step process in order to detect activities possibly overlapping\nin time. The first step is to detect and track the relevant objects, like\nbag-mask resuscitator, heart rate sensors etc., and the second step is to use\nthis information to recognize the resuscitation activities. The topic of this\npaper is the first step, and the object detection and tracking are based on\nconvolutional neural networks followed by post processing. Results: The\nperformance of the object detection during activities were 96.97 %\n(ventilations), 100 % (attaching/removing heart rate sensor) and 75 % (suction)\non a test set of 20 videos. The system also estimate the number of health care\nproviders present with a performance of 71.16 %. Conclusion: The proposed\nobject detection and tracking system provides promising results in noisy\nnewborn resuscitation videos. Significance: This is the first step in a\nthorough analysis of newborn resuscitation episodes, which could provide\nimportant insight about the importance and effect of different newborn\nresuscitation activities\n","authors":["Øyvind Meinich-Bache","Kjersti Engan","Ivar Austvoll","Trygve Eftestøl","Helge Myklebust","Ladislaus Blacy Yarrot","Hussein Kidanto","Hege Ersdal"],"pdf_url":"https://arxiv.org/pdf/2303.07790v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.03022v2","updated":"2023-03-14T11:04:43Z","published":"2022-12-06T14:49:41Z","title":"Iterative Next Boundary Detection for Instance Segmentation of Tree\n  Rings in Microscopy Images of Shrub Cross Sections","summary":"  We address the problem of detecting tree rings in microscopy images of shrub\ncross sections. This can be regarded as a special case of the instance\nsegmentation task with several unique challenges such as the concentric\ncircular ring shape of the objects and high precision requirements that result\nin inadequate performance of existing methods. We propose a new iterative\nmethod which we term Iterative Next Boundary Detection (INBD). It intuitively\nmodels the natural growth direction, starting from the center of the shrub\ncross section and detecting the next ring boundary in each iteration step. In\nour experiments, INBD shows superior performance to generic instance\nsegmentation methods and is the only one with a built-in notion of\nchronological order. Our dataset and source code are available at\nhttp://github.com/alexander-g/INBD.\n","authors":["Alexander Gillert","Giulia Resente","Alba Anadon-Rosell","Martin Wilmking","Uwe Freiherr von Lukas"],"pdf_url":"https://arxiv.org/pdf/2212.03022v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07789v1","updated":"2023-03-14T11:04:32Z","published":"2023-03-14T11:04:32Z","title":"Activity Recognition From Newborn Resuscitation Videos","summary":"  Objective: Birth asphyxia is one of the leading causes of neonatal deaths. A\nkey for survival is performing immediate and continuous quality newborn\nresuscitation. A dataset of recorded signals during newborn resuscitation,\nincluding videos, has been collected in Haydom, Tanzania, and the aim is to\nanalyze the treatment and its effect on the newborn outcome. An important step\nis to generate timelines of relevant resuscitation activities, including\nventilation, stimulation, suction, etc., during the resuscitation episodes.\nMethods: We propose a two-step deep neural network system, ORAA-net, utilizing\nlow-quality video recordings of resuscitation episodes to do activity\nrecognition during newborn resuscitation. The first step is to detect and track\nrelevant objects using Convolutional Neural Networks (CNN) and post-processing,\nand the second step is to analyze the proposed activity regions from step 1 to\ndo activity recognition using 3D CNNs. Results: The system recognized the\nactivities newborn uncovered, stimulation, ventilation and suction with a mean\nprecision of 77.67 %, a mean recall of 77,64 %, and a mean accuracy of 92.40 %.\nMoreover, the accuracy of the estimated number of Health Care Providers (HCPs)\npresent during the resuscitation episodes was 68.32 %. Conclusion: The results\nindicate that the proposed CNN-based two-step ORAAnet could be used for object\ndetection and activity recognition in noisy low-quality newborn resuscitation\nvideos. Significance: A thorough analysis of the effect the different\nresuscitation activities have on the newborn outcome could potentially allow us\nto optimize treatment guidelines, training, debriefing, and local quality\nimprovement in newborn resuscitation.\n","authors":["Øyvind Meinich-Bache","Simon Lennart Austnes","Kjersti Engan","Ivar Austvoll","Trygve Eftestøl","Helge Myklebust","Simeon Kusulla","Hussein Kidanto","Hege Ersdal"],"pdf_url":"https://arxiv.org/pdf/2303.07789v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2212.12902v2","updated":"2023-03-14T10:39:16Z","published":"2022-12-25T13:36:32Z","title":"TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose\n  Estimation","summary":"  In this paper, we introduce neural texture learning for 6D object pose\nestimation from synthetic data and a few unlabelled real images. Our major\ncontribution is a novel learning scheme which removes the drawbacks of previous\nworks, namely the strong dependency on co-modalities or additional refinement.\nThese have been previously necessary to provide training signals for\nconvergence. We formulate such a scheme as two sub-optimisation problems on\ntexture learning and pose learning. We separately learn to predict realistic\ntexture of objects from real image collections and learn pose estimation from\npixel-perfect synthetic data. Combining these two capabilities allows then to\nsynthesise photorealistic novel views to supervise the pose estimator with\naccurate geometry. To alleviate pose noise and segmentation imperfection\npresent during the texture learning phase, we propose a surfel-based\nadversarial training loss together with texture regularisation from synthetic\ndata. We demonstrate that the proposed approach significantly outperforms the\nrecent state-of-the-art methods without ground-truth pose annotations and\ndemonstrates substantial generalisation improvements towards unseen scenes.\nRemarkably, our scheme improves the adopted pose estimators substantially even\nwhen initialised with much inferior performance.\n","authors":["Hanzhi Chen","Fabian Manhardt","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2212.12902v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07775v1","updated":"2023-03-14T10:34:07Z","published":"2023-03-14T10:34:07Z","title":"Data-Free Sketch-Based Image Retrieval","summary":"  Rising concerns about privacy and anonymity preservation of deep learning\nmodels have facilitated research in data-free learning (DFL). For the first\ntime, we identify that for data-scarce tasks like Sketch-Based Image Retrieval\n(SBIR), where the difficulty in acquiring paired photos and hand-drawn sketches\nlimits data-dependent cross-modal learning algorithms, DFL can prove to be a\nmuch more practical paradigm. We thus propose Data-Free (DF)-SBIR, where,\nunlike existing DFL problems, pre-trained, single-modality classification\nmodels have to be leveraged to learn a cross-modal metric-space for retrieval\nwithout access to any training data. The widespread availability of pre-trained\nclassification models, along with the difficulty in acquiring paired\nphoto-sketch datasets for SBIR justify the practicality of this setting. We\npresent a methodology for DF-SBIR, which can leverage knowledge from models\nindependently trained to perform classification on photos and sketches. We\nevaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks,\ndesigning a variety of baselines based on state-of-the-art DFL literature, and\nobserve that our method surpasses all of them by significant margins. Our\nmethod also achieves mAPs competitive with data-dependent approaches, all the\nwhile requiring no training data. Implementation is available at\n\\url{https://github.com/abhrac/data-free-sbir}.\n","authors":["Abhra Chaudhuri","Ayan Kumar Bhunia","Yi-Zhe Song","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2303.07775v1.pdf","comment":"Computer Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.07771v1","updated":"2023-03-14T10:20:31Z","published":"2023-03-14T10:20:31Z","title":"Imbalanced Domain Generalization for Robust Single Cell Classification\n  in Hematological Cytomorphology","summary":"  Accurate morphological classification of white blood cells (WBCs) is an\nimportant step in the diagnosis of leukemia, a disease in which nonfunctional\nblast cells accumulate in the bone marrow. Recently, deep convolutional neural\nnetworks (CNNs) have been successfully used to classify leukocytes by training\nthem on single-cell images from a specific domain. Most CNN models assume that\nthe distributions of the training and test data are similar, i.e., that the\ndata are independently and identically distributed. Therefore, they are not\nrobust to different staining protocols, magnifications, resolutions, scanners,\nor imaging protocols, as well as variations in clinical centers or patient\ncohorts. In addition, domain-specific data imbalances affect the generalization\nperformance of classifiers. Here, we train a robust CNN for WBC classification\nby addressing cross-domain data imbalance and domain shifts. To this end, we\nuse two loss functions and demonstrate the effectiveness on out-of-distribution\n(OOD) generalization. Our approach achieves the best F1 macro score compared to\nother existing methods, and is able to consider rare cell types. This is the\nfirst demonstration of imbalanced domain generalization in hematological\ncytomorphology and paves the way for robust single cell classification methods\nfor the application in laboratories and clinics.\n","authors":["Rao Muhammad Umer","Armin Gruber","Sayedali Shetab Boushehri","Christian Metak","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2303.07771v1.pdf","comment":"Published as a ICLR 2023 workshop paper: What do we need for\n  successful domain generalization?"},{"id":"http://arxiv.org/abs/2303.07759v1","updated":"2023-03-14T10:06:19Z","published":"2023-03-14T10:06:19Z","title":"A Simple Baseline for Supervised Surround-view Depth Estimation","summary":"  Depth estimation has been widely studied and serves as the fundamental step\nof 3D perception for autonomous driving. Though significant progress has been\nmade for monocular depth estimation in the past decades, these attempts are\nmainly conducted on the KITTI benchmark with only front-view cameras, which\nignores the correlations across surround-view cameras. In this paper, we\npropose S3Depth, a Simple Baseline for Supervised Surround-view Depth\nEstimation, to jointly predict the depth maps across multiple surrounding\ncameras. Specifically, we employ a global-to-local feature extraction module\nwhich combines CNN with transformer layers for enriched representations.\nFurther, the Adjacent-view Attention mechanism is proposed to enable the\nintra-view and inter-view feature propagation. The former is achieved by the\nself-attention module within each view, while the latter is realized by the\nadjacent attention module, which computes the attention across multi-cameras to\nexchange the multi-scale representations across surround-view feature maps.\nExtensive experiments show that our method achieves superior performance over\nexisting state-of-the-art methods on both DDAD and nuScenes datasets.\n","authors":["Xianda Guo","Wenjie Yuan","Yunpeng Zhang","Tian Yang","Chenming Zhang","Zheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.07759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.13572v2","updated":"2023-03-14T09:53:55Z","published":"2022-11-24T12:44:33Z","title":"Real-Time Physics-Based Object Pose Tracking during Non-Prehensile\n  Manipulation","summary":"  We propose a method to track the 6D pose of an object over time, while the\nobject is under non-prehensile manipulation by a robot. At any given time\nduring the manipulation of the object, we assume access to the robot joint\ncontrols and an image from a camera. We use the robot joint controls to perform\na physics-based prediction of how the object might be moving. We then combine\nthis prediction with the observation coming from the camera, to estimate the\nobject pose as accurately as possible. We use a particle filtering approach to\ncombine the control information with the visual information. We compare the\nproposed method with two baselines: (i) using only an image-based pose\nestimation system at each time-step, and (ii) a particle filter which does not\nperform the computationally expensive physics predictions, but assumes the\nobject moves with constant velocity. Our results show that making physics-based\npredictions is worth the computational cost, resulting in more accurate\ntracking, and estimating object pose even when the object is not clearly\nvisible to the camera.\n","authors":["Zisong Xu","Rafael Papallas","Mehmet Dogar"],"pdf_url":"https://arxiv.org/pdf/2211.13572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07748v1","updated":"2023-03-14T09:48:59Z","published":"2023-03-14T09:48:59Z","title":"Generation-Guided Multi-Level Unified Network for Video Grounding","summary":"  Video grounding aims to locate the timestamps best matching the query\ndescription within an untrimmed video. Prevalent methods can be divided into\nmoment-level and clip-level frameworks. Moment-level approaches directly\npredict the probability of each transient moment to be the boundary in a global\nperspective, and they usually perform better in coarse grounding. On the other\nhand, clip-level ones aggregate the moments in different time windows into\nproposals and then deduce the most similar one, leading to its advantage in\nfine-grained grounding. In this paper, we propose a multi-level unified\nframework to enhance performance by leveraging the merits of both moment-level\nand clip-level methods. Moreover, a novel generation-guided paradigm in both\nlevels is adopted. It introduces a multi-modal generator to produce the\nimplicit boundary feature and clip feature, later regarded as queries to\ncalculate the boundary scores by a discriminator. The generation-guided\nsolution enhances video grounding from a two-unique-modals' match task to a\ncross-modal attention task, which steps out of the previous framework and\nobtains notable gains. The proposed Generation-guided Multi-level Unified\nnetwork (GMU) surpasses previous methods and reaches State-Of-The-Art on\nvarious benchmarks with disparate features, e.g., Charades-STA, ActivityNet\ncaptions.\n","authors":["Xing Cheng","Xiangyu Wu","Dong Shen","Hezheng Lin","Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07747v1","updated":"2023-03-14T09:44:29Z","published":"2023-03-14T09:44:29Z","title":"LoG-CAN: local-global Class-aware Network for semantic segmentation of\n  remote sensing images","summary":"  Remote sensing images are known of having complex backgrounds, high\nintra-class variance and large variation of scales, which bring challenge to\nsemantic segmentation. We present LoG-CAN, a multi-scale semantic segmentation\nnetwork with a global class-aware (GCA) module and local class-aware (LCA)\nmodules to remote sensing images. Specifically, the GCA module captures the\nglobal representations of class-wise context modeling to circumvent background\ninterference; the LCA modules generate local class representations as\nintermediate aware elements, indirectly associating pixels with global class\nrepresentations to reduce variance within a class; and a multi-scale\narchitecture with GCA and LCA modules yields effective segmentation of objects\nat different scales via cascaded refinement and fusion of features. Through the\nevaluation on the ISPRS Vaihingen dataset and the ISPRS Potsdam dataset,\nexperimental results indicate that LoG-CAN outperforms the state-of-the-art\nmethods for general semantic segmentation, while significantly reducing network\nparameters and computation. Code is available\nat~\\href{https://github.com/xwmaxwma/rssegmentation}{https://github.com/xwmaxwma/rssegmentation}.\n","authors":["Xiaowen Ma","Mengting Ma","Chenlu Hu","Zhiyuan Song","Ziyan Zhao","Tian Feng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07747v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07744v1","updated":"2023-03-14T09:42:49Z","published":"2023-03-14T09:42:49Z","title":"Sliding at first order: Higher-order momentum distributions for\n  discontinuous image registration","summary":"  In this paper, we propose a new approach to deformable image registration\nthat captures sliding motions. The large deformation diffeomorphic metric\nmapping (LDDMM) registration method faces challenges in representing sliding\nmotion since it per construction generates smooth warps. To address this issue,\nwe extend LDDMM by incorporating both zeroth- and first-order momenta with a\nnon-differentiable kernel. This allows to represent both discontinuous\ndeformation at switching boundaries and diffeomorphic deformation in\nhomogeneous regions. We provide a mathematical analysis of the proposed\ndeformation model from the viewpoint of discontinuous systems. To evaluate our\napproach, we conduct experiments on both artificial images and the publicly\navailable DIR-Lab 4DCT dataset. Results show the effectiveness of our approach\nin capturing plausible sliding motion.\n","authors":["Lili Bao","Jiahao Lu","Shihui Ying","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2303.07744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07740v1","updated":"2023-03-14T09:36:42Z","published":"2023-03-14T09:36:42Z","title":"Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening","summary":"  Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.\n","authors":["Min Cao","Yang Bai","Jingyao Wang","Ziqiang Cao","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07740v1.pdf","comment":"11 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.07034v2","updated":"2023-03-14T09:19:03Z","published":"2023-03-13T11:53:40Z","title":"Pretrained ViTs Yield Versatile Representations For Medical Images","summary":"  Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis, pushing the\nstate-of-the-art in classification, detection and segmentation tasks. Over the\nlast years, vision transformers (ViTs) have appeared as a competitive\nalternative to CNNs, yielding impressive levels of performance in the natural\nimage domain, while possessing several interesting properties that could prove\nbeneficial for medical imaging tasks. In this work, we explore the benefits and\ndrawbacks of transformer-based models for medical image classification. We\nconduct a series of experiments on several standard 2D medical image benchmark\ndatasets and tasks. Our findings show that, while CNNs perform better if\ntrained from scratch, off-the-shelf vision transformers can perform on par with\nCNNs when pretrained on ImageNet, both in a supervised and self-supervised\nsetting, rendering them as a viable alternative to CNNs.\n","authors":["Christos Matsoukas","Johan Fredin Haslum","Magnus Söderberg","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2303.07034v2.pdf","comment":"Extended version of arXiv:2108.09038 originally published at the ICCV\n  2021 Workshop on Computer Vision for Automated Medical Diagnosis"},{"id":"http://arxiv.org/abs/2303.04940v3","updated":"2023-03-14T09:10:20Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in misaligned hazy and clear image pairs. In this\npaper, we propose a non-aligned supervision framework that consists of three\nnetworks - dehazing, airlight, and transmission. In particular, we explore a\nnon-alignment setting by utilizing a clear reference image that is not aligned\nwith the hazy input image to supervise the dehazing network through a\nmulti-scale reference loss that compares the features of the two images. Our\nsetting makes it easier to collect hazy/clear image pairs in real-world\nenvironments, even under conditions of misalignment and shift views. To\ndemonstrate this, we have created a new hazy dataset called \"Phone-Hazy\", which\nwas captured using mobile phones in both rural and urban areas. Additionally,\nwe present a mean and variance self-attention network to model the infinite\nairlight using dark channel prior as position guidance, and employ a channel\nattention network to estimate the three-channel transmission. Experimental\nresults show that our framework outperforms current state-of-the-art methods in\nthe real-world image dehazing. Phone-Hazy and code will be available at\nhttps://github.com/hello2377/NSDNet.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14511v2","updated":"2023-03-14T09:10:14Z","published":"2023-02-28T12:01:16Z","title":"A Unified BEV Model for Joint Learning of 3D Local Features and Overlap\n  Estimation","summary":"  Pairwise point cloud registration is a critical task for many applications,\nwhich heavily depends on finding correct correspondences from the two point\nclouds. However, the low overlap between input point clouds causes the\nregistration to fail easily, leading to mistaken overlapping and mismatched\ncorrespondences, especially in scenes where non-overlapping regions contain\nsimilar structures. In this paper, we present a unified bird's-eye view (BEV)\nmodel for jointly learning of 3D local features and overlap estimation to\nfulfill pairwise registration and loop closure. Feature description is\nperformed by a sparse UNet-like network based on BEV representation, and 3D\nkeypoints are extracted by a detection head for 2D locations, and a regression\nhead for heights. For overlap detection, a cross-attention module is applied\nfor interacting contextual information of input point clouds, followed by a\nclassification head to estimate the overlapping region. We evaluate our unified\nmodel extensively on the KITTI dataset and Apollo-SouthBay dataset. The\nexperiments demonstrate that our method significantly outperforms existing\nmethods on overlap estimation, especially in scenes with small overlaps. It\nalso achieves top registration performance on both datasets in terms of\ntranslation and rotation errors.\n","authors":["Lin Li","Wendong Ding","Yongkun Wen","Yufei Liang","Yong Liu","Guowei Wan"],"pdf_url":"https://arxiv.org/pdf/2302.14511v2.pdf","comment":"8 pages. Accepted by ICRA-2023"},{"id":"http://arxiv.org/abs/2207.07921v2","updated":"2023-03-14T09:10:08Z","published":"2022-07-16T12:11:28Z","title":"CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image\n  Prior","summary":"  Euler's elastica constitute an appealing variational image inpainting model.\nIt minimises an energy that involves the total variation as well as the level\nline curvature. These components are transparent and make it attractive for\nshape completion tasks. However, its gradient flow is a singular, anisotropic,\nand nonlinear PDE of fourth order, which is numerically challenging: It is\ndifficult to find efficient algorithms that offer sharp edges and good rotation\ninvariance. As a remedy, we design the first neural algorithm that simulates\ninpainting with Euler's Elastica. We use the deep energy concept which employs\nthe variational energy as neural network loss. Furthermore, we pair it with a\ndeep image prior where the network architecture itself acts as a prior. This\nyields better inpaintings by steering the optimisation trajectory closer to the\ndesired solution. Our results are qualitatively on par with state-of-the-art\nalgorithms on elastica-based shape completion. They combine good rotation\ninvariance with sharp edges. Moreover, we benefit from the high efficiency and\neffortless parallelisation within a neural framework. Our neural elastica\napproach only requires 3x3 central difference stencils. It is thus much simpler\nthan other well-performing algorithms for elastica inpainting. Last but not\nleast, it is unsupervised as it requires no ground truth training data.\n","authors":["Karl Schrader","Tobias Alt","Joachim Weickert","Michael Ertel"],"pdf_url":"https://arxiv.org/pdf/2207.07921v2.pdf","comment":"In Proceedings of the 10th European Workshop on Visual Information\n  Processing, Lisbon, 2022"},{"id":"http://arxiv.org/abs/2303.07717v1","updated":"2023-03-14T09:05:19Z","published":"2023-03-14T09:05:19Z","title":"HALOS: Hallucination-free Organ Segmentation after Organ Resection\n  Surgery","summary":"  The wide range of research in deep learning-based medical image segmentation\npushed the boundaries in a multitude of applications. A clinically relevant\nproblem that received less attention is the handling of scans with irregular\nanatomy, e.g., after organ resection. State-of-the-art segmentation models\noften lead to organ hallucinations, i.e., false-positive predictions of organs,\nwhich cannot be alleviated by oversampling or post-processing. Motivated by the\nincreasing need to develop robust deep learning models, we propose HALOS for\nabdominal organ segmentation in MR images that handles cases after organ\nresection surgery. To this end, we combine missing organ classification and\nmulti-organ segmentation tasks into a multi-task model, yielding a\nclassification-assisted segmentation pipeline. The segmentation network learns\nto incorporate knowledge about organ existence via feature fusion modules.\nExtensive experiments on a small labeled test set and large-scale UK Biobank\ndata demonstrate the effectiveness of our approach in terms of higher\nsegmentation Dice scores and near-to-zero false positive prediction rate.\n","authors":["Anne-Marie Rickmann","Murong Xu","Tom Nuno Wolf","Oksana Kovalenko","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07717v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging (IPMI) 2023"},{"id":"http://arxiv.org/abs/2303.07716v1","updated":"2023-03-14T09:03:54Z","published":"2023-03-14T09:03:54Z","title":"BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow\n  Estimation","summary":"  Event cameras provide high temporal precision, low data rates, and high\ndynamic range visual perception, which are well-suited for optical flow\nestimation. While data-driven optical flow estimation has obtained great\nsuccess in RGB cameras, its generalization performance is seriously hindered in\nevent cameras mainly due to the limited and biased training data. In this\npaper, we present a novel simulator, BlinkSim, for the fast generation of\nlarge-scale data for event-based optical flow. BlinkSim consists of a\nconfigurable rendering engine and a flexible engine for event data simulation.\nBy leveraging the wealth of current 3D assets, the rendering engine enables us\nto automatically build up thousands of scenes with different objects, textures,\nand motion patterns and render very high-frequency images for realistic event\ndata simulation. Based on BlinkSim, we construct a large training dataset and\nevaluation benchmark BlinkFlow that contains sufficient, diversiform, and\nchallenging event data with optical flow ground truth. Experiments show that\nBlinkFlow improves the generalization performance of state-of-the-art methods\nby more than 40% on average and up to 90%. Moreover, we further propose an\nEvent optical Flow transFormer (E-FlowFormer) architecture. Powered by our\nBlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on MVSEC\ndataset and 14% on DSEC dataset and presents the best generalization\nperformance.\n","authors":["Yijin Li","Zhaoyang Huang","Shuo Chen","Xiaoyu Shi","Hongsheng Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14125v3","updated":"2023-03-14T08:59:16Z","published":"2022-08-30T10:21:40Z","title":"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images","summary":"  Diffusion models are a special type of generative model, capable of\nsynthesising new data from a learnt distribution. We introduce DISPR, a\ndiffusion-based model for solving the inverse problem of three-dimensional (3D)\ncell shape prediction from two-dimensional (2D) single cell microscopy images.\nUsing the 2D microscopy image as a prior, DISPR is conditioned to predict\nrealistic 3D shape reconstructions. To showcase the applicability of DISPR as a\ndata augmentation tool in a feature-based single cell classification task, we\nextract morphological features from the red blood cells grouped into six highly\nimbalanced classes. Adding features from the DISPR predictions to the three\nminority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm\n4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that\ndiffusion models can be successfully applied to inverse biomedical problems,\nand that they learn to reconstruct 3D shapes with realistic morphological\nfeatures from 2D microscopy images.\n","authors":["Dominik J. E. Waibel","Ernst Röell","Bastian Rieck","Raja Giryes","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2208.14125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07714v1","updated":"2023-03-14T08:55:24Z","published":"2023-03-14T08:55:24Z","title":"Freehand 2D Ultrasound Probe Calibration for Image Fusion with 3D MRI/CT","summary":"  The aim of this work is to implement a simple freehand ultrasound (US) probe\ncalibration technique. This will enable us to visualize US image data during\nsurgical procedures using augmented reality. The performance of the system was\nevaluated with different experiments using two different pose estimation\ntechniques. A near-millimeter accuracy can be achieved with the proposed\napproach. The developed system is cost-effective, simple and rapid with low\ncalibration error\n","authors":["Yogesh Langhe","Katrin Skerl","Adrien Bartoli"],"pdf_url":"https://arxiv.org/pdf/2303.07714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07709v1","updated":"2023-03-14T08:51:51Z","published":"2023-03-14T08:51:51Z","title":"3D Face Arbitrary Style Transfer","summary":"  Style transfer of 3D faces has gained more and more attention. However,\nprevious methods mainly use images of artistic faces for style transfer while\nignoring arbitrary style images such as abstract paintings. To solve this\nproblem, we propose a novel method, namely Face-guided Dual Style Transfer\n(FDST). To begin with, FDST employs a 3D decoupling module to separate facial\ngeometry and texture. Then we propose a style fusion strategy for facial\ngeometry. Subsequently, we design an optimization-based DDSG mechanism for\ntextures that can guide the style transfer by two style images. Besides the\nnormal style image input, DDSG can utilize the original face input as another\nstyle input as the face prior. By this means, high-quality face arbitrary style\ntransfer results can be obtained. Furthermore, FDST can be applied in many\ndownstream tasks, including region-controllable style transfer, high-fidelity\nface texture reconstruction, large-pose face reconstruction, and artistic face\nreconstruction. Comprehensive quantitative and qualitative results show that\nour method can achieve comparable performance. All source codes and pre-trained\nweights will be released to the public.\n","authors":["Xiangwen Deng","Yingshuang Zou","Yuanhao Cai","Chendong Zhao","Yang Liu","Zhifang Liu","Yuxiao Liu","Jiawei Zhou","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06615v2","updated":"2023-03-14T08:39:23Z","published":"2023-03-12T09:11:14Z","title":"Iterative Geometry Encoding Volume for Stereo Matching","summary":"  Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in\nmatching tasks. However, all-pairs correlations lack non-local geometry\nknowledge and have difficulties tackling local ambiguities in ill-posed\nregions. In this paper, we propose Iterative Geometry Encoding Volume\n(IGEV-Stereo), a new deep network architecture for stereo matching. The\nproposed IGEV-Stereo builds a combined geometry encoding volume that encodes\ngeometry and context information as well as local matching details, and\niteratively indexes it to update the disparity map. To speed up the\nconvergence, we exploit GEV to regress an accurate starting point for ConvGRUs\niterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)\namong all published methods and is the fastest among the top 10 methods. In\naddition, IGEV-Stereo has strong cross-dataset generalization as well as high\ninference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.\nIGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is\navailable at https://github.com/gangweiX/IGEV.\n","authors":["Gangwei Xu","Xianqi Wang","Xiaohuan Ding","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2303.06615v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07125v2","updated":"2023-03-14T08:29:49Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\nhttps://github.com/ai-med/PANIC .\n","authors":["Tom Nuno Wolf","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v2.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07700v1","updated":"2023-03-14T08:28:36Z","published":"2023-03-14T08:28:36Z","title":"PATS: Patch Area Transportation with Subdivision for Local Feature\n  Matching","summary":"  Local feature matching aims at establishing sparse correspondences between a\npair of images. Recently, detectorfree methods present generally better\nperformance but are not satisfactory in image pairs with large scale\ndifferences. In this paper, we propose Patch Area Transportation with\nSubdivision (PATS) to tackle this issue. Instead of building an expensive image\npyramid, we start by splitting the original image pair into equal-sized patches\nand gradually resizing and subdividing them into smaller patches with the same\nscale. However, estimating scale differences between these patches is\nnon-trivial since the scale differences are determined by both relative camera\nposes and scene structures, and thus spatially varying over image pairs.\nMoreover, it is hard to obtain the ground truth for real scenes. To this end,\nwe propose patch area transportation, which enables learning scale differences\nin a self-supervised manner. In contrast to bipartite graph matching, which\nonly handles one-to-one matching, our patch area transportation can deal with\nmany-to-many relationships. PATS improves both matching accuracy and coverage,\nand shows superior performance in downstream tasks, such as relative pose\nestimation, visual localization, and optical flow estimation. The source code\nwill be released to benefit the community.\n","authors":["Junjie Ni","Yijin Li","Zhaoyang Huang","Hongsheng Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07700v1.pdf","comment":"Project page: https://zju3dv.github.io/pats"},{"id":"http://arxiv.org/abs/2303.07697v1","updated":"2023-03-14T08:22:18Z","published":"2023-03-14T08:22:18Z","title":"DisCoHead: Audio-and-Video-Driven Talking Head Generation by\n  Disentangled Control of Head Pose and Facial Expressions","summary":"  For realistic talking head generation, creating natural head motion while\nmaintaining accurate lip synchronization is essential. To fulfill this\nchallenging task, we propose DisCoHead, a novel method to disentangle and\ncontrol head pose and facial expressions without supervision. DisCoHead uses a\nsingle geometric transformation as a bottleneck to isolate and extract head\nmotion from a head-driving video. Either an affine or a thin-plate spline\ntransformation can be used and both work well as geometric bottlenecks. We\nenhance the efficiency of DisCoHead by integrating a dense motion estimator and\nthe encoder of a generator which are originally separate modules. Taking a step\nfurther, we also propose a neural mix approach where dense motion is estimated\nand applied implicitly by the encoder. After applying the disentangled head\nmotion to a source identity, DisCoHead controls the mouth region according to\nspeech audio, and it blinks eyes and moves eyebrows following a separate\ndriving video of the eye region, via the weight modulation of convolutional\nneural networks. The experiments using multiple datasets show that DisCoHead\nsuccessfully generates realistic audio-and-video-driven talking heads and\noutperforms state-of-the-art methods. Project page:\nhttps://deepbrainai-research.github.io/discohead/\n","authors":["Geumbyeol Hwang","Sunwon Hong","Seunghyun Lee","Sungwoo Park","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07697v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2211.14769v3","updated":"2023-03-14T08:21:31Z","published":"2022-11-27T09:01:31Z","title":"Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied\n  Agents under Federated Learning","summary":"  Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n","authors":["Yunchao Zhang","Zonglin Di","Kaiwen Zhou","Cihang Xie","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.14769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11308v2","updated":"2023-03-14T08:16:51Z","published":"2022-08-24T05:29:47Z","title":"Deep model with built-in cross-attention alignment for acoustic echo\n  cancellation","summary":"  With recent research advances, deep learning models have become an attractive\nchoice for acoustic echo cancellation (AEC) in real-time teleconferencing\napplications. Since acoustic echo is one of the major sources of poor audio\nquality, a wide variety of deep models have been proposed. However, an\nimportant but often omitted requirement for good echo cancellation quality is\nthe synchronization of the microphone and far end signals. Typically\nimplemented using classical algorithms based on cross-correlation, the\nalignment module is a separate functional block with known design limitations.\nIn our work we propose a deep learning architecture with built-in\nself-attention based alignment, which is able to handle unaligned inputs,\nimproving echo cancellation performance while simplifying the communication\npipeline. Moreover, we show that our approach achieves significant improvements\nfor difficult delay estimation cases on real recordings from AEC Challenge data\nset.\n","authors":["Evgenii Indenbom","Nicolae-Cătălin Ristea","Ando Saabas","Tanel Pärnamaa","Jegor Gužvin"],"pdf_url":"https://arxiv.org/pdf/2208.11308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12268v3","updated":"2023-03-14T08:08:01Z","published":"2022-11-22T13:37:34Z","title":"Out-of-Candidate Rectification for Weakly Supervised Semantic\n  Segmentation","summary":"  Weakly supervised semantic segmentation is typically inspired by class\nactivation maps, which serve as pseudo masks with class-discriminative regions\nhighlighted. Although tremendous efforts have been made to recall precise and\ncomplete locations for each class, existing methods still commonly suffer from\nthe unsolicited Out-of-Candidate (OC) error predictions that not belongs to the\nlabel candidates, which could be avoidable since the contradiction with\nimage-level class tags is easy to be detected. In this paper, we develop a\ngroup ranking-based Out-of-Candidate Rectification (OCR) mechanism in a\nplug-and-play fashion. Firstly, we adaptively split the semantic categories\ninto In-Candidate (IC) and OC groups for each OC pixel according to their prior\nannotation correlation and posterior prediction correlation. Then, we derive a\ndifferentiable rectification loss to force OC pixels to shift to the IC group.\nIncorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM,\nMCTformer), we can achieve remarkable performance gains on both Pascal VOC\n(+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with\nnegligible extra training overhead, which justifies the effectiveness and\ngenerality of our OCR.\n","authors":["Zesen Cheng","Pengchong Qiao","Kehan Li","Siheng Li","Pengxu Wei","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2211.12268v3.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.07216v2","updated":"2023-03-14T07:48:31Z","published":"2023-03-13T15:51:38Z","title":"Parallel Vertex Diffusion for Unified Visual Grounding","summary":"  Unified visual grounding pursues a simple and generic technical route to\nleverage multi-task data with less task-specific design. The most advanced\nmethods typically present boxes and masks as vertex sequences to model\nreferring detection and segmentation as an autoregressive sequential vertex\ngeneration paradigm. However, generating high-dimensional vertex sequences\nsequentially is error-prone because the upstream of the sequence remains static\nand cannot be refined based on downstream vertex information, even if there is\na significant location gap. Besides, with limited vertexes, the inferior\nfitting of objects with complex contours restricts the performance upper bound.\nTo deal with this dilemma, we propose a parallel vertex generation paradigm for\nsuperior high-dimension scalability with a diffusion model by simply modifying\nthe noise dimension. An intuitive materialization of our paradigm is Parallel\nVertex Diffusion (PVD) to directly set vertex coordinates as the generation\ntarget and use a diffusion model to train and infer. We claim that it has two\nflaws: (1) unnormalized coordinate caused a high variance of loss value; (2)\nthe original training objective of PVD only considers point consistency but\nignores geometry consistency. To solve the first flaw, Center Anchor Mechanism\n(CAM) is designed to convert coordinates as normalized offset values to\nstabilize the training loss value. For the second flaw, Angle summation loss\n(ASL) is designed to constrain the geometry difference of prediction and ground\ntruth vertexes for geometry-level consistency. Empirical results show that our\nPVD achieves state-of-the-art in both referring detection and segmentation, and\nour paradigm is more scalable and efficient than sequential vertex generation\nwith high-dimension data.\n","authors":["Zesen Cheng","Kehan Li","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06373v2","updated":"2023-03-14T07:47:53Z","published":"2023-03-11T10:44:44Z","title":"Recursive Generalization Transformer for Image Super-Resolution","summary":"  Transformer architectures have exhibited remarkable performance in image\nsuper-resolution (SR). Since the quadratic computational complexity of the\nself-attention (SA) in Transformer, existing methods tend to adopt SA in a\nlocal region to reduce overheads. However, the local design restricts the\nglobal context exploitation, which is critical for accurate image\nreconstruction. In this work, we propose the Recursive Generalization\nTransformer (RGT) for image SR, which can capture global spatial information\nand is suitable for high-resolution images. Specifically, we propose the\nrecursive-generalization self-attention (RG-SA). It recursively aggregates\ninput features into representative feature maps, and then utilizes\ncross-attention to extract global information. Meanwhile, the channel\ndimensions of attention matrices (query, key, and value) are further scaled for\na better trade-off between computational overheads and performance.\nFurthermore, we combine the RG-SA with local self-attention to enhance the\nexploitation of the global context, and propose the hybrid adaptive integration\n(HAI) for module integration. The HAI allows the direct and effective fusion\nbetween features at different levels (local or global). Extensive experiments\ndemonstrate that our RGT outperforms recent state-of-the-art methods.\n","authors":["Zheng Chen","Yulun Zhang","Jinjin Gu","Linghe Kong","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.06373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07679v1","updated":"2023-03-14T07:42:02Z","published":"2023-03-14T07:42:02Z","title":"Feature representations useful for predicting image memorability","summary":"  Predicting image memorability has attracted interest in various fields.\nConsequently, prediction accuracy with convolutional neural network (CNN)\nmodels has been approaching the empirical upper bound estimated based on human\nconsistency. However, identifying which feature representations embedded in CNN\nmodels are responsible for such high prediction accuracy of memorability\nremains an open question. To tackle this problem, this study sought to identify\nmemorability-related feature representations in CNN models using brain\nsimilarity. Specifically, memorability prediction accuracy and brain similarity\nwere examined and assessed by Brain-Score across 16,860 layers in 64 CNN models\npretrained for object recognition. A clear tendency was shown in this\ncomprehensive analysis that layers with high memorability prediction accuracy\nhad higher brain similarity with the inferior temporal (IT) cortex, which is\nthe highest stage in the ventral visual pathway. Furthermore, fine-tuning the\n64 CNN models revealed that brain similarity with the IT cortex at the\npenultimate layer was positively correlated with memorability prediction\naccuracy. This analysis also showed that the best fine-tuned model provided\naccuracy comparable to the state-of-the-art CNN models developed specifically\nfor memorability prediction. Overall, this study's results indicated that the\nCNN models' great success in predicting memorability relies on feature\nrepresentation acquisition similar to the IT cortex. This study advanced our\nunderstanding of feature representations and its use for predicting image\nmemorability.\n","authors":["Takumi Harada","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2303.07679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00290v2","updated":"2023-03-14T07:30:03Z","published":"2022-12-01T05:31:07Z","title":"Component Segmentation of Engineering Drawings Using Graph Convolutional\n  Networks","summary":"  We present a data-driven framework to automate the vectorization and machine\ninterpretation of 2D engineering part drawings. In industrial settings, most\nmanufacturing engineers still rely on manual reads to identify the topological\nand manufacturing requirements from drawings submitted by designers. The\ninterpretation process is laborious and time-consuming, which severely inhibits\nthe efficiency of part quotation and manufacturing tasks. While recent advances\nin image-based computer vision methods have demonstrated great potential in\ninterpreting natural images through semantic segmentation approaches, the\napplication of such methods in parsing engineering technical drawings into\nsemantically accurate components remains a significant challenge. The severe\npixel sparsity in engineering drawings also restricts the effective\nfeaturization of image-based data-driven methods. To overcome these challenges,\nwe propose a deep learning based framework that predicts the semantic type of\neach vectorized component. Taking a raster image as input, we vectorize all\ncomponents through thinning, stroke tracing, and cubic bezier fitting. Then a\ngraph of such components is generated based on the connectivity between the\ncomponents. Finally, a graph convolutional neural network is trained on this\ngraph data to identify the semantic type of each component. We test our\nframework in the context of semantic segmentation of text, dimension and,\ncontour components in engineering drawings. Results show that our method yields\nthe best performance compared to recent image, and graph-based segmentation\nmethods.\n","authors":["Wentai Zhang","Joe Joseph","Yue Yin","Liuyue Xie","Tomotake Furuhata","Soji Yamakawa","Kenji Shimada","Levent Burak Kara"],"pdf_url":"https://arxiv.org/pdf/2212.00290v2.pdf","comment":"Preprint accepted to Computers in Industry"},{"id":"http://arxiv.org/abs/2303.07677v1","updated":"2023-03-14T07:26:55Z","published":"2023-03-14T07:26:55Z","title":"Sr-init: An interpretable layer pruning method","summary":"  Despite the popularization of deep neural networks (DNNs) in many fields, it\nis still challenging to deploy state-of-the-art models to resource-constrained\ndevices due to high computational overhead. Model pruning provides a feasible\nsolution to the aforementioned challenges. However, the interpretation of\nexisting pruning criteria is always overlooked. To counter this issue, we\npropose a novel layer pruning method by exploring the Stochastic\nRe-initialization. Our SR-init method is inspired by the discovery that the\naccuracy drop due to stochastic re-initialization of layer parameters differs\nin various layers. On the basis of this observation, we come up with a layer\npruning criterion, i.e., those layers that are not sensitive to stochastic\nre-initialization (low accuracy drop) produce less contribution to the model\nand could be pruned with acceptable loss. Afterward, we experimentally verify\nthe interpretability of SR-init via feature visualization. The visual\nexplanation demonstrates that SR-init is theoretically feasible, thus we\ncompare it with state-of-the-art methods to further evaluate its\npracticability. As for ResNet56 on CIFAR-10 and CIFAR-100, SR-init achieves a\ngreat reduction in parameters (63.98% and 37.71%) with an ignorable drop in\ntop-1 accuracy (-0.56% and 0.8%). With ResNet50 on ImageNet, we achieve a\n15.59% FLOPs reduction by removing 39.29% of the parameters, with only a drop\nof 0.6% in top-1 accuracy. Our code is available at\nhttps://github.com/huitang-zjut/SRinit.\n","authors":["Hui Tang","Yao Lu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2303.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07674v1","updated":"2023-03-14T07:25:38Z","published":"2023-03-14T07:25:38Z","title":"Koos Classification of Vestibular Schwannoma via Image Translation-Based\n  Unsupervised Cross-Modality Domain Adaptation","summary":"  The Koos grading scale is a classification system for vestibular schwannoma\n(VS) used to characterize the tumor and its effects on adjacent brain\nstructures. The Koos classification captures many of the characteristics of\ntreatment deci-sions and is often used to determine treatment plans. Although\nboth contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)\nscanning can be used for Koos Classification, hrT2 scanning is gaining interest\nbecause of its higher safety and cost-effectiveness. However, in the absence of\nannotations for hrT2 scans, deep learning methods often inevitably suffer from\nperformance deg-radation due to unsupervised learning. If ceT1 scans and their\nannotations can be used for unsupervised learning of hrT2 scans, the\nperformance of Koos classifi-cation using unlabeled hrT2 scans will be greatly\nimproved. In this regard, we propose an unsupervised cross-modality domain\nadaptation method based on im-age translation by transforming annotated ceT1\nscans into hrT2 modality and us-ing their annotations to achieve supervised\nlearning of hrT2 modality. Then, the VS and 7 adjacent brain structures related\nto Koos classification in hrT2 scans were segmented. Finally, handcrafted\nfeatures are extracted from the segmenta-tion results, and Koos grade is\nclassified using a random forest classifier. The proposed method received rank\n1 on the Koos classification task of the Cross-Modality Domain Adaptation\n(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of\n0.2148 for the validation set and 0.26 for the test set.\n","authors":["Tao Yang","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07674v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07670v1","updated":"2023-03-14T07:23:27Z","published":"2023-03-14T07:23:27Z","title":"Co-Salient Object Detection with Co-Representation Purification","summary":"  Co-salient object detection (Co-SOD) aims at discovering the common objects\nin a group of relevant images. Mining a co-representation is essential for\nlocating co-salient objects. Unfortunately, the current Co-SOD method does not\npay enough attention that the information not related to the co-salient object\nis included in the co-representation. Such irrelevant information in the\nco-representation interferes with its locating of co-salient objects. In this\npaper, we propose a Co-Representation Purification (CoRP) method aiming at\nsearching noise-free co-representation. We search a few pixel-wise embeddings\nprobably belonging to co-salient regions. These embeddings constitute our\nco-representation and guide our prediction. For obtaining purer\nco-representation, we use the prediction to iteratively reduce irrelevant\nembeddings in our co-representation. Experiments on three datasets demonstrate\nthat our CoRP achieves state-of-the-art performances on the benchmark datasets.\nOur source code is available at https://github.com/ZZY816/CoRP.\n","authors":["Ziyue Zhu","Zhao Zhang","Zheng Lin","Xing Sun","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.07670v1.pdf","comment":"Accepted by TPAMI 2023"},{"id":"http://arxiv.org/abs/2303.07662v1","updated":"2023-03-14T07:07:34Z","published":"2023-03-14T07:07:34Z","title":"One scalar is all you need -- absolute depth estimation using monocular\n  self-supervision","summary":"  Self-supervised monocular depth estimators can be trained or fine-tuned on\nnew scenes using only images and no ground-truth depth data, achieving good\naccuracy. However, these estimators suffer from the inherent ambiguity of the\ndepth scale, significantly limiting their applicability. In this work, we\npresent a method for transferring the depth-scale from existing source datasets\ncollected with ground-truth depths to depth estimators that are trained using\nself-supervision on a newly collected target dataset consisting of images only,\nsolving a significant limiting factor. We show that self-supervision based on\nprojective geometry results in predicted depths that are linearly correlated\nwith their ground-truth depths. Moreover, the linearity of this relationship\nalso holds when jointly training on images from two different (real or\nsynthetic) source and target domains. We utilize this observed property and\nmodel the relationship between the ground-truth and the predicted up-to-scale\ndepths of images from the source domain using a single global scalar. Then, we\nscale the predicted up-to-scale depths of images from the target domain using\nthe estimated global scaling factor, performing depth-scale transfer between\nthe two domains. This suggested method was evaluated on the target KITTI and\nDDAD datasets, while using other real or synthetic source datasets, that have a\nlarger field-of-view, other image style or structural content. Our approach\nachieves competitive accuracy on KITTI, even without using the specially\ntailored vKITTI or vKITTI2 datasets, and higher accuracy on DDAD, when using\nboth real or synthetic source datasets.\n","authors":["Alexandra Dana","Nadav Carmel","Amit Shomer","Ofer Manela","Tomer Peleg"],"pdf_url":"https://arxiv.org/pdf/2303.07662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09869v2","updated":"2023-03-14T07:03:54Z","published":"2023-01-24T09:09:35Z","title":"Image Super-Resolution using Efficient Striped Window Transformer","summary":"  Transformers have achieved remarkable results in single-image\nsuper-resolution (SR). However, the challenge of balancing model performance\nand complexity has hindered their application in lightweight SR (LSR). To\ntackle this challenge, we propose an efficient striped window transformer\n(ESWT). We revisit the normalization layer in the transformer and design a\nconcise and efficient transformer structure to build the ESWT. Furthermore, we\nintroduce a striped window mechanism to model long-term dependencies more\nefficiently. To fully exploit the potential of the ESWT, we propose a novel\nflexible window training strategy that can improve the performance of the ESWT\nwithout additional cost. Extensive experiments show that ESWT outperforms\nstate-of-the-art LSR transformers, and achieves a better trade-off between\nmodel performance and complexity. The ESWT requires fewer parameters, incurs\nfaster inference, smaller FLOPs, and less memory consumption, making it a\npromising solution for LSR.\n","authors":["Jinpeng Shi","Hui Li","Tianle Liu","Yulong Liu","Mingjian Zhang","Jinchen Zhu","Ling Zheng","Shizhuang Weng"],"pdf_url":"https://arxiv.org/pdf/2301.09869v2.pdf","comment":"SOTA lightweight super-resolution transformer. 8 pages, 9 figures and\n  6 tables. The Code is available at\n  https://github.com/Fried-Rice-Lab/FriedRiceLab"},{"id":"http://arxiv.org/abs/2303.07653v1","updated":"2023-03-14T06:45:13Z","published":"2023-03-14T06:45:13Z","title":"NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from\n  Multi-view Images","summary":"  We study the problem of reconstructing 3D feature curves of an object from a\nset of calibrated multi-view images. To do so, we learn a neural implicit field\nrepresenting the density distribution of 3D edges which we refer to as Neural\nEdge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based\nrendering loss where a 2D edge map is rendered at a given view and is compared\nto the ground-truth edge map extracted from the image of that view. The\nrendering-based differentiable optimization of NEF fully exploits 2D edge\ndetection, without needing a supervision of 3D edges, a 3D geometric operator\nor cross-view edge correspondence. Several technical designs are devised to\nensure learning a range-limited and view-independent NEF for robust edge\nextraction. The final parametric 3D curves are extracted from NEF with an\niterative optimization method. On our benchmark with synthetic data, we\ndemonstrate that NEF outperforms existing state-of-the-art methods on all\nmetrics. Project page: https://yunfan1202.github.io/NEF/.\n","authors":["Yunfan Ye","Renjiao Yi","Zhirui Gao","Chenyang Zhu","Zhiping Cai","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07653v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07651v1","updated":"2023-03-14T06:38:17Z","published":"2023-03-14T06:38:17Z","title":"Context Normalization for Robust Image Classification","summary":"  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n","authors":["Bilal Faye","Mohamed-Djallel Dilmi","Hanane Azzag","Mustapha Lebbah","Fangchen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07648v1","updated":"2023-03-14T06:30:55Z","published":"2023-03-14T06:30:55Z","title":"SimFLE: Simple Facial Landmark Encoding for Self-Supervised Facial\n  Expression Recognition in the Wild","summary":"  One of the key issues in facial expression recognition in the wild (FER-W) is\nthat curating large-scale labeled facial images is challenging due to the\ninherent complexity and ambiguity of facial images. Therefore, in this paper,\nwe propose a self-supervised simple facial landmark encoding (SimFLE) method\nthat can learn effective encoding of facial landmarks, which are important\nfeatures for improving the performance of FER-W, without expensive labels.\nSpecifically, we introduce novel FaceMAE module for this purpose. FaceMAE\nreconstructs masked facial images with elaborately designed semantic masking.\nUnlike previous random masking, semantic masking is conducted based on channel\ninformation processed in the backbone, so rich semantics of channels can be\nexplored. Additionally, the semantic masking process is fully trainable,\nenabling FaceMAE to guide the backbone to learn spatial details and contextual\nproperties of fine-grained facial landmarks. Experimental results on several\nFER-W benchmarks prove that the proposed SimFLE is superior in facial landmark\nlocalization and noticeably improved performance compared to the supervised\nbaseline and other self-supervised methods.\n","authors":["Jiyong Moon","Seongsik Park"],"pdf_url":"https://arxiv.org/pdf/2303.07648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08427v2","updated":"2023-03-14T06:07:21Z","published":"2022-07-18T08:22:18Z","title":"Adaptive Assignment for Geometry Aware Local Feature Matching","summary":"  The detector-free feature matching approaches are currently attracting great\nattention thanks to their excellent performance. However, these methods still\nstruggle at large-scale and viewpoint variations, due to the geometric\ninconsistency resulting from the application of the mutual nearest neighbour\ncriterion (\\ie, one-to-one assignment) in patch-level matching.Accordingly, we\nintroduce AdaMatcher, which first accomplishes the feature correlation and\nco-visible area estimation through an elaborate feature interaction module,\nthen performs adaptive assignment on patch-level matching while estimating the\nscales between images, and finally refines the co-visible matches through scale\nalignment and sub-pixel regression module.Extensive experiments show that\nAdaMatcher outperforms solid baselines and achieves state-of-the-art results on\nmany downstream tasks. Additionally, the adaptive assignment and sub-pixel\nrefinement module can be used as a refinement network for other matching\nmethods, such as SuperGlue, to boost their performance further. The code will\nbe publicly available at https://github.com/AbyssGaze/AdaMatcher.\n","authors":["Dihe Huang","Ying Chen","Shang Xu","Yong Liu","Wenlong Wu","Yikang Ding","Chengjie Wang","Fan Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08427v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07641v1","updated":"2023-03-14T06:03:57Z","published":"2023-03-14T06:03:57Z","title":"Rethinking Image-based Table Recognition Using Weakly Supervised Methods","summary":"  Most of the previous methods for table recognition rely on training datasets\ncontaining many richly annotated table images. Detailed table image annotation,\ne.g., cell or text bounding box annotation, however, is costly and often\nsubjective. In this paper, we propose a weakly supervised model named WSTabNet\nfor table recognition that relies only on HTML (or LaTeX) code-level\nannotations of table images. The proposed model consists of three main parts:\nan encoder for feature extraction, a structure decoder for generating table\nstructure, and a cell decoder for predicting the content of each cell in the\ntable. Our system is trained end-to-end by stochastic gradient descent\nalgorithms, requiring only table images and their ground-truth HTML (or LaTeX)\nrepresentations. To facilitate table recognition with deep learning, we create\nand release WikiTableSet, the largest publicly available image-based table\nrecognition dataset built from Wikipedia. WikiTableSet contains nearly 4\nmillion English table images, 590K Japanese table images, and 640k French table\nimages with corresponding HTML representation and cell bounding boxes. The\nextensive experiments on WikiTableSet and two large-scale datasets: FinTabNet\nand PubTabNet demonstrate that the proposed weakly supervised model achieves\nbetter, or similar accuracies compared to the state-of-the-art models on all\nbenchmark datasets.\n","authors":["Nam Tuan Ly","Atsuhiro Takasu","Phuc Nguyen","Hideaki Takeda"],"pdf_url":"https://arxiv.org/pdf/2303.07641v1.pdf","comment":"10 pages, ICPRAM2023"},{"id":"http://arxiv.org/abs/2209.14609v4","updated":"2023-03-14T05:56:18Z","published":"2022-09-29T07:58:32Z","title":"Dataset Distillation Using Parameter Pruning","summary":"  In many fields, the acquisition of advanced models depends on large datasets,\nmaking data storage and model training expensive. As a solution, dataset\ndistillation can synthesize a small dataset that preserves most information of\nthe original large dataset. The recently proposed dataset distillation method\nby matching network parameters has been proven effective for several datasets.\nHowever, the dimensions of network parameters are typically large. Furthermore,\nsome parameters are difficult to match during the distillation process,\ndegrading distillation performance. Based on this observation, this study\nproposes a novel dataset distillation method based on parameter pruning that\nsolves the problem. The proposed method can synthesize more robust distilled\ndatasets and improve distillation performance by pruning difficult-to-match\nparameters during the distillation process. Experimental results on three\ndatasets show that the proposed method outperforms other state-of-the-art\ndataset distillation methods.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2209.14609v4.pdf","comment":"Submitted as a journal paper at IEEE SPL"},{"id":"http://arxiv.org/abs/2303.07634v1","updated":"2023-03-14T05:29:34Z","published":"2023-03-14T05:29:34Z","title":"I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via\n  Raytracing in Neural SDFs","summary":"  In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene\nreconstruction and editing using differentiable Monte Carlo raytracing on\nneural signed distance fields (SDFs). Our holistic neural SDF-based framework\njointly recovers the underlying shapes, incident radiance and materials from\nmulti-view images. We introduce a novel bubble loss for fine-grained small\nobjects and error-guided adaptive sampling scheme to largely improve the\nreconstruction quality on large-scale indoor scenes. Further, we propose to\ndecompose the neural radiance field into spatially-varying material of the\nscene as a neural field through surface-based, differentiable Monte Carlo\nraytracing and emitter semantic segmentations, which enables physically based\nand photorealistic scene relighting and editing applications. Through a number\nof qualitative and quantitative experiments, we demonstrate the superior\nquality of our method on indoor scene reconstruction, novel view synthesis, and\nscene editing compared to state-of-the-art baselines.\n","authors":["Jingsen Zhu","Yuchi Huo","Qi Ye","Fujun Luan","Jifan Li","Dianbing Xi","Lisha Wang","Rui Tang","Wei Hua","Hujun Bao","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07634v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2209.13679v3","updated":"2023-03-14T04:58:08Z","published":"2022-09-27T20:34:41Z","title":"V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything\n  Perception","summary":"  Recent advancements in Vehicle-to-Everything communication technology have\nenabled autonomous vehicles to share sensory information to obtain better\nperception performance. With the rapid growth of autonomous vehicles and\nintelligent infrastructure, the V2X perception systems will soon be deployed at\nscale, which raises a safety-critical question: \\textit{how can we evaluate and\nimprove its performance under challenging traffic scenarios before the\nreal-world deployment?} Collecting diverse large-scale real-world test scenes\nseems to be the most straightforward solution, but it is expensive and\ntime-consuming, and the collections can only cover limited scenarios. To this\nend, we propose the first open adversarial scene generator V2XP-ASG that can\nproduce realistic, challenging scenes for modern LiDAR-based multi-agent\nperception systems. V2XP-ASG learns to construct an adversarial collaboration\ngraph and simultaneously perturb multiple agents' poses in an adversarial and\nplausible manner. The experiments demonstrate that V2XP-ASG can effectively\nidentify challenging scenes for a large range of V2X perception systems.\nMeanwhile, by training on the limited number of generated challenging scenes,\nthe accuracy of V2X perception systems can be further improved by 12.3\\% on\nchallenging and 4\\% on normal scenes. Our code will be released at\nhttps://github.com/XHwind/V2XP-ASG.\n","authors":["Hao Xiang","Runsheng Xu","Xin Xia","Zhaoliang Zheng","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2209.13679v3.pdf","comment":"ICRA 2023, see https://github.com/XHwind/V2XP-ASG"},{"id":"http://arxiv.org/abs/2303.07625v1","updated":"2023-03-14T04:48:18Z","published":"2023-03-14T04:48:18Z","title":"PlanarTrack: A Large-scale Challenging Benchmark for Planar Object\n  Tracking","summary":"  Planar object tracking is a critical computer vision problem and has drawn\nincreasing interest owing to its key roles in robotics, augmented reality, etc.\nDespite rapid progress, its further development, especially in the deep\nlearning era, is largely hindered due to the lack of large-scale challenging\nbenchmarks. Addressing this, we introduce PlanarTrack, a large-scale\nchallenging planar tracking benchmark. Specifically, PlanarTrack consists of\n1,000 videos with more than 490K images. All these videos are collected in\ncomplex unconstrained scenarios from the wild, which makes PlanarTrack,\ncompared with existing benchmarks, more challenging but realistic for\nreal-world applications. To ensure the high-quality annotation, each frame in\nPlanarTrack is manually labeled using four corners with multiple-round careful\ninspection and refinement. To our best knowledge, PlanarTrack, to date, is the\nlargest and most challenging dataset dedicated to planar object tracking. In\norder to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and\nconduct comprehensive comparisons and in-depth analysis. Our results, not\nsurprisingly, demonstrate that current top-performing planar trackers\ndegenerate significantly on the challenging PlanarTrack and more efforts are\nneeded to improve planar tracking in the future. In addition, we further derive\na variant named PlanarTrack$_{\\mathbf{BB}}$ for generic object tracking from\nPlanarTrack. Our evaluation of 10 excellent generic trackers on\nPlanarTrack$_{\\mathrm{BB}}$ manifests that, surprisingly,\nPlanarTrack$_{\\mathrm{BB}}$ is even more challenging than several popular\ngeneric tracking benchmarks and more attention should be paid to handle such\nplanar objects, though they are rigid. All benchmarks and evaluations will be\nreleased at the project webpage.\n","authors":["Xinran Liu","Xiaoqiong Liu","Ziruo Yi","Xin Zhou","Thanh Le","Libo Zhang","Yan Huang","Qing Yang","Heng Fan"],"pdf_url":"https://arxiv.org/pdf/2303.07625v1.pdf","comment":"Tech. Report"},{"id":"http://arxiv.org/abs/2303.07618v1","updated":"2023-03-14T03:57:16Z","published":"2023-03-14T03:57:16Z","title":"Medical Phrase Grounding with Region-Phrase Context Contrastive\n  Alignment","summary":"  Medical phrase grounding (MPG) aims to locate the most relevant region in a\nmedical image, given a phrase query describing certain medical findings, which\nis an important task for medical image analysis and radiological diagnosis.\nHowever, existing visual grounding methods rely on general visual features for\nidentifying objects in natural images and are not capable of capturing the\nsubtle and specialized features of medical findings, leading to sub-optimal\nperformance in MPG. In this paper, we propose MedRPG, an end-to-end approach\nfor MPG. MedRPG is built on a lightweight vision-language transformer encoder\nand directly predicts the box coordinates of mentioned medical findings, which\ncan be trained with limited medical data, making it a valuable tool in medical\nimage analysis. To enable MedRPG to locate nuanced medical findings with better\nregion-phrase correspondences, we further propose Tri-attention Context\ncontrastive alignment (TaCo). TaCo seeks context alignment to pull both the\nfeatures and attention outputs of relevant region-phrase pairs close together\nwhile pushing those of irrelevant regions far away. This ensures that the final\nbox prediction depends more on its finding-specific regions and phrases.\nExperimental results on three MPG datasets demonstrate that our MedRPG\noutperforms state-of-the-art visual grounding approaches by a large margin.\nAdditionally, the proposed TaCo strategy is effective in enhancing finding\nlocalization ability and reducing spurious region-phrase correlations.\n","authors":["Zhihao Chen","Yang Zhou","Anh Tran","Junting Zhao","Liang Wan","Gideon Ooi","Lionel Cheng","Choon Hua Thng","Xinxing Xu","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.07618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07615v1","updated":"2023-03-14T03:42:47Z","published":"2023-03-14T03:42:47Z","title":"Variation of Gender Biases in Visual Recognition Models Before and After\n  Finetuning","summary":"  We introduce a framework to measure how biases change before and after\nfine-tuning a large scale visual recognition model for a downstream task. Deep\nlearning models trained on increasing amounts of data are known to encode\nsocietal biases. Many computer vision systems today rely on models typically\npretrained on large scale datasets. While bias mitigation techniques have been\ndeveloped for tuning models for downstream tasks, it is currently unclear what\nare the effects of biases already encoded in a pretrained model. Our framework\nincorporates sets of canonical images representing individual and pairs of\nconcepts to highlight changes in biases for an array of off-the-shelf\npretrained models across model sizes, dataset sizes, and training objectives.\nThrough our analyses, we find that (1) supervised models trained on datasets\nsuch as ImageNet-21k are more likely to retain their pretraining biases\nregardless of the target dataset compared to self-supervised models. We also\nfind that (2) models finetuned on larger scale datasets are more likely to\nintroduce new biased associations. Our results also suggest that (3) biases can\ntransfer to finetuned models and the finetuning objective and dataset can\nimpact the extent of transferred biases.\n","authors":["Jaspreet Ranjit","Tianlu Wang","Baishakhi Ray","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2303.07615v1.pdf","comment":"10 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2301.07074v3","updated":"2023-03-14T03:36:41Z","published":"2023-01-17T18:36:57Z","title":"SegViz: A federated-learning based framework for multi-organ\n  segmentation on heterogeneous data sets with partial annotations","summary":"  Segmentation is one of the most primary tasks in deep learning for medical\nimaging, owing to its multiple downstream clinical applications. However,\ngenerating manual annotations for medical images is time-consuming, requires\nhigh skill, and is an expensive effort, especially for 3D images. One potential\nsolution is to aggregate knowledge from partially annotated datasets from\nmultiple groups to collaboratively train global models using Federated\nLearning. To this end, we propose SegViz, a federated learning-based framework\nto train a segmentation model from distributed non-i.i.d datasets with partial\nannotations. The performance of SegViz was compared against training individual\nmodels separately on each dataset as well as centrally aggregating all the\ndatasets in one place and training a single model. The SegViz framework using\nFedBN as the aggregation strategy demonstrated excellent performance on the\nexternal BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for\nsegmentation of liver, spleen, pancreas, and kidneys, respectively,\nsignificantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,\n0.83, 0.42, and 0.48 for the baseline models. In contrast, the central\naggregation model significantly ($p<0.05$) performed poorly on the test dataset\nwith dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the\npotential of the SegViz framework to train multi-task models from distributed\ndatasets with partial labels. All our implementations are open-source and\navailable at https://anonymous.4open.science/r/SegViz-B746\n","authors":["Adway U. Kanhere","Pranav Kulkarni","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.07074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.14360v2","updated":"2023-03-14T03:32:35Z","published":"2022-03-27T17:57:08Z","title":"Observation-Centric SORT: Rethinking SORT for Robust Multi-Object\n  Tracking","summary":"  Kalman filter (KF) based methods for multi-object tracking (MOT) make an\nassumption that objects move linearly. While this assumption is acceptable for\nvery short periods of occlusion, linear estimates of motion for prolonged time\ncan be highly inaccurate. Moreover, when there is no measurement available to\nupdate Kalman filter parameters, the standard convention is to trust the priori\nstate estimations for posteriori update. This leads to the accumulation of\nerrors during a period of occlusion. The error causes significant motion\ndirection variance in practice. In this work, we show that a basic Kalman\nfilter can still obtain state-of-the-art tracking performance if proper care is\ntaken to fix the noise accumulated during occlusion. Instead of relying only on\nthe linear state estimate (i.e., estimation-centric approach), we use object\nobservations (i.e., the measurements by object detector) to compute a virtual\ntrajectory over the occlusion period to fix the error accumulation of filter\nparameters during the occlusion period. This allows more time steps to correct\nerrors accumulated during occlusion. We name our method Observation-Centric\nSORT (OC-SORT). It remains Simple, Online, and Real-Time but improves\nrobustness during occlusion and non-linear motion. Given off-the-shelf\ndetections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves\nstate-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head\ntracking, and especially DanceTrack where the object motion is highly\nnon-linear. The code and models are available at\n\\url{https://github.com/noahcao/OC_SORT}.\n","authors":["Jinkun Cao","Jiangmiao Pang","Xinshuo Weng","Rawal Khirodkar","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2203.14360v2.pdf","comment":"Accepted by CVPR 2023. 8 pages + 10 pages of appendix. Renamed OOS as\n  Observation-centric Re-Update (ORU)"},{"id":"http://arxiv.org/abs/2303.07609v1","updated":"2023-03-14T03:09:56Z","published":"2023-03-14T03:09:56Z","title":"Training Robust Spiking Neural Networks with ViewPoint Transform and\n  SpatioTemporal Stretching","summary":"  Neuromorphic vision sensors (event cameras) simulate biological visual\nperception systems and have the advantages of high temporal resolution, less\ndata redundancy, low power consumption, and large dynamic range. Since both\nevents and spikes are modeled from neural signals, event cameras are inherently\nsuitable for spiking neural networks (SNNs), which are considered promising\nmodels for artificial intelligence (AI) and theoretical neuroscience. However,\nthe unconventional visual signals of these cameras pose a great challenge to\nthe robustness of spiking neural networks. In this paper, we propose a novel\ndata augmentation method, ViewPoint Transform and SpatioTemporal Stretching\n(VPT-STS). It improves the robustness of SNNs by transforming the rotation\ncenters and angles in the spatiotemporal domain to generate samples from\ndifferent viewpoints. Furthermore, we introduce the spatiotemporal stretching\nto avoid potential information loss in viewpoint transformation. Extensive\nexperiments on prevailing neuromorphic datasets demonstrate that VPT-STS is\nbroadly effective on multi-event representations and significantly outperforms\npure spatial geometric transformations. Notably, the SNNs model with VPT-STS\nachieves a state-of-the-art accuracy of 84.4\\% on the DVS-CIFAR10 dataset.\n","authors":["Haibo Shen","Juyu Xiao","Yihao Luo","Xiang Cao","Liangqi Zhang","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07609v1.pdf","comment":"Accepted by ICASSP 2023. arXiv admin note: text overlap with\n  arXiv:2207.11659"},{"id":"http://arxiv.org/abs/2209.07959v2","updated":"2023-03-14T03:03:52Z","published":"2022-09-16T14:19:48Z","title":"Towards Bridging the Performance Gaps of Joint Energy-based Models","summary":"  Can we train a hybrid discriminative-generative model within a single\nnetwork? This question has recently been answered in the affirmative,\nintroducing the field of Joint Energy-based Model (JEM), which achieves high\nclassification accuracy and image generation quality simultaneously. Despite\nrecent advances, there remain two performance gaps: the accuracy gap to the\nstandard softmax classifier, and the generation quality gap to state-of-the-art\ngenerative models. In this paper, we introduce a variety of training techniques\nto bridge the accuracy gap and the generation quality gap of JEM. 1) We\nincorporate a recently proposed sharpness-aware minimization (SAM) framework to\ntrain JEM, which promotes the energy landscape smoothness and the\ngeneralizability of JEM. 2) We exclude data augmentation from the maximum\nlikelihood estimate pipeline of JEM, and mitigate the negative impact of data\naugmentation to image generation quality. Extensive experiments on multiple\ndatasets demonstrate that our SADA-JEM achieves state-of-the-art performances\nand outperforms JEM in image classification, image generation, calibration,\nout-of-distribution detection and adversarial robustness by a notable margin.\nOur code is available at https://github.com/sndnyang/SADAJEM.\n","authors":["Xiulong Yang","Qing Su","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2209.07959v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07606v1","updated":"2023-03-14T03:01:24Z","published":"2023-03-14T03:01:24Z","title":"PSNet: a deep learning model based digital phase shifting algorithm from\n  a single fringe image","summary":"  As the gold standard for phase retrieval, phase-shifting algorithm (PS) has\nbeen widely used in optical interferometry, fringe projection profilometry,\netc. However, capturing multiple fringe patterns in PS limits the algorithm to\nonly a narrow range of application. To this end, a deep learning (DL) model\nbased digital PS algorithm from only a single fringe image is proposed. By\ntraining on a simulated dataset of PS fringe patterns, the learnt model,\ndenoted PSNet, can predict fringe patterns with other PS steps when given a\npattern with the first PS step. Simulation and experiment results demonstrate\nthe PSNet's promising performance on accurate prediction of digital PS\npatterns, and robustness to complex scenarios such as surfaces with varying\ncurvature and reflectance.\n","authors":["Zhaoshuai Qi","Xiaojun Liu","Xiaolin Liu","Jiaqi Yang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07606v1.pdf","comment":"5 pages9 figures, a letter"},{"id":"http://arxiv.org/abs/2303.07605v1","updated":"2023-03-14T02:58:27Z","published":"2023-03-14T02:58:27Z","title":"Modeling Continuous Motion for 3D Point Cloud Object Tracking","summary":"  The task of 3D single object tracking (SOT) with LiDAR point clouds is\ncrucial for various applications, such as autonomous driving and robotics.\nHowever, existing approaches have primarily relied on appearance matching or\nmotion modeling within only two successive frames, thereby overlooking the\nlong-range continuous motion property of objects in 3D space. To address this\nissue, this paper presents a novel approach that views each tracklet as a\ncontinuous stream: at each timestamp, only the current frame is fed into the\nnetwork to interact with multi-frame historical features stored in a memory\nbank, enabling efficient exploitation of sequential information. To achieve\neffective cross-frame message passing, a hybrid attention mechanism is designed\nto account for both long-range relation modeling and local geometric feature\nextraction. Furthermore, to enhance the utilization of multi-frame features for\nrobust tracking, a contrastive sequence enhancement strategy is designed, which\nuses ground truth tracklets to augment training sequences and promote\ndiscrimination against false positives in a contrastive manner. Extensive\nexperiments demonstrate that the proposed method outperforms the\nstate-of-the-art method by significant margins (approximately 8%, 6%, and 12%\nimprovements in the success performance on KITTI, nuScenes, and Waymo,\nrespectively).\n","authors":["Zhipeng Luo","Gongjie Zhang","Changqing Zhou","Zhonghua Wu","Qingyi Tao","Lewei Lu","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11659v3","updated":"2023-03-14T02:52:54Z","published":"2022-07-24T04:23:56Z","title":"Training Robust Spiking Neural Networks on Neuromorphic Data with\n  Spatiotemporal Fragments","summary":"  Neuromorphic vision sensors (event cameras) are inherently suitable for\nspiking neural networks (SNNs) and provide novel neuromorphic vision data for\nthis biomimetic model. Due to the spatiotemporal characteristics, novel data\naugmentations are required to process the unconventional visual signals of\nthese cameras. In this paper, we propose a novel Event SpatioTemporal Fragments\n(ESTF) augmentation method. It preserves the continuity of neuromorphic data by\ndrifting or inverting fragments of the spatiotemporal event stream to simulate\nthe disturbance of brightness variations, leading to more robust spiking neural\nnetworks. Extensive experiments are performed on prevailing neuromorphic\ndatasets. It turns out that ESTF provides substantial improvements over pure\ngeometric transformations and outperforms other event data augmentation\nmethods. It is worth noting that the SNNs with ESTF achieve the\nstate-of-the-art accuracy of 83.9\\% on the CIFAR10-DVS dataset.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11659v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2207.11670v2","updated":"2023-03-14T02:51:07Z","published":"2022-07-24T06:12:23Z","title":"Training Stronger Spiking Neural Networks with Biomimetic Adaptive\n  Internal Association Neurons","summary":"  As the third generation of neural networks, spiking neural networks (SNNs)\nare dedicated to exploring more insightful neural mechanisms to achieve\nnear-biological intelligence. Intuitively, biomimetic mechanisms are crucial to\nunderstanding and improving SNNs. For example, the associative long-term\npotentiation (ALTP) phenomenon suggests that in addition to learning mechanisms\nbetween neurons, there are associative effects within neurons. However, most\nexisting methods only focus on the former and lack exploration of the internal\nassociation effects. In this paper, we propose a novel Adaptive Internal\nAssociation~(AIA) neuron model to establish previously ignored influences\nwithin neurons. Consistent with the ALTP phenomenon, the AIA neuron model is\nadaptive to input stimuli, and internal associative learning occurs only when\nboth dendrites are stimulated at the same time. In addition, we employ weighted\nweights to measure internal associations and introduce intermediate caches to\nreduce the volatility of associations. Extensive experiments on prevailing\nneuromorphic datasets show that the proposed method can potentiate or depress\nthe firing of spikes more specifically, resulting in better performance with\nfewer spikes. It is worth noting that without adding any parameters at\ninference, the AIA model achieves state-of-the-art performance on\nDVS-CIFAR10~(83.9\\%) and N-CARS~(95.64\\%) datasets.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11670v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07601v1","updated":"2023-03-14T02:49:20Z","published":"2023-03-14T02:49:20Z","title":"V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle\n  Cooperative Perception","summary":"  Modern perception systems of autonomous vehicles are known to be sensitive to\nocclusions and lack the capability of long perceiving range. It has been one of\nthe key bottlenecks that prevents Level 5 autonomy. Recent research has\ndemonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system\nhas great potential to revolutionize the autonomous driving industry. However,\nthe lack of a real-world dataset hinders the progress of this field. To\nfacilitate the development of cooperative perception, we present V2V4Real, the\nfirst large-scale real-world multi-modal dataset for V2V perception. The data\nis collected by two vehicles equipped with multi-modal sensors driving together\nthrough diverse scenarios. Our V2V4Real dataset covers a driving area of 410\nkm, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding\nboxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real\nintroduces three perception tasks, including cooperative 3D object detection,\ncooperative 3D object tracking, and Sim2Real domain adaptation for cooperative\nperception. We provide comprehensive benchmarks of recent cooperative\nperception algorithms on three tasks. The V2V4Real dataset and codebase can be\nfound at https://github.com/ucla-mobility/V2V4Real.\n","authors":["Runsheng Xu","Xin Xia","Jinlong Li","Hanzhao Li","Shuo Zhang","Zhengzhong Tu","Zonglin Meng","Hao Xiang","Xiaoyu Dong","Rui Song","Hongkai Yu","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2303.07601v1.pdf","comment":"Accepted by CVPR2023. Code link:\n  https://github.com/ucla-mobility/V2V4Real"},{"id":"http://arxiv.org/abs/2210.04150v2","updated":"2023-03-14T02:48:42Z","published":"2022-10-09T02:57:32Z","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP","summary":"  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n","authors":["Feng Liang","Bichen Wu","Xiaoliang Dai","Kunpeng Li","Yinan Zhao","Hang Zhang","Peizhao Zhang","Peter Vajda","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2210.04150v2.pdf","comment":"CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg"},{"id":"http://arxiv.org/abs/2303.07598v1","updated":"2023-03-14T02:42:01Z","published":"2023-03-14T02:42:01Z","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision\n  Transformers via MAE+","summary":"  Unsupervised learning of vision transformers seeks to pretrain an encoder via\npretext tasks without labels. Among them is the Masked Image Modeling (MIM)\naligned with pretraining of language transformers by predicting masked patches\nas a pretext task. A criterion in unsupervised pretraining is the pretext task\nneeds to be sufficiently hard to prevent the transformer encoder from learning\ntrivial low-level features not generalizable well to downstream tasks. For this\npurpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It\ndistorts the local visual structures by perturbing the position encodings so\nthat the learned transformer cannot simply use the locally correlated patches\nto predict the missing ones. We hypothesize that it forces the transformer\nencoder to learn more discriminative features in a global context with stronger\ngeneralizability to downstream tasks. We will consider both absolute and\nrelative positional encodings, where adversarial positions can be imposed both\nin the embedding mode and the coordinate mode. We will also present a new MAE+\nbaseline that brings the performance of the MIM pretraining to a new level with\nthe AdPE. The experiments demonstrate that our approach can improve the\nfine-tuning accuracy of MAE by $0.8\\%$ and $0.4\\%$ over 1600 epochs of\npretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it\noutperforms the MAE with the ViT-B backbone by $2.6\\%$ in mIoU on ADE20K, and\nby $3.2\\%$ in AP$^{bbox}$ and $1.6\\%$ in AP$^{mask}$ on COCO, respectively.\nThese results are obtained with the AdPE being a pure MIM approach that does\nnot use any extra models or external datasets for pretraining. The code is\navailable at https://github.com/maple-research-lab/AdPE.\n","authors":["Xiao Wang","Ying Wang","Ziwei Xuan","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07598v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07596v1","updated":"2023-03-14T02:37:11Z","published":"2023-03-14T02:37:11Z","title":"Frequency-Modulated Point Cloud Rendering with Easy Editing","summary":"  We develop an effective point cloud rendering pipeline for novel view\nsynthesis, which enables high fidelity local detail reconstruction, real-time\nrendering and user-friendly editing. In the heart of our pipeline is an\nadaptive frequency modulation module called Adaptive Frequency Net (AFNet),\nwhich utilizes a hypernetwork to learn the local texture frequency encoding\nthat is consecutively injected into adaptive frequency activation layers to\nmodulate the implicit radiance signal. This mechanism improves the frequency\nexpressive ability of the network with richer frequency basis support, only at\na small computational budget. To further boost performance, a preprocessing\nmodule is also proposed for point cloud geometry optimization via point opacity\nestimation. In contrast to implicit rendering, our pipeline supports\nhigh-fidelity interactive editing based on point cloud manipulation. Extensive\nexperimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples\ndatasets demonstrate the superior performances achieved by our method in terms\nof PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.\n","authors":["Yi Zhang","Xiaoyang Huang","Bingbing Ni","Teng Li","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07596v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07582v1","updated":"2023-03-14T02:02:39Z","published":"2023-03-14T02:02:39Z","title":"Calibrated Teacher for Sparsely Annotated Object Detection","summary":"  Fully supervised object detection requires training images in which all\ninstances are annotated. This is actually impractical due to the high labor and\ntime costs and the unavoidable missing annotations. As a result, the incomplete\nannotation in each image could provide misleading supervision and harm the\ntraining. Recent works on sparsely annotated object detection alleviate this\nproblem by generating pseudo labels for the missing annotations. Such a\nmechanism is sensitive to the threshold of the pseudo label score. However, the\neffective threshold is different in different training stages and among\ndifferent object detectors. Therefore, the current methods with fixed\nthresholds have sub-optimal performance, and are difficult to be applied to\nother detectors. In order to resolve this obstacle, we propose a Calibrated\nTeacher, of which the confidence estimation of the prediction is well\ncalibrated to match its real precision. In this way, different detectors in\ndifferent training stages would share a similar distribution of the output\nconfidence, so that multiple detectors could share the same fixed threshold and\nachieve better performance. Furthermore, we present a simple but effective\nFocal IoU Weight (FIoU) for the classification loss. FIoU aims at reducing the\nloss weight of false negative samples caused by the missing annotation, and\nthus works as the complement of the teacher-student paradigm. Extensive\nexperiments show that our methods set new state-of-the-art under all different\nsparse settings in COCO. Code will be available at\nhttps://github.com/Whileherham/CalibratedTeacher.\n","authors":["Haohan Wang","Liang Liu","Boshen Zhang","Jiangning Zhang","Wuhao Zhang","Zhenye Gan","Yabiao Wang","Chengjie Wang","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07580v1","updated":"2023-03-14T01:56:15Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has used metamorphic testing (MT) to detect misclassified images. Most\nof them discuss metamorphic relations (MR), with little discussion on which\nregions should be transformed. We focus on the fact that there are sensitive\nregions where even a small transformation can easily change the prediction\nresults and propose an MT framework that efficiently tests for regions prone to\nmisclassification by transforming the sensitive regions. Our evaluation showed\nthat the sensitive regions can be specified by Explainable AI (XAI) and our\nframework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14402v3","updated":"2023-03-14T01:41:44Z","published":"2023-02-28T08:35:50Z","title":"Neural Video Compression with Diverse Contexts","summary":"  For any video codecs, the coding efficiency highly relies on whether the\ncurrent signal to be encoded can find the relevant contexts from the previous\nreconstructed signals. Traditional codec has verified more contexts bring\nsubstantial coding gain, but in a time-consuming manner. However, for the\nemerging neural video codec (NVC), its contexts are still limited, leading to\nlow compression ratio. To boost NVC, this paper proposes increasing the context\ndiversity in both temporal and spatial dimensions. First, we guide the model to\nlearn hierarchical quality patterns across frames, which enriches long-term and\nyet high-quality temporal contexts. Furthermore, to tap the potential of\noptical flow-based coding framework, we introduce a group-based offset\ndiversity where the cross-group interaction is proposed for better context\nmining. In addition, this paper also adopts a quadtree-based partition to\nincrease spatial context diversity when encoding the latent representation in\nparallel. Experiments show that our codec obtains 23.5% bitrate saving over\nprevious SOTA NVC. Better yet, our codec has surpassed the under-developing\nnext generation traditional codec/ECM in both RGB and YUV420 colorspaces, in\nterms of PSNR. The codes are at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.14402v3.pdf","comment":"Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC"},{"id":"http://arxiv.org/abs/2211.14462v2","updated":"2023-03-14T01:36:36Z","published":"2022-11-26T02:53:40Z","title":"Meta Architecture for Point Cloud Analysis","summary":"  Recent advances in 3D point cloud analysis bring a diverse set of network\narchitectures to the field. However, the lack of a unified framework to\ninterpret those networks makes any systematic comparison, contrast, or analysis\nchallenging, and practically limits healthy development of the field. In this\npaper, we take the initiative to explore and propose a unified framework called\nPointMeta, to which the popular 3D point cloud analysis approaches could fit.\nThis brings three benefits. First, it allows us to compare different approaches\nin a fair manner, and use quick experiments to verify any empirical\nobservations or assumptions summarized from the comparison. Second, the big\npicture brought by PointMeta enables us to think across different components,\nand revisit common beliefs and key design decisions made by the popular\napproaches. Third, based on the learnings from the previous two analyses, by\ndoing simple tweaks on the existing approaches, we are able to derive a basic\nbuilding block, termed PointMetaBase. It shows very strong performance in\nefficiency and effectiveness through extensive experiments on challenging\nbenchmarks, and thus verifies the necessity and benefits of high-level\ninterpretation, contrast, and comparison like PointMeta. In particular,\nPointMetaBase surpasses the previous state-of-the-art method by 0.7%/1.4/%2.1%\nmIoU with only 2%/11%/13% of the computation cost on the S3DIS datasets.\n","authors":["Haojia Lin","Xiawu Zheng","Lijiang Li","Fei Chao","Shanshan Wang","Yan Wang","Yonghong Tian","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2211.14462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13361v4","updated":"2023-03-14T01:28:35Z","published":"2023-01-31T01:31:43Z","title":"Iterative Loop Method Combining Active and Semi-Supervised Learning for\n  Domain Adaptive Semantic Segmentation","summary":"  Semantic segmentation is an important technique for environment perception in\nintelligent transportation systems. With the rapid development of convolutional\nneural networks (CNNs), road scene analysis can usually achieve satisfactory\nresults in the source domain. However, guaranteeing good generalization to\ndifferent target domain scenarios remains a significant challenge. Recently,\nsemi-supervised learning and active learning have been proposed to alleviate\nthis problem. Semisupervised learning can improve model accuracy with massive\nunlabeled data, but some pseudo labels containing noise would be generated with\nlimited or imbalanced training data. And there will be suboptimal models if\nhuman guidance is absent. Active learning can select more effective data to\nintervene, while the model accuracy can not be improved because the massive\nunlabeled data are not used. And the probability of querying sub-optimal\nsamples will increase when the domain difference is too large, increasing\nannotation cost. This paper proposes an iterative loop method combining active\nand semisupervised learning for domain adaptive semantic segmentation. The\nmethod first uses semi-supervised to learn massive unlabeled data to improve\nmodel accuracy and provide more accurate selection models for active learning.\nSecondly, combined with the predictive uncertainty sample selection strategy of\nactive learning, manual intervention is used to correct the pseudo-labels.\nFinally, flexible iterative loops achieve the best performance with minimal\nlabeling cost. Extensive experiments show that our method establishes\nstate-of-the-art performance on tasks of GTAV to Cityscapes, SYNTHIA to\nCityscapes, improving by 4.9% mIoU and 5.2% mIoU, compared to the previous best\nmethod, respectively.\n","authors":["Licong Guan","Xue Yuan"],"pdf_url":"https://arxiv.org/pdf/2301.13361v4.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2203.04446v3","updated":"2023-03-14T01:13:36Z","published":"2022-03-08T23:35:04Z","title":"Self-Supervised Domain Calibration and Uncertainty Estimation for Place\n  Recognition","summary":"  Visual place recognition techniques based on deep learning, which have\nimposed themselves as the state-of-the-art in recent years, do not generalize\nwell to environments visually different from the training set. Thus, to achieve\ntop performance, it is sometimes necessary to fine-tune the networks to the\ntarget environment. To this end, we propose a self-supervised domain\ncalibration procedure based on robust pose graph optimization from Simultaneous\nLocalization and Mapping (SLAM) as the supervision signal without requiring GPS\nor manual labeling. Moreover, we leverage the procedure to improve uncertainty\nestimation for place recognition matches which is important in safety critical\napplications. We show that our approach can improve the performance of a\nstate-of-the-art technique on a target environment dissimilar from its training\nset and that we can obtain uncertainty estimates. We believe that this approach\nwill help practitioners to deploy robust place recognition solutions in\nreal-world applications. Our code is available publicly:\nhttps://github.com/MISTLab/vpr-calibration-and-uncertainty\n","authors":["Pierre-Yves Lajoie","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2203.04446v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07564v1","updated":"2023-03-14T01:10:59Z","published":"2023-03-14T01:10:59Z","title":"Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow","summary":"  Optical flow has achieved great success under clean scenes, but suffers from\nrestricted performance under foggy scenes. To bridge the clean-to-foggy domain\ngap, the existing methods typically adopt the domain adaptation to transfer the\nmotion knowledge from clean to synthetic foggy domain. However, these methods\nunexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous\nwhen applied to real-world scenes. To handle the practical optical flow under\nreal foggy scenes, in this work, we propose a novel unsupervised cumulative\ndomain adaptation optical flow (UCDA-Flow) framework: depth-association motion\nadaptation and correlation-alignment motion adaptation. Specifically, we\ndiscover that depth is a key ingredient to influence the optical flow: the\ndeeper depth, the inferior optical flow, which motivates us to design a\ndepth-association motion adaptation module to bridge the clean-to-foggy domain\ngap. Moreover, we figure out that the cost volume correlation shares similar\ndistribution of the synthetic and real foggy images, which enlightens us to\ndevise a correlation-alignment motion adaptation module to distill motion\nknowledge of the synthetic foggy domain to the real foggy domain. Note that\nsynthetic fog is designed as the intermediate domain. Under this unified\nframework, the proposed cumulative adaptation progressively transfers knowledge\nfrom clean scenes to real foggy scenes. Extensive experiments have been\nperformed to verify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Wending Yan","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2303.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07560v1","updated":"2023-03-14T00:57:11Z","published":"2023-03-14T00:57:11Z","title":"Machine Learning Computer Vision Applications for Spatial AI Object\n  Recognition in Orange County, California","summary":"  We provide an integrated and systematic automation approach to spatial object\nrecognition and positional detection using AI machine learning and computer\nvision algorithms for Orange County, California. We describe a comprehensive\nmethodology for multi-sensor, high-resolution field data acquisition, along\nwith post-field processing and pre-analysis processing tasks. We developed a\nseries of algorithmic formulations and workflows that integrate convolutional\ndeep neural network learning with detected object positioning estimation in\n360{\\deg} equirectancular photosphere imagery. We provide examples of\napplication processing more than 800 thousand cardinal directions in\nphotosphere images across two areas in Orange County, and present detection\nresults for stop-sign and fire hydrant object recognition. We discuss the\nefficiency and effectiveness of our approach, along with broader inferences\nrelated to the performance and implications of this approach for future\ntechnological innovations, including automation of spatial data and public\nasset inventories, and near real-time AI field data systems.\n","authors":["Kostas Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2303.07560v1.pdf","comment":"24 pages, 15 figures, 8 tables"},{"id":"http://arxiv.org/abs/2302.07577v3","updated":"2023-03-14T00:48:26Z","published":"2023-02-15T10:40:19Z","title":"Efficient Teacher: Semi-Supervised Object Detection for YOLOv5","summary":"  Semi-Supervised Object Detection (SSOD) has been successful in improving the\nperformance of both R-CNN series and anchor-free detectors. However, one-stage\nanchor-based detectors lack the structure to generate high-quality or flexible\npseudo labels, leading to serious inconsistency problems in SSOD. In this\npaper, we propose the Efficient Teacher framework for scalable and effective\none-stage anchor-based SSOD training, consisting of Dense Detector, Pseudo\nLabel Assigner, and Epoch Adaptor. Dense Detector is a baseline model that\nextends RetinaNet with dense sampling techniques inspired by YOLOv5. The\nEfficient Teacher framework introduces a novel pseudo label assignment\nmechanism, named Pseudo Label Assigner, which makes more refined use of pseudo\nlabels from Dense Detector. Epoch Adaptor is a method that enables a stable and\nefficient end-to-end semi-supervised training schedule for Dense Detector. The\nPseudo Label Assigner prevents the occurrence of bias caused by a large number\nof low-quality pseudo labels that may interfere with the Dense Detector during\nthe student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes\ndomain and distribution adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training independent of the\nproportion of labeled data. Our experiments show that the Efficient Teacher\nframework achieves state-of-the-art results on VOC, COCO-standard, and\nCOCO-additional using fewer FLOPs than previous methods. To the best of our\nknowledge, this is the first attempt to apply Semi-Supervised Object Detection\nto YOLOv5.Code is available:\nhttps://github.com/AlibabaResearch/efficientteacher\n","authors":["Bowen Xu","Mingtao Chen","Wenlong Guan","Lulu Hu"],"pdf_url":"https://arxiv.org/pdf/2302.07577v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.07547v1","updated":"2023-03-14T00:30:24Z","published":"2023-03-14T00:30:24Z","title":"HazardNet: Road Debris Detection by Augmentation of Synthetic Models","summary":"  We present an algorithm to detect unseen road debris using a small set of\nsynthetic models. Early detection of road debris is critical for safe\nautonomous or assisted driving, yet the development of a robust road debris\ndetection model has not been widely discussed. There are two main challenges to\nbuilding a road debris detector: first, data collection of road debris is\nchallenging since hazardous objects on the road are rare to encounter in real\ndriving scenarios; second, the variability of road debris is broad, ranging\nfrom a very small brick to a large fallen tree. To overcome these challenges,\nwe propose a novel approach to few-shot learning of road debris that uses\nsemantic augmentation and domain randomization to augment real road images with\nsynthetic models. We constrain the problem domain to uncommon objects on the\nroad and allow the deep neural network, HazardNet, to learn the semantic\nmeaning of road debris to eventually detect unseen road debris. Our results\ndemonstrate that HazardNet is able to accurately detect real road debris when\nonly trained on synthetic objects in augmented images.\n","authors":["Tae Eun Choe","Jane Wu","Xiaolin Lin","Karen Kwon","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2303.07547v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2211.01324v5","updated":"2023-03-14T00:22:14Z","published":"2022-11-02T17:43:04Z","title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert\n  Denoisers","summary":"  Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I's \"paint-with-words\" capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/\n","authors":["Yogesh Balaji","Seungjun Nah","Xun Huang","Arash Vahdat","Jiaming Song","Qinsheng Zhang","Karsten Kreis","Miika Aittala","Timo Aila","Samuli Laine","Bryan Catanzaro","Tero Karras","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.01324v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07545v1","updated":"2023-03-14T00:19:11Z","published":"2023-03-14T00:19:11Z","title":"Implicit and Explicit Commonsense for Multi-sentence Video Captioning","summary":"  Existing dense or paragraph video captioning approaches rely on holistic\nrepresentations of videos, possibly coupled with learned object/action\nrepresentations, to condition hierarchical language decoders. However, they\nfundamentally lack the commonsense knowledge of the world required to reason\nabout progression of events, causality, and even function of certain objects\nwithin a scene. To address this limitation we propose a novel video captioning\nTransformer-based model, that takes into account both implicit (visuo-lingual\nand purely linguistic) and explicit (knowledge-base) commonsense knowledge. We\nshow that these forms of knowledge, in isolation and in combination, enhance\nthe quality of produced captions. Further, inspired by imitation learning, we\npropose a new task of instruction generation, where the goal is to produce a\nset of linguistic instructions from a video demonstration of its performance.\nWe formalize the task using ALFRED dataset [52] generated using an AI2-THOR\nenvironment. While instruction generation is conceptually similar to paragraph\ncaptioning, it differs in the fact that it exhibits stronger object\npersistence, as well as spatially-aware and causal sentence structure. We show\nthat our commonsense knowledge enhanced approach produces significant\nimprovements on this task (up to 57% in METEOR and 8.5% in CIDEr), as well as\nthe state-of-the-art result on more traditional video captioning in the\nActivityNet Captions dataset [29].\n","authors":["Shih-Han Chou","James J. Little","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2303.07545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v1","updated":"2023-03-14T00:13:57Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear\n  Discriminative Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminative Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07540v1","updated":"2023-03-14T00:05:08Z","published":"2023-03-14T00:05:08Z","title":"Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial\n  Wedge Pressure from Cardiac MRI","summary":"  Heart failure is a serious and life-threatening condition that can lead to\nelevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure\n(PAWP) is an important surrogate marker indicating high pressure in the left\nventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an\ninvasive procedure. A non-invasive method is useful in quickly identifying\nhigh-risk patients from a large population. In this work, we develop a tensor\nlearning-based pipeline for identifying PAWP from multimodal cardiac Magnetic\nResonance Imaging (MRI). This pipeline extracts spatial and temporal features\nfrom high-dimensional scans. For quality control, we incorporate an epistemic\nuncertainty-based binning strategy to identify poor-quality training samples.\nTo improve the performance, we learn complementary information by integrating\nfeatures from multimodal data: cardiac MRI with short-axis and four-chamber\nviews, and Electronic Health Records. The experimental analysis on a large\ncohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation\nindicates that the proposed pipeline has a diagnostic value and can produce\npromising performance with significant improvement over the baseline in\nclinical practice (i.e., $\\Delta$AUC $=0.10$, $\\Delta$Accuracy $=0.06$, and\n$\\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical\nutility of our method.\n","authors":["Prasun C. Tripathi","Mohammod N. I. Suvon","Lawrence Schobs","Shuo Zhou","Samer Alabed","Andrew J. Swift","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07540v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.08030v1","updated":"2023-03-14T16:10:55Z","published":"2023-03-14T16:10:55Z","title":"Improving information retrieval through correspondence analysis instead\n  of latent semantic analysis","summary":"  Both latent semantic analysis (LSA) and correspondence analysis (CA) are\ndimensionality reduction techniques that use singular value decomposition (SVD)\nfor information retrieval. Theoretically, the results of LSA display both the\nassociation between documents and terms, and marginal effects; in comparison,\nCA only focuses on the associations between documents and terms. Marginal\neffects are usually not relevant for information retrieval, and therefore, from\na theoretical perspective CA is more suitable for information retrieval.\n  In this paper, we empirically compare LSA and CA. The elements of the raw\ndocument-term matrix are weighted, and the weighting exponent of singular\nvalues is adjusted to improve the performance of LSA. We explore whether these\ntwo weightings also improve the performance of CA. In addition, we compare the\noptimal singular value weighting exponents for LSA and CA to identify what the\ninitial dimensions in LSA correspond to.\n  The results for four empirical datasets show that CA always performs better\nthan LSA. Weighting the elements of the raw data matrix can improve CA;\nhowever, it is data dependent and the improvement is small. Adjusting the\nsingular value weighting exponent usually improves the performance of CA;\nhowever, the extent of the improved performance depends on the dataset and\nnumber of dimensions. In general, CA needs a larger singular value weighting\nexponent than LSA to obtain the optimal performance. This indicates that CA\nemphasizes initial dimensions more than LSA, and thus, margins play an\nimportant role in the initial dimensions in LSA.\n","authors":["Qianqian Qi","David J. Hessen","Peter G. M. van der Heijden"],"pdf_url":"https://arxiv.org/pdf/2303.08030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07810v1","updated":"2023-03-14T11:28:53Z","published":"2023-03-14T11:28:53Z","title":"Disentangled Graph Social Recommendation","summary":"  Social recommender systems have drawn a lot of attention in many online web\nservices, because of the incorporation of social information between users in\nimproving recommendation results. Despite the significant progress made by\nexisting solutions, we argue that current methods fall short in two\nlimitations: (1) Existing social-aware recommendation models only consider\ncollaborative similarity between items, how to incorporate item-wise semantic\nrelatedness is less explored in current recommendation paradigms. (2) Current\nsocial recommender systems neglect the entanglement of the latent factors over\nheterogeneous relations (e.g., social connections, user-item interactions).\nLearning the disentangled representations with relation heterogeneity poses\ngreat challenge for social recommendation. In this work, we design a\nDisentangled Graph Neural Network (DGNN) with the integration of latent memory\nunits, which empowers DGNN to maintain factorized representations for\nheterogeneous types of user and item connections. Additionally, we devise new\nmemory-augmented message propagation and aggregation schemes under the graph\nneural architecture, allowing us to recursively distill semantic relatedness\ninto the representations of users and items in a fully automatic manner.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nour model by achieving great improvement over state-of-the-art recommendation\ntechniques. The source code is publicly available at:\nhttps://github.com/HKUDS/DGNN.\n","authors":["Lianghao Xia","Yizhen Shao","Chao Huang","Yong Xu","Huance Xu","Jian Pei"],"pdf_url":"https://arxiv.org/pdf/2303.07810v1.pdf","comment":"Accepted by IEEE ICDE 2023"},{"id":"http://arxiv.org/abs/2303.07797v1","updated":"2023-03-14T11:12:22Z","published":"2023-03-14T11:12:22Z","title":"Automated Self-Supervised Learning for Recommendation","summary":"  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm\nfor collaborative filtering (CF). To improve the representation quality over\nlimited labeled data, contrastive learning has attracted attention in\nrecommendation and benefited graph-based CF model recently. However, the\nsuccess of most contrastive methods heavily relies on manually generating\neffective contrastive views for heuristic-based data augmentation. This does\nnot generalize across different datasets and downstream recommendation tasks,\nwhich is difficult to be adaptive for data augmentation and robust to noise\nperturbation. To fill this crucial gap, this work proposes a unified Automated\nCollaborative Filtering (AutoCF) to automatically perform data augmentation for\nrecommendation. Specifically, we focus on the generative self-supervised\nlearning framework with a learnable augmentation paradigm that benefits the\nautomated distillation of important self-supervised signals. To enhance the\nrepresentation discrimination ability, our masked graph autoencoder is designed\nto aggregate global information during the augmentation via reconstructing the\nmasked subgraph structures. Experiments and ablation studies are performed on\nseveral public datasets for recommending products, venues, and locations.\nResults demonstrate the superiority of AutoCF against various baseline methods.\nWe release the model implementation at https://github.com/HKUDS/AutoCF.\n","authors":["Lianghao Xia","Chao Huang","Chunzhen Huang","Kangyi Lin","Tao Yu","Ben Kao"],"pdf_url":"https://arxiv.org/pdf/2303.07797v1.pdf","comment":"Accepted by ACM The Web Conference, 2023"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.07678v1","updated":"2023-03-14T07:27:30Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2303.07607v1","updated":"2023-03-14T03:03:19Z","published":"2023-03-14T03:03:19Z","title":"CoMeta: Enhancing Meta Embeddings with Collaborative Information in\n  Cold-start Problem of Recommendation","summary":"  The cold-start problem is quite challenging for existing recommendation\nmodels. Specifically, for the new items with only a few interactions, their ID\nembeddings are trained inadequately, leading to poor recommendation\nperformance. Some recent studies introduce meta learning to solve the\ncold-start problem by generating meta embeddings for new items as their initial\nID embeddings. However, we argue that the capability of these methods is\nlimited, because they mainly utilize item attribute features which only contain\nlittle information, but ignore the useful collaborative information contained\nin the ID embeddings of users and old items. To tackle this issue, we propose\nCoMeta to enhance the meta embeddings with the collaborative information.\nCoMeta consists of two submodules: B-EG and S-EG. Specifically, for a new item:\nB-EG calculates the similarity-based weighted sum of the ID embeddings of old\nitems as its base embedding; S-EG generates its shift embedding not only with\nits attribute features but also with the average ID embedding of the users who\ninteracted with it. The final meta embedding is obtained by adding up the base\nembedding and the shift embedding. We conduct extensive experiments on two\npublic datasets. The experimental results demonstrate both the effectiveness\nand the compatibility of CoMeta.\n","authors":["Haonan Hu","Dazhong Rong","Jianhai Chen","Qinming He","Zhenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06095v2","updated":"2023-03-14T02:34:12Z","published":"2023-03-10T17:24:41Z","title":"HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical\n  Information Extraction","summary":"  Multi-scenario & multi-task learning has been widely applied to many\nrecommendation systems in industrial applications, wherein an effective and\npractical approach is to carry out multi-scenario transfer learning on the\nbasis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based\nmethod, which aims to project all information in the same feature space, cannot\neffectively deal with the complex relationships inherent among various\nscenarios and tasks, resulting in unsatisfactory performance. To tackle the\nproblem, we propose a Hierarchical information extraction Network (HiNet) for\nmulti-scenario and multi-task recommendation, which achieves hierarchical\nextraction based on coarse-to-fine knowledge transfer scheme. The multiple\nextraction layers of the hierarchical network enable the model to enhance the\ncapability of transferring valuable information across scenarios while\npreserving specific features of scenarios and tasks. Furthermore, a novel\nscenario-aware attentive network module is proposed to model correlations\nbetween scenarios explicitly. Comprehensive experiments conducted on real-world\nindustrial datasets from Meituan Meishi platform demonstrate that HiNet\nachieves a new state-of-the-art performance and significantly outperforms\nexisting solutions. HiNet is currently fully deployed in two scenarios and has\nachieved 2.87% and 1.75% order quantity gain respectively.\n","authors":["Jie Zhou","Xianshuai Cao","Wenhao Li","Lin Bo","Kun Zhang","Chuan Luo","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2303.06095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08157v1","updated":"2023-03-14T18:14:40Z","published":"2023-03-14T18:14:40Z","title":"Graph Neural Network Surrogates of Fair Graph Filtering","summary":"  Graph filters that transform prior node values to posterior scores via edge\npropagation often support graph mining tasks affecting humans, such as\nrecommendation and ranking. Thus, it is important to make them fair in terms of\nsatisfying statistical parity constraints between groups of nodes (e.g.,\ndistribute score mass between genders proportionally to their representation).\nTo achieve this while minimally perturbing the original posteriors, we\nintroduce a filter-aware universal approximation framework for posterior\nobjectives. This defines appropriate graph neural networks trained at runtime\nto be similar to filters but also locally optimize a large class of objectives,\nincluding fairness-aware ones. Experiments on a collection of 8 filters and 5\ngraphs show that our approach performs equally well or better than alternatives\nin meeting parity constraints while preserving the AUC of score-based community\nmember recommendation and creating minimal utility loss in prior diffusion.\n","authors":["Emmanouil Krasanakis","Symeon Papadopulos"],"pdf_url":"https://arxiv.org/pdf/2303.08157v1.pdf","comment":"40 pages, 5 figures, 5 papers"},{"id":"http://arxiv.org/abs/2303.11160v1","updated":"2023-03-14T06:45:28Z","published":"2023-03-14T06:45:28Z","title":"Explaining Recommendation System Using Counterfactual Textual\n  Explanations","summary":"  Currently, there is a significant amount of research being conducted in the\nfield of artificial intelligence to improve the explainability and\ninterpretability of deep learning models. It is found that if end-users\nunderstand the reason for the production of some output, it is easier to trust\nthe system. Recommender systems are one example of systems that great efforts\nhave been conducted to make their output more explainable. One method for\nproducing a more explainable output is using counterfactual reasoning, which\ninvolves altering minimal features to generate a counterfactual item that\nresults in changing the output of the system. This process allows the\nidentification of input features that have a significant impact on the desired\noutput, leading to effective explanations. In this paper, we present a method\nfor generating counterfactual explanations for both tabular and textual\nfeatures. We evaluated the performance of our proposed method on three\nreal-world datasets and demonstrated a +5\\% improvement on finding effective\nfeatures (based on model-based measures) compared to the baseline method.\n","authors":["Niloofar Ranjbar","Saeedeh Momtazi","MohammadMehdi Homayounpour"],"pdf_url":"https://arxiv.org/pdf/2303.11160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11174v1","updated":"2023-03-14T01:58:04Z","published":"2023-03-14T01:58:04Z","title":"Metric Search for Rank List Compatibility Matching with Applications","summary":"  As online dating has become more popular in the past few years, an efficient\nand effective algorithm to match users is needed. In this project, we proposed\na new dating matching algorithm that uses Kendall-Tau distance to measure the\nsimilarity between users based on their ranking for items in a list. (e.g.,\ntheir favourite sports, music, etc.) To increase the performance of the search\nprocess, we applied a tree-based searching structure, Cascading Metric Tree\n(CMT), on this metric. The tree is built on ranked lists from all the users;\nwhen a query target and a radius are provided, our algorithm can return users\nwithin the radius of the target. We tested the scaling of this searching method\non a synthetic dataset by varying list length, population size, and query\nradius. We observed that the algorithm is able to query the best matching\npeople for the user in a practical time, given reasonable parameters. We also\nprovided potential future improvements that can be made to this algorithm based\non the limitations. Finally, we offered more use cases of this search structure\non Kendall-Tau distance and new insight into real-world applications of\ndistance search structures.\n","authors":["Wenqi Guo","Jeffrey Uhlmann"],"pdf_url":"https://arxiv.org/pdf/2303.11174v1.pdf","comment":"Paper for 2023 Multidisciplinary Undergraduate Research Conference\n  (MURC)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.08133v1","updated":"2023-03-14T17:59:01Z","published":"2023-03-14T17:59:01Z","title":"MeshDiffusion: Score-based Generative 3D Mesh Modeling","summary":"  We consider the task of generating realistic 3D shapes, which is useful for a\nvariety of applications such as automatic scene generation and physical\nsimulation. Compared to other 3D representations like voxels and point clouds,\nmeshes are more desirable in practice, because (1) they enable easy and\narbitrary manipulation of shapes for relighting and simulation, and (2) they\ncan fully leverage the power of modern graphics pipelines which are mostly\noptimized for meshes. Previous scalable methods for generating meshes typically\nrely on sub-optimal post-processing, and they tend to produce overly-smooth or\nnoisy surfaces without fine-grained geometric details. To overcome these\nshortcomings, we take advantage of the graph structure of meshes and use a\nsimple yet very effective generative modeling method to generate 3D meshes.\nSpecifically, we represent meshes with deformable tetrahedral grids, and then\ntrain a diffusion model on this direct parametrization. We demonstrate the\neffectiveness of our model on multiple generative tasks.\n","authors":["Zhen Liu","Yao Feng","Michael J. Black","Derek Nowrouzezahrai","Liam Paull","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08133v1.pdf","comment":"Published in ICLR 2023 (Spotlight, Notable-top-25%)"},{"id":"http://arxiv.org/abs/2303.03340v2","updated":"2023-03-14T17:57:32Z","published":"2023-03-06T18:13:14Z","title":"Symbolic Synthesis of Neural Networks","summary":"  Neural networks adapt very well to distributed and continuous\nrepresentations, but struggle to generalize from small amounts of data.\nSymbolic systems commonly achieve data efficient generalization by exploiting\nmodularity to benefit from local and discrete features of a representation.\nThese features allow symbolic programs to be improved one module at a time and\nto experience combinatorial growth in the values they can successfully process.\nHowever, it is difficult to design a component that can be used to form\nsymbolic abstractions and which is adequately overparametrized to learn\narbitrary high-dimensional transformations. I present Graph-based Symbolically\nSynthesized Neural Networks (G-SSNNs), a class of neural modules that operate\non representations modified with synthesized symbolic programs to include a\nfixed set of local and discrete features. I demonstrate that the choice of\ninjected features within a G-SSNN module modulates the data efficiency and\ngeneralization of baseline neural models, creating predictable patterns of both\nheightened and curtailed generalization. By training G-SSNNs, we also derive\ninformation about desirable semantics of symbolic programs without manual\nengineering. This information is compact and amenable to abstraction, but can\nalso be flexibly recontextualized for other high-dimensional settings. In\nfuture work, I will investigate data efficient generalization and the\ntransferability of learned symbolic representations in more complex G-SSNN\ndesigns based on more complex classes of symbolic programs. Experimental code\nand data are available at\nhttps://github.com/shlomenu/symbolically_synthesized_networks .\n","authors":["Eli Whitehouse"],"pdf_url":"https://arxiv.org/pdf/2303.03340v2.pdf","comment":"8 pages, 1 figure. Minor formula correction and minor textual\n  revision"},{"id":"http://arxiv.org/abs/2303.08127v1","updated":"2023-03-14T17:57:06Z","published":"2023-03-14T17:57:06Z","title":"CB2: Collaborative Natural Language Interaction Research Platform","summary":"  CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n","authors":["Jacob Sharf","Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2303.08127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08871v4","updated":"2023-03-14T17:50:47Z","published":"2021-10-17T17:15:13Z","title":"Expectation Distance-based Distributional Clustering for\n  Noise-Robustness","summary":"  This paper presents a clustering technique that reduces the susceptibility to\ndata noise by learning and clustering the data-distribution and then assigning\nthe data to the cluster of its distribution. In the process, it reduces the\nimpact of noise on clustering results. This method involves introducing a new\ndistance among distributions, namely the expectation distance (denoted, ED),\nthat goes beyond the state-of-art distribution distance of optimal mass\ntransport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends\nonly on the marginal distributions while the former also employs the\ninformation about the joint distributions. Using the ED, the paper extends the\nclassical $K$-means and $K$-medoids clustering to those over data-distributions\n(rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also\npresents the closed-form expressions of the $W_2$ and ED distance measures. The\nimplementation results of the proposed ED and the $W_2$ distance measures to\ncluster real-world weather data as well as stock data are also presented, which\ninvolves efficiently extracting and using the underlying data distributions --\nGaussians for weather data versus lognormals for stock data. The results show\nstriking performance improvement over classical clustering of raw-data, with\nhigher accuracy realized for ED. Also, not only does the distribution-based\nclustering offer higher accuracy, but it also lowers the computation time due\nto reduced time-complexity.\n","authors":["Rahmat Adesunkanmi","Ratnesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2110.08871v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08117v1","updated":"2023-03-14T17:49:50Z","published":"2023-03-14T17:49:50Z","title":"Do Transformers Parse while Predicting the Masked Word?","summary":"  Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.\n","authors":["Haoyu Zhao","Abhishek Panigrahi","Rong Ge","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2303.08117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.01680v3","updated":"2023-03-14T17:47:50Z","published":"2022-01-05T16:19:16Z","title":"Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems","summary":"  TWe establish regret lower bounds for adaptively controlling an unknown\nlinear Gaussian system with quadratic costs. We combine ideas from experiment\ndesign, estimation theory and a perturbation bound of certain information\nmatrices to derive regret lower bounds exhibiting scaling on the order of\nmagnitude $\\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the\nrole of control-theoretic parameters and we are able to show that systems that\nare hard to control are also hard to learn to control; when instantiated to\nstate feedback systems we recover the dimensional dependency of earlier work\nbut with improved scaling with system-theoretic constants such as system costs\nand Gramians. Furthermore, we extend our results to a class of partially\nobserved systems and demonstrate that systems with poor observability structure\nalso are hard to learn to control.\n","authors":["Ingvar Ziemann","Henrik Sandberg"],"pdf_url":"https://arxiv.org/pdf/2201.01680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08114v1","updated":"2023-03-14T17:47:25Z","published":"2023-03-14T17:47:25Z","title":"Simfluence: Modeling the Influence of Individual Training Examples by\n  Simulating Training Runs","summary":"  Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.\n","authors":["Kelvin Guu","Albert Webson","Ellie Pavlick","Lucas Dixon","Ian Tenney","Tolga Bolukbasi"],"pdf_url":"https://arxiv.org/pdf/2303.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08112v1","updated":"2023-03-14T17:47:09Z","published":"2023-03-14T17:47:09Z","title":"Eliciting Latent Predictions from Transformers with the Tuned Lens","summary":"  We analyze transformers from the perspective of iterative inference, seeking\nto understand how model predictions are refined layer by layer. To do so, we\ntrain an affine probe for each block in a frozen pretrained model, making it\npossible to decode every hidden state into a distribution over the vocabulary.\nOur method, the \\emph{tuned lens}, is a refinement of the earlier ``logit\nlens'' technique, which yielded useful insights but is often brittle.\n  We test our method on various autoregressive language models with up to 20B\nparameters, showing it to be more predictive, reliable and unbiased than the\nlogit lens. With causal experiments, we show the tuned lens uses similar\nfeatures to the model itself. We also find the trajectory of latent predictions\ncan be used to detect malicious inputs with high accuracy. All code needed to\nreproduce our results can be found at\nhttps://github.com/AlignmentResearch/tuned-lens.\n","authors":["Nora Belrose","Zach Furman","Logan Smith","Danny Halawi","Igor Ostrovsky","Lev McKinney","Stella Biderman","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2303.08112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08109v1","updated":"2023-03-14T17:44:23Z","published":"2023-03-14T17:44:23Z","title":"Vision-based route following by an embodied insect-inspired sparse\n  neural network","summary":"  We compared the efficiency of the FlyHash model, an insect-inspired sparse\nneural network (Dasgupta et al., 2017), to similar but non-sparse models in an\nembodied navigation task. This requires a model to control steering by\ncomparing current visual inputs to memories stored along a training route. We\nconcluded the FlyHash model is more efficient than others, especially in terms\nof data encoding.\n","authors":["Lu Yihe","Rana Alkhoury Maroun","Barbara Webb"],"pdf_url":"https://arxiv.org/pdf/2303.08109v1.pdf","comment":"8 pages, 4 figures; work-in-progress submission, accepted as a poster\n  at ICLR 2023 Workshop on Sparsity in Neural Networks; non-archival"},{"id":"http://arxiv.org/abs/2303.08102v1","updated":"2023-03-14T17:41:31Z","published":"2023-03-14T17:41:31Z","title":"Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice","summary":"  We investigate the problem of bandits with expert advice when the experts are\nfixed and known distributions over the actions. Improving on previous analyses,\nwe show that the regret in this setting is controlled by information-theoretic\nquantities that measure the similarity between experts. In some natural special\ncases, this allows us to obtain the first regret bound for EXP4 that can get\narbitrarily close to zero if the experts are similar enough. While for a\ndifferent algorithm, we provide another bound that describes the similarity\nbetween the experts in terms of the KL-divergence, and we show that this bound\ncan be smaller than the one of EXP4 in some cases. Additionally, we provide\nlower bounds for certain classes of experts showing that the algorithms we\nanalyzed are nearly optimal in some cases.\n","authors":["Khaled Eldowa","Nicolò Cesa-Bianchi","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2303.08102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13059v3","updated":"2023-03-14T17:36:44Z","published":"2022-02-26T04:49:01Z","title":"Variational Inference with Gaussian Mixture by Entropy Approximation","summary":"  Variational inference is a technique for approximating intractable posterior\ndistributions in order to quantify the uncertainty of machine learning.\nAlthough the unimodal Gaussian distribution is usually chosen as a parametric\ndistribution, it hardly approximates the multimodality. In this paper, we\nemploy the Gaussian mixture distribution as a parametric distribution. A main\ndifficulty of variational inference with the Gaussian mixture is how to\napproximate the entropy of the Gaussian mixture. We approximate the entropy of\nthe Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which\ncan be analytically calculated. In addition, we theoretically analyze the\napproximation error between the true entropy and approximated one in order to\nreveal when our approximation works well. Specifically, the approximation error\nis controlled by the ratios of the distances between the means to the sum of\nthe variances of the Gaussian mixture. Furthermore, it converges to zero when\nthe ratios go to infinity. This situation seems to be more likely to occur in\nhigher dimensional parametric spaces because of the curse of dimensionality.\nTherefore, our result guarantees that our approximation works well, for\nexample, in neural networks that assume a large number of weights.\n","authors":["Takashi Furuya","Hiroyuki Kusumoto","Koichi Taniguchi","Naoya Kanno","Kazuma Suetake"],"pdf_url":"https://arxiv.org/pdf/2202.13059v3.pdf","comment":"35 pages, 3 figures"},{"id":"http://arxiv.org/abs/2302.06025v2","updated":"2023-03-14T17:23:52Z","published":"2023-02-12T23:20:41Z","title":"Statistical Complexity and Optimal Algorithms for Non-linear Ridge\n  Bandits","summary":"  We consider the sequential decision-making problem where the mean outcome is\na non-linear function of the chosen action. Compared with the linear model, two\ncurious phenomena arise in non-linear models: first, in addition to the\n\"learning phase\" with a standard parametric rate for estimation or regret,\nthere is an \"burn-in period\" with a fixed cost determined by the non-linear\nfunction; second, achieving the smallest burn-in cost requires new exploration\nalgorithms. For a special family of non-linear functions named ridge functions\nin the literature, we derive upper and lower bounds on the optimal burn-in\ncost, and in addition, on the entire learning trajectory during the burn-in\nperiod via differential equations. In particular, a two-stage algorithm that\nfirst finds a good initial action and then treats the problem as locally linear\nis statistically optimal. In contrast, several classical algorithms, such as\nUCB and algorithms relying on regression oracles, are provably suboptimal.\n","authors":["Nived Rajaraman","Yanjun Han","Jiantao Jiao","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2302.06025v2.pdf","comment":"Title change; add a new lower bound for linear bandits in Theorem 13"},{"id":"http://arxiv.org/abs/2212.00881v2","updated":"2023-03-14T17:22:41Z","published":"2022-12-01T21:39:48Z","title":"Investigating Deep Learning Model Calibration for Classification\n  Problems in Mechanics","summary":"  Recently, there has been a growing interest in applying machine learning\nmethods to problems in engineering mechanics. In particular, there has been\nsignificant interest in applying deep learning techniques to predicting the\nmechanical behavior of heterogeneous materials and structures. Researchers have\nshown that deep learning methods are able to effectively predict mechanical\nbehavior with low error for systems ranging from engineered composites, to\ngeometrically complex metamaterials, to heterogeneous biological tissue.\nHowever, there has been comparatively little attention paid to deep learning\nmodel calibration, i.e., the match between predicted probabilities of outcomes\nand the true probabilities of outcomes. In this work, we perform a\ncomprehensive investigation into ML model calibration across seven open access\nengineering mechanics datasets that cover three distinct types of mechanical\nproblems. Specifically, we evaluate both model and model calibration error for\nmultiple machine learning methods, and investigate the influence of ensemble\naveraging and post hoc model calibration via temperature scaling. Overall, we\nfind that ensemble averaging of deep neural networks is both an effective and\nconsistent tool for improving model calibration, while temperature scaling has\ncomparatively limited benefits. Looking forward, we anticipate that this\ninvestigation will lay the foundation for future work in developing mechanics\nspecific approaches to deep learning model calibration.\n","authors":["Saeed Mohammadzadeh","Peerasait Prachaseree","Emma Lejeune"],"pdf_url":"https://arxiv.org/pdf/2212.00881v2.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.08081v1","updated":"2023-03-14T17:13:01Z","published":"2023-03-14T17:13:01Z","title":"Explanation Shift: Investigating Interactions between Models and\n  Shifting Data Distributions","summary":"  As input data distributions evolve, the predictive performance of machine\nlearning models tends to deteriorate. In practice, new input data tend to come\nwithout target labels. Then, state-of-the-art techniques model input data\ndistributions or model prediction distributions and try to understand issues\nregarding the interactions between learned models and shifting distributions.\nWe suggest a novel approach that models how explanation characteristics shift\nwhen affected by distribution shifts. We find that the modeling of explanation\nshifts can be a better indicator for detecting out-of-distribution model\nbehaviour than state-of-the-art techniques. We analyze different types of\ndistribution shifts using synthetic examples and real-world data sets. We\nprovide an algorithmic method that allows us to inspect the interaction between\ndata set features and learned models and compare them to the state-of-the-art.\nWe release our methods in an open-source Python package, as well as the code\nused to reproduce our experiments.\n","authors":["Carlos Mougan","Klaus Broelemann","David Masip","Gjergji Kasneci","Thanassis Thiropanis","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2303.08081v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2210.12369"},{"id":"http://arxiv.org/abs/2302.08893v2","updated":"2023-03-14T17:09:47Z","published":"2023-02-17T14:24:13Z","title":"A survey on online active learning","summary":"  Online active learning is a paradigm in machine learning that aims to select\nthe most informative data points to label from a data stream. The problem of\nminimizing the cost associated with collecting labeled observations has gained\na lot of attention in recent years, particularly in real-world applications\nwhere data is only available in an unlabeled form. Annotating each observation\ncan be time-consuming and costly, making it difficult to obtain large amounts\nof labeled data. To overcome this issue, many active learning strategies have\nbeen proposed in the last decades, aiming to select the most informative\nobservations for labeling in order to improve the performance of machine\nlearning models. These approaches can be broadly divided into two categories:\nstatic pool-based and stream-based active learning. Pool-based active learning\ninvolves selecting a subset of observations from a closed pool of unlabeled\ndata, and it has been the focus of many surveys and literature reviews.\nHowever, the growing availability of data streams has led to an increase in the\nnumber of approaches that focus on online active learning, which involves\ncontinuously selecting and labeling observations as they arrive in a stream.\nThis work aims to provide an overview of the most recently proposed approaches\nfor selecting the most informative observations from data streams in the\ncontext of online active learning. We review the various techniques that have\nbeen proposed and discuss their strengths and limitations, as well as the\nchallenges and opportunities that exist in this area of research. Our review\naims to provide a comprehensive and up-to-date overview of the field and to\nhighlight directions for future work.\n","authors":["Davide Cacciarelli","Murat Kulahci"],"pdf_url":"https://arxiv.org/pdf/2302.08893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03052v2","updated":"2023-03-14T17:04:49Z","published":"2023-03-06T11:51:28Z","title":"Masked Images Are Counterfactual Samples for Robust Fine-tuning","summary":"  Deep learning models are challenged by the distribution shift between the\ntraining data and test data. Recently, the large models pre-trained on diverse\ndata demonstrate unprecedented robustness to various distribution shifts.\nHowever, fine-tuning on these models can lead to a trade-off between\nin-distribution (ID) performance and out-of-distribution (OOD) robustness.\nExisting methods for tackling this trade-off do not explicitly address the OOD\nrobustness problem. In this paper, based on causal analysis on the\naforementioned problems, we propose a novel fine-tuning method, which use\nmasked images as counterfactual samples that help improving the robustness of\nthe fine-tuning model. Specifically, we mask either the semantics-related or\nsemantics-unrelated patches of the images based on class activation map to\nbreak the spurious correlation, and refill the masked patches with patches from\nother images. The resulting counterfactual samples are used in feature-based\ndistillation with the pre-trained model. Extensive experiments verify that\nregularizing the fine-tuning with the proposed masked images can achieve a\nbetter trade-off between ID and OOD performance, surpassing previous methods on\nthe OOD performance. Our code will be publicly available.\n","authors":["Yao Xiao","Ziyi Tang","Pengxu Wei","Cong Liu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.03052v2.pdf","comment":"Accepted by CVPR 2023 (v2: improve the clarity)"},{"id":"http://arxiv.org/abs/2210.10272v2","updated":"2023-03-14T17:02:34Z","published":"2022-10-19T03:29:58Z","title":"Training set cleansing of backdoor poisoning by self-supervised\n  representation learning","summary":"  A backdoor or Trojan attack is an important type of data poisoning attack\nagainst deep neural network (DNN) classifiers, wherein the training dataset is\npoisoned with a small number of samples that each possess the backdoor pattern\n(usually a pattern that is either imperceptible or innocuous) and which are\nmislabeled to the attacker's target class. When trained on a backdoor-poisoned\ndataset, a DNN behaves normally on most benign test samples but makes incorrect\npredictions to the target class when the test sample has the backdoor pattern\nincorporated (i.e., contains a backdoor trigger). Here we focus on image\nclassification tasks and show that supervised training may build stronger\nassociation between the backdoor pattern and the associated target class than\nthat between normal features and the true class of origin. By contrast,\nself-supervised representation learning ignores the labels of samples and\nlearns a feature embedding based on images' semantic content. %We thus propose\nto use unsupervised representation learning to avoid emphasising\nbackdoor-poisoned training samples and learn a similar feature embedding for\nsamples of the same class. Using a feature embedding found by self-supervised\nrepresentation learning, a data cleansing method, which combines sample\nfiltering and re-labeling, is developed. Experiments on CIFAR-10 benchmark\ndatasets show that our method achieves state-of-the-art performance in\nmitigating backdoor attacks.\n","authors":["H. Wang","S. Karami","O. Dia","H. Ritter","E. Emamjomeh-Zadeh","J. Chen","Z. Xiang","D. J. Miller","G. Kesidis"],"pdf_url":"https://arxiv.org/pdf/2210.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04115v2","updated":"2023-03-14T16:57:30Z","published":"2023-03-07T18:28:39Z","title":"Predicted Embedding Power Regression for Large-Scale Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) inputs can compromise the performance and safety of\nreal world machine learning systems. While many methods exist for OOD detection\nand work well on small scale datasets with lower resolution and few classes,\nfew methods have been developed for large-scale OOD detection. Existing\nlarge-scale methods generally depend on maximum classification probability,\nsuch as the state-of-the-art grouped softmax method. In this work, we develop a\nnovel approach that calculates the probability of the predicted class label\nbased on label distributions learned during the training process. Our method\nperforms better than current state-of-the-art methods with only a negligible\nincrease in compute cost. We evaluate our method against contemporary methods\nacross $14$ datasets and achieve a statistically significant improvement with\nrespect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).\n","authors":["Hong Yang","William Gebhardt","Alexander G. Ororbia","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2303.04115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.11741v2","updated":"2023-03-14T16:57:14Z","published":"2022-09-21T21:17:56Z","title":"Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking\n  Neural Networks with Learnable Neuronal Dynamics","summary":"  Event-based cameras have recently shown great potential for high-speed motion\nestimation owing to their ability to capture temporally rich information\nasynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired\nevent-driven processing can efficiently handle such asynchronous data, while\nneuron models such as the leaky-integrate and fire (LIF) can keep track of the\nquintessential timing information contained in the inputs. SNNs achieve this by\nmaintaining a dynamic state in the neuron memory, retaining important\ninformation while forgetting redundant data over time. Thus, we posit that SNNs\nwould allow for better performance on sequential regression tasks compared to\nsimilarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult\nto train due to vanishing spikes at later layers. To that effect, we propose an\nadaptive fully-spiking framework with learnable neuronal dynamics to alleviate\nthe spike vanishing problem. We utilize surrogate gradient-based\nbackpropagation through time (BPTT) to train our deep SNNs from scratch. We\nvalidate our approach for the task of optical flow estimation on the\nMulti-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.\nOur experiments on these datasets show an average reduction of 13% in average\nendpoint error (AEE) compared to state-of-the-art ANNs. We also explore several\ndown-scaled models and observe that our SNN models consistently outperform\nsimilarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the\nimportance of SNNs for smaller models and their suitability at the edge. In\nterms of efficiency, our SNNs offer substantial savings in network parameters\n(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE\ncompared to the state-of-the-art ANN implementations.\n","authors":["Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2209.11741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02390v2","updated":"2023-03-14T16:54:14Z","published":"2022-10-05T17:05:56Z","title":"Bayesian Prompt Learning for Image-Language Model Generalization","summary":"  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\n","authors":["Mohammad Mahdi Derakhshani","Enrique Sanchez","Adrian Bulat","Victor Guilherme Turrisi da Costa","Cees G. M. Snoek","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2210.02390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08059v1","updated":"2023-03-14T16:51:14Z","published":"2023-03-14T16:51:14Z","title":"Fast Rates for Maximum Entropy Exploration","summary":"  We consider the reinforcement learning (RL) setting, in which the agent has\nto act in unknown environment driven by a Markov Decision Process (MDP) with\nsparse or even reward free signals. In this situation, exploration becomes the\nmain challenge. In this work, we study the maximum entropy exploration problem\nof two different types. The first type is visitation entropy maximization that\nwas previously considered by Hazan et al. (2019) in the discounted setting. For\nthis type of exploration, we propose an algorithm based on a game theoretic\nrepresentation that has $\\widetilde{\\mathcal{O}}(H^3 S^2 A / \\varepsilon^2)$\nsample complexity thus improving the $\\varepsilon$-dependence of Hazan et al.\n(2019), where $S$ is a number of states, $A$ is a number of actions, $H$ is an\nepisode length, and $\\varepsilon$ is a desired accuracy. The second type of\nentropy we study is the trajectory entropy. This objective function is closely\nrelated to the entropy-regularized MDPs, and we propose a simple modification\nof the UCBVI algorithm that has a sample complexity of order\n$\\widetilde{\\mathcal{O}}(1/\\varepsilon)$ ignoring dependence in $S, A, H$.\nInterestingly enough, it is the first theoretical result in RL literature\nestablishing that the exploration problem for the regularized MDPs can be\nstatistically strictly easier (in terms of sample complexity) than for the\nordinary MDPs.\n","authors":["Daniil Tiapkin","Denis Belomestny","Daniele Calandriello","Eric Moulines","Remi Munos","Alexey Naumov","Pierre Perrault","Yunhao Tang","Michal Valko","Pierre Menard"],"pdf_url":"https://arxiv.org/pdf/2303.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08054v1","updated":"2023-03-14T16:37:38Z","published":"2023-03-14T16:37:38Z","title":"Statistical Hardware Design With Multi-model Active Learning","summary":"  With the rising complexity of numerous novel applications that serve our\nmodern society comes the strong need to design efficient computing platforms.\nDesigning efficient hardware is, however, a complex multi-objective problem\nthat deals with multiple parameters and their interactions. Given that there\nare a large number of parameters and objectives involved in hardware design,\nsynthesizing all possible combinations is not a feasible method to find the\noptimal solution. One promising approach to tackle this problem is statistical\nmodeling of a desired hardware performance. Here, we propose a model-based\nactive learning approach to solve this problem. Our proposed method uses\nBayesian models to characterize various aspects of hardware performance. We\nalso use transfer learning and Gaussian regression bootstrapping techniques in\nconjunction with active learning to create more accurate models. Our proposed\nstatistical modeling method provides hardware models that are sufficiently\naccurate to perform design space exploration as well as performance prediction\nsimultaneously. We use our proposed method to perform design space exploration\nand performance prediction for various hardware setups, such as\nmicro-architecture design and OpenCL kernels for FPGA targets. Our experiments\nshow that the number of samples required to create performance models\nsignificantly reduces while maintaining the predictive power of our proposed\nstatistical models. For instance, in our performance prediction setting, the\nproposed method needs 65\\% fewer samples to create the model, and in the design\nspace exploration setting, our proposed method can find the best parameter\nsettings by exploring less than 50 samples.\n","authors":["Alireza Ghaffari","Masoud Asgharian","Yvon Savaria"],"pdf_url":"https://arxiv.org/pdf/2303.08054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08040v1","updated":"2023-03-14T16:19:44Z","published":"2023-03-14T16:19:44Z","title":"Demographic Parity Inspector: Fairness Audits via the Explanation Space","summary":"  Even if deployed with the best intentions, machine learning methods can\nperpetuate, amplify or even create social biases. Measures of (un-)fairness\nhave been proposed as a way to gauge the (non-)discriminatory nature of machine\nlearning models. However, proxies of protected attributes causing\ndiscriminatory effects remain challenging to address. In this work, we propose\na new algorithmic approach that measures group-wise demographic parity\nviolations and allows us to inspect the causes of inter-group discrimination.\nOur method relies on the novel idea of measuring the dependence of a model on\nthe protected attribute based on the explanation space, an informative space\nthat allows for more sensitive audits than the primary space of input data or\nprediction distributions, and allowing for the assertion of theoretical\ndemographic parity auditing guarantees. We provide a mathematical analysis,\nsynthetic examples, and experimental evaluation of real-world data. We release\nan open-source Python package with methods, routines, and tutorials.\n","authors":["Carlos Mougan","Laura State","Antonio Ferrara","Salvatore Ruggieri","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2303.08040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06710v2","updated":"2023-03-14T16:16:58Z","published":"2023-03-12T17:22:54Z","title":"Decision Making for Human-in-the-loop Robotic Agents via\n  Uncertainty-Aware Reinforcement Learning","summary":"  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly\nautonomously in solving a task, but can request help from an external expert\nwhen needed. However, knowing when to request such assistance is critical: too\nfew requests can lead to the robot making mistakes, but too many requests can\noverload the expert. In this paper, we present a Reinforcement Learning based\napproach to this problem, where a semi-autonomous agent asks for external\nassistance when it has low confidence in the eventual success of the task. The\nconfidence level is computed by estimating the variance of the return from the\ncurrent state. We show that this estimate can be iteratively improved during\ntraining using a Bellman-like recursion. On discrete navigation problems with\nboth fully- and partially-observable state information, we show that our method\nmakes effective use of a limited budget of expert calls at run-time, despite\nhaving no access to the expert at training time.\n","authors":["Siddharth Singi","Zhanpeng He","Alvin Pan","Sandip Patel","Gunnar A. Sigurdsson","Robinson Piramuthu","Shuran Song","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2303.06710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08035v1","updated":"2023-03-14T16:15:28Z","published":"2023-03-14T16:15:28Z","title":"ISimDL: Importance Sampling-Driven Acceleration of Fault Injection\n  Simulations for Evaluating the Robustness of Deep Learning","summary":"  Deep Learning (DL) systems have proliferated in many applications, requiring\nspecialized hardware accelerators and chips. In the nano-era, devices have\nbecome increasingly more susceptible to permanent and transient faults.\nTherefore, we need an efficient methodology for analyzing the resilience of\nadvanced DL systems against such faults, and understand how the faults in\nneural accelerator chips manifest as errors at the DL application level, where\nfaults can lead to undetectable and unrecoverable errors. Using fault\ninjection, we can perform resilience investigations of the DL system by\nmodifying neuron weights and outputs at the software-level, as if the hardware\nhad been affected by a transient fault. Existing fault models reduce the search\nspace, allowing faster analysis, but requiring a-priori knowledge on the model,\nand not allowing further analysis of the filtered-out search space. Therefore,\nwe propose ISimDL, a novel methodology that employs neuron sensitivity to\ngenerate importance sampling-based fault-scenarios. Without any a-priori\nknowledge of the model-under-test, ISimDL provides an equivalent reduction of\nthe search space as existing works, while allowing long simulations to cover\nall the possible faults, improving on existing model requirements. Our\nexperiments show that the importance sampling provides up to 15x higher\nprecision in selecting critical faults than the random uniform sampling,\nreaching such precision in less than 100 faults. Additionally, we showcase\nanother practical use-case for importance sampling for reliable DNN design,\nnamely Fault Aware Training (FAT). By using ISimDL to select the faults leading\nto errors, we can insert the faults during the DNN training process to harden\nthe DNN against such faults. Using importance sampling in FAT reduces the\noverhead required for finding faults that lead to a predetermined drop in\naccuracy by more than 12x.\n","authors":["Alessio Colucci","Andreas Steininger","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.08035v1.pdf","comment":"Under review at IJCNN2023"},{"id":"http://arxiv.org/abs/2211.05634v2","updated":"2023-03-14T16:12:11Z","published":"2022-11-07T07:58:29Z","title":"Generalization of generative model for neuronal ensemble inference\n  method","summary":"  Various brain functions that are necessary to maintain life activities\nmaterialize through the interaction of countless neurons. Therefore, it is\nimportant to analyze the structure of functional neuronal network. To elucidate\nthe mechanism of brain function, many studies are being actively conducted on\nthe structure of functional neuronal ensemble and hub, including all areas of\nneuroscience. In addition, recent study suggests that the existence of\nfunctional neuronal ensembles and hubs contributes to the efficiency of\ninformation processing. For these reasons, there is a demand for methods to\ninfer functional neuronal ensembles from neuronal activity data, and methods\nbased on Bayesian inference have been proposed. However, there is a problem in\nmodeling the activity in Bayesian inference. The features of each neuron's\nactivity have non-stationarity depending on physiological experimental\nconditions. As a result, the assumption of stationarity in Bayesian inference\nmodel impedes inference, which leads to destabilization of inference results\nand degradation of inference accuracy. In this study, we extend the range of\nthe variable for expressing the neuronal state, and generalize the likelihood\nof the model for extended variables. By comparing with the previous study, our\nmodel can express the neuronal state in larger space. This generalization\nwithout restriction of the binary input enables us to perform soft clustering\nand apply the method to non-stationary neuroactivity data. In addition, for the\neffectiveness of the method, we apply the developed method to multiple\nsynthetic fluorescence data generated from the electrical potential data in\nleaky integrated-and-fire model.\n","authors":["Shun Kimura","Koujin Takeda"],"pdf_url":"https://arxiv.org/pdf/2211.05634v2.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08032v1","updated":"2023-03-14T16:11:47Z","published":"2023-03-14T16:11:47Z","title":"BODEGA: Benchmark for Adversarial Example Generation in Credibility\n  Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we introduce BODEGA: a benchmark for testing both victim models\nand attack methods on four misinformation detection tasks in an evaluation\nframework designed to simulate real use-cases of content moderation. We also\nsystematically test the robustness of popular text classifiers against\navailable attacking techniques and discover that, indeed, in some cases barely\nsignificant changes in input text can mislead the models. We openly share the\nBODEGA code and data in hope of enhancing the comparability and replicability\nof further research in this area.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v3","updated":"2023-03-14T16:11:35Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v3.pdf","comment":"ACCV 2022 (Best Paper Award)\n  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html"},{"id":"http://arxiv.org/abs/2303.08027v1","updated":"2023-03-14T16:08:45Z","published":"2023-03-14T16:08:45Z","title":"A Hierarchical Regression Chain Framework for Affective Vocal Burst\n  Recognition","summary":"  As a common way of emotion signaling via non-linguistic vocalizations, vocal\nburst (VB) plays an important role in daily social interaction. Understanding\nand modeling human vocal bursts are indispensable for developing robust and\ngeneral artificial intelligence. Exploring computational approaches for\nunderstanding vocal bursts is attracting increasing research attention. In this\nwork, we propose a hierarchical framework, based on chain regression models,\nfor affective recognition from VBs, that explicitly considers multiple\nrelationships: (i) between emotional states and diverse cultures; (ii) between\nlow-dimensional (arousal & valence) and high-dimensional (10 emotion classes)\nemotion spaces; and (iii) between various emotion classes within the\nhigh-dimensional space. To address the challenge of data sparsity, we also use\nself-supervised learning (SSL) representations with layer-wise and temporal\naggregation modules. The proposed systems participated in the ACII Affective\nVocal Burst (A-VB) Challenge 2022 and ranked first in the \"TWO'' and \"CULTURE''\ntasks. Experimental results based on the ACII Challenge 2022 dataset\ndemonstrate the superior performance of the proposed system and the\neffectiveness of considering multiple relationships using hierarchical\nregression chain models.\n","authors":["Jinchao Li","Xixin Wu","Kaitao Song","Dongsheng Li","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08027v1.pdf","comment":"5 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.08019v1","updated":"2023-03-14T16:03:28Z","published":"2023-03-14T16:03:28Z","title":"Leveraging Pretrained Representations with Task-related Keywords for\n  Alzheimer's Disease Detection","summary":"  With the global population aging rapidly, Alzheimer's disease (AD) is\nparticularly prominent in older adults, which has an insidious onset and leads\nto a gradual, irreversible deterioration in cognitive domains (memory,\ncommunication, etc.). Speech-based AD detection opens up the possibility of\nwidespread screening and timely disease intervention. Recent advances in\npre-trained models motivate AD detection modeling to shift from low-level\nfeatures to high-level representations. This paper presents several efficient\nmethods to extract better AD-related cues from high-level acoustic and\nlinguistic features. Based on these features, the paper also proposes a novel\ntask-oriented approach by modeling the relationship between the participants'\ndescription and the cognitive task. Experiments are carried out on the ADReSS\ndataset in a binary classification setup, and models are evaluated on the\nunseen test set. Results and comparison with recent literature demonstrate the\nefficiency and superior performance of proposed acoustic, linguistic and\ntask-oriented methods. The findings also show the importance of semantic and\nsyntactic information, and feasibility of automation and generalization with\nthe promising audio-only and task-oriented methods for the AD detection task.\n","authors":["Jinchao Li","Kaitao Song","Junan Li","Bo Zheng","Dongsheng Li","Xixin Wu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08019v1.pdf","comment":"5 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.08017v1","updated":"2023-03-14T16:02:46Z","published":"2023-03-14T16:02:46Z","title":"Reliable Beamforming at Terahertz Bands: Are Causal Representations the\n  Way Forward?","summary":"  Future wireless services, such as the metaverse require high information\nrate, reliability, and low latency. Multi-user wireless systems can meet such\nrequirements by utilizing the abundant terahertz bandwidth with a massive\nnumber of antennas, creating narrow beamforming solutions. However, existing\nsolutions lack proper modeling of channel dynamics, resulting in inaccurate\nbeamforming solutions in high-mobility scenarios. Herein, a dynamic,\nsemantically aware beamforming solution is proposed for the first time,\nutilizing novel artificial intelligence algorithms in variational causal\ninference to compute the time-varying dynamics of the causal representation of\nmulti-modal data and the beamforming. Simulations show that the proposed\ncausality-guided approach for Terahertz (THz) beamforming outperforms classical\nMIMO beamforming techniques.\n","authors":["Christo Kurisummoottil Thomas","Walid Saad"],"pdf_url":"https://arxiv.org/pdf/2303.08017v1.pdf","comment":"Accepted at IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08010v1","updated":"2023-03-14T15:57:54Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07991v1","updated":"2023-03-14T15:45:35Z","published":"2023-03-14T15:45:35Z","title":"Finding the Needle in a Haystack: Unsupervised Rationale Extraction from\n  Long Text Classifiers","summary":"  Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.\n","authors":["Kamil Bujel","Andrew Caines","Helen Yannakoudakis","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2303.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07988v1","updated":"2023-03-14T15:44:40Z","published":"2023-03-14T15:44:40Z","title":"Partial Neural Optimal Transport","summary":"  We propose a novel neural method to compute partial optimal transport (OT)\nmaps, i.e., OT maps between parts of measures of the specified masses. We test\nour partial neural optimal transport algorithm on synthetic examples.\n","authors":["Milena Gazdieva","Alexander Korotin","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2303.07988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07987v1","updated":"2023-03-14T15:44:20Z","published":"2023-03-14T15:44:20Z","title":"Practically Solving LPN in High Noise Regimes Faster Using Neural\n  Networks","summary":"  We conduct a systematic study of solving the learning parity with noise\nproblem (LPN) using neural networks. Our main contribution is designing\nfamilies of two-layer neural networks that practically outperform classical\nalgorithms in high-noise, low-dimension regimes. We consider three settings\nwhere the numbers of LPN samples are abundant, very limited, and in between. In\neach setting we provide neural network models that solve LPN as fast as\npossible. For some settings we are also able to provide theories that explain\nthe rationale of the design of our models. Comparing with the previous\nexperiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$,\nnoise rate $\\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm\ntakes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66\nminutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms\nfor solving middle or large dimension LPN instances.\n","authors":["Haozhe Jiang","Kaiyue Wen","Yilei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07987v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2303.07971v1","updated":"2023-03-14T15:24:05Z","published":"2023-03-14T15:24:05Z","title":"A Theory of Emergent In-Context Learning as Implicit Structure Induction","summary":"  Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.\n","authors":["Michael Hahn","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2303.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09078v3","updated":"2023-03-14T15:13:06Z","published":"2022-09-19T15:12:47Z","title":"NIERT: Accurate Numerical Interpolation through Unifying Scattered Data\n  Representations using Transformer Encoder","summary":"  Interpolation for scattered data is a classical problem in numerical\nanalysis, with a long history of theoretical and practical contributions.\nRecent advances have utilized deep neural networks to construct interpolators,\nexhibiting excellent and generalizable performance. However, they still fall\nshort in two aspects: \\textbf{1) inadequate representation learning}, resulting\nfrom separate embeddings of observed and target points in popular\nencoder-decoder frameworks and \\textbf{2) limited generalization power}, caused\nby overlooking prior interpolation knowledge shared across different domains.\nTo overcome these limitations, we present a \\textbf{N}umerical\n\\textbf{I}nterpolation approach using \\textbf{E}ncoder \\textbf{R}epresentation\nof \\textbf{T}ransformers (called \\textbf{NIERT}). On one hand, NIERT utilizes\nan encoder-only framework rather than the encoder-decoder structure. This way,\nNIERT can embed observed and target points into a unified encoder\nrepresentation space, thus effectively exploiting the correlations among them\nand obtaining more precise representations. On the other hand, we propose to\npre-train NIERT on large-scale synthetic mathematical functions to acquire\nprior interpolation knowledge, and transfer it to multiple interpolation\ndomains with consistent performance gain. On both synthetic and real-world\ndatasets, NIERT outperforms the existing approaches by a large margin, i.e.,\n4.3$\\sim$14.3$\\times$ lower MAE on TFRD subsets, and 1.7/1.8/8.7$\\times$ lower\nMSE on Mathit/PhysioNet/PTV datasets. The source code of NIERT is available at\nhttps://github.com/DingShizhe/NIERT.\n","authors":["Shizhe Ding","Boyang Xia","Milong Ren","Dongbo Bu"],"pdf_url":"https://arxiv.org/pdf/2209.09078v3.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2206.04636v3","updated":"2023-03-14T15:07:16Z","published":"2022-06-09T17:34:39Z","title":"Spatial Entropy as an Inductive Bias for Vision Transformers","summary":"  Recent work on Vision Transformers (VTs) showed that introducing a local\ninductive bias in the VT architecture helps reducing the number of samples\nnecessary for training. However, the architecture modifications lead to a loss\nof generality of the Transformer backbone, partially contradicting the push\ntowards the development of uniform architectures, shared, e.g., by both the\nComputer Vision and the Natural Language Processing areas. In this work, we\npropose a different and complementary direction, in which a local bias is\nintroduced using an auxiliary self-supervised task, performed jointly with\nstandard supervised training. Specifically, we exploit the observation that the\nattention maps of VTs, when trained with self-supervision, can contain a\nsemantic segmentation structure which does not spontaneously emerge when\ntraining is supervised. Thus, we explicitly encourage the emergence of this\nspatial clustering as a form of training regularization. In more detail, we\nexploit the assumption that, in a given image, objects usually correspond to\nfew connected regions, and we propose a spatial formulation of the information\nentropy to quantify this object-based inductive bias. By minimizing the\nproposed spatial entropy, we include an additional self-supervised signal\nduring training. Using extensive experiments, we show that the proposed\nregularization leads to equivalent or better results than other VT proposals\nwhich include a local bias by changing the basic Transformer architecture, and\nit can drastically boost the VT final accuracy when using small-medium training\nsets. The code is available at https://github.com/helia95/SAR.\n","authors":["Elia Peruzzo","Enver Sangineto","Yahui Liu","Marco De Nadai","Wei Bi","Bruno Lepri","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2206.04636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00841v3","updated":"2023-03-14T15:04:33Z","published":"2022-10-03T11:57:30Z","title":"Smooth image-to-image translations with latent space interpolations","summary":"  Multi-domain image-to-image (I2I) translations can transform a source image\naccording to the style of a target domain. One important, desired\ncharacteristic of these transformations, is their graduality, which corresponds\nto a smooth change between the source and the target image when their\nrespective latent-space representations are linearly interpolated. However,\nstate-of-the-art methods usually perform poorly when evaluated using\ninter-domain interpolations, often producing abrupt changes in the appearance\nor non-realistic intermediate images. In this paper, we argue that one of the\nmain reasons behind this problem is the lack of sufficient inter-domain\ntraining data and we propose two different regularization methods to alleviate\nthis issue: a new shrinkage loss, which compacts the latent space, and a Mixup\ndata-augmentation strategy, which flattens the style representations between\ndomains. We also propose a new metric to quantitatively evaluate the degree of\nthe interpolation smoothness, an aspect which is not sufficiently covered by\nthe existing I2I translation metrics. Using both our proposed metric and\nstandard evaluation protocols, we show that our regularization techniques can\nimprove the state-of-the-art multi-domain I2I translations by a large margin.\nOur code will be made publicly available upon the acceptance of this article.\n","authors":["Yahui Liu","Enver Sangineto","Yajing Chen","Linchao Bao","Haoxian Zhang","Nicu Sebe","Bruno Lepri","Marco De Nadai"],"pdf_url":"https://arxiv.org/pdf/2210.00841v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01286v2","updated":"2023-03-14T14:53:27Z","published":"2022-12-31T22:56:04Z","title":"Pseudo-Inverted Bottleneck Convolution for DARTS Search Space","summary":"  Differentiable Architecture Search (DARTS) has attracted considerable\nattention as a gradient-based neural architecture search method. Since the\nintroduction of DARTS, there has been little work done on adapting the action\nspace based on state-of-art architecture design principles for CNNs. In this\nwork, we aim to address this gap by incrementally augmenting the DARTS search\nspace with micro-design changes inspired by ConvNeXt and studying the trade-off\nbetween accuracy, evaluation layer count, and computational cost. We introduce\nthe Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the\ncomputational footprint of the inverted bottleneck block proposed in ConvNeXt.\nOur proposed architecture is much less sensitive to evaluation layer count and\noutperforms a DARTS network with similar size significantly, at layer counts as\nsmall as 2. Furthermore, with less layers, not only does it achieve higher\naccuracy with lower computational footprint (measured in GMACs) and parameter\ncount, GradCAM comparisons show that our network can better detect distinctive\nfeatures of target objects compared to DARTS. Code is available from\nhttps://github.com/mahdihosseini/PIBConv.\n","authors":["Arash Ahmadian","Louis S. P. Liu","Yue Fei","Konstantinos N. Plataniotis","Mahdi S. Hosseini"],"pdf_url":"https://arxiv.org/pdf/2301.01286v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.06274v2","updated":"2023-03-14T14:53:19Z","published":"2023-03-11T01:21:13Z","title":"CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,\n  Segmentation, Classification and Counting","summary":"  Nuclear detection, segmentation and morphometric profiling are essential in\nhelping us further understand the relationship between histology and patient\noutcome. To drive innovation in this area, we setup a community-wide challenge\nusing the largest available dataset of its kind to assess nuclear segmentation\nand cellular composition. Our challenge, named CoNIC, stimulated the\ndevelopment of reproducible algorithms for cellular recognition with real-time\nresult inspection on public leaderboards. We conducted an extensive\npost-challenge analysis based on the top-performing models using 1,658\nwhole-slide images of colon tissue. With around 700 million detected nuclei per\nmodel, associated features were used for dysplasia grading and survival\nanalysis, where we demonstrated that the challenge's improvement over the\nprevious state-of-the-art led to significant boosts in downstream performance.\nOur findings also suggest that eosinophils and neutrophils play an important\nrole in the tumour microevironment. We release challenge models and WSI-level\nresults to foster the development of further methods for biomarker discovery.\n","authors":["Simon Graham","Quoc Dang Vu","Mostafa Jahanifar","Martin Weigert","Uwe Schmidt","Wenhua Zhang","Jun Zhang","Sen Yang","Jinxi Xiang","Xiyue Wang","Josef Lorenz Rumberger","Elias Baumann","Peter Hirsch","Lihao Liu","Chenyang Hong","Angelica I. Aviles-Rivero","Ayushi Jain","Heeyoung Ahn","Yiyu Hong","Hussam Azzuni","Min Xu","Mohammad Yaqub","Marie-Claire Blache","Benoît Piégu","Bertrand Vernay","Tim Scherr","Moritz Böhland","Katharina Löffler","Jiachen Li","Weiqin Ying","Chixin Wang","Dagmar Kainmueller","Carola-Bibiane Schönlieb","Shuolin Liu","Dhairya Talsania","Yughender Meda","Prakash Mishra","Muhammad Ridzuan","Oliver Neumann","Marcel P. Schilling","Markus Reischl","Ralf Mikut","Banban Huang","Hsiang-Chin Chien","Ching-Ping Wang","Chia-Yen Lee","Hong-Kun Lin","Zaiyi Liu","Xipeng Pan","Chu Han","Jijun Cheng","Muhammad Dawood","Srijay Deshpande","Raja Muhammad Saad Bashir","Adam Shephard","Pedro Costa","João D. Nunes","Aurélio Campilho","Jaime S. Cardoso","Hrishikesh P S","Densen Puthussery","Devika R G","Jiji C V","Ye Zhang","Zijie Fang","Zhifan Lin","Yongbing Zhang","Chunhui Lin","Liukun Zhang","Lijian Mao","Min Wu","Vi Thi-Tuong Vo","Soo-Hyung Kim","Taebum Lee","Satoshi Kondo","Satoshi Kasai","Pranay Dumbhare","Vedant Phuse","Yash Dubey","Ankush Jamthikar","Trinh Thi Le Vuong","Jin Tae Kwak","Dorsa Ziaei","Hyun Jung","Tianyi Miao","David Snead","Shan E Ahmed Raza","Fayyaz Minhas","Nasir M. Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2303.06274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06871v2","updated":"2023-03-14T14:49:08Z","published":"2023-03-13T05:42:58Z","title":"Physics-driven machine learning models coupling PyTorch and Firedrake","summary":"  Partial differential equations (PDEs) are central to describing and modelling\ncomplex physical systems that arise in many disciplines across science and\nengineering. However, in many realistic applications PDE modelling provides an\nincomplete description of the physics of interest. PDE-based machine learning\ntechniques are designed to address this limitation. In this approach, the PDE\nis used as an inductive bias enabling the coupled model to rely on fundamental\nphysical laws while requiring less training data. The deployment of\nhigh-performance simulations coupling PDEs and machine learning to complex\nproblems necessitates the composition of capabilities provided by machine\nlearning and PDE-based frameworks. We present a simple yet effective coupling\nbetween the machine learning framework PyTorch and the PDE system Firedrake\nthat provides researchers, engineers and domain specialists with a high\nproductive way of specifying coupled models while only requiring trivial\nchanges to existing code.\n","authors":["Nacime Bouziani","David A. Ham"],"pdf_url":"https://arxiv.org/pdf/2303.06871v2.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2207.14653v2","updated":"2023-03-14T14:47:08Z","published":"2022-07-29T12:57:50Z","title":"Ensemble forecasts in reproducing kernel Hilbert space family: dynamical\n  systems in Wonderland","summary":"  A methodological framework for ensemble-based estimation and simulation of\nhigh dimensional dynamical systems such as the oceanic or atmospheric flows is\nproposed. To that end, the dynamical system is embedded in a family of\nreproducing kernel Hilbert spaces with kernel functions driven by the dynamics.\nThis family is nicknamed Wonderland for its appealing properties. In Wonderland\nthe Koopman and Perron-Frobenius operators are unitary and uniformly\ncontinuous. This property warrants they can be expressed in exponential series\nof diagonalizable bounded infinitesimal generators. Access to Lyapunov\nexponents and to exact ensemble based expressions of the tangent linear\ndynamics are directly available as well. Wonderland enables us the devise of\nstrikingly simple ensemble data assimilation methods for trajectory\nreconstructions in terms of constant-in-time linear combinations of trajectory\nsamples. Such an embarrassingly simple strategy is made possible through a\nfully justified superposition principle ensuing from several fundamental\ntheorems.\n","authors":["Benjamin Dufée","Bérenger Hug","Etienne Memin","Gilles Tissot"],"pdf_url":"https://arxiv.org/pdf/2207.14653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07940v1","updated":"2023-03-14T14:25:54Z","published":"2023-03-14T14:25:54Z","title":"On the Connection between Concept Drift and Uncertainty in Industrial\n  Artificial Intelligence","summary":"  AI-based digital twins are at the leading edge of the Industry 4.0\nrevolution, which are technologically empowered by the Internet of Things and\nreal-time data analysis. Information collected from industrial assets is\nproduced in a continuous fashion, yielding data streams that must be processed\nunder stringent timing constraints. Such data streams are usually subject to\nnon-stationary phenomena, causing that the data distribution of the streams may\nchange, and thus the knowledge captured by models used for data analysis may\nbecome obsolete (leading to the so-called concept drift effect). The early\ndetection of the change (drift) is crucial for updating the model's knowledge,\nwhich is challenging especially in scenarios where the ground truth associated\nto the stream data is not readily available. Among many other techniques, the\nestimation of the model's confidence has been timidly suggested in a few\nstudies as a criterion for detecting drifts in unsupervised settings. The goal\nof this manuscript is to confirm and expose solidly the connection between the\nmodel's confidence in its output and the presence of a concept drift,\nshowcasing it experimentally and advocating for a major consideration of\nuncertainty estimation in comparative studies to be reported in the future.\n","authors":["Jesus L. Lobo","Ibai Laña","Eneko Osaba","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2303.07940v1.pdf","comment":"2 pages, 1 figure, 2023 IEEE Conference on Artificial Intelligence\n  (IEEE CAI)"},{"id":"http://arxiv.org/abs/2303.07925v1","updated":"2023-03-14T14:10:37Z","published":"2023-03-14T14:10:37Z","title":"Understanding Model Complexity for temporal tabular and multi-variate\n  time series, case study with Numerai data science tournament","summary":"  In this paper, we explore the use of different feature engineering and\ndimensionality reduction methods in multi-variate time-series modelling. Using\na feature-target cross correlation time series dataset created from Numerai\ntournament, we demonstrate under over-parameterised regime, both the\nperformance and predictions from different feature engineering methods converge\nto the same equilibrium, which can be characterised by the reproducing kernel\nHilbert space. We suggest a new Ensemble method, which combines different\nrandom non-linear transforms followed by ridge regression for modelling high\ndimensional time-series. Compared to some commonly used deep learning models\nfor sequence modelling, such as LSTM and transformers, our method is more\nrobust (lower model variance over different random seeds and less sensitive to\nthe choice of architecture) and more efficient. An additional advantage of our\nmethod is model simplicity as there is no need to use sophisticated deep\nlearning frameworks such as PyTorch. The learned feature rankings are then\napplied to the temporal tabular prediction problem in the Numerai tournament,\nand the predictive power of feature rankings obtained from our method is better\nthan the baseline prediction model based on moving averages\n","authors":["Thomas Wong","Prof. Mauricio Barahona"],"pdf_url":"https://arxiv.org/pdf/2303.07925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07924v1","updated":"2023-03-14T14:10:16Z","published":"2023-03-14T14:10:16Z","title":"Improving Accented Speech Recognition with Multi-Domain Training","summary":"  Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.\n","authors":["Lucas Maison","Yannick Estève"],"pdf_url":"https://arxiv.org/pdf/2303.07924v1.pdf","comment":"5 pages, 2 figures. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07917v1","updated":"2023-03-14T14:00:32Z","published":"2023-03-14T14:00:32Z","title":"Reachability Analysis of Neural Networks with Uncertain Parameters","summary":"  The literature on reachability analysis methods for neural networks currently\nonly focuses on uncertainties on the network's inputs. In this paper, we\nintroduce two new approaches for the reachability analysis of neural networks\nwith additional uncertainties on their internal parameters (weight matrices and\nbias vectors of each layer), which may open the field of formal methods on\nneural networks to new topics, such as safe training or network repair. The\nfirst and main method that we propose relies on existing reachability analysis\napproach based on mixed monotonicity (initially introduced for dynamical\nsystems). The second proposed approach extends the ESIP (Error-based Symbolic\nInterval Propagation) approach which was first implemented in the verification\ntool Neurify, and first mentioned in the publication of the tool VeriNet.\nAlthough the ESIP approach has been shown to often outperform the\nmixed-monotonicity reachability analysis in the classical case with\nuncertainties only on the network's inputs, we show in this paper through\nnumerical simulations that the situation is greatly reversed (in terms of\nprecision, computation time, memory usage, and broader applicability) when\ndealing with uncertainties on the weights and biases.\n","authors":["Pierre-Jean Meyer"],"pdf_url":"https://arxiv.org/pdf/2303.07917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07909v1","updated":"2023-03-14T13:49:54Z","published":"2023-03-14T13:49:54Z","title":"Text-to-image Diffusion Model in Generative AI: A Survey","summary":"  This survey reviews text-to-image diffusion models in the context that\ndiffusion models have emerged to be popular for a wide range of generative\ntasks. As a self-contained work, this survey starts with a brief introduction\nof how a basic diffusion model works for image synthesis, followed by how\ncondition or guidance improves learning. Based on that, we present a review of\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\ntext-to-image. We further summarize applications beyond text-to-image\ngeneration: text-guided creative generation and text-guided image editing.\nBeyond the progress made so far, we discuss existing challenges and promising\nfuture directions.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Mengchun Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.07909v1.pdf","comment":"First survey on the recent progress of text-to-image generation based\n  on the diffusion model (under progress)"},{"id":"http://arxiv.org/abs/2303.06965v2","updated":"2023-03-14T13:47:14Z","published":"2023-03-13T10:06:41Z","title":"Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction\n  Pretraining and Conditional Molecule Generation","summary":"  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. In recent years, there has been a growing need for\na large-scale deep-learning framework that can efficiently capture the basic\nrules of chemical reactions. In this paper, we have proposed a unified\nframework that addresses both the reaction representation learning and molecule\ngeneration tasks, which allows for a more holistic approach. Inspired by the\norganic chemistry mechanism, we develop a novel pretraining framework that\nenables us to incorporate inductive biases into the model. Our framework\nachieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, this framework can be applied to reaction-based\ngenerative models, overcoming the limitations of current molecule generation\nmodels that rely on a small number of reaction templates. In the extensive\nexperiments, our model generates synthesizable drug-like structures of high\nquality. Overall, our work presents a significant step toward a large-scale\ndeep-learning framework for a variety of reaction-based applications.\n","authors":["Bo Qiang","Yiran Zhou","Yuheng Ding","Ningfeng Liu","Song Song","Liangren Zhang","Bo Huang","Zhenming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07900v1","updated":"2023-03-14T13:41:28Z","published":"2023-03-14T13:41:28Z","title":"Generalised Scale-Space Properties for Probabilistic Diffusion Models","summary":"  Probabilistic diffusion models enjoy increasing popularity in the deep\nlearning community. They generate convincing samples from a learned\ndistribution of input images with a wide field of practical applications.\nOriginally, these approaches were motivated from drift-diffusion processes, but\nthese origins find less attention in recent, practice-oriented publications.\n  We investigate probabilistic diffusion models from the viewpoint of\nscale-space research and show that they fulfil generalised scale-space\nproperties on evolving probability distributions. Moreover, we discuss\nsimilarities and differences between interpretations of the physical core\nconcept of drift-diffusion in the deep learning and model-based world. To this\nend, we examine relations of probabilistic diffusion to osmosis filters.\n","authors":["Pascal Peter"],"pdf_url":"https://arxiv.org/pdf/2303.07900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07127v2","updated":"2023-03-14T13:26:03Z","published":"2023-03-13T13:58:03Z","title":"Improving physics-informed neural networks with meta-learned\n  optimization","summary":"  We show that the error achievable using physics-informed neural networks for\nsolving systems of differential equations can be substantially reduced when\nthese networks are trained using meta-learned optimization methods rather than\nto using fixed, hand-crafted optimizers as traditionally done. We choose a\nlearnable optimization method based on a shallow multi-layer perceptron that is\nmeta-trained for specific classes of differential equations. We illustrate\nmeta-trained optimizers for several equations of practical relevance in\nmathematical physics, including the linear advection equation, Poisson's\nequation, the Korteweg--de Vries equation and Burgers' equation. We also\nillustrate that meta-learned optimizers exhibit transfer learning abilities, in\nthat a meta-trained optimizer on one differential equation can also be\nsuccessfully deployed on another differential equation.\n","authors":["Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2303.07127v2.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2103.03076v2","updated":"2023-03-14T13:05:12Z","published":"2021-03-04T14:57:53Z","title":"Dynamic Efficient Adversarial Training Guided by Gradient Magnitude","summary":"  Adversarial training is an effective but time-consuming way to train robust\ndeep neural networks that can withstand strong adversarial attacks. As a\nresponse to its inefficiency, we propose Dynamic Efficient Adversarial Training\n(DEAT), which gradually increases the adversarial iteration during training. We\ndemonstrate that the gradient's magnitude correlates with the curvature of the\ntrained model's loss landscape, allowing it to reflect the effect of\nadversarial training. Therefore, based on the magnitude of the gradient, we\npropose a general acceleration strategy, M+ acceleration, which enables an\nautomatic and highly effective method of adjusting the training procedure. M+\nacceleration is computationally efficient and easy to implement. It is suited\nfor DEAT and compatible with the majority of existing adversarial training\ntechniques. Extensive experiments have been done on CIFAR-10 and ImageNet\ndatasets with various training environments. The results show that the proposed\nM+ acceleration significantly improves the training efficiency of existing\nadversarial training methods while achieving similar robustness performance.\nThis demonstrates that the strategy is highly adaptive and offers a valuable\nsolution for automatic adversarial training.\n","authors":["Fu Wang","Yanghao Zhang","Yanbin Zheng","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2103.03076v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07864v1","updated":"2023-03-14T12:55:42Z","published":"2023-03-14T12:55:42Z","title":"DualMix: Unleashing the Potential of Data Augmentation for Online\n  Class-Incremental Learning","summary":"  Online Class-Incremental (OCI) learning has sparked new approaches to expand\nthe previously trained model knowledge from sequentially arriving data streams\nwith new classes. Unfortunately, OCI learning can suffer from catastrophic\nforgetting (CF) as the decision boundaries for old classes can become\ninaccurate when perturbated by new ones. Existing literature have applied the\ndata augmentation (DA) to alleviate the model forgetting, while the role of DA\nin OCI has not been well understood so far. In this paper, we theoretically\nshow that augmented samples with lower correlation to the original data are\nmore effective in preventing forgetting. However, aggressive augmentation may\nalso reduce the consistency between data and corresponding labels, which\nmotivates us to exploit proper DA to boost the OCI performance and prevent the\nCF problem. We propose the Enhanced Mixup (EnMix) method that mixes the\naugmented samples and their labels simultaneously, which is shown to enhance\nthe sample diversity while maintaining strong consistency with corresponding\nlabels. Further, to solve the class imbalance problem, we design an Adaptive\nMixup (AdpMix) method to calibrate the decision boundaries by mixing samples\nfrom both old and new classes and dynamically adjusting the label mixing ratio.\nOur approach is demonstrated to be effective on several benchmark datasets\nthrough extensive experiments, and it is shown to be compatible with other\nreplay-based techniques.\n","authors":["Yunfeng Fan","Wenchao Xu","Haozhao Wang","Jiaqi Zhu","Junxiao Wang","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2303.07864v1.pdf","comment":"10 pages, 7 figures and 3 tables"},{"id":"http://arxiv.org/abs/2204.10416v2","updated":"2023-03-14T12:49:22Z","published":"2022-04-21T21:43:23Z","title":"CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile\n  Motion Sensors","summary":"  In cities worldwide, cars cause health and traffic problems whichcould be\npartly mitigated through an increased modal share of bicycles. Many people,\nhowever, avoid cycling due to a lack of perceived safety. For city planners,\naddressing this is hard as they lack insights intowhere cyclists feel safe and\nwhere they do not. To gain such insights,we have in previous work proposed the\ncrowdsourcing platform SimRa,which allows cyclists to record their rides and\nreport near miss incidentsvia a smartphone app. In this paper, we present\nCycleSense, a combination of signal pro-cessing and Machine Learning\ntechniques, which partially automatesthe detection of near miss incidents, thus\nmaking the reporting of nearmiss incidents easier. Using the SimRa data set, we\nevaluate CycleSenseby comparing it to a baseline method used by SimRa and show\nthat itsignificantly improves incident detection.\n","authors":["Ahmet-Serdar Karakaya","Thomas Ritter","Felix Biessmann","David Bermbach"],"pdf_url":"https://arxiv.org/pdf/2204.10416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07853v1","updated":"2023-03-14T12:46:52Z","published":"2023-03-14T12:46:52Z","title":"BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised\n  Semantic Segmentation of Medical Images","summary":"  Weakly Supervised Semantic Segmentation (WSSS) with only image-level\nsupervision is a promising approach to deal with the need for Segmentation\nnetworks, especially for generating a large number of pixel-wise masks in a\ngiven dataset. However, most state-of-the-art image-level WSSS techniques lack\nan understanding of the geometric features embedded in the images since the\nnetwork cannot derive any object boundary information from just image-level\nlabels. We define a boundary here as the line separating an object and its\nbackground, or two different objects. To address this drawback, we propose our\nnovel BoundaryCAM framework, which deploys state-of-the-art class activation\nmaps combined with various post-processing techniques in order to achieve\nfine-grained higher-accuracy segmentation masks. To achieve this, we\ninvestigate a state-of-the-art unsupervised semantic segmentation network that\ncan be used to construct a boundary map, which enables BoundaryCAM to predict\nobject locations with sharper boundaries. By applying our method to WSSS\npredictions, we were able to achieve up to 10% improvements even to the benefit\nof the current state-of-the-art WSSS methods for medical imaging. The framework\nis open-source and accessible online at\nhttps://github.com/bharathprabakaran/BoundaryCAM.\n","authors":["Bharath Srinivas Prabakaran","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07852v1","updated":"2023-03-14T12:46:48Z","published":"2023-03-14T12:46:48Z","title":"FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network\n  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features","summary":"  Ultrasound imaging is one of the most prominent technologies to evaluate the\ngrowth, progression, and overall health of a fetus during its gestation.\nHowever, the interpretation of the data obtained from such studies is best left\nto expert physicians and technicians who are trained and well-versed in\nanalyzing such images. To improve the clinical workflow and potentially develop\nan at-home ultrasound-based fetal monitoring platform, we present a novel fetus\nphantom ultrasound dataset, FPUS23, which can be used to identify (1) the\ncorrect diagnostic planes for estimating fetal biometric values, (2) fetus\norientation, (3) their anatomical features, and (4) bounding boxes of the fetus\nphantom anatomies at 23 weeks gestation. The entire dataset is composed of\n15,728 images, which are used to train four different Deep Neural Network\nmodels, built upon a ResNet34 backbone, for detecting aforementioned fetus\nfeatures and use-cases. We have also evaluated the models trained using our\nFPUS23 dataset, to show that the information learned by these models can be\nused to substantially increase the accuracy on real-world ultrasound fetus\ndatasets. We make the FPUS23 dataset and the pre-trained models publicly\naccessible at https://github.com/bharathprabakaran/FPUS23, which will further\nfacilitate future research on fetal ultrasound imaging and analysis.\n","authors":["Bharath Srinivas Prabakaran","Paul Hamelmann","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.03540v2","updated":"2023-03-14T12:38:35Z","published":"2021-10-07T15:01:33Z","title":"A Broad Ensemble Learning System for Drifting Stream Classification","summary":"  In a data stream environment, classification models must handle concept drift\nefficiently and effectively. Ensemble methods are widely used for this purpose;\nhowever, the ones available in the literature either use a large data chunk to\nupdate the model or learn the data one by one. In the former, the model may\nmiss the changes in the data distribution, and in the latter, the model may\nsuffer from inefficiency and instability. To address these issues, we introduce\na novel ensemble approach based on the Broad Learning System (BLS), where mini\nchunks are used at each update. BLS is an effective lightweight neural\narchitecture recently developed for incremental learning. Although it is fast,\nit requires huge data chunks for effective updates, and is unable to handle\ndynamic changes observed in data streams. Our proposed approach named Broad\nEnsemble Learning System (BELS) uses a novel updating method that significantly\nimproves best-in-class model accuracy. It employs an ensemble of output layers\nto address the limitations of BLS and handle drifts. Our model tracks the\nchanges in the accuracy of the ensemble components and react to these changes.\nWe present the mathematical derivation of BELS, perform comprehensive\nexperiments with 20 datasets that demonstrate the adaptability of our model to\nvarious drift types, and provide hyperparameter and ablation analysis of our\nproposed model. Our experiments show that the proposed approach outperforms\nnine state-of-the-art baselines and supplies an overall improvement of 13.28%\nin terms of average prequential accuracy.\n","authors":["Sepehr Bakhshi","Pouya Ghahramanian","Hamed Bonab","Fazli Can"],"pdf_url":"https://arxiv.org/pdf/2110.03540v2.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2303.07847v1","updated":"2023-03-14T12:37:22Z","published":"2023-03-14T12:37:22Z","title":"Transfer Learning for Real-time Deployment of a Screening Tool for\n  Depression Detection Using Actigraphy","summary":"  Automated depression screening and diagnosis is a highly relevant problem\ntoday. There are a number of limitations of the traditional depression\ndetection methods, namely, high dependence on clinicians and biased\nself-reporting. In recent years, research has suggested strong potential in\nmachine learning (ML) based methods that make use of the user's passive data\ncollected via wearable devices. However, ML is data hungry. Especially in the\nhealthcare domain primary data collection is challenging. In this work, we\npresent an approach based on transfer learning, from a model trained on a\nsecondary dataset, for the real time deployment of the depression screening\ntool based on the actigraphy data of users. This approach enables machine\nlearning modelling even with limited primary data samples. A modified version\nof leave one out cross validation approach performed on the primary set\nresulted in mean accuracy of 0.96, where in each iteration one subject's data\nfrom the primary set was set aside for testing.\n","authors":["Rajanikant Ghate","Nayan Kalnad","Rahee Walambe","Ketan Kotecha"],"pdf_url":"https://arxiv.org/pdf/2303.07847v1.pdf","comment":"5 pages, 4 figures, conference, to be published in UKSIM23"},{"id":"http://arxiv.org/abs/2303.07846v1","updated":"2023-03-14T12:36:01Z","published":"2023-03-14T12:36:01Z","title":"Sample-efficient Adversarial Imitation Learning","summary":"  Imitation learning, in which learning is performed by demonstration, has been\nstudied and advanced for sequential decision-making tasks in which a reward\nfunction is not predefined. However, imitation learning methods still require\nnumerous expert demonstration samples to successfully imitate an expert's\nbehavior. To improve sample efficiency, we utilize self-supervised\nrepresentation learning, which can generate vast training signals from the\ngiven data. In this study, we propose a self-supervised representation-based\nadversarial imitation learning method to learn state and action representations\nthat are robust to diverse distortions and temporally predictive, on non-image\ncontrol tasks. In particular, in comparison with existing self-supervised\nlearning methods for tabular data, we propose a different corruption method for\nstate and action representations that is robust to diverse distortions. We\ntheoretically and empirically observe that making an informative feature\nmanifold with less sample complexity significantly improves the performance of\nimitation learning. The proposed method shows a 39% relative improvement over\nexisting adversarial imitation learning methods on MuJoCo in a setting limited\nto 100 expert state-action pairs. Moreover, we conduct comprehensive ablations\nand additional experiments using demonstrations with varying optimality to\nprovide insights into a range of factors.\n","authors":["Dahuin Jung","Hyungyu Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.07846v1.pdf","comment":"A preliminary version of this manuscript was presented at Deep RL\n  Workshop, NeurIPS 2022"},{"id":"http://arxiv.org/abs/2301.00656v2","updated":"2023-03-14T12:23:33Z","published":"2022-12-12T05:55:07Z","title":"TriNet: stabilizing self-supervised learning from complete or slow\n  collapse on ASR","summary":"  Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n","authors":["Lixin Cao","Jun Wang","Ben Yang","Dan Su","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2301.00656v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.02314v2","updated":"2023-03-14T12:12:00Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  Most computer vision models are developed based on either convolutional\nneural network (CNN) or transformer, while the former (latter) method captures\nlocal (global) features. To relieve model performance limitations due to the\nlack of global (local) features, we develop a novel classification network CECT\nby controllable ensemble CNN and transformer. CECT is composed of a\nconvolutional encoder block, a transposed-convolutional decoder block, and a\ntransformer classification block. Different from conventional CNN- or\ntransformer-based methods, our CECT can capture features at both multi-local\nand global scales. Besides, the contribution of local features at different\nscales can be controlled with the proposed ensemble coefficients. We evaluate\nCECT on two public COVID-19 datasets and it outperforms existing\nstate-of-the-art methods on all evaluation metrics. With remarkable feature\ncapture ability, we believe CECT can be extended to other medical image\nclassification scenarios as a diagnosis assistant.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.09100v2","updated":"2023-03-14T12:08:11Z","published":"2022-12-18T14:56:22Z","title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images","summary":"  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset will be made public with the code and\nmodels on the project website https://abdullahamdi.com/sparf/ .\n","authors":["Abdullah Hamdi","Bernard Ghanem","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2212.09100v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2109.09500v2","updated":"2023-03-14T11:57:10Z","published":"2021-09-20T12:53:01Z","title":"Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale\n  Confirmatory Item Factor Analysis","summary":"  We investigate novel parameter estimation and goodness-of-fit (GOF)\nassessment methods for large-scale confirmatory item factor analysis (IFA) with\nmany respondents, items, and latent factors. For parameter estimation, we\nextend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to\nthe confirmatory setting by showing how to handle constraints on loadings and\nfactor correlations. For GOF assessment, we explore simulation-based tests and\nindices that extend the classifier two-sample test (C2ST), a method that tests\nwhether a deep neural network can distinguish between observed data and\nsynthetic data sampled from a fitted IFA model. Proposed extensions include a\ntest of approximate fit wherein the user specifies what percentage of observed\nand synthetic data should be distinguishable as well as a relative fit index\n(RFI) that is similar in spirit to the RFIs used in structural equation\nmodeling. Via simulation studies, we show that: (1) the confirmatory extension\nof Urban and Bauer's (2021) algorithm obtains comparable estimates to a\nstate-of-the-art estimation procedure in less time; (2) C2ST-based GOF tests\ncontrol the empirical type I error rate and detect when the latent\ndimensionality is misspecified; and (3) the sampling distribution of the\nC2ST-based RFI depends on the sample size.\n","authors":["Christopher J. Urban","Daniel J. Bauer"],"pdf_url":"https://arxiv.org/pdf/2109.09500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07814v1","updated":"2023-03-14T11:44:58Z","published":"2023-03-14T11:44:58Z","title":"Kinematic Data-Based Action Segmentation for Surgical Applications","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nIn the context of surgical procedures, action segmentation is critical for\nworkflow analysis algorithms. This work presents two contributions related to\naction segmentation on kinematic data. Firstly, we introduce two multi-stage\narchitectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Horizontal-Flip, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieve state-of-the-art\nperformance on all benchmark datasets and establish a strong baseline for the\nBRS dataset.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2110.01303v3","updated":"2023-03-14T11:40:35Z","published":"2021-10-04T10:19:53Z","title":"Incremental Class Learning using Variational Autoencoders with\n  Similarity Learning","summary":"  Catastrophic forgetting in neural networks during incremental learning\nremains a challenging problem. Previous research investigated catastrophic\nforgetting in fully connected networks, with some earlier work exploring\nactivation functions and learning algorithms. Applications of neural networks\nhave been extended to include similarity learning. Understanding how similarity\nlearning loss functions would be affected by catastrophic forgetting is of\nsignificant interest. Our research investigates catastrophic forgetting for\nfour well-known similarity-based loss functions during incremental class\nlearning. The loss functions are Angular, Contrastive, Center, and Triplet\nloss. Our results show that the catastrophic forgetting rate differs across\nloss functions on multiple datasets. The Angular loss was least affected,\nfollowed by Contrastive, Triplet loss, and Center loss with good mining\ntechniques. We implemented three existing incremental learning techniques,\niCaRL, EWC, and EBLL. We further proposed a novel technique using Variational\nAutoencoders (VAEs) to generate representation as exemplars passed through the\nnetwork's intermediate layers. Our method outperformed three existing\nstate-of-the-art techniques. We show that one does not require stored images\n(exemplars) for incremental learning with similarity learning. The generated\nrepresentations from VAEs help preserve regions of the embedding space used by\nprior knowledge so that new knowledge does not ``overwrite'' it.\n","authors":["Jiahao Huo","Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2110.01303v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07811v1","updated":"2023-03-14T11:31:45Z","published":"2023-03-14T11:31:45Z","title":"ICICLE: Interpretable Class Incremental Continual Learning","summary":"  Continual learning enables incremental learning of new tasks without\nforgetting those previously learned, resulting in positive knowledge transfer\nthat can enhance performance on both new and old tasks. However, continual\nlearning poses new challenges for interpretability, as the rationale behind\nmodel predictions may change over time, leading to interpretability concept\ndrift. We address this problem by proposing Interpretable Class-InCremental\nLEarning (ICICLE), an exemplar-free approach that adopts a prototypical\npart-based approach. It consists of three crucial novelties: interpretability\nregularization that distills previously learned concepts while preserving\nuser-friendly positive reasoning; proximity-based prototype initialization\nstrategy dedicated to the fine-grained setting; and task-recency bias\ncompensation devoted to prototypical parts. Our experimental results\ndemonstrate that ICICLE reduces the interpretability concept drift and\noutperforms the existing exemplar-free methods of common class-incremental\nlearning when applied to concept-based models. We make the code available.\n","authors":["Dawid Rymarczyk","Joost van de Weijer","Bartosz Zieliński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2303.07811v1.pdf","comment":"Under review, code will be shared after the acceptance"},{"id":"http://arxiv.org/abs/2301.13060v3","updated":"2023-03-14T11:30:48Z","published":"2023-01-30T17:02:23Z","title":"Zero-One Laws of Graph Neural Networks","summary":"  Graph neural networks (GNNs) are de facto standard deep learning\narchitectures for machine learning on graphs. This has led to a large body of\nwork analyzing the capabilities and limitations of these models, particularly\npertaining to their representation and extrapolation capacity. We offer a novel\ntheoretical perspective on the representation and extrapolation capacity of\nGNNs, by answering the question: how do GNNs behave as the number of graph\nnodes become very large? Under mild assumptions, we show that when we draw\ngraphs of increasing size from the Erd\\H{o}s-R\\'enyi model, the probability\nthat such graphs are mapped to a particular output by a class of GNN\nclassifiers tends to either zero or to one. This class includes the popular\ngraph convolutional network architecture. The result establishes 'zero-one\nlaws' for these GNNs, and analogously to other convergence laws, entails\ntheoretical limitations on their capacity. We empirically verify our results,\nobserving that the theoretical asymptotic limits are evident already on\nrelatively small graphs.\n","authors":["Sam Adam-Day","Theodor Mihai Iliant","İsmail İlkan Ceylan"],"pdf_url":"https://arxiv.org/pdf/2301.13060v3.pdf","comment":"8 pages + references + 9 pages appendices, 2 figures"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2303.07778v1","updated":"2023-03-14T10:39:58Z","published":"2023-03-14T10:39:58Z","title":"GANN: Graph Alignment Neural Network for Semi-Supervised Learning","summary":"  Graph neural networks (GNNs) have been widely investigated in the field of\nsemi-supervised graph machine learning. Most methods fail to exploit adequate\ngraph information when labeled data is limited, leading to the problem of\noversmoothing. To overcome this issue, we propose the Graph Alignment Neural\nNetwork (GANN), a simple and effective graph neural architecture. A unique\nlearning algorithm with three alignment rules is proposed to thoroughly explore\nhidden information for insufficient labels. Firstly, to better investigate\nattribute specifics, we suggest the feature alignment rule to align the inner\nproduct of both the attribute and embedding matrices. Secondly, to properly\nutilize the higher-order neighbor information, we propose the cluster center\nalignment rule, which involves aligning the inner product of the cluster center\nmatrix with the unit matrix. Finally, to get reliable prediction results with\nfew labels, we establish the minimum entropy alignment rule by lining up the\nprediction probability matrix with its sharpened result. Extensive studies on\ngraph benchmark datasets demonstrate that GANN can achieve considerable\nbenefits in semi-supervised node classification and outperform state-of-the-art\ncompetitors.\n","authors":["Linxuan Song","Wenxuan Tu","Sihang Zhou","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.07778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05735v2","updated":"2023-03-14T10:38:14Z","published":"2023-03-10T06:44:49Z","title":"Hardware Acceleration of Neural Graphics","summary":"  Rendering and inverse-rendering algorithms that drive conventional computer\ngraphics have recently been superseded by neural representations (NR). NRs have\nrecently been used to learn the geometric and the material properties of the\nscenes and use the information to synthesize photorealistic imagery, thereby\npromising a replacement for traditional rendering algorithms with scalable\nquality and predictable performance. In this work we ask the question: Does\nneural graphics (NG) need hardware support? We studied representative NG\napplications showing that, if we want to render 4k res. at 60FPS there is a gap\nof 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,\nthere is an even larger gap of 2-4 OOM between the desired performance and the\nrequired system power. We identify that the input encoding and the MLP kernels\nare the performance bottlenecks, consuming 72%,60% and 59% of application time\nfor multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,\nrespectively. We propose a NG processing cluster, a scalable and flexible\nhardware architecture that directly accelerates the input encoding and MLP\nkernels through dedicated engines and supports a wide range of NG applications.\nWe also accelerate the rest of the kernels by fusing them together in Vulkan,\nwhich leads to 9.94X kernel-level performance improvement compared to un-fused\nimplementation of the pre-processing and the post-processing kernels. Our\nresults show that, NGPC gives up to 58X end-to-end application-level\nperformance improvement, for multi res. hashgrid encoding on average across the\nfour NG applications, the performance benefits are 12X,20X,33X and 39X for the\nscaling factor of 8,16,32 and 64, respectively. Our results show that with\nmulti res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS\nfor NeRF and 8k res. at 120FPS for all our other NG applications.\n","authors":["Muhammad Husnain Mubarik","Ramakrishna Kanungo","Tobias Zirr","Rakesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.05735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15567v2","updated":"2023-03-14T10:37:52Z","published":"2022-05-31T06:57:56Z","title":"Few-Shot Unlearning by Model Inversion","summary":"  We consider a practical scenario of machine unlearning to erase a target\ndataset, which causes unexpected behavior from the trained model. The target\ndataset is often assumed to be fully identifiable in a standard unlearning\nscenario. Such a flawless identification, however, is almost impossible if the\ntraining dataset is inaccessible at the time of unlearning. Unlike previous\napproaches requiring a complete set of targets, we consider few-shot unlearning\nscenario when only a few samples of target data are available. To this end, we\nformulate the few-shot unlearning problem specifying intentions behind the\nunlearning request (e.g., purely unlearning, mislabel correction, privacy\nprotection), and we devise a straightforward framework that (i) retrieves a\nproxy of the training data via model inversion fully exploiting information\navailable in the context of unlearning; (ii) adjusts the proxy according to the\nunlearning intention; and (iii) updates the model with the adjusted proxy. We\ndemonstrate that our method using only a subset of target data can outperform\nthe state-of-the-art unlearning methods even with a complete indication of\ntarget data.\n","authors":["Youngsik Yoon","Jinhwan Nam","Hyojeong Yun","Jaeho Lee","Dongwoo Kim","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2205.15567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01372v2","updated":"2023-03-14T10:34:55Z","published":"2023-03-02T15:58:09Z","title":"High-dimensional analysis of double descent for linear regression with\n  random projections","summary":"  We consider linear regression problems with a varying number of random\nprojections, where we provably exhibit a double descent curve for a fixed\nprediction problem, with a high-dimensional analysis based on random matrix\ntheory. We first consider the ridge regression estimator and review earlier\nresults using classical notions from non-parametric statistics, namely degrees\nof freedom, also known as effective dimensionality. We then compute asymptotic\nequivalents of the generalization performance (in terms of squared bias and\nvariance) of the minimum norm least-squares fit with random projections,\nproviding simple expressions for the double descent phenomenon.\n","authors":["Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2303.01372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2110.03501v3","updated":"2023-03-14T10:30:51Z","published":"2021-10-07T14:37:06Z","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!","summary":"  Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.\n","authors":["Kimia Noorbakhsh","Modar Sulaiman","Mahdi Sharifi","Kallol Roy","Pooyan Jamshidi"],"pdf_url":"https://arxiv.org/pdf/2110.03501v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.07774v1","updated":"2023-03-14T10:25:56Z","published":"2023-03-14T10:25:56Z","title":"Testing Causality for High Dimensional Data","summary":"  Determining causal relationship between high dimensional observations are\namong the most important tasks in scientific discoveries. In this paper, we\nrevisited the \\emph{linear trace method}, a technique proposed\nin~\\citep{janzing2009telling,zscheischler2011testing} to infer the causal\ndirection between two random variables of high dimensions. We strengthen the\nexisting results significantly by providing an improved tail analysis in\naddition to extending the results to nonlinear trace functionals with sharper\nconfidence bounds under certain distributional assumptions. We obtain our\nresults by interpreting the trace estimator in the causal regime as a function\nover random orthogonal matrices, where the concentration of Lipschitz functions\nover such space could be applied. We additionally propose a novel\nridge-regularized variant of the estimator in \\cite{zscheischler2011testing},\nand give provable bounds relating the ridge-estimated terms to their\nground-truth counterparts. We support our theoretical results with encouraging\nexperiments on synthetic datasets, more prominently, under high-dimension low\nsample size regime.\n","authors":["Arun Jambulapati","Hilaf Hasson","Youngsuk Park","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01717v3","updated":"2023-03-14T10:21:56Z","published":"2022-11-03T11:13:02Z","title":"Learning Hypergraphs From Signals With Dual Smoothness Prior","summary":"  Hypergraph structure learning, which aims to learn the hypergraph structures\nfrom the observed signals to capture the intrinsic high-order relationships\namong the entities, becomes crucial when a hypergraph topology is not readily\navailable in the datasets. There are two challenges that lie at the heart of\nthis problem: 1) how to handle the huge search space of potential hyperedges,\nand 2) how to define meaningful criteria to measure the relationship between\nthe signals observed on nodes and the hypergraph structure. In this paper, for\nthe first challenge, we adopt the assumption that the ideal hypergraph\nstructure can be derived from a learnable graph structure that captures the\npairwise relations within signals. Further, we propose a hypergraph structure\nlearning framework HGSL with a novel dual smoothness prior that reveals a\nmapping between the observed node signals and the hypergraph structure, whereby\neach hyperedge corresponds to a subgraph with both node signal smoothness and\nedge signal smoothness in the learnable graph structure. Finally, we conduct\nextensive experiments to evaluate HGSL on both synthetic and real world\ndatasets. Experiments show that HGSL can efficiently infer meaningful\nhypergraph topologies from observed signals.\n","authors":["Bohan Tang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2211.01717v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07768v1","updated":"2023-03-14T10:18:31Z","published":"2023-03-14T10:18:31Z","title":"DBSCAN of Multi-Slice Clustering for three-order Tensor","summary":"  Several methods for triclustering three-dimensional data require the cluster\nsize or the number of clusters in each dimension to be specified. To address\nthis issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal\nslices that lie in a low dimensional subspace for a rank-one tensor dataset in\norder to find a cluster based on the threshold similarity. We propose an\nextension algorithm called MSC-DBSCAN to extract the different clusters of\nslices that lie in the different subspaces from the data if the dataset is a\nsum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC\nalgorithm and can find the same solution for rank-one tensor data as MSC.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.07758v1","updated":"2023-03-14T10:03:37Z","published":"2023-03-14T10:03:37Z","title":"Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from\n  Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle\n  Detectors","summary":"  The global trends of urbanization and increased personal mobility force us to\nrethink the way we live and use urban space. The Traffic4cast competition\nseries tackles this problem in a data-driven way, advancing the latest methods\nin machine learning for modeling complex spatial systems over time. In this\nedition, our dynamic road graph data combine information from road maps,\n$10^{12}$ probe data points, and stationary vehicle detectors in three cities\nover the span of two years. While stationary vehicle detectors are the most\naccurate way to capture traffic volume, they are only available in few\nlocations. Traffic4cast 2022 explores models that have the ability to\ngeneralize loosely related temporal vertex data on just a few nodes to predict\ndynamic future traffic states on the edges of the entire road graph. In the\ncore challenge, participants are invited to predict the likelihoods of three\ncongestion classes derived from the speed levels in the GPS data for the entire\nroad graph in three cities 15 min into the future. We only provide vehicle\ncount data from spatially sparse stationary vehicle detectors in these three\ncities as model input for this task. The data are aggregated in 15 min time\nbins for one hour prior to the prediction time. For the extended challenge,\nparticipants are tasked to predict the average travel times on super-segments\n15 min into the future - super-segments are longer sequences of road segments\nin the graph. The competition results provide an important advance in the\nprediction of complex city-wide traffic states just from publicly available\nsparse vehicle data and without the need for large amounts of real-time\nfloating vehicle data.\n","authors":["Moritz Neun","Christian Eichenberger","Henry Martin","Markus Spanring","Rahul Siripurapu","Daniel Springer","Leyan Deng","Chenwang Wu","Defu Lian","Min Zhou","Martin Lumiste","Andrei Ilie","Xinhua Wu","Cheng Lyu","Qing-Long Lu","Vishal Mahajan","Yichao Lu","Jiezhang Li","Junjun Li","Yue-Jiao Gong","Florian Grötschla","Joël Mathys","Ye Wei","He Haitao","Hui Fang","Kevin Malm","Fei Tang","Michael Kopp","David Kreil","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2303.07758v1.pdf","comment":"Pre-print under review, submitted to Proceedings of Machine Learning\n  Research"},{"id":"http://arxiv.org/abs/2303.07757v1","updated":"2023-03-14T10:02:52Z","published":"2023-03-14T10:02:52Z","title":"Multiway clustering of 3-order tensor via affinity matrix","summary":"  We propose a new method of multiway clustering for 3-order tensors via\naffinity matrix (MCAM). Based on a notion of similarity between the tensor\nslices and the spread of information of each slice, our model builds an\naffinity/similarity matrix on which we apply advanced clustering methods. The\ncombination of all clusters of the three modes delivers the desired multiway\nclustering. Finally, MCAM achieves competitive results compared with other\nknown algorithms on synthetics and real datasets.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07742v1","updated":"2023-03-14T09:40:37Z","published":"2023-03-14T09:40:37Z","title":"ForDigitStress: A multi-modal stress dataset employing a digital job\n  interview scenario","summary":"  We present a multi-modal stress dataset that uses digital job interviews to\ninduce stress. The dataset provides multi-modal data of 40 participants\nincluding audio, video (motion capturing, facial recognition, eye tracking) as\nwell as physiological information (photoplethysmography, electrodermal\nactivity). In addition to that, the dataset contains time-continuous\nannotations for stress and occurred emotions (e.g. shame, anger, anxiety,\nsurprise). In order to establish a baseline, five different machine learning\nclassifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest,\nLong-Short-Term Memory Network) have been trained and evaluated on the proposed\ndataset for a binary stress classification task. The best-performing classifier\nachieved an accuracy of 88.3% and an F1-score of 87.5%.\n","authors":["Alexander Heimerl","Pooja Prajod","Silvan Mertes","Tobias Baur","Matthias Kraus","Ailin Liu","Helen Risack","Nicolas Rohleder","Elisabeth André","Linda Becker"],"pdf_url":"https://arxiv.org/pdf/2303.07742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07735v1","updated":"2023-03-14T09:30:52Z","published":"2023-03-14T09:30:52Z","title":"Can neural networks do arithmetic? A survey on the elementary numerical\n  skills of state-of-the-art deep learning models","summary":"  Creating learning models that can exhibit sophisticated reasoning skills is\none of the greatest challenges in deep learning research, and mathematics is\nrapidly becoming one of the target domains for assessing scientific progress in\nthis direction. In the past few years there has been an explosion of neural\nnetwork architectures, data sets, and benchmarks specifically designed to\ntackle mathematical problems, reporting notable success in disparate fields\nsuch as automated theorem proving, numerical integration, and discovery of new\nconjectures or matrix multiplication algorithms. However, despite these\nimpressive achievements it is still unclear whether deep learning models\npossess an elementary understanding of quantities and symbolic numbers. In this\nsurvey we critically examine the recent literature, concluding that even\nstate-of-the-art architectures often fall short when probed with relatively\nsimple tasks designed to test basic numerical and arithmetic knowledge.\n","authors":["Alberto Testolin"],"pdf_url":"https://arxiv.org/pdf/2303.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04488v2","updated":"2023-03-14T09:23:25Z","published":"2023-01-11T14:33:42Z","title":"WuYun: Exploring hierarchical skeleton-guided melody generation using\n  knowledge-enhanced deep learning","summary":"  Although deep learning has revolutionized music generation, existing methods\nfor structured melody generation follow an end-to-end left-to-right\nnote-by-note generative paradigm and treat each note equally. Here, we present\nWuYun, a knowledge-enhanced deep learning architecture for improving the\nstructure of generated melodies, which first generates the most structurally\nimportant notes to construct a melodic skeleton and subsequently infills it\nwith dynamically decorative notes into a full-fledged melody. Specifically, we\nuse music domain knowledge to extract melodic skeletons and employ sequence\nlearning to reconstruct them, which serve as additional knowledge to provide\nauxiliary guidance for the melody generation process. We demonstrate that WuYun\ncan generate melodies with better long-term structure and musicality and\noutperforms other state-of-the-art methods by 0.51 on average on all subjective\nevaluation metrics. Our study provides a multidisciplinary lens to design\nmelodic hierarchical structures and bridge the gap between data-driven and\nknowledge-based approaches for numerous music generation tasks.\n","authors":["Kejun Zhang","Xinda Wu","Tieyao Zhang","Zhijie Huang","Xu Tan","Qihao Liang","Songruoyao Wu","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2301.04488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07726v1","updated":"2023-03-14T09:15:51Z","published":"2023-03-14T09:15:51Z","title":"Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme\n  Conversion","summary":"  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.\n","authors":["Jungjun Kim","Changjin Han","Gyuhyeon Nam","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07726v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.04660v2","updated":"2023-03-14T09:12:11Z","published":"2023-03-08T15:27:29Z","title":"Neural Probabilistic Logic Programming in Discrete-Continuous Domains","summary":"  Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic\nbackground knowledge in the form of logic. It has been shown to aid learning in\nthe limited data regime and to facilitate inference on out-of-distribution\ndata. Probabilistic NeSy focuses on integrating neural networks with both logic\nand probability theory, which additionally allows learning under uncertainty. A\nmajor limitation of current probabilistic NeSy systems, such as DeepProbLog, is\ntheir restriction to finite probability distributions, i.e., discrete random\nvariables. In contrast, deep probabilistic programming (DPP) excels in\nmodelling and optimising continuous probability distributions. Hence, we\nintroduce DeepSeaProbLog, a neural probabilistic logic programming language\nthat incorporates DPP techniques into NeSy. Doing so results in the support of\ninference and learning of both discrete and continuous probability\ndistributions under logical constraints. Our main contributions are 1) the\nsemantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a\nproven asymptotically unbiased learning algorithm, and 3) a series of\nexperiments that illustrate the versatility of our approach.\n","authors":["Lennert De Smet","Pedro Zuidberg Dos Martires","Robin Manhaeve","Giuseppe Marra","Angelika Kimmig","Luc De Raedt"],"pdf_url":"https://arxiv.org/pdf/2303.04660v2.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2302.02601v2","updated":"2023-03-14T09:12:00Z","published":"2023-02-06T07:45:57Z","title":"Learning Representations of Bi-level Knowledge Graphs for Reasoning\n  beyond Link Prediction","summary":"  Knowledge graphs represent known facts using triplets. While existing\nknowledge graph embedding methods only consider the connections between\nentities, we propose considering the relationships between triplets. For\nexample, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is\n(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,\nAcademy_Awards). Given these two base-level triplets, we see that $T_1$ is a\nprerequisite for $T_2$. In this paper, we define a higher-level triplet to\nrepresent a relationship between triplets, e.g., $\\langle T_1$,\nPrerequisiteFor, $T_2\\rangle$ where PrerequisiteFor is a higher-level relation.\nWe define a bi-level knowledge graph that consists of the base-level and the\nhigher-level triplets. We also propose a data augmentation strategy based on\nthe random walks on the bi-level knowledge graph to augment plausible triplets.\nOur model called BiVE learns embeddings by taking into account the structures\nof the base-level and the higher-level triplets, with additional consideration\nof the augmented triplets. We propose two new tasks: triplet prediction and\nconditional link prediction. Given a triplet $T_1$ and a higher-level relation,\nthe triplet prediction predicts a triplet that is likely to be connected to\n$T_1$ by the higher-level relation, e.g., $\\langle T_1$, PrerequisiteFor,\n?$\\rangle$. The conditional link prediction predicts a missing entity in a\ntriplet conditioned on another triplet, e.g., $\\langle T_1$, PrerequisiteFor,\n(Avatar, Wins, ?)$\\rangle$. Experimental results show that BiVE significantly\noutperforms all other methods in the two new tasks and the typical base-level\nlink prediction in real-world bi-level knowledge graphs.\n","authors":["Chanyoung Chung","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2302.02601v2.pdf","comment":"14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2207.07921v2","updated":"2023-03-14T09:10:08Z","published":"2022-07-16T12:11:28Z","title":"CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image\n  Prior","summary":"  Euler's elastica constitute an appealing variational image inpainting model.\nIt minimises an energy that involves the total variation as well as the level\nline curvature. These components are transparent and make it attractive for\nshape completion tasks. However, its gradient flow is a singular, anisotropic,\nand nonlinear PDE of fourth order, which is numerically challenging: It is\ndifficult to find efficient algorithms that offer sharp edges and good rotation\ninvariance. As a remedy, we design the first neural algorithm that simulates\ninpainting with Euler's Elastica. We use the deep energy concept which employs\nthe variational energy as neural network loss. Furthermore, we pair it with a\ndeep image prior where the network architecture itself acts as a prior. This\nyields better inpaintings by steering the optimisation trajectory closer to the\ndesired solution. Our results are qualitatively on par with state-of-the-art\nalgorithms on elastica-based shape completion. They combine good rotation\ninvariance with sharp edges. Moreover, we benefit from the high efficiency and\neffortless parallelisation within a neural framework. Our neural elastica\napproach only requires 3x3 central difference stencils. It is thus much simpler\nthan other well-performing algorithms for elastica inpainting. Last but not\nleast, it is unsupervised as it requires no ground truth training data.\n","authors":["Karl Schrader","Tobias Alt","Joachim Weickert","Michael Ertel"],"pdf_url":"https://arxiv.org/pdf/2207.07921v2.pdf","comment":"In Proceedings of the 10th European Workshop on Visual Information\n  Processing, Lisbon, 2022"},{"id":"http://arxiv.org/abs/2303.06836v2","updated":"2023-03-14T09:06:45Z","published":"2023-03-13T03:46:37Z","title":"Label Information Bottleneck for Label Enhancement","summary":"  In this work, we focus on the challenging problem of Label Enhancement (LE),\nwhich aims to exactly recover label distributions from logical labels, and\npresent a novel Label Information Bottleneck (LIB) method for LE. For the\nrecovery process of label distributions, the label irrelevant information\ncontained in the dataset may lead to unsatisfactory recovery performance. To\naddress this limitation, we make efforts to excavate the essential label\nrelevant information to improve the recovery performance. Our method formulates\nthe LE problem as the following two joint processes: 1) learning the\nrepresentation with the essential label relevant information, 2) recovering\nlabel distributions based on the learned representation. The label relevant\ninformation can be excavated based on the \"bottleneck\" formed by the learned\nrepresentation. Significantly, both the label relevant information about the\nlabel assignments and the label relevant information about the label gaps can\nbe explored in our method. Evaluation experiments conducted on several\nbenchmark label distribution learning datasets verify the effectiveness and\ncompetitiveness of LIB. Our source codes are available\nhttps://github.com/qinghai-zheng/LIBLE\n","authors":["Qinghai Zheng","Jihua Zhu","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06836v2.pdf","comment":"Accepted by CVPR 2023, our source codes are available at\n  https://github.com/qinghai-zheng/LIBLE"},{"id":"http://arxiv.org/abs/2303.00515v4","updated":"2023-03-14T09:01:28Z","published":"2023-02-28T04:37:26Z","title":"Interpretable Water Level Forecaster with Spatiotemporal Causal\n  Attention Mechanisms","summary":"  Forecasting the water level of the Han river is important to control traffic\nand avoid natural disasters. There are many variables related to the Han river\nand they are intricately connected. In this work, we propose a novel\ntransformer that exploits the causal relationship based on the prior knowledge\namong the variables and forecasts the water level at the Jamsu bridge in the\nHan river. Our proposed model considers both spatial and temporal causation by\nformalizing the causal structure as a multilayer network and using masking\nmethods. Due to this approach, we can have interpretability that consistent\nwith prior knowledge. In real data analysis, we use the Han river dataset from\n2016 to 2021 and compare the proposed model with deep learning models.\n","authors":["Sunghcul Hong","Yunjin Choi","Jong-June Jeon"],"pdf_url":"https://arxiv.org/pdf/2303.00515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14125v3","updated":"2023-03-14T08:59:16Z","published":"2022-08-30T10:21:40Z","title":"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images","summary":"  Diffusion models are a special type of generative model, capable of\nsynthesising new data from a learnt distribution. We introduce DISPR, a\ndiffusion-based model for solving the inverse problem of three-dimensional (3D)\ncell shape prediction from two-dimensional (2D) single cell microscopy images.\nUsing the 2D microscopy image as a prior, DISPR is conditioned to predict\nrealistic 3D shape reconstructions. To showcase the applicability of DISPR as a\ndata augmentation tool in a feature-based single cell classification task, we\nextract morphological features from the red blood cells grouped into six highly\nimbalanced classes. Adding features from the DISPR predictions to the three\nminority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm\n4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that\ndiffusion models can be successfully applied to inverse biomedical problems,\nand that they learn to reconstruct 3D shapes with realistic morphological\nfeatures from 2D microscopy images.\n","authors":["Dominik J. E. Waibel","Ernst Röell","Bastian Rieck","Raja Giryes","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2208.14125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07125v2","updated":"2023-03-14T08:29:49Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\nhttps://github.com/ai-med/PANIC .\n","authors":["Tom Nuno Wolf","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v2.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07697v1","updated":"2023-03-14T08:22:18Z","published":"2023-03-14T08:22:18Z","title":"DisCoHead: Audio-and-Video-Driven Talking Head Generation by\n  Disentangled Control of Head Pose and Facial Expressions","summary":"  For realistic talking head generation, creating natural head motion while\nmaintaining accurate lip synchronization is essential. To fulfill this\nchallenging task, we propose DisCoHead, a novel method to disentangle and\ncontrol head pose and facial expressions without supervision. DisCoHead uses a\nsingle geometric transformation as a bottleneck to isolate and extract head\nmotion from a head-driving video. Either an affine or a thin-plate spline\ntransformation can be used and both work well as geometric bottlenecks. We\nenhance the efficiency of DisCoHead by integrating a dense motion estimator and\nthe encoder of a generator which are originally separate modules. Taking a step\nfurther, we also propose a neural mix approach where dense motion is estimated\nand applied implicitly by the encoder. After applying the disentangled head\nmotion to a source identity, DisCoHead controls the mouth region according to\nspeech audio, and it blinks eyes and moves eyebrows following a separate\ndriving video of the eye region, via the weight modulation of convolutional\nneural networks. The experiments using multiple datasets show that DisCoHead\nsuccessfully generates realistic audio-and-video-driven talking heads and\noutperforms state-of-the-art methods. Project page:\nhttps://deepbrainai-research.github.io/discohead/\n","authors":["Geumbyeol Hwang","Sunwon Hong","Seunghyun Lee","Sungwoo Park","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07697v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07200v2","updated":"2023-03-14T08:17:19Z","published":"2023-03-10T17:09:55Z","title":"Supervised Feature Selection with Neuron Evolution in Sparse Neural\n  Networks","summary":"  Feature selection that selects an informative subset of variables from data\nnot only enhances the model interpretability and performance but also\nalleviates the resource demands. Recently, there has been growing attention on\nfeature selection using neural networks. However, existing methods usually\nsuffer from high computational costs when applied to high-dimensional datasets.\nIn this paper, inspired by evolution processes, we propose a novel\nresource-efficient supervised feature selection method using sparse neural\nnetworks, named \\enquote{NeuroFS}. By gradually pruning the uninformative\nfeatures from the input layer of a sparse neural network trained from scratch,\nNeuroFS derives an informative subset of features efficiently. By performing\nseveral experiments on $11$ low and high-dimensional real-world benchmarks of\ndifferent types, we demonstrate that NeuroFS achieves the highest ranking-based\nscore among the considered state-of-the-art supervised feature selection\nmodels. The code is available on GitHub.\n","authors":["Zahra Atashgahi","Xuhao Zhang","Neil Kichler","Shiwei Liu","Lu Yin","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2303.07200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05660v2","updated":"2023-03-14T08:15:35Z","published":"2023-03-10T02:22:33Z","title":"Towards better traffic volume estimation: Tackling both underdetermined\n  and non-equilibrium problems via a correlation-adaptive graph convolution\n  network","summary":"  Traffic volume is an indispensable ingredient to provide fine-grained\ninformation for traffic management and control. However, due to limited\ndeployment of traffic sensors, obtaining full-scale volume information is far\nfrom easy. Existing works on this topic primarily focus on improving the\noverall estimation accuracy of a particular method and ignore the underlying\nchallenges of volume estimation, thereby having inferior performances on some\ncritical tasks. This paper studies two key problems with regard to traffic\nvolume estimation: (1) underdetermined traffic flows caused by undetected\nmovements, and (2) non-equilibrium traffic flows arise from congestion\npropagation. Here we demonstrate a graph-based deep learning method that can\noffer a data-driven, model-free and correlation adaptive approach to tackle the\nabove issues and perform accurate network-wide traffic volume estimation.\nParticularly, in order to quantify the dynamic and nonlinear relationships\nbetween traffic speed and volume for the estimation of underdetermined flows, a\nspeed patternadaptive adjacent matrix based on graph attention is developed and\nintegrated into the graph convolution process, to capture non-local\ncorrelations between sensors. To measure the impacts of non-equilibrium flows,\na temporal masked and clipped attention combined with a gated temporal\nconvolution layer is customized to capture time-asynchronous correlations\nbetween upstream and downstream sensors. We then evaluate our model on a\nreal-world highway traffic volume dataset and compare it with several benchmark\nmodels. It is demonstrated that the proposed model achieves high estimation\naccuracy even under 20% sensor coverage rate and outperforms other baselines\nsignificantly, especially on underdetermined and non-equilibrium flow\nlocations. Furthermore, comprehensive quantitative model analysis are also\ncarried out to justify the model designs.\n","authors":["Tong Nie","Guoyang Qin","Yunpeng Wang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2303.05660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07693v1","updated":"2023-03-14T08:13:21Z","published":"2023-03-14T08:13:21Z","title":"Adaptive Policy Learning for Offline-to-Online Reinforcement Learning","summary":"  Conventional reinforcement learning (RL) needs an environment to collect\nfresh data, which is impractical when online interactions are costly. Offline\nRL provides an alternative solution by directly learning from the previously\ncollected dataset. However, it will yield unsatisfactory performance if the\nquality of the offline datasets is poor. In this paper, we consider an\noffline-to-online setting where the agent is first learned from the offline\ndataset and then trained online, and propose a framework called Adaptive Policy\nLearning for effectively taking advantage of offline and online data.\nSpecifically, we explicitly consider the difference between the online and\noffline data and apply an adaptive update scheme accordingly, that is, a\npessimistic update strategy for the offline dataset and an optimistic/greedy\nupdate scheme for the online dataset. Such a simple and effective method\nprovides a way to mix the offline and online RL and achieve the best of both\nworlds. We further provide two detailed algorithms for implementing the\nframework through embedding value or policy-based RL algorithms into it.\nFinally, we conduct extensive experiments on popular continuous control tasks,\nand results show that our algorithm can learn the expert policy with high\nsample efficiency even when the quality of offline dataset is poor, e.g.,\nrandom dataset.\n","authors":["Han Zheng","Xufang Luo","Pengfei Wei","Xuan Song","Dongsheng Li","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07693v1.pdf","comment":"AAAI2023"},{"id":"http://arxiv.org/abs/2108.07472v6","updated":"2023-03-14T08:08:20Z","published":"2021-08-17T07:06:46Z","title":"Is Nash Equilibrium Approximator Learnable?","summary":"  In this paper, we investigate the learnability of the function approximator\nthat approximates Nash equilibrium (NE) for games generated from a\ndistribution. First, we offer a generalization bound using the Probably\nApproximately Correct (PAC) learning model. The bound describes the gap between\nthe expected loss and empirical loss of the NE approximator. Afterward, we\nprove the agnostic PAC learnability of the Nash approximator. In addition to\ntheoretical analysis, we demonstrate an application of NE approximator in\nexperiments. The trained NE approximator can be used to warm-start and\naccelerate classical NE solvers. Together, our results show the practicability\nof approximating NE through function approximation.\n","authors":["Zhijian Duan","Wenhan Huang","Dinghuai Zhang","Yali Du","Jun Wang","Yaodong Yang","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2108.07472v6.pdf","comment":"Accepted by AAMAS 2023"},{"id":"http://arxiv.org/abs/2303.07685v1","updated":"2023-03-14T07:55:50Z","published":"2023-03-14T07:55:50Z","title":"FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting","summary":"  Traffic flow forecasting is challenging due to the intricate spatio-temporal\ncorrelations in traffic flow data. Existing Transformer-based methods usually\ntreat traffic flow forecasting as multivariate time series (MTS) forecasting.\nHowever, too many sensors can cause a vector with a dimension greater than 800,\nwhich is difficult to process without information loss. In addition, these\nmethods design complex mechanisms to capture spatial dependencies in MTS,\nresulting in slow forecasting speed. To solve the abovementioned problems, we\npropose a Fast Pure Transformer Network (FPTN) in this paper. First, the\ntraffic flow data are divided into sequences along the sensor dimension instead\nof the time dimension. Then, to adequately represent complex spatio-temporal\ncorrelations, Three types of embeddings are proposed for projecting these\nvectors into a suitable vector space. After that, to capture the complex\nspatio-temporal correlations simultaneously in these vectors, we utilize\nTransformer encoder and stack it with several layers. Extensive experiments are\nconducted with 4 real-world datasets and 13 baselines, which demonstrate that\nFPTN outperforms the state-of-the-art on two metrics. Meanwhile, the\ncomputational time of FPTN spent is less than a quarter of other\nstate-of-the-art Transformer-based models spent, and the requirements for\ncomputing resources are significantly reduced.\n","authors":["Junhao Zhang","Junjie Tang","Juncheng Jin","Zehui Qu"],"pdf_url":"https://arxiv.org/pdf/2303.07685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07679v1","updated":"2023-03-14T07:42:02Z","published":"2023-03-14T07:42:02Z","title":"Feature representations useful for predicting image memorability","summary":"  Predicting image memorability has attracted interest in various fields.\nConsequently, prediction accuracy with convolutional neural network (CNN)\nmodels has been approaching the empirical upper bound estimated based on human\nconsistency. However, identifying which feature representations embedded in CNN\nmodels are responsible for such high prediction accuracy of memorability\nremains an open question. To tackle this problem, this study sought to identify\nmemorability-related feature representations in CNN models using brain\nsimilarity. Specifically, memorability prediction accuracy and brain similarity\nwere examined and assessed by Brain-Score across 16,860 layers in 64 CNN models\npretrained for object recognition. A clear tendency was shown in this\ncomprehensive analysis that layers with high memorability prediction accuracy\nhad higher brain similarity with the inferior temporal (IT) cortex, which is\nthe highest stage in the ventral visual pathway. Furthermore, fine-tuning the\n64 CNN models revealed that brain similarity with the IT cortex at the\npenultimate layer was positively correlated with memorability prediction\naccuracy. This analysis also showed that the best fine-tuned model provided\naccuracy comparable to the state-of-the-art CNN models developed specifically\nfor memorability prediction. Overall, this study's results indicated that the\nCNN models' great success in predicting memorability relies on feature\nrepresentation acquisition similar to the IT cortex. This study advanced our\nunderstanding of feature representations and its use for predicting image\nmemorability.\n","authors":["Takumi Harada","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2303.07679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00290v2","updated":"2023-03-14T07:30:03Z","published":"2022-12-01T05:31:07Z","title":"Component Segmentation of Engineering Drawings Using Graph Convolutional\n  Networks","summary":"  We present a data-driven framework to automate the vectorization and machine\ninterpretation of 2D engineering part drawings. In industrial settings, most\nmanufacturing engineers still rely on manual reads to identify the topological\nand manufacturing requirements from drawings submitted by designers. The\ninterpretation process is laborious and time-consuming, which severely inhibits\nthe efficiency of part quotation and manufacturing tasks. While recent advances\nin image-based computer vision methods have demonstrated great potential in\ninterpreting natural images through semantic segmentation approaches, the\napplication of such methods in parsing engineering technical drawings into\nsemantically accurate components remains a significant challenge. The severe\npixel sparsity in engineering drawings also restricts the effective\nfeaturization of image-based data-driven methods. To overcome these challenges,\nwe propose a deep learning based framework that predicts the semantic type of\neach vectorized component. Taking a raster image as input, we vectorize all\ncomponents through thinning, stroke tracing, and cubic bezier fitting. Then a\ngraph of such components is generated based on the connectivity between the\ncomponents. Finally, a graph convolutional neural network is trained on this\ngraph data to identify the semantic type of each component. We test our\nframework in the context of semantic segmentation of text, dimension and,\ncontour components in engineering drawings. Results show that our method yields\nthe best performance compared to recent image, and graph-based segmentation\nmethods.\n","authors":["Wentai Zhang","Joe Joseph","Yue Yin","Liuyue Xie","Tomotake Furuhata","Soji Yamakawa","Kenji Shimada","Levent Burak Kara"],"pdf_url":"https://arxiv.org/pdf/2212.00290v2.pdf","comment":"Preprint accepted to Computers in Industry"},{"id":"http://arxiv.org/abs/2210.10691v2","updated":"2023-03-14T07:27:43Z","published":"2022-10-19T16:06:12Z","title":"Provably Safe Reinforcement Learning via Action Projection using\n  Reachability Analysis and Polynomial Zonotopes","summary":"  While reinforcement learning produces very promising results for many\napplications, its main disadvantage is the lack of safety guarantees, which\nprevents its use in safety-critical systems. In this work, we address this\nissue by a safety shield for nonlinear continuous systems that solve\nreach-avoid tasks. Our safety shield prevents applying potentially unsafe\nactions from a reinforcement learning agent by projecting the proposed action\nto the closest safe action. This approach is called action projection and is\nimplemented via mixed-integer optimization. The safety constraints for action\nprojection are obtained by applying parameterized reachability analysis using\npolynomial zonotopes, which enables to accurately capture the nonlinear effects\nof the actions on the system. In contrast to other state-of-the-art approaches\nfor action projection, our safety shield can efficiently handle input\nconstraints and dynamic obstacles, eases incorporation of the spatial robot\ndimensions into the safety constraints, guarantees robust safety despite\nprocess noise and measurement errors, and is well suited for high-dimensional\nsystems, as we demonstrate on several challenging benchmark systems.\n","authors":["Niklas Kochdumper","Hanna Krasowski","Xiao Wang","Stanley Bak","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2210.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07675v1","updated":"2023-03-14T07:25:44Z","published":"2023-03-14T07:25:44Z","title":"Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems\n  Using Optimal Transport","summary":"  Predicting how distributions over discrete variables vary over time is a\ncommon task in time series forecasting. But whereas most approaches focus on\nmerely predicting the distribution at subsequent time steps, a crucial piece of\ninformation in many settings is to determine how this probability mass flows\nbetween the different elements over time. We propose a new approach to\npredicting such mass flow over time using optimal transport. Specifically, we\npropose a generic approach to predicting transport matrices in end-to-end deep\nlearning systems, replacing the standard softmax operation with Sinkhorn\niterations. We apply our approach to the task of predicting how communities\nwill evolve over time in social network settings, and show that the approach\nimproves substantially over alternative prediction methods. We specifically\nhighlight results on the task of predicting faction evolution in Ukrainian\nparliamentary voting.\n","authors":["Mukul Bhutani","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2303.07675v1.pdf","comment":"A prior version of the work appeared in the Optimal Transport\n  Workshop at NeurIPS 2019"},{"id":"http://arxiv.org/abs/2303.07674v1","updated":"2023-03-14T07:25:38Z","published":"2023-03-14T07:25:38Z","title":"Koos Classification of Vestibular Schwannoma via Image Translation-Based\n  Unsupervised Cross-Modality Domain Adaptation","summary":"  The Koos grading scale is a classification system for vestibular schwannoma\n(VS) used to characterize the tumor and its effects on adjacent brain\nstructures. The Koos classification captures many of the characteristics of\ntreatment deci-sions and is often used to determine treatment plans. Although\nboth contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)\nscanning can be used for Koos Classification, hrT2 scanning is gaining interest\nbecause of its higher safety and cost-effectiveness. However, in the absence of\nannotations for hrT2 scans, deep learning methods often inevitably suffer from\nperformance deg-radation due to unsupervised learning. If ceT1 scans and their\nannotations can be used for unsupervised learning of hrT2 scans, the\nperformance of Koos classifi-cation using unlabeled hrT2 scans will be greatly\nimproved. In this regard, we propose an unsupervised cross-modality domain\nadaptation method based on im-age translation by transforming annotated ceT1\nscans into hrT2 modality and us-ing their annotations to achieve supervised\nlearning of hrT2 modality. Then, the VS and 7 adjacent brain structures related\nto Koos classification in hrT2 scans were segmented. Finally, handcrafted\nfeatures are extracted from the segmenta-tion results, and Koos grade is\nclassified using a random forest classifier. The proposed method received rank\n1 on the Koos classification task of the Cross-Modality Domain Adaptation\n(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of\n0.2148 for the validation set and 0.26 for the test set.\n","authors":["Tao Yang","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07674v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07669v1","updated":"2023-03-14T07:23:16Z","published":"2023-03-14T07:23:16Z","title":"AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph\n  Neural Networks","summary":"  AutoML has demonstrated remarkable success in finding an effective neural\narchitecture for a given machine learning task defined by a specific dataset\nand an evaluation metric. However, most present AutoML techniques consider each\ntask independently from scratch, which requires exploring many architectures,\nleading to high computational cost. Here we propose AutoTransfer, an AutoML\nsolution that improves search efficiency by transferring the prior\narchitectural design knowledge to the novel task of interest. Our key\ninnovation includes a task-model bank that captures the model performance over\na diverse set of GNN architectures and tasks, and a computationally efficient\ntask embedding that can accurately measure the similarity among different\ntasks. Based on the task-model bank and the task embeddings, we estimate the\ndesign priors of desirable models of the novel task, by aggregating a\nsimilarity-weighted sum of the top-K design distributions on tasks that are\nsimilar to the task of interest. The computed design priors can be used with\nany AutoML search algorithm. We evaluate AutoTransfer on six datasets in the\ngraph machine learning domain. Experiments demonstrate that (i) our proposed\ntask embedding can be computed efficiently, and that tasks with similar\nembeddings have similar best-performing architectures; (ii) AutoTransfer\nsignificantly improves search efficiency with the transferred design priors,\nreducing the number of explored architectures by an order of magnitude.\nFinally, we release GNN-Bank-101, a large-scale dataset of detailed GNN\ntraining information of 120,000 task-model combinations to facilitate and\ninspire future research.\n","authors":["Kaidi Cao","Jiaxuan You","Jiaju Liu","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2303.07669v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2206.07883v3","updated":"2023-03-14T07:16:43Z","published":"2022-06-16T02:19:37Z","title":"Combinatorial Pure Exploration of Causal Bandits","summary":"  The combinatorial pure exploration of causal bandits is the following online\nlearning task: given a causal graph with unknown causal inference\ndistributions, in each round we choose a subset of variables to intervene or do\nno intervention, and observe the random outcomes of all random variables, with\nthe goal that using as few rounds as possible, we can output an intervention\nthat gives the best (or almost best) expected outcome on the reward variable\n$Y$ with probability at least $1-\\delta$, where $\\delta$ is a given confidence\nlevel. We provide the first gap-dependent and fully adaptive pure exploration\nalgorithms on two types of causal models -- the binary generalized linear model\n(BGLM) and general graphs. For BGLM, our algorithm is the first to be designed\nspecifically for this setting and achieves polynomial sample complexity, while\nall existing algorithms for general graphs have either sample complexity\nexponential to the graph size or some unreasonable assumptions. For general\ngraphs, our algorithm provides a significant improvement on sample complexity,\nand it nearly matches the lower bound we prove. Our algorithms achieve such\nimprovement by a novel integration of prior causal bandit algorithms and prior\nadaptive pure exploration algorithms, the former of which utilize the rich\nobservational feedback in causal bandits but are not adaptive to reward gaps,\nwhile the latter of which have the issue in reverse.\n","authors":["Nuoya Xiong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2206.07883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07666v1","updated":"2023-03-14T07:15:41Z","published":"2023-03-14T07:15:41Z","title":"Relational Multi-Task Learning: Modeling Relations between Data and\n  Tasks","summary":"  A key assumption in multi-task learning is that at the inference time the\nmulti-task model only has access to a given data point but not to the data\npoint's labels from other tasks. This presents an opportunity to extend\nmulti-task learning to utilize data point's labels from other auxiliary tasks,\nand this way improves performance on the new task. Here we introduce a novel\nrelational multi-task learning setting where we leverage data point labels from\nauxiliary tasks to make more accurate predictions on the new task. We develop\nMetaLink, where our key innovation is to build a knowledge graph that connects\ndata points and tasks and thus allows us to leverage labels from auxiliary\ntasks. The knowledge graph consists of two types of nodes: (1) data nodes,\nwhere node features are data embeddings computed by the neural network, and (2)\ntask nodes, with the last layer's weights for each task as node features. The\nedges in this knowledge graph capture data-task relationships, and the edge\nlabel captures the label of a data point on a particular task. Under MetaLink,\nwe reformulate the new task as a link label prediction problem between a data\nnode and a task node. The MetaLink framework provides flexibility to model\nknowledge transfer from auxiliary task labels to the task of interest. We\nevaluate MetaLink on 6 benchmark datasets in both biochemical and vision\ndomains. Experiments demonstrate that MetaLink can successfully utilize the\nrelations among different tasks, outperforming the state-of-the-art methods\nunder the proposed relational multi-task learning setting, with up to 27%\nimprovement in ROC AUC.\n","authors":["Kaidi Cao","Jiaxuan You","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2303.07666v1.pdf","comment":"ICLR 2022 Spotlight"},{"id":"http://arxiv.org/abs/2207.03902v2","updated":"2023-03-14T07:13:04Z","published":"2022-07-08T13:42:54Z","title":"Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning","summary":"  Deep cooperative multi-agent reinforcement learning has demonstrated its\nremarkable success over a wide spectrum of complex control tasks. However,\nrecent advances in multi-agent learning mainly focus on value decomposition\nwhile leaving entity interactions still intertwined, which easily leads to\nover-fitting on noisy interactions between entities. In this work, we introduce\na novel interactiOn Pattern disenTangling (OPT) method, to disentangle not only\nthe joint value function into agent-wise value functions for decentralized\nexecution, but also the entity interactions into interaction prototypes, each\nof which represents an underlying interaction pattern within a subgroup of the\nentities. OPT facilitates filtering the noisy interactions between irrelevant\nentities and thus significantly improves generalizability as well as\ninterpretability. Specifically, OPT introduces a sparse disagreement mechanism\nto encourage sparsity and diversity among discovered interaction prototypes.\nThen the model selectively restructures these prototypes into a compact\ninteraction pattern by an aggregator with learnable weights. To alleviate the\ntraining instability issue caused by partial observability, we propose to\nmaximize the mutual information between the aggregation weights and the history\nbehaviors of each agent. Experiments on both single-task and multi-task\nbenchmarks demonstrate that the proposed method yields results superior to the\nstate-of-the-art counterparts. Our code is available at\nhttps://github.com/liushunyu/OPT.\n","authors":["Shunyu Liu","Jie Song","Yihe Zhou","Na Yu","Kaixuan Chen","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2207.03902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07651v1","updated":"2023-03-14T06:38:17Z","published":"2023-03-14T06:38:17Z","title":"Context Normalization for Robust Image Classification","summary":"  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n","authors":["Bilal Faye","Mohamed-Djallel Dilmi","Hanane Azzag","Mustapha Lebbah","Fangchen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07647v1","updated":"2023-03-14T06:15:17Z","published":"2023-03-14T06:15:17Z","title":"Recent Advances and Applications of Machine Learning in Experimental\n  Solid Mechanics: A Review","summary":"  For many decades, experimental solid mechanics has played a crucial role in\ncharacterizing and understanding the mechanical properties of natural and novel\nmaterials. Recent advances in machine learning (ML) provide new opportunities\nfor the field, including experimental design, data analysis, uncertainty\nquantification, and inverse problems. As the number of papers published in\nrecent years in this emerging field is exploding, it is timely to conduct a\ncomprehensive and up-to-date review of recent ML applications in experimental\nsolid mechanics. Here, we first provide an overview of common ML algorithms and\nterminologies that are pertinent to this review, with emphasis placed on\nphysics-informed and physics-based ML methods. Then, we provide thorough\ncoverage of recent ML applications in traditional and emerging areas of\nexperimental mechanics, including fracture mechanics, biomechanics, nano- and\nmicro-mechanics, architected materials, and 2D material. Finally, we highlight\nsome current challenges of applying ML to multi-modality and multi-fidelity\nexperimental datasets and propose several future research directions. This\nreview aims to provide valuable insights into the use of ML methods as well as\na variety of examples for researchers in solid mechanics to integrate into\ntheir experiments.\n","authors":["Hanxun Jin","Enrui Zhang","Horacio D. Espinosa"],"pdf_url":"https://arxiv.org/pdf/2303.07647v1.pdf","comment":"76 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07646v1","updated":"2023-03-14T06:11:57Z","published":"2023-03-14T06:11:57Z","title":"Clustering with Simplicial Complexes","summary":"  In this work, we propose a new clustering algorithm to group nodes in\nnetworks based on second-order simplices (aka filled triangles) to leverage\nhigher-order network interactions. We define a simplicial conductance function,\nwhich on minimizing, yields an optimal partition with a higher density of\nfilled triangles within the set while the density of filled triangles is\nsmaller across the sets. To this end, we propose a simplicial adjacency\noperator that captures the relation between the nodes through second-order\nsimplices. This allows us to extend the well-known Cheeger inequality to\ncluster a simplicial complex. Then, leveraging the Cheeger inequality, we\npropose the simplicial spectral clustering algorithm. We report results from\nnumerical experiments on synthetic and real-world network data to demonstrate\nthe efficacy of the proposed approach.\n","authors":["Thummaluru Siddartha Reddy","Sundeep Prabhakar Chepuri","Pierre Borgnat"],"pdf_url":"https://arxiv.org/pdf/2303.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14829v3","updated":"2023-03-14T06:05:01Z","published":"2023-02-22T07:56:45Z","title":"Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time\n  Series Forecasting","summary":"  The distribution shift in Time Series Forecasting (TSF), indicating series\ndistribution changes over time, largely hinders the performance of TSF models.\nExisting works towards distribution shift in time series are mostly limited in\nthe quantification of distribution and, more importantly, overlook the\npotential shift between lookback and horizon windows. To address above\nchallenges, we systematically summarize the distribution shift in TSF into two\ncategories. Regarding lookback windows as input-space and horizon windows as\noutput-space, there exist (i) intra-space shift, that the distribution within\nthe input-space keeps shifted over time, and (ii) inter-space shift, that the\ndistribution is shifted between input-space and output-space. Then we\nintroduce, Dish-TS, a general neural paradigm for alleviating distribution\nshift in TSF. Specifically, for better distribution estimation, we propose the\ncoefficient net (CONET), which can be any neural architectures, to map input\nsequences into learnable distribution coefficients. To relieve intra-space and\ninter-space shift, we organize Dish-TS as a Dual-CONET framework to separately\nlearn the distribution of input- and output-space, which naturally captures the\ndistribution difference of two spaces. In addition, we introduce a more\neffective training strategy for intractable CONET learning. Finally, we conduct\nextensive experiments on several datasets coupled with different\nstate-of-the-art forecasting models. Experimental results show Dish-TS\nconsistently boosts them with a more than 20% average improvement. Code is\navailable.\n","authors":["Wei Fan","Pengyang Wang","Dongkun Wang","Dongjie Wang","Yuanchun Zhou","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2302.14829v3.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2209.14609v4","updated":"2023-03-14T05:56:18Z","published":"2022-09-29T07:58:32Z","title":"Dataset Distillation Using Parameter Pruning","summary":"  In many fields, the acquisition of advanced models depends on large datasets,\nmaking data storage and model training expensive. As a solution, dataset\ndistillation can synthesize a small dataset that preserves most information of\nthe original large dataset. The recently proposed dataset distillation method\nby matching network parameters has been proven effective for several datasets.\nHowever, the dimensions of network parameters are typically large. Furthermore,\nsome parameters are difficult to match during the distillation process,\ndegrading distillation performance. Based on this observation, this study\nproposes a novel dataset distillation method based on parameter pruning that\nsolves the problem. The proposed method can synthesize more robust distilled\ndatasets and improve distillation performance by pruning difficult-to-match\nparameters during the distillation process. Experimental results on three\ndatasets show that the proposed method outperforms other state-of-the-art\ndataset distillation methods.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2209.14609v4.pdf","comment":"Submitted as a journal paper at IEEE SPL"},{"id":"http://arxiv.org/abs/2211.02247v2","updated":"2023-03-14T05:24:20Z","published":"2022-11-04T03:45:17Z","title":"Music Mixing Style Transfer: A Contrastive Learning Approach to\n  Disentangle Audio Effects","summary":"  We propose an end-to-end music mixing style transfer system that converts the\nmixing style of an input multitrack to that of a reference song. This is\nachieved with an encoder pre-trained with a contrastive objective to extract\nonly audio effects related information from a reference music recording. All\nour models are trained in a self-supervised manner from an already-processed\nwet multitrack dataset with an effective data preprocessing method that\nalleviates the data scarcity of obtaining unprocessed dry data. We analyze the\nproposed encoder for the disentanglement capability of audio effects and also\nvalidate its performance for mixing style transfer through both objective and\nsubjective evaluations. From the results, we show the proposed system not only\nconverts the mixing style of multitrack audio close to a reference but is also\nrobust with mixture-wise style transfer upon using a music source separation\nmodel.\n","authors":["Junghyun Koo","Marco A. Martínez-Ramírez","Wei-Hsiang Liao","Stefan Uhlich","Kyogu Lee","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2211.02247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10325v9","updated":"2023-03-14T05:23:24Z","published":"2021-10-20T00:47:31Z","title":"One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and\n  Its Application to Tumour Segmentation for Breast Cancer","summary":"  Recent studies have demonstrated the effectiveness of the combination of\nmachine learning and logical reasoning, including data-driven logical\nreasoning, knowledge driven machine learning and abductive learning, in\ninventing advanced artificial intelligence technologies. One-step abductive\nmulti-target learning (OSAMTL), an approach inspired by abductive learning, via\nsimply combining machine learning and logical reasoning in a one-step balanced\nway, has as well shown its effectiveness in handling complex noisy labels of a\nsingle noisy sample in medical histopathology whole slide image analysis\n(MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy\nsamples (DiNS) are provided for a learning task. In this paper, giving\ndefinition of DiNS, we propose one-step abductive multi-target learning with\nDiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels\nof DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in\nMHWSIA, we show that OSAMTL-DiNS is able to enable various state-of-the-art\napproaches for learning from noisy labels to achieve more rational predictions.\n","authors":["Yongquan Yang","Fengling Li","Yani Wei","Jie Chen","Ning Chen","Hong Bu"],"pdf_url":"https://arxiv.org/pdf/2110.10325v9.pdf","comment":"66 pages"},{"id":"http://arxiv.org/abs/2303.07627v1","updated":"2023-03-14T04:51:24Z","published":"2023-03-14T04:51:24Z","title":"Best arm identification in rare events","summary":"  We consider the best arm identification problem in the stochastic multi-armed\nbandit framework where each arm has a tiny probability of realizing large\nrewards while with overwhelming probability the reward is zero. A key\napplication of this framework is in online advertising where click rates of\nadvertisements could be a fraction of a single percent and final conversion to\nsales, while highly profitable, may again be a small fraction of the click\nrates. Lately, algorithms for BAI problems have been developed that minimise\nsample complexity while providing statistical guarantees on the correct arm\nselection. As we observe, these algorithms can be computationally prohibitive.\nWe exploit the fact that the reward process for each arm is well approximated\nby a Compound Poisson process to arrive at algorithms that are faster, with a\nsmall increase in sample complexity. We analyze the problem in an asymptotic\nregime as rarity of reward occurrence reduces to zero, and reward amounts\nincrease to infinity. This helps illustrate the benefits of the proposed\nalgorithm. It also sheds light on the underlying structure of the optimal BAI\nalgorithms in the rare event setting.\n","authors":["Anirban Bhattacharjee","Sushant Vijayan","Sandeep K Juneja"],"pdf_url":"https://arxiv.org/pdf/2303.07627v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2206.10696v3","updated":"2023-03-14T04:43:23Z","published":"2022-06-21T19:31:25Z","title":"Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting\n  Epidemics","summary":"  Infectious diseases remain among the top contributors to human illness and\ndeath worldwide, among which many diseases produce epidemic waves of infection.\nThe unavailability of specific drugs and ready-to-use vaccines to prevent most\nof these epidemics makes the situation worse. These force public health\nofficials and policymakers to rely on early warning systems generated by\nreliable and accurate forecasts of epidemics. Accurate forecasts of epidemics\ncan assist stakeholders in tailoring countermeasures, such as vaccination\ncampaigns, staff scheduling, and resource allocation, to the situation at hand,\nwhich could translate to reductions in the impact of a disease. Unfortunately,\nmost of these past epidemics exhibit nonlinear and non-stationary\ncharacteristics due to their spreading fluctuations based on seasonal-dependent\nvariability and the nature of these epidemics. We analyse a wide variety of\nepidemic time series datasets using a maximal overlap discrete wavelet\ntransform (MODWT) based autoregressive neural network and call it EWNet model.\nMODWT techniques effectively characterize non-stationary behavior and seasonal\ndependencies in the epidemic time series and improve the nonlinear forecasting\nscheme of the autoregressive neural network in the proposed ensemble wavelet\nnetwork framework. From a nonlinear time series viewpoint, we explore the\nasymptotic stationarity of the proposed EWNet model to show the asymptotic\nbehavior of the associated Markov Chain. We also theoretically investigate the\neffect of learning stability and the choice of hidden neurons in the proposal.\nFrom a practical perspective, we compare our proposed EWNet framework with\nseveral statistical, machine learning, and deep learning models. Experimental\nresults show that the proposed EWNet is highly competitive compared to the\nstate-of-the-art epidemic forecasting methods.\n","authors":["Madhurima Panja","Tanujit Chakraborty","Uttam Kumar","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2206.10696v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07622v1","updated":"2023-03-14T04:20:59Z","published":"2023-03-14T04:20:59Z","title":"RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via\n  Language-Based Feedback","summary":"  Reinforcement learning-based policies for continuous control robotic\nnavigation tasks often fail to adapt to changes in the environment during\nreal-time deployment, which may result in catastrophic failures. To address\nthis limitation, we propose a novel approach called RE-MOVE (\\textbf{RE}quest\nhelp and \\textbf{MOVE} on), which uses language-based feedback to adjust\ntrained policies to real-time changes in the environment. In this work, we\nenable the trained policy to decide \\emph{when to ask for feedback} and\n\\emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates\nepistemic uncertainty to determine the optimal time to request feedback from\nhumans and uses language-based feedback for real-time adaptation. We perform\nextensive synthetic and real-world evaluations to demonstrate the benefits of\nour proposed approach in several test-time dynamic navigation scenarios. Our\napproach enable robots to learn from human feedback and adapt to previously\nunseen adversarial situations.\n","authors":["Souradip Chakraborty","Kasun Weerakoon","Prithvi Poddar","Pratap Tokekar","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.07622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07074v3","updated":"2023-03-14T03:36:41Z","published":"2023-01-17T18:36:57Z","title":"SegViz: A federated-learning based framework for multi-organ\n  segmentation on heterogeneous data sets with partial annotations","summary":"  Segmentation is one of the most primary tasks in deep learning for medical\nimaging, owing to its multiple downstream clinical applications. However,\ngenerating manual annotations for medical images is time-consuming, requires\nhigh skill, and is an expensive effort, especially for 3D images. One potential\nsolution is to aggregate knowledge from partially annotated datasets from\nmultiple groups to collaboratively train global models using Federated\nLearning. To this end, we propose SegViz, a federated learning-based framework\nto train a segmentation model from distributed non-i.i.d datasets with partial\nannotations. The performance of SegViz was compared against training individual\nmodels separately on each dataset as well as centrally aggregating all the\ndatasets in one place and training a single model. The SegViz framework using\nFedBN as the aggregation strategy demonstrated excellent performance on the\nexternal BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for\nsegmentation of liver, spleen, pancreas, and kidneys, respectively,\nsignificantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,\n0.83, 0.42, and 0.48 for the baseline models. In contrast, the central\naggregation model significantly ($p<0.05$) performed poorly on the test dataset\nwith dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the\npotential of the SegViz framework to train multi-task models from distributed\ndatasets with partial labels. All our implementations are open-source and\navailable at https://anonymous.4open.science/r/SegViz-B746\n","authors":["Adway U. Kanhere","Pranav Kulkarni","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.07074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00351v3","updated":"2023-03-14T03:12:12Z","published":"2023-01-01T05:26:33Z","title":"Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation","summary":"  An unbiased scene graph generation (SGG) algorithm referred to as Skew\nClass-balanced Re-weighting (SCR) is proposed for considering the unbiased\npredicate prediction caused by the long-tailed distribution. The prior works\nfocus mainly on alleviating the deteriorating performances of the minority\npredicate predictions, showing drastic dropping recall scores, i.e., losing the\nmajority predicate performances. It has not yet correctly analyzed the\ntrade-off between majority and minority predicate performances in the limited\nSGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced\nRe-weighting (SCR) loss function is considered for the unbiased SGG models.\nLeveraged by the skewness of biased predicate predictions, the SCR estimates\nthe target predicate weight coefficient and then re-weights more to the biased\npredicates for better trading-off between the majority predicates and the\nminority ones. Extensive experiments conducted on the standard Visual Genome\ndataset and Open Image V4 \\& V6 show the performances and generality of the SCR\nwith the traditional SGG models.\n","authors":["Haeyong Kang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2301.00351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07608v1","updated":"2023-03-14T03:04:37Z","published":"2023-03-14T03:04:37Z","title":"On the Implicit Geometry of Cross-Entropy Parameterizations for\n  Label-Imbalanced Data","summary":"  Various logit-adjusted parameterizations of the cross-entropy (CE) loss have\nbeen proposed as alternatives to weighted CE for training large models on\nlabel-imbalanced data far beyond the zero train error regime. The driving force\nbehind those designs has been the theory of implicit bias, which for\nlinear(ized) models, explains why they successfully induce bias on the\noptimization path towards solutions that favor minorities. Aiming to extend\nthis theory to non-linear models, we investigate the implicit geometry of\nclassifiers and embeddings that are learned by different CE parameterizations.\nOur main result characterizes the global minimizers of a non-convex\ncost-sensitive SVM classifier for the unconstrained features model, which\nserves as an abstraction of deep nets. We derive closed-form formulas for the\nangles and norms of classifiers and embeddings as a function of the number of\nclasses, the imbalance and the minority ratios, and the loss hyperparameters.\nUsing these, we show that logit-adjusted parameterizations can be appropriately\ntuned to learn symmetric geometries irrespective of the imbalance ratio. We\ncomplement our analysis with experiments and an empirical study of convergence\naccuracy in deep-nets.\n","authors":["Tina Behnia","Ganesh Ramachandra Kini","Vala Vakilian","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2303.07608v1.pdf","comment":"Short version of this accepted at AISTATS 2023"},{"id":"http://arxiv.org/abs/2207.11659v3","updated":"2023-03-14T02:52:54Z","published":"2022-07-24T04:23:56Z","title":"Training Robust Spiking Neural Networks on Neuromorphic Data with\n  Spatiotemporal Fragments","summary":"  Neuromorphic vision sensors (event cameras) are inherently suitable for\nspiking neural networks (SNNs) and provide novel neuromorphic vision data for\nthis biomimetic model. Due to the spatiotemporal characteristics, novel data\naugmentations are required to process the unconventional visual signals of\nthese cameras. In this paper, we propose a novel Event SpatioTemporal Fragments\n(ESTF) augmentation method. It preserves the continuity of neuromorphic data by\ndrifting or inverting fragments of the spatiotemporal event stream to simulate\nthe disturbance of brightness variations, leading to more robust spiking neural\nnetworks. Extensive experiments are performed on prevailing neuromorphic\ndatasets. It turns out that ESTF provides substantial improvements over pure\ngeometric transformations and outperforms other event data augmentation\nmethods. It is worth noting that the SNNs with ESTF achieve the\nstate-of-the-art accuracy of 83.9\\% on the CIFAR10-DVS dataset.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11659v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2207.11670v2","updated":"2023-03-14T02:51:07Z","published":"2022-07-24T06:12:23Z","title":"Training Stronger Spiking Neural Networks with Biomimetic Adaptive\n  Internal Association Neurons","summary":"  As the third generation of neural networks, spiking neural networks (SNNs)\nare dedicated to exploring more insightful neural mechanisms to achieve\nnear-biological intelligence. Intuitively, biomimetic mechanisms are crucial to\nunderstanding and improving SNNs. For example, the associative long-term\npotentiation (ALTP) phenomenon suggests that in addition to learning mechanisms\nbetween neurons, there are associative effects within neurons. However, most\nexisting methods only focus on the former and lack exploration of the internal\nassociation effects. In this paper, we propose a novel Adaptive Internal\nAssociation~(AIA) neuron model to establish previously ignored influences\nwithin neurons. Consistent with the ALTP phenomenon, the AIA neuron model is\nadaptive to input stimuli, and internal associative learning occurs only when\nboth dendrites are stimulated at the same time. In addition, we employ weighted\nweights to measure internal associations and introduce intermediate caches to\nreduce the volatility of associations. Extensive experiments on prevailing\nneuromorphic datasets show that the proposed method can potentiate or depress\nthe firing of spikes more specifically, resulting in better performance with\nfewer spikes. It is worth noting that without adding any parameters at\ninference, the AIA model achieves state-of-the-art performance on\nDVS-CIFAR10~(83.9\\%) and N-CARS~(95.64\\%) datasets.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11670v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07122v2","updated":"2023-03-14T02:50:50Z","published":"2023-02-22T19:35:28Z","title":"Quantifying Causes of Arctic Amplification via Deep Learning based\n  Time-series Causal Inference","summary":"  The warming of the Arctic, also known as Arctic amplification, is led by\nseveral atmospheric and oceanic drivers, however, the details of its underlying\nthermodynamic causes are still unknown. Inferring the causal effects of\natmospheric processes on sea ice melt using fixed treatment effect strategies\nleads to unrealistic counterfactual estimations. Such models are also prone to\nbias due to time-varying confoundedness. In order to tackle these challenges,\nwe propose TCINet - time-series causal inference model to infer causation under\ncontinuous treatment using recurrent neural networks. Through experiments on\nsynthetic and observational data, we show how our research can substantially\nimprove the ability to quantify the leading causes of Arctic sea ice melt.\n","authors":["Sahara Ali","Omar Faruque","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04150v2","updated":"2023-03-14T02:48:42Z","published":"2022-10-09T02:57:32Z","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP","summary":"  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n","authors":["Feng Liang","Bichen Wu","Xiaoliang Dai","Kunpeng Li","Yinan Zhao","Hang Zhang","Peizhao Zhang","Peter Vajda","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2210.04150v2.pdf","comment":"CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg"},{"id":"http://arxiv.org/abs/2303.07600v1","updated":"2023-03-14T02:46:42Z","published":"2023-03-14T02:46:42Z","title":"Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC)\n  Countries using Machine Learning","summary":"  COVID-19 has infected more than 68 million people worldwide since it was\nfirst detected about a year ago. Machine learning time series models have been\nimplemented to forecast COVID-19 infections. In this paper, we develop time\nseries models for the Gulf Cooperation Council (GCC) countries using the public\nCOVID-19 dataset from Johns Hopkins. The dataset set includes the one-year\ncumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed\ndifferent models for the countries under study based on the spatial\ndistribution of the infection data. Our experimental results show that the\ndeveloped models can forecast COVID-19 infections with high precision.\n","authors":["Leila Ismail","Huned Materwala","Alain Hennebelle"],"pdf_url":"https://arxiv.org/pdf/2303.07600v1.pdf","comment":"9 pages, Proceedings of the 13th International Conference on Computer\n  Modeling and Simulation, ICCMS 2021, Autoregressive integrated moving\n  average, ARIMA, Coronavirus, COVID-19, Damped Trend, Holt Linear Trend,\n  Machine learning, Pandemic, Time series"},{"id":"http://arxiv.org/abs/2303.07599v1","updated":"2023-03-14T02:45:41Z","published":"2023-03-14T02:45:41Z","title":"A Contrastive Knowledge Transfer Framework for Model Compression and\n  Transfer Learning","summary":"  Knowledge Transfer (KT) achieves competitive performance and is widely used\nfor image classification tasks in model compression and transfer learning.\nExisting KT works transfer the information from a large model (\"teacher\") to\ntrain a small model (\"student\") by minimizing the difference of their\nconditionally independent output distributions. However, these works overlook\nthe high-dimension structural knowledge from the intermediate representations\nof the teacher, which leads to limited effectiveness, and they are motivated by\nvarious heuristic intuitions, which makes it difficult to generalize. This\npaper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which\nenables the transfer of sufficient structural knowledge from the teacher to the\nstudent by optimizing multiple contrastive objectives across the intermediate\nrepresentations between them. Also, CKTF provides a generalized agreement to\nexisting KT techniques and increases their performance significantly by\nderiving them as specific cases of CKTF. The extensive evaluation shows that\nCKTF consistently outperforms the existing KT works by 0.04% to 11.59% in model\ncompression and by 0.4% to 4.75% in transfer learning on various models and\ndatasets.\n","authors":["Kaiqi Zhao","Yitao Chen","Ming Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.07725v2","updated":"2023-03-14T02:45:14Z","published":"2021-02-15T18:19:07Z","title":"Neural Network Compression for Noisy Storage Devices","summary":"  Compression and efficient storage of neural network (NN) parameters is\ncritical for applications that run on resource-constrained devices. Despite the\nsignificant progress in NN model compression, there has been considerably less\ninvestigation in the actual \\textit{physical} storage of NN parameters.\nConventionally, model compression and physical storage are decoupled, as\ndigital storage media with error-correcting codes (ECCs) provide robust\nerror-free storage. However, this decoupled approach is inefficient as it\nignores the overparameterization present in most NNs and forces the memory\ndevice to allocate the same amount of resources to every bit of information\nregardless of its importance. In this work, we investigate analog memory\ndevices as an alternative to digital media -- one that naturally provides a way\nto add more protection for significant bits unlike its counterpart, but is\nnoisy and may compromise the stored model's performance if used naively. We\ndevelop a variety of robust coding strategies for NN weight storage on analog\ndevices, and propose an approach to jointly optimize model compression and\nmemory resource allocation. We then demonstrate the efficacy of our approach on\nmodels trained on MNIST, CIFAR-10 and ImageNet datasets for existing\ncompression techniques. Compared to conventional error-free digital storage,\nour method reduces the memory footprint by up to one order of magnitude,\nwithout significantly compromising the stored model's accuracy.\n","authors":["Berivan Isik","Kristy Choi","Xin Zheng","Tsachy Weissman","Stefano Ermon","H. -S. Philip Wong","Armin Alaghi"],"pdf_url":"https://arxiv.org/pdf/2102.07725v2.pdf","comment":"Published at the ACM Transactions on Embedded Computing Systems\n  (TECS), 2023"},{"id":"http://arxiv.org/abs/2303.00320v3","updated":"2023-03-14T02:43:29Z","published":"2023-03-01T08:33:16Z","title":"TimeMAE: Self-Supervised Representations of Time Series with Decoupled\n  Masked Autoencoders","summary":"  Enhancing the expressive capacity of deep learning-based time series models\nwith self-supervised pre-training has become ever-increasingly prevalent in\ntime series classification. Even though numerous efforts have been devoted to\ndeveloping self-supervised models for time series data, we argue that the\ncurrent methods are not sufficient to learn optimal time series representations\ndue to solely unidirectional encoding over sparse point-wise input units. In\nthis work, we propose TimeMAE, a novel self-supervised paradigm for learning\ntransferrable time series representations based on transformer networks. The\ndistinct characteristics of the TimeMAE lie in processing each time series into\na sequence of non-overlapping sub-series via window-slicing partitioning,\nfollowed by random masking strategies over the semantic units of localized\nsub-series. Such a simple yet effective setting can help us achieve the goal of\nkilling three birds with one stone, i.e., (1) learning enriched contextual\nrepresentations of time series with a bidirectional encoding scheme; (2)\nincreasing the information density of basic semantic units; (3) efficiently\nencoding representations of time series using transformer networks.\nNevertheless, it is a non-trivial to perform reconstructing task over such a\nnovel formulated modeling paradigm. To solve the discrepancy issue incurred by\nnewly injected masked embeddings, we design a decoupled autoencoder\narchitecture, which learns the representations of visible (unmasked) positions\nand masked ones with two different encoder modules, respectively. Furthermore,\nwe construct two types of informative targets to accomplish the corresponding\npretext tasks. One is to create a tokenizer module that assigns a codeword to\neach masked region, allowing the masked codeword classification (MCC) task to\nbe completed effectively...\n","authors":["Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00320v3.pdf","comment":"Submitted to IEEE TRANSACTIONS ON KNOWLEDGE AND DATA\n  ENGINEERING(TKDE), under review"},{"id":"http://arxiv.org/abs/2303.07598v1","updated":"2023-03-14T02:42:01Z","published":"2023-03-14T02:42:01Z","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision\n  Transformers via MAE+","summary":"  Unsupervised learning of vision transformers seeks to pretrain an encoder via\npretext tasks without labels. Among them is the Masked Image Modeling (MIM)\naligned with pretraining of language transformers by predicting masked patches\nas a pretext task. A criterion in unsupervised pretraining is the pretext task\nneeds to be sufficiently hard to prevent the transformer encoder from learning\ntrivial low-level features not generalizable well to downstream tasks. For this\npurpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It\ndistorts the local visual structures by perturbing the position encodings so\nthat the learned transformer cannot simply use the locally correlated patches\nto predict the missing ones. We hypothesize that it forces the transformer\nencoder to learn more discriminative features in a global context with stronger\ngeneralizability to downstream tasks. We will consider both absolute and\nrelative positional encodings, where adversarial positions can be imposed both\nin the embedding mode and the coordinate mode. We will also present a new MAE+\nbaseline that brings the performance of the MIM pretraining to a new level with\nthe AdPE. The experiments demonstrate that our approach can improve the\nfine-tuning accuracy of MAE by $0.8\\%$ and $0.4\\%$ over 1600 epochs of\npretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it\noutperforms the MAE with the ViT-B backbone by $2.6\\%$ in mIoU on ADE20K, and\nby $3.2\\%$ in AP$^{bbox}$ and $1.6\\%$ in AP$^{mask}$ on COCO, respectively.\nThese results are obtained with the AdPE being a pure MIM approach that does\nnot use any extra models or external datasets for pretraining. The code is\navailable at https://github.com/maple-research-lab/AdPE.\n","authors":["Xiao Wang","Ying Wang","Ziwei Xuan","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07598v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07597v1","updated":"2023-03-14T02:40:46Z","published":"2023-03-14T02:40:46Z","title":"Fast Regularized Discrete Optimal Transport with Group-Sparse\n  Regularizers","summary":"  Regularized discrete optimal transport (OT) is a powerful tool to measure the\ndistance between two discrete distributions that have been constructed from\ndata samples on two different domains. While it has a wide range of\napplications in machine learning, in some cases the sampled data from only one\nof the domains will have class labels such as unsupervised domain adaptation.\nIn this kind of problem setting, a group-sparse regularizer is frequently\nleveraged as a regularization term to handle class labels. In particular, it\ncan preserve the label structure on the data samples by corresponding the data\nsamples with the same class label to one group-sparse regularization term. As a\nresult, we can measure the distance while utilizing label information by\nsolving the regularized optimization problem with gradient-based algorithms.\nHowever, the gradient computation is expensive when the number of classes or\ndata samples is large because the number of regularization terms and their\nrespective sizes also turn out to be large. This paper proposes fast discrete\nOT with group-sparse regularizers. Our method is based on two ideas. The first\nis to safely skip the computations of the gradients that must be zero. The\nsecond is to efficiently extract the gradients that are expected to be nonzero.\nOur method is guaranteed to return the same value of the objective function as\nthat of the original method. Experiments show that our method is up to 8.6\ntimes faster than the original method without degrading accuracy.\n","authors":["Yasutoshi Ida","Sekitoshi Kanai","Kazuki Adachi","Atsutoshi Kumagai","Yasuhiro Fujiwara"],"pdf_url":"https://arxiv.org/pdf/2303.07597v1.pdf","comment":"This is an extended version of the paper accepted by the 37th AAAI\n  Conference on Artificial Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2303.07589v1","updated":"2023-03-14T02:26:34Z","published":"2023-03-14T02:26:34Z","title":"Sequential three-way decisions with a single hidden layer feedforward\n  neural network","summary":"  The three-way decisions strategy has been employed to construct network\ntopology in a single hidden layer feedforward neural network (SFNN). However,\nthis model has a general performance, and does not consider the process costs,\nsince it has fixed threshold parameters. Inspired by the sequential three-way\ndecisions (STWD), this paper proposes STWD with an SFNN (STWD-SFNN) to enhance\nthe performance of networks on structured datasets. STWD-SFNN adopts\nmulti-granularity levels to dynamically learn the number of hidden layer nodes\nfrom coarse to fine, and set the sequential threshold parameters. Specifically,\nat the coarse granular level, STWD-SFNN handles easy-to-classify instances by\napplying strict threshold conditions, and with the increasing number of hidden\nlayer nodes at the fine granular level, STWD-SFNN focuses more on disposing of\nthe difficult-to-classify instances by applying loose threshold conditions,\nthereby realizing the classification of instances. Moreover, STWD-SFNN\nconsiders and reports the process cost produced from each granular level. The\nexperimental results verify that STWD-SFNN has a more compact network on\nstructured datasets than other SFNN models, and has better generalization\nperformance than the competitive models. All models and datasets can be\ndownloaded from https://github.com/wuc567/Machine-learning/tree/main/STWD-SFNN.\n","authors":["Youxi Wu","Shuhui Cheng","Yan Li","Rongjie Lv","Fan Min"],"pdf_url":"https://arxiv.org/pdf/2303.07589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07586v1","updated":"2023-03-14T02:12:00Z","published":"2023-03-14T02:12:00Z","title":"Teacher-Student Knowledge Distillation for Radar Perception on Embedded\n  Accelerators","summary":"  Many radar signal processing methodologies are being developed for critical\nroad safety perception tasks. Unfortunately, these signal processing algorithms\nare often poorly suited to run on embedded hardware accelerators used in\nautomobiles. Conversely, end-to-end machine learning (ML) approaches better\nexploit the performance gains brought by specialized accelerators. In this\npaper, we propose a teacher-student knowledge distillation approach for\nlow-level radar perception tasks. We utilize a hybrid model for stationary\nobject detection as a teacher to train an end-to-end ML student model. The\nstudent can efficiently harness embedded compute for real-time deployment. We\ndemonstrate that the proposed student model runs at speeds 100x faster than the\nteacher model.\n","authors":["Steven Shaw","Kanishka Tyagi","Shan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07586v1.pdf","comment":"submitted at ASILOMAR,2023"},{"id":"http://arxiv.org/abs/2211.12712v2","updated":"2023-03-14T02:07:45Z","published":"2022-11-23T05:18:42Z","title":"Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition","summary":"  Value Decomposition (VD) aims to deduce the contributions of agents for\ndecentralized policies in the presence of only global rewards, and has recently\nemerged as a powerful credit assignment paradigm for tackling cooperative\nMulti-Agent Reinforcement Learning (MARL) problems. One of the main challenges\nin VD is to promote diverse behaviors among agents, while existing methods\ndirectly encourage the diversity of learned agent networks with various\nstrategies. However, we argue that these dedicated designs for agent networks\nare still limited by the indistinguishable VD network, leading to homogeneous\nagent behaviors and thus downgrading the cooperation capability. In this paper,\nwe propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly\nboosting the credit-level distinguishability of the VD network to break the\nbottleneck of multi-agent diversity. Specifically, our approach leverages\ncontrastive learning to maximize the mutual information between the temporal\ncredits and identity representations of different agents, encouraging the full\nexpressiveness of credit assignment and further the emergence of\nindividualities. The algorithm implementation of the proposed CIA module is\nsimple yet effective that can be readily incorporated into various VD\narchitectures. Experiments on the SMAC benchmarks and across different VD\nbackbones demonstrate that the proposed method yields results superior to the\nstate-of-the-art counterparts. Our code is available at\nhttps://github.com/liushunyu/CIA.\n","authors":["Shunyu Liu","Yihe Zhou","Jie Song","Tongya Zheng","Kaixuan Chen","Tongtian Zhu","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2211.12712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07580v1","updated":"2023-03-14T01:56:15Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has used metamorphic testing (MT) to detect misclassified images. Most\nof them discuss metamorphic relations (MR), with little discussion on which\nregions should be transformed. We focus on the fact that there are sensitive\nregions where even a small transformation can easily change the prediction\nresults and propose an MT framework that efficiently tests for regions prone to\nmisclassification by transforming the sensitive regions. Our evaluation showed\nthat the sensitive regions can be specified by Explainable AI (XAI) and our\nframework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07578v1","updated":"2023-03-14T01:55:41Z","published":"2023-03-14T01:55:41Z","title":"VANI: Very-lightweight Accent-controllable TTS for Native and Non-native\n  speakers with Identity Preservation","summary":"  We introduce VANI, a very lightweight multi-lingual accent controllable\nspeech synthesis system. Our model builds upon disentanglement strategies\nproposed in RADMMM and supports explicit control of accent, language, speaker\nand fine-grained $F_0$ and energy features for speech synthesis. We utilize the\nIndic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal\nProcessing Grand Challenge, to synthesize speech in 3 different languages. Our\nmodel supports transferring the language of a speaker while retaining their\nvoice and the native accent of the target language. We utilize the\nlarge-parameter RADMMM model for Track $1$ and lightweight VANI model for Track\n$2$ and $3$ of the competition.\n","authors":["Rohan Badlani","Akshit Arora","Subhankar Ghosh","Rafael Valle","Kevin J. Shih","João Felipe Santos","Boris Ginsburg","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2303.07578v1.pdf","comment":"Presentation accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07560v1","updated":"2023-03-14T00:57:11Z","published":"2023-03-14T00:57:11Z","title":"Machine Learning Computer Vision Applications for Spatial AI Object\n  Recognition in Orange County, California","summary":"  We provide an integrated and systematic automation approach to spatial object\nrecognition and positional detection using AI machine learning and computer\nvision algorithms for Orange County, California. We describe a comprehensive\nmethodology for multi-sensor, high-resolution field data acquisition, along\nwith post-field processing and pre-analysis processing tasks. We developed a\nseries of algorithmic formulations and workflows that integrate convolutional\ndeep neural network learning with detected object positioning estimation in\n360{\\deg} equirectancular photosphere imagery. We provide examples of\napplication processing more than 800 thousand cardinal directions in\nphotosphere images across two areas in Orange County, and present detection\nresults for stop-sign and fire hydrant object recognition. We discuss the\nefficiency and effectiveness of our approach, along with broader inferences\nrelated to the performance and implications of this approach for future\ntechnological innovations, including automation of spatial data and public\nasset inventories, and near real-time AI field data systems.\n","authors":["Kostas Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2303.07560v1.pdf","comment":"24 pages, 15 figures, 8 tables"},{"id":"http://arxiv.org/abs/2303.07557v1","updated":"2023-03-14T00:49:09Z","published":"2023-03-14T00:49:09Z","title":"Lifelong Learning for Anomaly Detection: New Challenges, Perspectives,\n  and Insights","summary":"  Anomaly detection is of paramount importance in many real-world domains,\ncharacterized by evolving behavior. Lifelong learning represents an emerging\ntrend, answering the need for machine learning models that continuously adapt\nto new challenges in dynamic environments while retaining past knowledge.\nHowever, limited efforts are dedicated to building foundations for lifelong\nanomaly detection, which provides intrinsically different challenges compared\nto the more widely explored classification setting. In this paper, we face this\nissue by exploring, motivating, and discussing lifelong anomaly detection,\ntrying to build foundations for its wider adoption. First, we explain why\nlifelong anomaly detection is relevant, defining challenges and opportunities\nto design anomaly detection methods that deal with lifelong learning\ncomplexities. Second, we characterize learning settings and a scenario\ngeneration procedure that enables researchers to experiment with lifelong\nanomaly detection using existing datasets. Third, we perform experiments with\npopular anomaly detection methods on proposed lifelong scenarios, emphasizing\nthe gap in performance that could be gained with the adoption of lifelong\nlearning. Overall, we conclude that the adoption of lifelong anomaly detection\nis important to design more robust models that provide a comprehensive view of\nthe environment, as well as simultaneous adaptation and knowledge retention.\n","authors":["Kamil Faber","Roberto Corizzo","Bartlomiej Sniezynski","Nathalie Japkowicz"],"pdf_url":"https://arxiv.org/pdf/2303.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07551v1","updated":"2023-03-14T00:43:48Z","published":"2023-03-14T00:43:48Z","title":"Merging Decision Transformers: Weight Averaging for Forming Multi-Task\n  Policies","summary":"  Recent work has shown the promise of creating generalist, transformer-based,\npolicies for language, vision, and sequential decision-making problems. To\ncreate such models, we generally require centralized training objectives, data,\nand compute. It is of interest if we can more flexibly create generalist\npolicies, by merging together multiple, task-specific, individually trained\npolicies. In this work, we take a preliminary step in this direction through\nmerging, or averaging, subsets of Decision Transformers in weight space trained\non different MuJoCo locomotion problems, forming multi-task models without\ncentralized training. We also propose that when merging policies, we can obtain\nbetter results if all policies start from common, pre-trained initializations,\nwhile also co-training on shared auxiliary tasks during problem-specific\nfinetuning. In general, we believe research in this direction can help\ndemocratize and distribute the process of which forms generally capable agents.\n","authors":["Daniel Lawson","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2303.07551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07546v1","updated":"2023-03-14T00:27:33Z","published":"2023-03-14T00:27:33Z","title":"Constrained Adversarial Learning and its applicability to Automated\n  Software Testing: a systematic review","summary":"  Every novel technology adds hidden vulnerabilities ready to be exploited by a\ngrowing number of cyber-attacks. Automated software testing can be a promising\nsolution to quickly analyze thousands of lines of code by generating and\nslightly modifying function-specific testing data to encounter a multitude of\nvulnerabilities and attack vectors. This process draws similarities to the\nconstrained adversarial examples generated by adversarial learning methods, so\nthere could be significant benefits to the integration of these methods in\nautomated testing tools. Therefore, this systematic review is focused on the\ncurrent state-of-the-art of constrained data generation methods applied for\nadversarial learning and software testing, aiming to guide researchers and\ndevelopers to enhance testing tools with adversarial learning methods and\nimprove the resilience and robustness of their digital systems. The found\nconstrained data generation applications for adversarial machine learning were\nsystematized, and the advantages and limitations of approaches specific for\nsoftware testing were thoroughly analyzed, identifying research gaps and\nopportunities to improve testing tools with adversarial attack methods.\n","authors":["João Vitorino","Tiago Dias","Tiago Fonseca","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2303.07546v1.pdf","comment":"32 pages, 5 tables, 2 figures, Information and Software Technology\n  journal"},{"id":"http://arxiv.org/abs/2211.01324v5","updated":"2023-03-14T00:22:14Z","published":"2022-11-02T17:43:04Z","title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert\n  Denoisers","summary":"  Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I's \"paint-with-words\" capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/\n","authors":["Yogesh Balaji","Seungjun Nah","Xun Huang","Arash Vahdat","Jiaming Song","Qinsheng Zhang","Karsten Kreis","Miika Aittala","Timo Aila","Samuli Laine","Bryan Catanzaro","Tero Karras","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.01324v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v1","updated":"2023-03-14T00:13:57Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear\n  Discriminative Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminative Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07540v1","updated":"2023-03-14T00:05:08Z","published":"2023-03-14T00:05:08Z","title":"Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial\n  Wedge Pressure from Cardiac MRI","summary":"  Heart failure is a serious and life-threatening condition that can lead to\nelevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure\n(PAWP) is an important surrogate marker indicating high pressure in the left\nventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an\ninvasive procedure. A non-invasive method is useful in quickly identifying\nhigh-risk patients from a large population. In this work, we develop a tensor\nlearning-based pipeline for identifying PAWP from multimodal cardiac Magnetic\nResonance Imaging (MRI). This pipeline extracts spatial and temporal features\nfrom high-dimensional scans. For quality control, we incorporate an epistemic\nuncertainty-based binning strategy to identify poor-quality training samples.\nTo improve the performance, we learn complementary information by integrating\nfeatures from multimodal data: cardiac MRI with short-axis and four-chamber\nviews, and Electronic Health Records. The experimental analysis on a large\ncohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation\nindicates that the proposed pipeline has a diagnostic value and can produce\npromising performance with significant improvement over the baseline in\nclinical practice (i.e., $\\Delta$AUC $=0.10$, $\\Delta$Accuracy $=0.06$, and\n$\\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical\nutility of our method.\n","authors":["Prasun C. Tripathi","Mohammod N. I. Suvon","Lawrence Schobs","Shuo Zhou","Samer Alabed","Andrew J. Swift","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02131v2","updated":"2023-03-14T23:51:53Z","published":"2022-08-03T15:11:01Z","title":"Masked Vision and Language Modeling for Multi-modal Representation\n  Learning","summary":"  In this paper, we study how to use masked signal modeling in vision and\nlanguage (V+L) representation learning. Instead of developing masked language\nmodeling (MLM) and masked image modeling (MIM) independently, we propose to\nbuild joint masked vision and language modeling, where the masked signal of one\nmodality is reconstructed with the help from another modality. This is\nmotivated by the nature of image-text paired data that both of the image and\nthe text convey almost the same information but in different formats. The\nmasked signal reconstruction of one modality conditioned on another modality\ncan also implicitly learn cross-modal alignment between language tokens and\nimage patches. Our experiments on various V+L tasks show that the proposed\nmethod, along with common V+L alignment losses, achieves state-of-the-art\nperformance in the regime of millions of pre-training data. Also, we\noutperforms the other competitors by a significant margin in limited data\nscenarios.\n","authors":["Gukyeong Kwon","Zhaowei Cai","Avinash Ravichandran","Erhan Bas","Rahul Bhotika","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2208.02131v2.pdf","comment":"International Conference on Learning Representations (ICLR) 2023"},{"id":"http://arxiv.org/abs/2303.08273v1","updated":"2023-03-14T23:29:20Z","published":"2023-03-14T23:29:20Z","title":"Towards a Deep Learning Pain-Level Detection Deployment at UAE for\n  Patient-Centric-Pain Management and Diagnosis Support: Framework and\n  Performance Evaluation","summary":"  The outbreak of the COVID-19 pandemic revealed the criticality of timely\nintervention in a situation exacerbated by a shortage in medical staff and\nequipment. Pain-level screening is the initial step toward identifying the\nseverity of patient conditions. Automatic recognition of state and feelings\nhelp in identifying patient symptoms to take immediate adequate action and\nproviding a patient-centric medical plan tailored to a patient's state. In this\npaper, we propose a framework for pain-level detection for deployment in the\nUnited Arab Emirates and assess its performance using the most used approaches\nin the literature. Our results show that a deployment of a pain-level deep\nlearning detection framework is promising in identifying the pain level\naccurately.\n","authors":["Leila Ismail","Muhammad Danish Waseem"],"pdf_url":"https://arxiv.org/pdf/2303.08273v1.pdf","comment":"9 pages, conference, deep learning methods, automatic pain\n  recognition, framework, performance evaluation"},{"id":"http://arxiv.org/abs/2303.08272v1","updated":"2023-03-14T23:26:55Z","published":"2023-03-14T23:26:55Z","title":"Automated patent extraction powers generative modeling in focused\n  chemical spaces","summary":"  Deep generative models have emerged as an exciting avenue for inverse\nmolecular design, with progress coming from the interplay between training\nalgorithms and molecular representations. One of the key challenges in their\napplicability to materials science and chemistry has been the lack of access to\nsizeable training datasets with property labels. Published patents contain the\nfirst disclosure of new materials prior to their publication in journals, and\nare a vast source of scientific knowledge that has remained relatively untapped\nin the field of data-driven molecular design. Because patents are filed seeking\nto protect specific uses, molecules in patents can be considered to be weakly\nlabeled into application classes. Furthermore, patents published by the US\nPatent and Trademark Office (USPTO) are downloadable and have machine-readable\ntext and molecular structures. In this work, we train domain-specific\ngenerative models using patent data sources by developing an automated pipeline\nto go from USPTO patent digital files to the generation of novel candidates\nwith minimal human intervention. We test the approach on two in-class extracted\ndatasets, one in organic electronics and another in tyrosine kinase inhibitors.\nWe then evaluate the ability of generative models trained on these in-class\ndatasets on two categories of tasks (distribution learning and property\noptimization), identify strengths and limitations, and suggest possible\nexplanations and remedies that could be used to overcome these in practice.\n","authors":["Akshay Subramanian","Kevin Greenman","Alexis Gervaix","Tzuhsiung Yang","Rafael Gómez-Bombarelli"],"pdf_url":"https://arxiv.org/pdf/2303.08272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08271v1","updated":"2023-03-14T23:22:32Z","published":"2023-03-14T23:22:32Z","title":"Act-Then-Measure: Reinforcement Learning for Partially Observable\n  Environments with Active Measuring","summary":"  We study Markov decision processes (MDPs), where agents have direct control\nover when and how they gather information, as formalized by action-contingent\nnoiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of\ntwo components: a control action that affects the environment, and a\nmeasurement action that affects what the agent can observe. To solve ACNO-MDPs,\nwe introduce the act-then-measure (ATM) heuristic, which assumes that we can\nignore future state uncertainty when choosing control actions. We show how\nfollowing this heuristic may lead to shorter policy computation times and prove\na bound on the performance loss incurred by the heuristic. To decide whether or\nnot to take a measurement action, we introduce the concept of measuring value.\nWe develop a reinforcement learning algorithm based on the ATM heuristic, using\na Dyna-Q variant adapted for partially observable domains, and showcase its\nsuperior performance compared to prior methods on a number of\npartially-observable environments.\n","authors":["Merlijn Krale","Thiago D. Simão","Nils Jansen"],"pdf_url":"https://arxiv.org/pdf/2303.08271v1.pdf","comment":"Accecpted at ICAPS 2023"},{"id":"http://arxiv.org/abs/2303.08269v1","updated":"2023-03-14T23:16:22Z","published":"2023-03-14T23:16:22Z","title":"PULSNAR -- Positive unlabeled learning selected not at random: class\n  proportion estimation when the SCAR assumption does not hold","summary":"  Positive and Unlabeled (PU) learning is a type of semi-supervised binary\nclassification where the machine learning algorithm differentiates between a\nset of positive instances (labeled) and a set of both positive and negative\ninstances (unlabeled). PU learning has broad applications in settings where\nconfirmed negatives are unavailable or difficult to obtain, and there is value\nin discovering positives among the unlabeled (e.g., viable drugs among untested\ncompounds). Most PU learning algorithms make the selected completely at random\n(SCAR) assumption, namely that positives are selected independently of their\nfeatures. However, in many real-world applications, such as healthcare,\npositives are not SCAR (e.g., severe cases are more likely to be diagnosed),\nleading to a poor estimate of the proportion, $\\alpha$, of positives among\nunlabeled examples and poor model calibration, resulting in an uncertain\ndecision threshold for selecting positives. PU learning algorithms can estimate\n$\\alpha$ or the probability of an individual unlabeled instance being positive\nor both. We propose two PU learning algorithms to estimate $\\alpha$, calculate\ncalibrated probabilities for PU instances, and improve classification metrics:\ni) PULSCAR (positive unlabeled learning selected completely at random), and ii)\nPULSNAR (positive unlabeled learning selected not at random). PULSNAR uses a\ndivide-and-conquer approach that creates and solves several SCAR-like\nsub-problems using PULSCAR. In our experiments, PULSNAR outperformed\nstate-of-the-art approaches on both synthetic and real-world benchmark\ndatasets.\n","authors":["Praveen Kumar","Christophe G. Lambert"],"pdf_url":"https://arxiv.org/pdf/2303.08269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03023v2","updated":"2023-03-14T23:06:48Z","published":"2022-12-06T14:51:27Z","title":"FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar\n  Tablature Transcription","summary":"  In recent years, the task of Automatic Music Transcription (AMT), whereby\nvarious attributes of music notes are estimated from audio, has received\nincreasing attention. At the same time, the related task of Multi-Pitch\nEstimation (MPE) remains a challenging but necessary component of almost all\nAMT approaches, even if only implicitly. In the context of AMT, pitch\ninformation is typically quantized to the nominal pitches of the Western music\nscale. Even in more general contexts, MPE systems typically produce pitch\npredictions with some degree of quantization. In certain applications of AMT,\nsuch as Guitar Tablature Transcription (GTT), it is more meaningful to estimate\ncontinuous-valued pitch contours. Guitar tablature has the capacity to\nrepresent various playing techniques, some of which involve pitch modulation.\nContemporary approaches to AMT do not adequately address pitch modulation, and\noffer only less quantization at the expense of more model complexity. In this\npaper, we present a GTT formulation that estimates continuous-valued pitch\ncontours, grouping them according to their string and fret of origin. We\ndemonstrate that for this task, the proposed method significantly improves the\nresolution of MPE and simultaneously yields tablature estimation results\ncompetitive with baseline models.\n","authors":["Frank Cwitkowitz","Toni Hirvonen","Anssi Klapuri"],"pdf_url":"https://arxiv.org/pdf/2212.03023v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08268v1","updated":"2023-03-14T23:01:27Z","published":"2023-03-14T23:01:27Z","title":"Chat with the Environment: Interactive Multimodal Perception using Large\n  Language Models","summary":"  Programming robot behaviour in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in zero-shot robotic planning. However, it remains challenging to\nground LLMs in multimodal sensory input and continuous action output, while\nenabling a robot to interact with its environment and acquire novel information\nas its policies unfold. We develop a robot interaction scenario with a\npartially observable state, which necessitates a robot to decide on a range of\nepistemic actions in order to sample sensory information among multiple\nmodalities, before being able to execute the task correctly. An interactive\nperception framework is therefore proposed with an LLM as its backbone, whose\nability is exploited to instruct epistemic actions and to reason over the\nresulting multimodal sensations (vision, sound, haptics, proprioception), as\nwell as to plan an entire task execution based on the interactively acquired\ninformation. Our study demonstrates that LLMs can provide high-level planning\nand reasoning skills and control interactive robot behaviour in a multimodal\nenvironment, while multimodal modules with the context of the environmental\nstate help ground the LLMs and extend their processing ability.\n","authors":["Xufeng Zhao","Mengdi Li","Cornelius Weber","Muhammad Burhan Hafez","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2303.08268v1.pdf","comment":"See website at https://xf-zhao.github.io/projects/Matcha/"},{"id":"http://arxiv.org/abs/2209.01610v2","updated":"2023-03-14T22:58:26Z","published":"2022-09-04T12:48:30Z","title":"Generalization in Neural Networks: A Broad Survey","summary":"  This paper reviews concepts, modeling approaches, and recent findings along a\nspectrum of different levels of abstraction of neural network models including\ngeneralization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks,\n(5) Modalities, and (6) Scopes. Results on (1) sample generalization show that,\nin the case of ImageNet, nearly all the recent improvements reduced training\nerror while overfitting stayed flat; with nearly all the training error\neliminated, future progress will require a focus on reducing overfitting.\nPerspectives from statistics highlight how (2) distribution generalization can\nbe viewed alternately as a change in sample weights or a change in the\ninput-output relationship; thus, techniques that have been successful in domain\ngeneralization have the potential to be applied to difficult forms of sample or\ndistribution generalization. Transfer learning approaches to (3) domain\ngeneralization are summarized, as are recent advances and the wealth of domain\nadaptation benchmark datasets available. Recent breakthroughs surveyed in (4)\ntask generalization include few-shot meta-learning approaches and the BERT NLP\nengine, and recent (5) modality generalization studies are discussed that\nintegrate image and text data and that apply a biologically-inspired network\nacross olfactory, visual, and auditory modalities. Recent (6) scope\ngeneralization results are reviewed that embed knowledge graphs into deep NLP\napproaches. Additionally, concepts from neuroscience are discussed on the\nmodular architecture of brains and the steps by which dopamine-driven\nconditioning leads to abstract thinking.\n","authors":["Chris Rohlfs"],"pdf_url":"https://arxiv.org/pdf/2209.01610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.06808v3","updated":"2023-03-14T22:29:46Z","published":"2021-02-12T22:54:24Z","title":"Planning and Learning Using Adaptive Entropy Tree Search","summary":"  Recent breakthroughs in Artificial Intelligence have shown that the\ncombination of tree-based planning with deep learning can lead to superior\nperformance. We present Adaptive Entropy Tree Search (ANTS) - a novel algorithm\ncombining planning and learning in the maximum entropy paradigm. Through a\ncomprehensive suite of experiments on the Atari benchmark we show that ANTS\nsignificantly outperforms PUCT, the planning component of the state-of-the-art\nAlphaZero system. ANTS builds upon recent work on maximum entropy planning\nmethods - which however, as we show, fail in combination with learning. ANTS\nresolves this issue to reach state-of-the-art performance. We further find that\nANTS exhibits superior robustness to different hyperparameter choices, compared\nto the previous algorithms. We believe that the high performance and robustness\nof ANTS can bring tree search planning one step closer to wide practical\nadoption.\n","authors":["Piotr Kozakowski","Mikołaj Pacek","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2102.06808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08255v1","updated":"2023-03-14T22:11:34Z","published":"2023-03-14T22:11:34Z","title":"Model-to-Circuit Cross-Approximation For Printed Machine Learning\n  Classifiers","summary":"  Printed electronics (PE) promises on-demand fabrication, low non-recurring\nengineering costs, and sub-cent fabrication costs. It also allows for high\ncustomization that would be infeasible in silicon, and bespoke architectures\nprevail to improve the efficiency of emerging PE machine learning (ML)\napplications. Nevertheless, large feature sizes in PE prohibit the realization\nof complex ML models in PE, even with bespoke architectures. In this work, we\npresent an automated, cross-layer approximation framework tailored to bespoke\narchitectures that enable complex ML models, such as Multi-Layer Perceptrons\n(MLPs) and Support Vector Machines (SVMs), in PE. Our framework adopts\ncooperatively a hardware-driven coefficient approximation of the ML model at\nalgorithmic level, a netlist pruning at logic level, and a voltage over-scaling\nat the circuit level. Extensive experimental evaluation on 12 MLPs and 12 SVMs\nand more than 6000 approximate and exact designs demonstrates that our\nmodel-to-circuit cross-approximation delivers power and area optimal designs\nthat, compared to the state-of-the-art exact designs, feature on average 51%\nand 66% area and power reduction, respectively, for less than 5% accuracy loss.\nFinally, we demonstrate that our framework enables 80% of the examined\nclassifiers to be battery-powered with almost identical accuracy with the exact\ndesigns, paving thus the way towards smart complex printed applications.\n","authors":["Giorgos Armeniakos","Georgios Zervakis","Dimitrios Soudris","Mehdi B. Tahoori","Jörg Henkel"],"pdf_url":"https://arxiv.org/pdf/2303.08255v1.pdf","comment":"Accepted for publication by IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems, March 2023. arXiv admin note: text\n  overlap with arXiv:2203.05915"},{"id":"http://arxiv.org/abs/2303.08253v1","updated":"2023-03-14T21:59:21Z","published":"2023-03-14T21:59:21Z","title":"R^2: Range Regularization for Model Compression and Quantization","summary":"  Model parameter regularization is a widely used technique to improve\ngeneralization, but also can be used to shape the weight distributions for\nvarious purposes. In this work, we shed light on how weight regularization can\nassist model quantization and compression techniques, and then propose range\nregularization (R^2) to further boost the quality of model optimization by\nfocusing on the outlier prevention. By effectively regulating the minimum and\nmaximum weight values from a distribution, we mold the overall distribution\ninto a tight shape so that model compression and quantization techniques can\nbetter utilize their limited numeric representation powers. We introduce L-inf\nregularization, its extension margin regularization and a new soft-min-max\nregularization to be used as a regularization loss during full-precision model\ntraining. Coupled with state-of-the-art quantization and compression\ntechniques, models trained with R^2 perform better on an average, specifically\nat lower bit weights with 16x compression ratio. We also demonstrate that R^2\nhelps parameter constrained models like MobileNetV1 achieve significant\nimprovement of around 8% for 2 bit quantization and 7% for 1 bit compression.\n","authors":["Arnav Kundu","Chungkuk Yoo","Srijan Mishra","Minsik Cho","Saurabh Adya"],"pdf_url":"https://arxiv.org/pdf/2303.08253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08250v1","updated":"2023-03-14T21:52:27Z","published":"2023-03-14T21:52:27Z","title":"Learning to Grow Artificial Hippocampi in Vision Transformers for\n  Resilient Lifelong Learning","summary":"  Lifelong learning without catastrophic forgetting (i.e., resiliency)\npossessed by human intelligence is entangled with sophisticated memory\nmechanisms in the brain, especially the long-term memory (LM) maintained by\nHippocampi. To a certain extent, Transformers have emerged as the counterpart\n``Brain\" of Artificial Intelligence (AI), and yet leave the LM component\nunder-explored for lifelong learning settings. This paper presents a method of\nlearning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers\n(ViTs) for resilient lifelong learning. With a comprehensive ablation study,\nthe final linear projection layer in the multi-head self-attention (MHSA) block\nis selected in realizing and growing ArtiHippo. ArtiHippo is represented by a\nmixture of experts (MoEs). Each expert component is an on-site variant of the\nlinear projection layer, maintained via neural architecture search (NAS) with\nthe search space defined by four basic growing operations -- skip, reuse,\nadapt, and new in lifelong learning. The LM of a task consists of two parts:\nthe dedicated expert components (as model parameters) at different layers of a\nViT learned via NAS, and the mean class-tokens (as stored latent vectors for\nmeasuring task similarity) associated with the expert components. For a new\ntask, a hierarchical task-similarity-oriented exploration-exploitation sampling\nbased NAS is proposed to learn the expert components. The task similarity is\nmeasured based on the normalized cosine similarity between the mean class-token\nof the new task and those of old tasks. The proposed method is complementary to\nprompt-based lifelong learningwith ViTs. In experiments, the proposed method is\ntested on the challenging Visual Domain Decathlon (VDD) benchmark and the\nrecently proposed 5-Dataset benchmark. It obtains consistently better\nperformance than the prior art with sensible ArtiHippo learned continually.\n","authors":["Chinmay Savadikar","Michelle Dai","Tianfu Wu"],"pdf_url":"https://arxiv.org/pdf/2303.08250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08249v1","updated":"2023-03-14T21:51:08Z","published":"2023-03-14T21:51:08Z","title":"Systematic design space exploration by learning the explored space using\n  Machine Learning","summary":"  Current practice in parameter space exploration in euclidean space is\ndominated by randomized sampling or design of experiment methods. The biggest\nissue with these methods is not keeping track of what part of parameter space\nhas been explored and what has not. In this context, we utilize the geometric\nlearning of explored data space using modern machine learning methods to keep\ntrack of already explored regions and samples from the regions that are\nunexplored. For this purpose, we use a modified version of a robust random-cut\nforest along with other heuristic-based approaches. We demonstrate our method\nand its progression in two-dimensional Euclidean space but it can be extended\nto any dimension since the underlying method is generic.\n","authors":["Avinash Kumar","Anish Kumar","Sumit Sharma","Surjeet Singh","Kumar Vardhan"],"pdf_url":"https://arxiv.org/pdf/2303.08249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08242v1","updated":"2023-03-14T21:26:30Z","published":"2023-03-14T21:26:30Z","title":"Optimal Sampling Designs for Multi-dimensional Streaming Time Series\n  with Application to Power Grid Sensor Data","summary":"  The Internet of Things (IoT) system generates massive high-speed temporally\ncorrelated streaming data and is often connected with online inference tasks\nunder computational or energy constraints. Online analysis of these streaming\ntime series data often faces a trade-off between statistical efficiency and\ncomputational cost. One important approach to balance this trade-off is\nsampling, where only a small portion of the sample is selected for the model\nfitting and update. Motivated by the demands of dynamic relationship analysis\nof IoT system, we study the data-dependent sample selection and online\ninference problem for a multi-dimensional streaming time series, aiming to\nprovide low-cost real-time analysis of high-speed power grid electricity\nconsumption data. Inspired by D-optimality criterion in design of experiments,\nwe propose a class of online data reduction methods that achieve an optimal\nsampling criterion and improve the computational efficiency of the online\nanalysis. We show that the optimal solution amounts to a strategy that is a\nmixture of Bernoulli sampling and leverage score sampling. The leverage score\nsampling involves auxiliary estimations that have a computational advantage\nover recursive least squares updates. Theoretical properties of the auxiliary\nestimations involved are also discussed. When applied to European power grid\nconsumption data, the proposed leverage score based sampling methods outperform\nthe benchmark sampling method in online estimation and prediction. The general\napplicability of the sampling-assisted online estimation method is assessed via\nsimulation studies.\n","authors":["Rui Xie","Shuyang Bai","Ping Ma"],"pdf_url":"https://arxiv.org/pdf/2303.08242v1.pdf","comment":"Accepted by The Annals of Applied Statistics"},{"id":"http://arxiv.org/abs/2303.08230v1","updated":"2023-03-14T20:50:12Z","published":"2023-03-14T20:50:12Z","title":"Bayesian Beta-Bernoulli Process Sparse Coding with Deep Neural Networks","summary":"  Several approximate inference methods have been proposed for deep discrete\nlatent variable models. However, non-parametric methods which have previously\nbeen successfully employed for classical sparse coding models have largely been\nunexplored in the context of deep models. We propose a non-parametric iterative\nalgorithm for learning discrete latent representations in such deep models.\nAdditionally, to learn scale invariant discrete features, we propose local data\nscaling variables. Lastly, to encourage sparsity in our representations, we\npropose a Beta-Bernoulli process prior on the latent factors. We evaluate our\nspare coding model coupled with different likelihood models. We evaluate our\nmethod across datasets with varying characteristics and compare our results to\ncurrent amortized approximate inference methods.\n","authors":["Arunesh Mittal","Kai Yang","Paul Sajda","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2303.08230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08227v1","updated":"2023-03-14T20:46:05Z","published":"2023-03-14T20:46:05Z","title":"Hall effect thruster design via deep neural network for additive\n  manufacturing","summary":"  Hall effect thrusters are one of the most versatile and popular electric\npropulsion systems for space use. Industry trends towards interplanetary\nmissions arise advances in design development of such propulsion systems. It is\nunderstood that correct sizing of discharge channel in Hall effect thruster\nimpact performance greatly. Since the complete physics model of such propulsion\nsystem is not yet optimized for fast computations and design iterations, most\nthrusters are being designed using so-called scaling laws. But this work\nfocuses on rather novel approach, which is outlined less frequently than\nordinary scaling design approach in literature. Using deep machine learning it\nis possible to create predictive performance model, which can be used to\neffortlessly get design of required hall thruster with required characteristics\nusing way less computational power than design from scratch and way more\nflexible than usual scaling approach.\n","authors":["Konstantin Korolev"],"pdf_url":"https://arxiv.org/pdf/2303.08227v1.pdf","comment":"12 pages, 19 figures"},{"id":"http://arxiv.org/abs/2303.08226v1","updated":"2023-03-14T20:42:38Z","published":"2023-03-14T20:42:38Z","title":"DeepAxe: A Framework for Exploration of Approximation and Reliability\n  Trade-offs in DNN Accelerators","summary":"  While the role of Deep Neural Networks (DNNs) in a wide range of\nsafety-critical applications is expanding, emerging DNNs experience massive\ngrowth in terms of computation power. It raises the necessity of improving the\nreliability of DNN accelerators yet reducing the computational burden on the\nhardware platforms, i.e. reducing the energy consumption and execution time as\nwell as increasing the efficiency of DNN accelerators. Therefore, the trade-off\nbetween hardware performance, i.e. area, power and delay, and the reliability\nof the DNN accelerator implementation becomes critical and requires tools for\nanalysis. In this paper, we propose a framework DeepAxe for design space\nexploration for FPGA-based implementation of DNNs by considering the trilateral\nimpact of applying functional approximation on accuracy, reliability and\nhardware performance. The framework enables selective approximation of\nreliability-critical DNNs, providing a set of Pareto-optimal DNN implementation\ndesign space points for the target resource utilization requirements. The\ndesign flow starts with a pre-trained network in Keras, uses an innovative\nhigh-level synthesis environment DeepHLS and results in a set of Pareto-optimal\ndesign space points as a guide for the designer. The framework is demonstrated\nin a case study of custom and state-of-the-art DNNs and datasets.\n","authors":["Mahdi Taheri","Mohammad Riazati","Mohammad Hasan Ahmadilivani","Maksim Jenihhin","Masoud Daneshtalab","Jaan Raik","Mikael Sjodin","Bjorn Lisper"],"pdf_url":"https://arxiv.org/pdf/2303.08226v1.pdf","comment":"This paper is accepted at the 24th International Symposium on Quality\n  Electronic Design (ISQED) 2023, 8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.08224v1","updated":"2023-03-14T20:35:14Z","published":"2023-03-14T20:35:14Z","title":"Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic\n  Meta-Learning and Brain MRI","summary":"  For machine learning applications in medical imaging, the availability of\ntraining data is often limited, which hampers the design of radiological\nclassifiers for subtle conditions such as autism spectrum disorder (ASD).\nTransfer learning is one method to counter this problem of low training data\nregimes. Here we explore the use of meta-learning for very low data regimes in\nthe context of having prior data from multiple sites - an approach we term\nsite-agnostic meta-learning. Inspired by the effectiveness of meta-learning for\noptimizing a model across multiple tasks, here we propose a framework to adapt\nit to learn across multiple sites. We tested our meta-learning model for\nclassifying ASD versus typically developing controls in 2,201 T1-weighted\n(T1-w) MRI scans collected from 38 imaging sites as part of Autism Brain\nImaging Data Exchange (ABIDE) [age: 5.2-64.0 years]. The method was trained to\nfind a good initialization state for our model that can quickly adapt to data\nfrom new unseen sites by fine-tuning on the limited data that is available. The\nproposed method achieved an ROC-AUC=0.857 on 370 scans from 7 unseen sites in\nABIDE using a few-shot setting of 2-way 20-shot i.e., 20 training samples per\nsite. Our results outperformed a transfer learning baseline by generalizing\nacross a wider range of sites as well as other related prior work. We also\ntested our model in a zero-shot setting on an independent test site without any\nadditional fine-tuning. Our experiments show the promise of the proposed\nsite-agnostic meta-learning framework for challenging neuroimaging tasks\ninvolving multi-site heterogeneity with limited availability of training data.\n","authors":["Nikhil J. Dhinagar","Vignesh Santhalingam","Katherine E. Lawrence","Emily Laltoo","Paul M. Thompson"],"pdf_url":"https://arxiv.org/pdf/2303.08224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08219v1","updated":"2023-03-14T20:25:56Z","published":"2023-03-14T20:25:56Z","title":"A 2-opt Algorithm for Locally Optimal Set Partition Optimization","summary":"  Our research deals with the optimization version of the set partition\nproblem, where the objective is to minimize the absolute difference between the\nsums of the two disjoint partitions. Although this problem is known to be\nNP-hard and requires exponential time to solve, we propose a less demanding\nversion of this problem where the goal is to find a locally optimal solution.\nIn our approach, we consider the local optimality in respect to any movement of\nat most two elements. To accomplish this, we developed an algorithm that can\ngenerate a locally optimal solution in at most $O(N^2)$ time and $O(N)$ space.\nOur algorithm can handle arbitrary input precisions and does not require\npositive or integer inputs. Hence, it can be applied in various problem\nscenarios with ease.\n","authors":["Kaan Gokcesu","Hakan Gokcesu"],"pdf_url":"https://arxiv.org/pdf/2303.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.03973v2","updated":"2023-03-14T20:23:40Z","published":"2021-01-11T15:28:38Z","title":"Load Encoding for Learning AC-OPF","summary":"  The AC Optimal Power Flow (AC-OPF) problem is a core building block in\nelectrical transmission system. It seeks the most economical active and\nreactive generation dispatch to meet demands while satisfying transmission\noperational limits. It is often solved repeatedly, especially in regions with\nlarge penetration of wind farms to avoid violating operational and physical\nlimits. Recent work has shown that deep learning techniques have huge potential\nin providing accurate approximations of AC-OPF solutions. However, deep\nlearning approaches often suffer from scalability issues, especially when\napplied to real life power grids. This paper focuses on the scalability\nlimitation and proposes a load compression embedding scheme to reduce training\nmodel sizes using a 3-step approach. The approach is evaluated experimentally\non large-scale test cases from the PGLib, and produces an order of magnitude\nimprovements in training convergence and prediction accuracy.\n","authors":["Terrence W. K. Mak","Ferdinando Fioretto","Pascal VanHentenryck"],"pdf_url":"https://arxiv.org/pdf/2101.03973v2.pdf","comment":"5 pages, IEEE PES Annual Meeting version"},{"id":"http://arxiv.org/abs/2303.08216v1","updated":"2023-03-14T20:18:12Z","published":"2023-03-14T20:18:12Z","title":"Efficiently Training Vision Transformers on Structural MRI Scans for\n  Alzheimer's Disease Detection","summary":"  Neuroimaging of large populations is valuable to identify factors that\npromote or resist brain disease, and to assist diagnosis, subtyping, and\nprognosis. Data-driven models such as convolutional neural networks (CNNs) have\nincreasingly been applied to brain images to perform diagnostic and prognostic\ntasks by learning robust features. Vision transformers (ViT) - a new class of\ndeep learning architectures - have emerged in recent years as an alternative to\nCNNs for several computer vision applications. Here we tested variants of the\nViT architecture for a range of desired neuroimaging downstream tasks based on\ndifficulty, in this case for sex and Alzheimer's disease (AD) classification\nbased on 3D brain MRI. In our experiments, two vision transformer architecture\nvariants achieved an AUC of 0.987 for sex and 0.892 for AD classification,\nrespectively. We independently evaluated our models on data from two benchmark\nAD datasets. We achieved a performance boost of 5% and 9-10% upon fine-tuning\nvision transformer models pre-trained on synthetic (generated by a latent\ndiffusion model) and real MRI scans, respectively. Our main contributions\ninclude testing the effects of different ViT training strategies including\npre-training, data augmentation and learning rate warm-ups followed by\nannealing, as pertaining to the neuroimaging domain. These techniques are\nessential for training ViT-like models for neuroimaging applications where\ntraining data is usually limited. We also analyzed the effect of the amount of\ntraining data utilized on the test-time performance of the ViT via data-model\nscaling curves.\n","authors":["Nikhil J. Dhinagar","Sophia I. Thomopoulos","Emily Laltoo","Paul M. Thompson"],"pdf_url":"https://arxiv.org/pdf/2303.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08207v1","updated":"2023-03-14T19:52:09Z","published":"2023-03-14T19:52:09Z","title":"Is forgetting less a good inductive bias for forward transfer?","summary":"  One of the main motivations of studying continual learning is that the\nproblem setting allows a model to accrue knowledge from past tasks to learn new\ntasks more efficiently. However, recent studies suggest that the key metric\nthat continual learning algorithms optimize, reduction in catastrophic\nforgetting, does not correlate well with the forward transfer of knowledge. We\nbelieve that the conclusion previous works reached is due to the way they\nmeasure forward transfer. We argue that the measure of forward transfer to a\ntask should not be affected by the restrictions placed on the continual learner\nin order to preserve knowledge of previous tasks. Instead, forward transfer\nshould be measured by how easy it is to learn a new task given a set of\nrepresentations produced by continual learning on previous tasks. Under this\nnotion of forward transfer, we evaluate different continual learning algorithms\non a variety of image classification benchmarks. Our results indicate that less\nforgetful representations lead to a better forward transfer suggesting a strong\ncorrelation between retaining past information and learning efficiency on new\ntasks. Further, we found less forgetful representations to be more diverse and\ndiscriminative compared to their forgetful counterparts.\n","authors":["Jiefeng Chen","Timothy Nguyen","Dilan Gorur","Arslan Chaudhry"],"pdf_url":"https://arxiv.org/pdf/2303.08207v1.pdf","comment":"Published as a conference paper at ICLR 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07863v1","updated":"2023-03-14T12:53:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v1.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2302.10657v2","updated":"2023-03-14T11:35:27Z","published":"2023-02-21T13:19:19Z","title":"DasFormer: Deep Alternating Spectrogram Transformer for\n  Multi/Single-Channel Speech Separation","summary":"  For the task of speech separation, previous study usually treats\nmulti-channel and single-channel scenarios as two research tracks with\nspecialized solutions developed respectively. Instead, we propose a simple and\nunified architecture - DasFormer (Deep alternating spectrogram transFormer) to\nhandle both of them in the challenging reverberant environments. Unlike\nframe-wise sequence modeling, each TF-bin in the spectrogram is assigned with\nan embedding encoding spectral and spatial information. With such input,\nDasFormer is then formed by multiple repetition of simple blocks each of which\nintegrates 1) two multi-head self-attention (MHSA) modules alternately\nprocessing within each frequency bin & temporal frame of the spectrogram 2)\nMBConv before each MHSA for modeling local features on the spectrogram.\nExperiments show that DasFormer has a powerful ability to model the\ntime-frequency representation, whose performance far exceeds the current SOTA\nmodels in multi-channel speech separation, and also achieves single-channel\nSOTA in the more challenging yet realistic reverberation scenario.\n","authors":["Shuo Wang","Xiangyu Kong","Xiulian Peng","Mahmood Movassagh","Vinod Prakash","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.10657v2.pdf","comment":"5 pages, accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2303.07794v1","updated":"2023-03-14T11:09:12Z","published":"2023-03-14T11:09:12Z","title":"DiffuseRoll: Multi-track multi-category music generation based on\n  diffusion model","summary":"  Recent advancements in generative models have shown remarkable progress in\nmusic generation. However, most existing methods focus on generating monophonic\nor homophonic music, while the generation of polyphonic and multi-track music\nwith rich attributes is still a challenging task. In this paper, we propose a\nnovel approach for multi-track, multi-attribute symphonic music generation\nusing the diffusion model. Specifically, we generate piano-roll representations\nwith a diffusion model and map them to MIDI format for output. To capture rich\nattribute information, we introduce a color coding scheme to encode note\nsequences into color and position information that represents pitch,velocity,\nand instrument. This scheme enables a seamless mapping between discrete music\nsequences and continuous images. We also propose a post-processing method to\noptimize the generated scores for better performance. Experimental results show\nthat our method outperforms state-of-the-art methods in terms of polyphonic\nmusic generation with rich attribute information compared to the figure\nmethods.\n","authors":["Hongfei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07748v1","updated":"2023-03-14T09:48:59Z","published":"2023-03-14T09:48:59Z","title":"Generation-Guided Multi-Level Unified Network for Video Grounding","summary":"  Video grounding aims to locate the timestamps best matching the query\ndescription within an untrimmed video. Prevalent methods can be divided into\nmoment-level and clip-level frameworks. Moment-level approaches directly\npredict the probability of each transient moment to be the boundary in a global\nperspective, and they usually perform better in coarse grounding. On the other\nhand, clip-level ones aggregate the moments in different time windows into\nproposals and then deduce the most similar one, leading to its advantage in\nfine-grained grounding. In this paper, we propose a multi-level unified\nframework to enhance performance by leveraging the merits of both moment-level\nand clip-level methods. Moreover, a novel generation-guided paradigm in both\nlevels is adopted. It introduces a multi-modal generator to produce the\nimplicit boundary feature and clip feature, later regarded as queries to\ncalculate the boundary scores by a discriminator. The generation-guided\nsolution enhances video grounding from a two-unique-modals' match task to a\ncross-modal attention task, which steps out of the previous framework and\nobtains notable gains. The proposed Generation-guided Multi-level Unified\nnetwork (GMU) surpasses previous methods and reaches State-Of-The-Art on\nvarious benchmarks with disparate features, e.g., Charades-STA, ActivityNet\ncaptions.\n","authors":["Xing Cheng","Xiangyu Wu","Dong Shen","Hezheng Lin","Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07626v1","updated":"2023-03-14T04:50:52Z","published":"2023-03-14T04:50:52Z","title":"CAT: Causal Audio Transformer for Audio Classification","summary":"  The attention-based Transformers have been increasingly applied to audio\nclassification because of their global receptive field and ability to handle\nlong-term dependency. However, the existing frameworks which are mainly\nextended from the Vision Transformers are not perfectly compatible with audio\nsignals. In this paper, we introduce a Causal Audio Transformer (CAT)\nconsisting of a Multi-Resolution Multi-Feature (MRMF) feature extraction with\nan acoustic attention block for more optimized audio modeling. In addition, we\npropose a causal module that alleviates over-fitting, helps with knowledge\ntransfer, and improves interpretability. CAT obtains higher or comparable\nstate-of-the-art classification performance on ESC50, AudioSet and UrbanSound8K\ndatasets, and can be easily generalized to other Transformer-based models.\n","authors":["Xiaoyu Liu","Hanlin Lu","Jianbo Yuan","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2303.07626v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.14402v3","updated":"2023-03-14T01:41:44Z","published":"2023-02-28T08:35:50Z","title":"Neural Video Compression with Diverse Contexts","summary":"  For any video codecs, the coding efficiency highly relies on whether the\ncurrent signal to be encoded can find the relevant contexts from the previous\nreconstructed signals. Traditional codec has verified more contexts bring\nsubstantial coding gain, but in a time-consuming manner. However, for the\nemerging neural video codec (NVC), its contexts are still limited, leading to\nlow compression ratio. To boost NVC, this paper proposes increasing the context\ndiversity in both temporal and spatial dimensions. First, we guide the model to\nlearn hierarchical quality patterns across frames, which enriches long-term and\nyet high-quality temporal contexts. Furthermore, to tap the potential of\noptical flow-based coding framework, we introduce a group-based offset\ndiversity where the cross-group interaction is proposed for better context\nmining. In addition, this paper also adopts a quadtree-based partition to\nincrease spatial context diversity when encoding the latent representation in\nparallel. Experiments show that our codec obtains 23.5% bitrate saving over\nprevious SOTA NVC. Better yet, our codec has surpassed the under-developing\nnext generation traditional codec/ECM in both RGB and YUV420 colorspaces, in\nterms of PSNR. The codes are at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.14402v3.pdf","comment":"Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC"},{"id":"http://arxiv.org/abs/2303.05007v2","updated":"2023-03-14T20:10:00Z","published":"2023-03-09T03:16:04Z","title":"Towards Robust Image-in-Audio Deep Steganography","summary":"  The field of steganography has experienced a surge of interest due to the\nrecent advancements in AI-powered techniques, particularly in the context of\nmultimodal setups that enable the concealment of signals within signals of a\ndifferent nature. The primary objectives of all steganographic methods are to\nachieve perceptual transparency, robustness, and large embedding capacity -\nwhich often present conflicting goals that classical methods have struggled to\nreconcile. This paper extends and enhances an existing image-in-audio deep\nsteganography method by focusing on improving its robustness. The proposed\nenhancements include modifications to the loss function, utilization of the\nShort-Time Fourier Transform (STFT), introduction of redundancy in the encoding\nprocess for error correction, and buffering of additional information in the\npixel subconvolution operation. The results demonstrate that our approach\noutperforms the existing method in terms of robustness and perceptual\ntransparency.\n","authors":["Jaume Ros","Margarita Geleta","Jordi Pons","Xavier Giro-i-Nieto"],"pdf_url":"https://arxiv.org/pdf/2303.05007v2.pdf","comment":"8 pages, 5 figures, 2 tables"}]},"2023-03-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.08809v1","updated":"2023-03-15T17:57:22Z","published":"2023-03-15T17:57:22Z","title":"Cascading and Direct Approaches to Unsupervised Constituency Parsing on\n  Spoken Sentences","summary":"  Past work on unsupervised parsing is constrained to written form. In this\npaper, we present the first study on unsupervised spoken constituency parsing\ngiven unlabeled spoken sentences and unpaired textual data. The goal is to\ndetermine the spoken sentences' hierarchical syntactic structure in the form of\nconstituency parse trees, such that each node is a span of audio that\ncorresponds to a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an unsupervised\nparser to obtain parse trees on ASR transcripts, and (2) direct training an\nunsupervised parser on continuous word-level speech representations. This is\ndone by first splitting utterances into sequences of word-level segments, and\naggregating self-supervised speech representations within segments to obtain\nsegment embeddings. We find that separately training a parser on the unpaired\ntext and directly applying it on ASR transcripts for inference produces better\nresults for unsupervised parsing. Additionally, our results suggest that\naccurate segmentation alone may be sufficient to parse spoken sentences\naccurately. Finally, we show the direct approach may learn head-directionality\ncorrectly for both head-initial and head-final languages without any explicit\ninductive bias.\n","authors":["Yuan Tseng","Cheng-I Lai","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2303.08809v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08774v1","updated":"2023-03-15T17:15:04Z","published":"2023-03-15T17:15:04Z","title":"GPT-4 Technical Report","summary":"  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n","authors":[" OpenAI"],"pdf_url":"https://arxiv.org/pdf/2303.08774v1.pdf","comment":"99 pages"},{"id":"http://arxiv.org/abs/2209.07496v2","updated":"2023-03-15T17:08:31Z","published":"2022-09-15T17:37:08Z","title":"Unsupervised Opinion Summarization Using Approximate Geodesics","summary":"  Opinion summarization is the task of creating summaries capturing popular\nopinions from user reviews. In this paper, we introduce Geodesic Summarizer\n(GeoSumm), a novel system to perform unsupervised extractive opinion\nsummarization. GeoSumm involves an encoder-decoder based representation\nlearning model, that generates representations of text as a distribution over\nlatent semantic units. GeoSumm generates these representations by performing\ndictionary learning over pre-trained text representations at multiple decoder\nlayers. We then use these representations to quantify the relevance of review\nsentences using a novel approximate geodesic distance based scoring mechanism.\nWe use the relevance scores to identify popular opinions in order to compose\ngeneral and aspect-specific summaries. Our proposed model, GeoSumm, achieves\nstate-of-the-art performance on three opinion summarization datasets. We\nperform additional experiments to analyze the functioning of our model and\nshowcase the generalization ability of {\\X} across different domains.\n","authors":["Somnath Basu Roy Chowdhury","Nicholas Monath","Avinava Dubey","Amr Ahmed","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2209.07496v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2301.08104v2","updated":"2023-03-15T16:26:17Z","published":"2023-01-19T14:50:36Z","title":"Author as Character and Narrator: Deconstructing Personal Narratives\n  from the r/AmITheAsshole Reddit Community","summary":"  In the r/AmITheAsshole subreddit, people anonymously share first person\nnarratives that contain some moral dilemma or conflict and ask the community to\njudge who is at fault (i.e., who is \"the asshole\"). In general, first person\nnarratives are a unique storytelling domain where the author is the narrator\n(the person telling the story) but can also be a character (the person living\nthe story) and, thus, the author has two distinct voices presented in the\nstory. In this study, we identify linguistic and narrative features associated\nwith the author as the character or as a narrator. We use these features to\nanswer the following questions: (1) what makes an asshole character and (2)\nwhat makes an asshole narrator? We extract both Author-as-Character features\n(e.g., demographics, narrative event chain, and emotional arc) and\nAuthor-as-Narrator features (i.e., the style and emotion of the story as a\nwhole) in order to identify which aspects of the narrative are correlated with\nthe final moral judgment. Our work shows that \"assholes\" as Characters frame\nthemselves as lacking agency with a more positive personal arc, while\n\"assholes\" as Narrators will tell emotional and opinionated stories.\n","authors":["Salvatore Giorgi","Ke Zhao","Alexander H. Feng","Lara J. Martin"],"pdf_url":"https://arxiv.org/pdf/2301.08104v2.pdf","comment":"Accepted to the 17th International AAAI Conference on Web and Social\n  Media (ICWSM), 2023"},{"id":"http://arxiv.org/abs/2303.07519v2","updated":"2023-03-15T16:07:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100% rate.\nAccuracy shows great improvement when scaling the models, with the largest\nmodel (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for\ndifferent prompt categories. We open source the finetuned Architext models and\nour synthetic dataset, hoping to inspire experimentation in this exciting area\nof design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08721v1","updated":"2023-03-15T16:05:11Z","published":"2023-03-15T16:05:11Z","title":"Artificial Influence: An Analysis Of AI-Driven Persuasion","summary":"  Persuasion is a key aspect of what it means to be human, and is central to\nbusiness, politics, and other endeavors. Advancements in artificial\nintelligence (AI) have produced AI systems that are capable of persuading\nhumans to buy products, watch videos, click on search results, and more. Even\nsystems that are not explicitly designed to persuade may do so in practice. In\nthe future, increasingly anthropomorphic AI systems may form ongoing\nrelationships with users, increasing their persuasive power. This paper\ninvestigates the uncertain future of persuasive AI systems. We examine ways\nthat AI could qualitatively alter our relationship to and views regarding\npersuasion by shifting the balance of persuasive power, allowing personalized\npersuasion to be deployed at scale, powering misinformation campaigns, and\nchanging the way humans can shape their own discourse. We consider ways\nAI-driven persuasion could differ from human-driven persuasion. We warn that\nubiquitous highlypersuasive AI systems could alter our information environment\nso significantly so as to contribute to a loss of human control of our own\nfuture. In response, we examine several potential responses to AI-driven\npersuasion: prohibition, identification of AI agents, truthful AI, and legal\nremedies. We conclude that none of these solutions will be airtight, and that\nindividuals and governments will need to take active steps to guard against the\nmost pernicious effects of persuasive AI.\n","authors":["Matthew Burtell","Thomas Woodside"],"pdf_url":"https://arxiv.org/pdf/2303.08721v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.08697v1","updated":"2023-03-15T15:31:51Z","published":"2023-03-15T15:31:51Z","title":"Mirror: A Natural Language Interface for Data Querying, Summarization,\n  and Visualization","summary":"  We present Mirror, an open-source platform for data exploration and analysis\npowered by large language models. Mirror offers an intuitive natural language\ninterface for querying databases, and automatically generates executable SQL\ncommands to retrieve relevant data and summarize it in natural language. In\naddition, users can preview and manually edit the generated SQL commands to\nensure the accuracy of their queries. Mirror also generates visualizations to\nfacilitate understanding of the data. Designed with flexibility and human input\nin mind, Mirror is suitable for both experienced data analysts and\nnon-technical professionals looking to gain insights from their data.\n","authors":["Canwen Xu","Julian McAuley","Penghan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08697v1.pdf","comment":"The Web Conference (WWW 2023) Demo"},{"id":"http://arxiv.org/abs/2302.14680v2","updated":"2023-03-15T14:38:21Z","published":"2023-02-28T15:45:20Z","title":"Which One Are You Referring To? Multimodal Object Identification in\n  Situated Dialogue","summary":"  The demand for multimodal dialogue systems has been rising in various\ndomains, emphasizing the importance of interpreting multimodal inputs from\nconversational and situational contexts. We explore three methods to tackle\nthis problem and evaluate them on the largest situated dialogue dataset, SIMMC\n2.1. Our best method, scene-dialogue alignment, improves the performance by\n~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and\ndiscussion regarding the limitation of our methods and the potential directions\nfor future works. Our code is publicly available at\nhttps://github.com/holylovenia/multimodal-object-identification.\n","authors":["Holy Lovenia","Samuel Cahyawijaya","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2302.14680v2.pdf","comment":"Accepted at EACL SRW 2023"},{"id":"http://arxiv.org/abs/2302.01973v2","updated":"2023-03-15T14:36:49Z","published":"2023-02-03T19:47:22Z","title":"Measuring The Impact Of Programming Language Distribution","summary":"  Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.\n","authors":["Gabriel Orlanski","Kefan Xiao","Xavier Garcia","Jeffrey Hui","Joshua Howland","Jonathan Malmaud","Jacob Austin","Rishah Singh","Michele Catasta"],"pdf_url":"https://arxiv.org/pdf/2302.01973v2.pdf","comment":"Code and data release: https://github.com/google-research/babelcode"},{"id":"http://arxiv.org/abs/2303.08652v1","updated":"2023-03-15T14:32:00Z","published":"2023-03-15T14:32:00Z","title":"Automated Query Generation for Evidence Collection from Web Search\n  Engines","summary":"  It is widely accepted that so-called facts can be checked by searching for\ninformation on the Internet. This process requires a fact-checker to formulate\na search query based on the fact and to present it to a search engine. Then,\nrelevant and believable passages need to be identified in the search results\nbefore a decision is made. This process is carried out by sub-editors at many\nnews and media organisations on a daily basis. Here, we ask the question as to\nwhether it is possible to automate the first step, that of query generation.\nCan we automatically formulate search queries based on factual statements which\nare similar to those formulated by human experts? Here, we consider similarity\nboth in terms of textual similarity and with respect to relevant documents\nbeing returned by a search engine. First, we introduce a moderate-sized\nevidence collection dataset which includes 390 factual statements together with\nassociated human-generated search queries and search results. Then, we\ninvestigate generating queries using a number of rule-based and automatic text\ngeneration methods based on pre-trained large language models (LLMs). We show\nthat these methods have different merits and propose a hybrid approach which\nhas superior performance in practice.\n","authors":["Nestor Prieto-Chavana","Julie Weeds","David Weir"],"pdf_url":"https://arxiv.org/pdf/2303.08652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11799v2","updated":"2023-03-15T14:31:56Z","published":"2023-02-23T06:25:51Z","title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question\n  Answering","summary":"  Knowledge-aware question answering (KAQA) requires the model to answer\nquestions over a knowledge base, which is essential for both open-domain QA and\ndomain-specific QA, especially when language models alone cannot provide all\nthe knowledge needed. Despite the promising result of recent KAQA systems which\ntend to integrate linguistic knowledge from pre-trained language models (PLM)\nand factual knowledge from knowledge graphs (KG) to answer complex questions, a\nbottleneck exists in effectively fusing the representations from PLMs and KGs\nbecause of (i) the semantic and distributional gaps between them, and (ii) the\ndifficulties in joint reasoning over the provided knowledge from both\nmodalities. To address the above two problems, we propose a Fine-grained\nTwo-stage training framework (FiTs) to boost the KAQA system performance: The\nfirst stage aims at aligning representations from the PLM and the KG, thus\nbridging the modality gaps between them, named knowledge adaptive\npost-training. The second stage, called knowledge-aware fine-tuning, aims to\nimprove the model's joint reasoning ability based on the aligned\nrepresentations. In detail, we fine-tune the post-trained model via two\nauxiliary self-supervised tasks in addition to the QA supervision. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,\nOpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.\n","authors":["Qichen Ye","Bowen Cao","Nuo Chen","Weiyuan Xu","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2302.11799v2.pdf","comment":"Accepted in AAAI 2023, oral"},{"id":"http://arxiv.org/abs/2303.08606v1","updated":"2023-03-15T13:26:25Z","published":"2023-03-15T13:26:25Z","title":"On the Calibration and Uncertainty with Pólya-Gamma Augmentation for\n  Dialog Retrieval Models","summary":"  Deep neural retrieval models have amply demonstrated their power but\nestimating the reliability of their predictions remains challenging. Most\ndialog response retrieval models output a single score for a response on how\nrelevant it is to a given question. However, the bad calibration of deep neural\nnetwork results in various uncertainty for the single score such that the\nunreliable predictions always misinform user decisions. To investigate these\nissues, we present an efficient calibration and uncertainty estimation\nframework PG-DRR for dialog response retrieval models which adds a Gaussian\nProcess layer to a deterministic deep neural network and recovers conjugacy for\ntractable posterior inference by P\\'{o}lya-Gamma augmentation. Finally, PG-DRR\nachieves the lowest empirical calibration error (ECE) in the in-domain datasets\nand the distributional shift task while keeping $R_{10}@1$ and MAP performance.\n","authors":["Tong Ye","Shijing Si","Jianzong Wang","Ning Cheng","Zhitao Li","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.08606v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08601v1","updated":"2023-03-15T13:15:22Z","published":"2023-03-15T13:15:22Z","title":"GCRE-GPT: A Generative Model for Comparative Relation Extraction","summary":"  Given comparative text, comparative relation extraction aims to extract two\ntargets (\\eg two cameras) in comparison and the aspect they are compared for\n(\\eg image quality). The extracted comparative relations form the basis of\nfurther opinion analysis.Existing solutions formulate this task as a sequence\nlabeling task, to extract targets and aspects. However, they cannot directly\nextract comparative relation(s) from text. In this paper, we show that\ncomparative relations can be directly extracted with high accuracy, by\ngenerative model. Based on GPT-2, we propose a Generation-based Comparative\nRelation Extractor (GCRE-GPT). Experiment results show that \\modelname achieves\nstate-of-the-art accuracy on two datasets.\n","authors":["Yequan Wang","Hengran Zhang","Aixin Sun","Xuying Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08599v1","updated":"2023-03-15T13:12:16Z","published":"2023-03-15T13:12:16Z","title":"Efficient Uncertainty Estimation with Gaussian Process for Reliable\n  Dialog Response Retrieval","summary":"  Deep neural networks have achieved remarkable performance in retrieval-based\ndialogue systems, but they are shown to be ill calibrated. Though basic\ncalibration methods like Monte Carlo Dropout and Ensemble can calibrate well,\nthese methods are time-consuming in the training or inference stages. To tackle\nthese challenges, we propose an efficient uncertainty calibration framework\nGPF-BERT for BERT-based conversational search, which employs a Gaussian Process\nlayer and the focal loss on top of the BERT architecture to achieve a\nhigh-quality neural ranker. Extensive experiments are conducted to verify the\neffectiveness of our method. In comparison with basic calibration methods,\nGPF-BERT achieves the lowest empirical calibration error (ECE) in three\nin-domain datasets and the distributional shift tasks, while yielding the\nhighest $R_{10}@1$ and MAP performance on most cases. In terms of time\nconsumption, our GPF-BERT has an 8$\\times$ speedup.\n","authors":["Tong Ye","Zhitao Li","Jianzong Wang","Ning Cheng","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.08599v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08559v1","updated":"2023-03-15T12:20:13Z","published":"2023-03-15T12:20:13Z","title":"Large Language Model Is Not a Good Few-shot Information Extractor, but a\n  Good Reranker for Hard Samples!","summary":"  Large Language Models (LLMs) have made remarkable strides in various tasks.\nHowever, whether they are competitive few-shot solvers for information\nextraction (IE) tasks and surpass fine-tuned small Pre-trained Language Models\n(SLMs) remains an open problem. This paper aims to provide a thorough answer to\nthis problem, and moreover, to explore an approach towards effective and\neconomical IE systems that combine the strengths of LLMs and SLMs. Through\nextensive experiments on eight datasets across three IE tasks, we show that\nLLMs are not effective few-shot information extractors in general, given their\nunsatisfactory performance in most settings and the high latency and budget\nrequirements. However, we demonstrate that LLMs can well complement SLMs and\neffectively solve hard samples that SLMs struggle with. Building on these\nfindings, we propose an adaptive filter-then-rerank paradigm, in which SLMs act\nas filters and LLMs act as rerankers. By utilizing LLMs to rerank a small\nportion of difficult samples identified by SLMs, our preliminary system\nconsistently achieves promising improvements (2.1% F1-gain on average) on\nvarious IE tasks, with acceptable cost of time and money.\n","authors":["Yubo Ma","Yixin Cao","YongChing Hong","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2303.08559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.10076v2","updated":"2023-03-15T12:19:58Z","published":"2021-06-18T11:54:33Z","title":"Label prompt for multi-label text classification","summary":"  One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.\n","authors":["Rui Song","Xingbing Chen","Zelong Liu","Haining An","Zhiqi Zhang","Xiaoguang Wang","Hao Xu"],"pdf_url":"https://arxiv.org/pdf/2106.10076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08553v1","updated":"2023-03-15T12:13:30Z","published":"2023-03-15T12:13:30Z","title":"The Image of the Process Interpretation of Regular Expressions is Not\n  Closed under Bisimulation Collapse","summary":"  Axiomatization and expressibility problems for Milner's process semantics\n(1984) of regular expressions modulo bisimilarity have turned out to be\ndifficult for the full class of expressions with deadlock 0 and empty step~1.\nWe report on a phenomenon that arises from the added presence of 1 when 0 is\navailable, and that brings a crucial reason for this difficulty into focus. To\nwit, while interpretations of 1-free regular expressions are closed under\nbisimulation collapse, this is not the case for the interpretations of\narbitrary regular expressions.\n  Process graph interpretations of 1-free regular expressions satisfy the loop\nexistence and elimination property LEE, which is preserved under bisimulation\ncollapse. These features of LEE were applied for showing that an equational\nproof system for 1-free regular expressions modulo bisimilarity is complete,\nand that it is decidable in polynomial time whether a process graph is\nbisimilar to the interpretation of a 1-free regular expression.\n  While interpretations of regular expressions do not satisfy the property LEE\nin general, we show that LEE can be recovered by refined interpretations as\ngraphs with 1-transitions refined interpretations with 1-transitions (which are\nsimilar to silent steps for automata). This suggests that LEE can be expedient\nalso for the general axiomatization and expressibility problems. But a new\nphenomenon emerges that needs to be addressed: the property of a process graph\n`to can be refined into a process graph with 1-transitions and with LEE' is not\npreserved under bisimulation collapse. We provide a 10-vertex graph with two\n1-transitions that satisfies LEE, and in which a pair of bisimilar vertices\ncannot be collapsed on to each other while preserving the refinement property.\nThis implies that the image of the process interpretation of regular\nexpressions is not closed under bisimulation collapse.\n","authors":["Clemens Grabmayer"],"pdf_url":"https://arxiv.org/pdf/2303.08553v1.pdf","comment":"Report (14 pages, 9 pages appendix) written for a submission in\n  January 2021 (now slightly adapted, with added explanation of the relation\n  with subsequent work that was published earlier) concerning the crucial\n  observation underlying the crystallization process described in\n  arXiv:2209.12188"},{"id":"http://arxiv.org/abs/2303.08518v1","updated":"2023-03-15T10:53:49Z","published":"2023-03-15T10:53:49Z","title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation","summary":"  Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs.\n","authors":["Daixuan Cheng","Shaohan Huang","Junyu Bi","Yuefeng Zhan","Jianfeng Liu","Yujing Wang","Hao Sun","Furu Wei","Denvy Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15447v2","updated":"2023-03-15T10:52:03Z","published":"2022-10-27T14:09:48Z","title":"Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised\n  Learning for Text-To-Speech","summary":"  This paper proposes Virtuoso, a massively multilingual speech-text joint\nsemi-supervised learning framework for text-to-speech synthesis (TTS) models.\nExisting multilingual TTS typically supports tens of languages, which are a\nsmall fraction of the thousands of languages in the world. One difficulty to\nscale multilingual TTS to hundreds of languages is collecting high-quality\nspeech-text paired data in low-resource languages. This study extends Maestro,\na speech-text joint pretraining framework for automatic speech recognition\n(ASR), to speech generation tasks. To train a TTS model from various types of\nspeech and text data, different training schemes are designed to handle\nsupervised (paired TTS and ASR data) and unsupervised (untranscribed speech and\nunspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS\nmodels trained on Virtuoso can achieve significantly better naturalness and\nintelligibility than baseline ones in seen languages, and 2) they can\nsynthesize reasonably intelligible and naturally sounding speech for unseen\nlanguages where no high-quality paired TTS data is available.\n","authors":["Takaaki Saeki","Heiga Zen","Zhehuai Chen","Nobuyuki Morioka","Gary Wang","Yu Zhang","Ankur Bapna","Andrew Rosenberg","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2210.15447v2.pdf","comment":"To appear in ICASSP 2023"},{"id":"http://arxiv.org/abs/2201.09227v2","updated":"2023-03-15T09:23:54Z","published":"2022-01-23T11:17:53Z","title":"A Large and Diverse Arabic Corpus for Language Modeling","summary":"  Language models (LMs) have introduced a major paradigm shift in Natural\nLanguage Processing (NLP) modeling where large pre-trained LMs became integral\nto most of the NLP tasks. The LMs are intelligent enough to find useful and\nrelevant representations of the language without any supervision. Perhaps,\nthese models are used to fine-tune typical NLP tasks with significantly high\naccuracy as compared to the traditional approaches. Conversely, the training of\nthese models requires a massively large corpus that is a good representation of\nthe language. English LMs generally perform better than their other language\ncounterparts, due to the availability of massive English corpora. This work\nelaborates on the design and development of a large Arabic corpus. It consists\nof over 500 GB of Arabic cleaned text targeted at improving cross-domain\nknowledge and downstream generalization capability of large-scale language\nmodels. Moreover, the corpus is utilized in the training of a large Arabic LM.\nIn order to evaluate the effectiveness of the LM, a number of typical NLP tasks\nare fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when\ncompared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my\nknowledge, this is currently the largest clean and diverse Arabic corpus ever\ncollected.\n","authors":["Abbas Raza Ali","Muhammad Ajmal Siddiqui","Rema Algunaibet","Hasan Raza Ali"],"pdf_url":"https://arxiv.org/pdf/2201.09227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08448v1","updated":"2023-03-15T08:44:07Z","published":"2023-03-15T08:44:07Z","title":"A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP\n  Algorithms on Electronic Health Records","summary":"  Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.\n","authors":["Sicheng Zhou","Nan Wang","Liwei Wang","Ju Sun","Anne Blaes","Hongfang Liu","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08448v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.15115v2","updated":"2023-03-15T07:35:09Z","published":"2022-11-28T08:05:45Z","title":"Generalized Category Discovery with Decoupled Prototypical Network","summary":"  Generalized Category Discovery (GCD) aims to recognize both known and novel\ncategories from a set of unlabeled data, based on another dataset labeled with\nonly known categories. Without considering differences between known and novel\ncategories, current methods learn about them in a coupled manner, which can\nhurt model's generalization and discriminative ability. Furthermore, the\ncoupled training approach prevents these models transferring category-specific\nknowledge explicitly from labeled data to unlabeled data, which can lose\nhigh-level semantic information and impair model performance. To mitigate above\nlimitations, we present a novel model called Decoupled Prototypical Network\n(DPN). By formulating a bipartite matching problem for category prototypes, DPN\ncan not only decouple known and novel categories to achieve different training\ntargets effectively, but also align known categories in labeled and unlabeled\ndata to transfer category-specific knowledge explicitly and capture high-level\nsemantics. Furthermore, DPN can learn more discriminative features for both\nknown and novel categories through our proposed Semantic-aware Prototypical\nLearning (SPL). Besides capturing meaningful semantic information, SPL can also\nalleviate the noise of hard pseudo labels through semantic-weighted soft\nassignment. Extensive experiments show that DPN outperforms state-of-the-art\nmodels by a large margin on all evaluation metrics across multiple benchmark\ndatasets. Code and data are available at https://github.com/Lackel/DPN.\n","authors":["Wenbin An","Feng Tian","Qinghua Zheng","Wei Ding","QianYing Wang","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15115v2.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2211.02332v3","updated":"2023-03-15T07:07:24Z","published":"2022-11-04T09:19:13Z","title":"Once-for-All Sequence Compression for Self-Supervised Speech Models","summary":"  The sequence length along the time axis is often the dominant factor of the\ncomputation in speech processing. Works have been proposed to reduce the\nsequence length for lowering the computational cost in self-supervised speech\nmodels. However, different downstream tasks have different tolerance of\nsequence compressing, so a model that produces a fixed compressing rate may not\nfit all tasks. In this work, we introduce a once-for-all (OFA) sequence\ncompression framework for self-supervised speech models that supports a\ncontinuous range of operating compressing rates. The framework is evaluated on\nvarious tasks, showing marginal degradation compared to the fixed compressing\nrate variants with a smooth performance-efficiency trade-off. We further\nexplore adaptive compressing rate learning, demonstrating the ability to select\ntask-specific preferred frame periods without needing a grid search.\n","authors":["Hsuan-Jui Chen","Yen Meng","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2211.02332v3.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.05892v2","updated":"2023-03-15T06:57:46Z","published":"2022-10-12T03:13:28Z","title":"Perplexity from PLM Is Unreliable for Evaluating Text Quality","summary":"  Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality\nof the generated text. They suppose that if the value of PPL is smaller, the\nquality(i.e. fluency) of the text to be evaluated is better. However, we find\nthat the PPL referee is unqualified and it cannot evaluate the generated text\nfairly for the following reasons: (i) The PPL of short text is larger than long\ntext, which goes against common sense, (ii) The repeated text span could damage\nthe performance of PPL, and (iii) The punctuation marks could affect the\nperformance of PPL heavily. Experiments show that the PPL is unreliable for\nevaluating the quality of given text. Last, we discuss the key problems with\nevaluating text quality using language models.\n","authors":["Yequan Wang","Jiawen Deng","Aixin Sun","Xuying Meng"],"pdf_url":"https://arxiv.org/pdf/2210.05892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08389v1","updated":"2023-03-15T06:37:26Z","published":"2023-03-15T06:37:26Z","title":"PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning","summary":"  Vulnerability to lexical perturbation is a critical weakness of automatic\nevaluation metrics for image captioning. This paper proposes Perturbation\nRobust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such\nperturbations, as a novel reference-free image captioning metric applicable to\nmultiple languages. To achieve perturbation robustness, we fine-tune the text\nencoder of CLIP with our language-agnostic method to distinguish the perturbed\ntext from the original text. To verify the robustness of PR-MCS, we introduce a\nnew fine-grained evaluation dataset consisting of detailed captions, critical\nobjects, and the relationships between the objects for 3, 000 images in five\nlanguages. In our experiments, PR-MCS significantly outperforms baseline\nmetrics in capturing lexical noise of all various perturbation types in all\nfive languages, proving that PR-MCS is highly robust to lexical perturbations.\n","authors":["Yongil Kim","Yerin Hwang","Hyeongu Yun","Seunghyun Yoon","Trung Bui","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2303.08389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.03454v3","updated":"2023-03-15T06:28:19Z","published":"2022-10-07T10:54:49Z","title":"DABERT: Dual Attention Enhanced BERT for Semantic Matching","summary":"  Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.\n","authors":["Sirui Wang","Di Liang","Jian Song","Yuntao Li","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2210.03454v3.pdf","comment":"Accepted by COLING 2022"},{"id":"http://arxiv.org/abs/2210.08471v3","updated":"2023-03-15T04:29:17Z","published":"2022-10-16T07:17:27Z","title":"Improving Semantic Matching through Dependency-Enhanced Pre-trained\n  Model with Adaptive Fusion","summary":"  Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.\n","authors":["Jian Song","Di Liang","Rumei Li","Yuntao Li","Sirui Wang","Minlong Peng","Wei Wu","Yongxin Yu"],"pdf_url":"https://arxiv.org/pdf/2210.08471v3.pdf","comment":"Accepted by Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2211.14865v2","updated":"2023-03-15T03:54:14Z","published":"2022-11-27T15:48:14Z","title":"Understanding BLOOM: An empirical study on diverse NLP tasks","summary":"  We view the landscape of large language models (LLMs) through the lens of the\nrecently released BLOOM model to understand the performance of BLOOM and other\ndecoder-only LLMs compared to BERT-style encoder-only models. We achieve this\nby evaluating the smaller BLOOM model variants (\\textit{350m/560m} and\n\\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards.\nWe make the following observations: (1) BLOOM performance does not scale with\nparameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning\nBLOOM models show that the 560m variant performs similarly to or better than\nthe 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning\nexperiments show that BLOOM is at par or worse than monolingual GPT-2 models,\nand (3) Toxicity analysis of prompt-based text generation using the\nRealToxicityPrompts dataset shows that the text generated by BLOOM is at least\n17\\% less toxic than GPT-2 and GPT-3 models.\n","authors":["Parag Pravin Dakle","SaiKrishna Rallabandi","Preethi Raghavan"],"pdf_url":"https://arxiv.org/pdf/2211.14865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08335v1","updated":"2023-03-15T02:51:57Z","published":"2023-03-15T02:51:57Z","title":"FactReranker: Fact-guided Reranker for Faithful Radiology Report\n  Summarization","summary":"  Automatic radiology report summarization is a crucial clinical task, whose\nkey challenge is to maintain factual accuracy between produced summaries and\nground truth radiology findings. Existing research adopts reinforcement\nlearning to directly optimize factual consistency metrics such as CheXBert or\nRadGraph score. However, their decoding method using greedy search or beam\nsearch considers no factual consistency when picking the optimal candidate,\nleading to limited factual consistency improvement. To address it, we propose a\nnovel second-stage summarizing approach FactReranker, the first attempt that\nlearns to choose the best summary from all candidates based on their estimated\nfactual consistency score. We propose to extract medical facts of the input\nmedical report, its gold summary, and candidate summaries based on the RadGraph\nschema and design the fact-guided reranker to efficiently incorporate the\nextracted medical facts for selecting the optimal summary. We decompose the\nfact-guided reranker into the factual knowledge graph generation and the\nfactual scorer, which allows the reranker to model the mapping between the\nmedical facts of the input text and its gold summary, thus can select the\noptimal summary even the gold summary can't be observed during inference. We\nalso present a fact-based ranking metric (RadMRR) for measuring the ability of\nthe reranker on selecting factual consistent candidates. Experimental results\non two benchmark datasets demonstrate the superiority of our method in\ngenerating summaries with higher factual consistency scores when compared with\nexisting methods.\n","authors":["Qianqian Xie","Jinpeng Hu","Jiayu Zhou","Yifan Peng","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08335v1.pdf","comment":"11 pages, 5 figures, 7 tables, submitted to KDD 2023"},{"id":"http://arxiv.org/abs/2303.08329v1","updated":"2023-03-15T02:34:03Z","published":"2023-03-15T02:34:03Z","title":"Cross-speaker Emotion Transfer by Manipulating Speech Style Latents","summary":"  In recent years, emotional text-to-speech has shown considerable progress.\nHowever, it requires a large amount of labeled data, which is not easily\naccessible. Even if it is possible to acquire an emotional speech dataset,\nthere is still a limitation in controlling emotion intensity. In this work, we\npropose a novel method for cross-speaker emotion transfer and manipulation\nusing vector arithmetic in latent style space. By leveraging only a few labeled\nsamples, we generate emotional speech from reading-style speech without losing\nthe speaker identity. Furthermore, emotion strength is readily controllable\nusing a scalar value, providing an intuitive way for users to manipulate\nspeech. Experimental results show the proposed method affords superior\nperformance in terms of expressiveness, naturalness, and controllability,\npreserving speaker identity.\n","authors":["Suhee Jo","Younggun Lee","Yookyung Shin","Yeongtae Hwang","Taesu Kim"],"pdf_url":"https://arxiv.org/pdf/2303.08329v1.pdf","comment":"accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08302v1","updated":"2023-03-15T01:27:15Z","published":"2023-03-15T01:27:15Z","title":"A Comprehensive Study on Post-Training Quantization for Large Language\n  Models","summary":"  Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce the memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study on those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bits is similar to 5-bits). We also\npresent recommendations about how to utilize quantization for \\llms with\ndifferent sizes, and leave suggestions of future opportunities and system work\nthat are not resolved in this work.\n","authors":["Zhewei Yao","Cheng Li","Xiaoxia Wu","Stephen Youn","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.08302v1.pdf","comment":"25 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.08290v1","updated":"2023-03-15T00:37:18Z","published":"2023-03-15T00:37:18Z","title":"Rediscovery of CNN's Versatility for Text-based Encoding of Raw\n  Electronic Health Records","summary":"  Making the most use of abundant information in electronic health records\n(EHR) is rapidly becoming an important topic in the medical domain. Recent work\npresented a promising framework that embeds entire features in raw EHR data\nregardless of its form and medical code standards. The framework, however, only\nfocuses on encoding EHR with minimal preprocessing and fails to consider how to\nlearn efficient EHR representation in terms of computation and memory usage. In\nthis paper, we search for a versatile encoder not only reducing the large data\ninto a manageable size but also well preserving the core information of\npatients to perform diverse clinical tasks. We found that hierarchically\nstructured Convolutional Neural Network (CNN) often outperforms the\nstate-of-the-art model on diverse tasks such as reconstruction, prediction, and\ngeneration, even with fewer parameters and less training time. Moreover, it\nturns out that making use of the inherent hierarchy of EHR data can boost the\nperformance of any kind of backbone models and clinical tasks performed.\nThrough extensive experiments, we present concrete evidence to generalize our\nresearch findings into real-world practice. We give a clear guideline on\nbuilding the encoder based on the research findings captured while exploring\nnumerous settings.\n","authors":["Eunbyeol Cho","Min Jae Lee","Kyunghoon Hur","Jiyoun Kim","Jinsung Yoon","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2303.08290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08288v1","updated":"2023-03-15T00:23:49Z","published":"2023-03-15T00:23:49Z","title":"Attention-likelihood relationship in transformers","summary":"  We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.\n","authors":["Valeria Ruscio","Valentino Maiorca","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2303.08288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15551v2","updated":"2023-03-15T00:02:07Z","published":"2022-10-27T15:41:46Z","title":"Terminology-aware Medical Dialogue Generation","summary":"  Medical dialogue generation aims to generate responses according to a history\nof dialogue turns between doctors and patients. Unlike open-domain dialogue\ngeneration, this requires background knowledge specific to the medical domain.\nExisting generative frameworks for medical dialogue generation fall short of\nincorporating domain-specific knowledge, especially with regard to medical\nterminology. In this paper, we propose a novel framework to improve medical\ndialogue generation by considering features centered on domain-specific\nterminology. We leverage an attention mechanism to incorporate terminologically\ncentred features, and fill in the semantic gap between medical background\nknowledge and common utterances by enforcing language models to learn\nterminology representations with an auxiliary terminology recognition task.\nExperimental results demonstrate the effectiveness of our approach, in which\nour proposed framework outperforms SOTA language models. Additionally, we\nprovide a new dataset with medical terminology annotations to support the\nresearch on medical dialogue generation. Our dataset and code are available at\nhttps://github.com/tangg555/meddialog.\n","authors":["Chen Tang","Hongbo Zhang","Tyler Loakman","Chenghua Lin","Frank Guerin"],"pdf_url":"https://arxiv.org/pdf/2210.15551v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.07179v2","updated":"2023-03-15T00:02:06Z","published":"2022-10-13T17:02:23Z","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for\n  Vision-Language Few-Shot Prompting","summary":"  Large pre-trained models have proved to be remarkable zero- and\n(prompt-based) few-shot learners in unimodal vision and language tasks. We\npropose MAPL, a simple and parameter-efficient method that reuses frozen\npre-trained unimodal models and leverages their strong generalization\ncapabilities in multimodal vision-language (VL) settings. MAPL learns a\nlightweight mapping between the representation spaces of unimodal models using\naligned image-text data, and can generalize to unseen VL tasks from just a few\nin-context examples. The small number of trainable parameters makes MAPL\neffective at low-data and in-domain learning. Moreover, MAPL's modularity\nenables easy extension to other pre-trained models. Extensive experiments on\nseveral visual question answering and image captioning benchmarks show that\nMAPL achieves superior or competitive performance compared to similar methods\nwhile training orders of magnitude fewer parameters. MAPL can be trained in\njust a few hours using modest computational resources and public datasets. We\nrelease our code and pre-trained model weights at\nhttps://github.com/mair-lab/mapl.\n","authors":["Oscar Mañas","Pau Rodriguez","Saba Ahmadi","Aida Nematzadeh","Yash Goyal","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2210.07179v2.pdf","comment":"Accepted at EACL 2023 (main track); 26 pages, 21 figures, 6 tables;\n  Pau Rodriguez and Saba Ahmadi had equal contributions"},{"id":"http://arxiv.org/abs/2301.09790v2","updated":"2023-03-15T23:47:00Z","published":"2023-01-24T02:44:02Z","title":"Can Very Large Pretrained Language Models Learn Storytelling With A Few\n  Examples?","summary":"  While pre-trained language models can generate individually fluent sentences\nfor automatic story generation, they struggle to generate stories that are\ncoherent, sensible and interesting. Current state-of-the-art (SOTA) story\ngeneration models explore using higher-level features such as plots or\ncommonsense knowledge to improve the quality of generated stories. Prompt-based\nlearning using very large pre-trained language models (VLPLMs) such as GPT3 has\ndemonstrated impressive performance even across various NLP tasks. In this\npaper, we present an extensive study using automatic and human evaluation to\ncompare the story generation capability of VLPLMs to those SOTA models in three\ndifferent datasets where stories differ in style, register and length. Our\nresults show that VLPLMs generate much higher quality stories than other story\ngeneration models, and to a certain extent rival human authors, although\npreliminary investigation also reveals that they tend to ``plagiarise'' real\nstories in scenarios that involve world knowledge.\n","authors":["Zhuohan Xie","Trevor Cohn","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2301.09790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08991v1","updated":"2023-03-15T23:45:54Z","published":"2023-03-15T23:45:54Z","title":"DeltaScore: Evaluating Story Generation with Differentiating\n  Perturbations","summary":"  Various evaluation metrics exist for natural language generation tasks, but\nthey have limited utility for story generation since they generally do not\ncorrelate well with human judgments and do not measure fine-grained story\naspects, such as fluency versus relatedness, as they are intended to assess\noverall generation quality. In this paper, we propose deltascore, an approach\nthat utilizes perturbation to evaluate fine-grained story aspects. Our core\nidea is based on the hypothesis that the better the story performs in a\nspecific aspect (e.g., fluency), the more it will be affected by a particular\nperturbation (e.g., introducing typos). To measure the impact, we calculate the\nlikelihood difference between the pre- and post-perturbation stories using a\nlanguage model. We evaluate deltascore against state-of-the-art model-based and\ntraditional similarity-based metrics across multiple story domains, and\ninvestigate its correlation with human judgments on five fine-grained story\naspects: fluency, coherence, relatedness, logicality, and interestingness. Our\nresults demonstrate that deltascore performs impressively in evaluating\nfine-grained story aspects, and we discovered a striking outcome where a\nspecific perturbation appears to be highly effective in measuring most aspects.\n","authors":["Zhuohan Xie","Miao Li","Trevor Cohn","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2303.08991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08985v1","updated":"2023-03-15T23:11:30Z","published":"2023-03-15T23:11:30Z","title":"Cross-domain Sentiment Classification in Spanish","summary":"  Sentiment Classification is a fundamental task in the field of Natural\nLanguage Processing, and has very important academic and commercial\napplications. It aims to automatically predict the degree of sentiment present\nin a text that contains opinions and subjectivity at some level, like product\nand movie reviews, or tweets. This can be really difficult to accomplish, in\npart, because different domains of text contains different words and\nexpressions. In addition, this difficulty increases when text is written in a\nnon-English language due to the lack of databases and resources. As a\nconsequence, several cross-domain and cross-language techniques are often\napplied to this task in order to improve the results. In this work we perform a\nstudy on the ability of a classification system trained with a large database\nof product reviews to generalize to different Spanish domains. Reviews were\ncollected from the MercadoLibre website from seven Latin American countries,\nallowing the creation of a large and balanced dataset. Results suggest that\ngeneralization across domains is feasible though very challenging when trained\nwith these product reviews, and can be improved by pre-training and fine-tuning\nthe classification model.\n","authors":["Lautaro Estienne","Matias Vera","Leonardo Rey Vega"],"pdf_url":"https://arxiv.org/pdf/2303.08985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.01636v2","updated":"2023-03-15T22:57:03Z","published":"2021-09-03T17:28:04Z","title":"Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding","summary":"  With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n","authors":["Xin Chen","Qi Zhao","Xinyang Liu"],"pdf_url":"https://arxiv.org/pdf/2109.01636v2.pdf","comment":"Want to review again"},{"id":"http://arxiv.org/abs/2303.08954v1","updated":"2023-03-15T21:51:13Z","published":"2023-03-15T21:51:13Z","title":"PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented\n  Dialogs","summary":"  Research interest in task-oriented dialogs has increased as systems such as\nGoogle Assistant, Alexa and Siri have become ubiquitous in everyday life.\nHowever, the impact of academic research in this area has been limited by the\nlack of datasets that realistically capture the wide array of user pain points.\nTo enable research on some of the more challenging aspects of parsing realistic\nconversations, we introduce PRESTO, a public dataset of over 550K contextual\nmultilingual conversations between humans and virtual assistants. PRESTO\ncontains a diverse array of challenges that occur in real-world NLU tasks such\nas disfluencies, code-switching, and revisions. It is the only large scale\nhuman generated conversational parsing dataset that provides structured context\nsuch as a user's contacts and lists for each example. Our mT5 model based\nbaselines demonstrate that the conversational phenomenon present in PRESTO are\nchallenging to model, which is further pronounced in a low-resource setup.\n","authors":["Rahul Goel","Waleed Ammar","Aditya Gupta","Siddharth Vashishtha","Motoki Sano","Faiz Surani","Max Chang","HyunJeong Choe","David Greene","Kyle He","Rattima Nitisaroj","Anna Trukhina","Shachi Paul","Pararth Shah","Rushin Shah","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2303.08954v1.pdf","comment":"PRESTO v1 Release"},{"id":"http://arxiv.org/abs/2303.08928v1","updated":"2023-03-15T20:55:25Z","published":"2023-03-15T20:55:25Z","title":"Applying unsupervised keyphrase methods on concepts extracted from\n  discharge sheets","summary":"  Clinical notes containing valuable patient information are written by\ndifferent health care providers with various scientific levels and writing\nstyles. It might be helpful for clinicians and researchers to understand what\ninformation is essential when dealing with extensive electronic medical\nrecords. Entities recognizing and mapping them to standard terminologies is\ncrucial in reducing ambiguity in processing clinical notes. Although named\nentity recognition and entity linking are critical steps in clinical natural\nlanguage processing, they can also result in the production of repetitive and\nlow-value concepts. In other hand, all parts of a clinical text do not share\nthe same importance or content in predicting the patient's condition. As a\nresult, it is necessary to identify the section in which each content is\nrecorded and also to identify key concepts to extract meaning from clinical\ntexts. In this study, these challenges have been addressed by using clinical\nnatural language processing techniques. In addition, in order to identify key\nconcepts, a set of popular unsupervised key phrase extraction methods has been\nverified and evaluated. Considering that most of the clinical concepts are in\nthe form of multi-word expressions and their accurate identification requires\nthe user to specify n-gram range, we have proposed a shortcut method to\npreserve the structure of the expression based on TF-IDF. In order to evaluate\nthe pre-processing method and select the concepts, we have designed two types\nof downstream tasks (multiple and binary classification) using the capabilities\nof transformer-based models. The obtained results show the superiority of\nproposed method in combination with SciBERT model, also offer an insight into\nthe efficacy of general extracting essential phrase methods for clinical notes.\n","authors":["Hoda Memarzadeh","Nasser Ghadiri","Matthias Samwald","Maryam Lotfi Shahreza"],"pdf_url":"https://arxiv.org/pdf/2303.08928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.04889v3","updated":"2023-03-15T19:53:01Z","published":"2022-09-11T15:04:11Z","title":"Chain of Explanation: New Prompting Method to Generate Higher Quality\n  Natural Language Explanation for Implicit Hate Speech","summary":"  Recent studies have exploited advanced generative language models to generate\nNatural Language Explanations (NLE) for why a certain text could be hateful. We\npropose the Chain of Explanation (CoE) Prompting method, using the heuristic\nwords and target group, to generate high-quality NLE for implicit hate speech.\nWe improved the BLUE score from 44.0 to 62.3 for NLE generation by providing\naccurate target information. We then evaluate the quality of generated NLE\nusing various automatic metrics and human annotations of informativeness and\nclarity scores.\n","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2209.04889v3.pdf","comment":"Accepted by The Web Conference Companion, 2023"},{"id":"http://arxiv.org/abs/2303.08896v1","updated":"2023-03-15T19:31:21Z","published":"2023-03-15T19:31:21Z","title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models","summary":"  Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to token-level output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nblack-box models in a zero-resource fashion, i.e. without an external database.\nSelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given\nconcept, sampled responses are likely to be similar and contain consistent\nfacts. However, for hallucinated facts, stochastically sampled responses are\nlikely to diverge and contradict one another. We investigate this approach by\nusing GPT-3 to generate passages about individuals from the WikiBio dataset,\nand manually annotate the factuality of the generated passages. We demonstrate\nthat SelfCheckGPT can: i) detect non-factual and factual sentences; and ii)\nrank passages in terms of factuality. We compare our approach to several\nexisting baselines and show that in sentence hallucination detection, our\napproach has AUC-PR scores comparable to grey-box methods, while SelfCheckGPT\nis best at passage factuality assessment.\n","authors":["Potsawee Manakul","Adian Liusie","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2303.08896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08894v1","updated":"2023-03-15T19:29:40Z","published":"2023-03-15T19:29:40Z","title":"A Formalization of Operads in Coq","summary":"  What provides the highest level of assurance for correctness of execution\nwithin a programming language? One answer, and our solution in particular, to\nthis problem is to provide a formalization for, if it exists, the denotational\nsemantics of a programming language. Achieving such a formalization provides a\ngold standard for ensuring a programming language is correct-by-construction.\nIn our effort on the DARPA V-SPELLS program, we worked to provide a foundation\nfor the denotational semantics of a meta-language using a mathematical object\nknown as an operad. This object has compositional properties which are vital to\nbuilding languages from smaller pieces. In this paper, we discuss our\nformalization of an operad in the proof assistant Coq. Moreover, our definition\nwithin Coq is capable of providing proofs that objects specified within Coq are\noperads. This work within Coq provides a formal mathematical basis for our\nmeta-language development within V-SPELLS. Our work also provides, to our\nknowledge, the first known formalization of operads within a proof assistant\nthat has significant automation, as well as a model that can be replicated\nwithout knowledge of Homotopy Type Theory.\n","authors":["Zachary Flores","Angelo Taranto","Eric Bond","Yakir Forman"],"pdf_url":"https://arxiv.org/pdf/2303.08894v1.pdf","comment":"Repository for code to follow shortly"},{"id":"http://arxiv.org/abs/2302.07736v2","updated":"2023-03-15T19:16:45Z","published":"2023-02-11T03:13:54Z","title":"Is ChatGPT better than Human Annotators? Potential and Limitations of\n  ChatGPT in Explaining Implicit Hate Speech","summary":"  Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.\n","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2302.07736v2.pdf","comment":"Accepted by The Web Conference Companion, 2023"},{"id":"http://arxiv.org/abs/2302.07268v4","updated":"2023-03-15T18:46:38Z","published":"2023-02-14T06:42:09Z","title":"AI Chat Assistants can Improve Conversations about Divisive Topics","summary":"  A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.\n","authors":["Lisa P. Argyle","Ethan Busby","Joshua Gubler","Chris Bail","Thomas Howe","Christopher Rytting","David Wingate"],"pdf_url":"https://arxiv.org/pdf/2302.07268v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08877v1","updated":"2023-03-15T18:44:37Z","published":"2023-03-15T18:44:37Z","title":"ROSE: A Neurocomputational Architecture for Syntax","summary":"  A comprehensive model of natural language processing in the brain must\naccommodate four components: representations, operations, structures and\nencoding. It further requires a principled account of how these components\nmechanistically, and causally, relate to each another. While previous models\nhave isolated regions of interest for structure-building and lexical access,\nmany gaps remain with respect to bridging distinct scales of neural complexity.\nBy expanding existing accounts of how neural oscillations can index various\nlinguistic processes, this article proposes a neurocomputational architecture\nfor syntax, termed the ROSE model (Representation, Operation, Structure,\nEncoding). Under ROSE, the basic data structures of syntax are atomic features,\ntypes of mental representations (R), and are coded at the single-unit and\nensemble level. Elementary computations (O) that transform these units into\nmanipulable objects accessible to subsequent structure-building levels are\ncoded via high frequency gamma activity. Low frequency synchronization and\ncross-frequency coupling code for recursive categorial inferences (S). Distinct\nforms of low frequency coupling and phase-amplitude coupling (delta-theta\ncoupling via pSTS-IFG; theta-gamma coupling via IFG to conceptual hubs) then\nencode these structures onto distinct workspaces (E). Causally connecting R to\nO is spike-phase/LFP coupling; connecting O to S is phase-amplitude coupling;\nconnecting S to E is a system of frontotemporal traveling oscillations;\nconnecting E to lower levels is low-frequency phase resetting of spike-LFP\ncoupling. ROSE is reliant on neurophysiologically plausible mechanisms, is\nsupported at all four levels by a range of recent empirical research, and\nprovides an anatomically precise and falsifiable grounding for the basic\nproperty of natural language syntax: hierarchical, recursive\nstructure-building.\n","authors":["Elliot Murphy"],"pdf_url":"https://arxiv.org/pdf/2303.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08806v1","updated":"2023-03-15T17:56:34Z","published":"2023-03-15T17:56:34Z","title":"Understanding Post-hoc Explainers: The Case of Anchors","summary":"  In many scenarios, the interpretability of machine learning models is a\nhighly required but difficult task. To explain the individual predictions of\nsuch models, local model-agnostic approaches have been proposed. However, the\nprocess generating the explanations can be, for a user, as mysterious as the\nprediction to be explained. Furthermore, interpretability methods frequently\nlack theoretical guarantees, and their behavior on simple models is frequently\nunknown. While it is difficult, if not impossible, to ensure that an explainer\nbehaves as expected on a cutting-edge model, we can at least ensure that\neverything works on simple, already interpretable models. In this paper, we\npresent a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular\nrule-based interpretability method that highlights a small set of words to\nexplain a text classifier's decision. After formalizing its algorithm and\nproviding useful insights, we demonstrate mathematically that Anchors produces\nmeaningful results when used with linear text classifiers on top of a TF-IDF\nvectorization. We believe that our analysis framework can aid in the\ndevelopment of new explainability methods based on solid theoretical\nfoundations.\n","authors":["Gianluigi Lopardo","Frederic Precioso","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2303.08806v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2205.13789"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2212.01160v2","updated":"2023-03-15T17:59:57Z","published":"2022-12-02T13:34:56Z","title":"High-Res Facial Appearance Capture from Polarized Smartphone Images","summary":"  We propose a novel method for high-quality facial texture reconstruction from\nRGB images using a novel capturing routine based on a single smartphone which\nwe equip with an inexpensive polarization foil. Specifically, we turn the\nflashlight into a polarized light source and add a polarization filter on top\nof the camera. Leveraging this setup, we capture the face of a subject with\ncross-polarized and parallel-polarized light. For each subject, we record two\nshort sequences in a dark environment under flash illumination with different\nlight polarization using the modified smartphone. Based on these observations,\nwe reconstruct an explicit surface mesh of the face using structure from\nmotion. We then exploit the camera and light co-location within a\ndifferentiable renderer to optimize the facial textures using an\nanalysis-by-synthesis approach. Our method optimizes for high-resolution normal\ntextures, diffuse albedo, and specular albedo using a coarse-to-fine\noptimization scheme. We show that the optimized textures can be used in a\nstandard rendering pipeline to synthesize high-quality photo-realistic 3D\ndigital humans in novel environments.\n","authors":["Dejan Azinović","Olivier Maury","Christophe Hery","Mathias Nießner","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2212.01160v2.pdf","comment":"Project page: https://dazinovic.github.io/polface/ Video:\n  https://www.youtube.com/watch?v=jnb4V0qURtc"},{"id":"http://arxiv.org/abs/2303.08817v1","updated":"2023-03-15T17:59:55Z","published":"2023-03-15T17:59:55Z","title":"DeepMIM: Deep Supervision for Masked Image Modeling","summary":"  Deep supervision, which involves extra supervisions to the intermediate\nfeatures of a neural network, was widely used in image classification in the\nearly deep learning era since it significantly reduces the training difficulty\nand eases the optimization like avoiding gradient vanish over the vanilla\ntraining. Nevertheless, with the emergence of normalization techniques and\nresidual connection, deep supervision in image classification was gradually\nphased out. In this paper, we revisit deep supervision for masked image\nmodeling (MIM) that pre-trains a Vision Transformer (ViT) via a\nmask-and-predict scheme. Experimentally, we find that deep supervision drives\nthe shallower layers to learn more meaningful representations, accelerates\nmodel convergence, and expands attention diversities. Our approach, called\nDeepMIM, significantly boosts the representation capability of each layer. In\naddition, DeepMIM is compatible with many MIM models across a range of\nreconstruction targets. For instance, using ViT-B, DeepMIM on MAE achieves 84.2\ntop-1 accuracy on ImageNet, outperforming MAE by +0.6. By combining DeepMIM\nwith a stronger tokenizer CLIP, our model achieves state-of-the-art performance\non various downstream tasks, including image classification (85.6 top-1\naccuracy on ImageNet-1K, outperforming MAE-CLIP by +0.8), object detection\n(52.8 APbox on COCO) and semantic segmentation (53.1 mIoU on ADE20K). Code and\nmodels are available at https://github.com/OliverRensu/DeepMIM.\n","authors":["Sucheng Ren","Fangyun Wei","Samuel Albanie","Zheng Zhang","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2303.08817v1.pdf","comment":"Code and models are available at\n  https://github.com/OliverRensu/DeepMIM"},{"id":"http://arxiv.org/abs/2212.07398v3","updated":"2023-03-15T17:59:29Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v3.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2303.08815v1","updated":"2023-03-15T17:59:13Z","published":"2023-03-15T17:59:13Z","title":"Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online\n  Lane Graph Construction","summary":"  Online lane graph construction is a promising but challenging task in\nautonomous driving. Previous methods usually model the lane graph at the pixel\nor piece level, and recover the lane graph by pixel-wise or piece-wise\nconnection, which breaks down the continuity of the lane. Human drivers focus\non and drive along the continuous and complete paths instead of considering\nlane pieces. Autonomous vehicles also require path-specific guidance from lane\ngraph for trajectory planning. We argue that the path, which indicates the\ntraffic flow, is the primitive of the lane graph. Motivated by this, we propose\nto model the lane graph in a novel path-wise manner, which well preserves the\ncontinuity of the lane and encodes traffic information for planning. We present\na path-based online lane graph construction method, termed LaneGAP, which\nend-to-end learns the path and recovers the lane graph via a Path2Graph\nalgorithm. We qualitatively and quantitatively demonstrate the superiority of\nLaneGAP over conventional pixel-based and piece-based methods. Abundant\nvisualizations show LaneGAP can cope with diverse traffic conditions. Code and\nmodels will be released at \\url{https://github.com/hustvl/LaneGAP} for\nfacilitating future research.\n","authors":["Bencheng Liao","Shaoyu Chen","Bo Jiang","Tianheng Cheng","Qian Zhang","Wenyu Liu","Chang Huang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08810v1","updated":"2023-03-15T17:58:46Z","published":"2023-03-15T17:58:46Z","title":"BiFormer: Vision Transformer with Bi-Level Routing Attention","summary":"  As the core building block of vision transformers, attention is a powerful\ntool to capture long-range dependency. However, such power comes at a cost: it\nincurs a huge computation burden and heavy memory footprint as pairwise token\ninteraction across all spatial locations is computed. A series of works attempt\nto alleviate this problem by introducing handcrafted and content-agnostic\nsparsity into attention, such as restricting the attention operation to be\ninside local windows, axial stripes, or dilated windows. In contrast to these\napproaches, we propose a novel dynamic sparse attention via bi-level routing to\nenable a more flexible allocation of computations with content awareness.\nSpecifically, for a query, irrelevant key-value pairs are first filtered out at\na coarse region level, and then fine-grained token-to-token attention is\napplied in the union of remaining candidate regions (\\ie, routed regions). We\nprovide a simple yet effective implementation of the proposed bi-level routing\nattention, which utilizes the sparsity to save both computation and memory\nwhile involving only GPU-friendly dense matrix multiplications. Built with the\nproposed bi-level routing attention, a new general vision transformer, named\nBiFormer, is then presented. As BiFormer attends to a small subset of relevant\ntokens in a \\textbf{query adaptive} manner without distraction from other\nirrelevant ones, it enjoys both good performance and high computational\nefficiency, especially in dense prediction tasks. Empirical results across\nseveral computer vision tasks such as image classification, object detection,\nand semantic segmentation verify the effectiveness of our design. Code is\navailable at \\url{https://github.com/rayleizhu/BiFormer}.\n","authors":["Lei Zhu","Xinjiang Wang","Zhanghan Ke","Wayne Zhang","Rynson Lau"],"pdf_url":"https://arxiv.org/pdf/2303.08810v1.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2303.08808v1","updated":"2023-03-15T17:57:13Z","published":"2023-03-15T17:57:13Z","title":"Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB\n  videos","summary":"  Human reconstruction and synthesis from monocular RGB videos is a challenging\nproblem due to clothing, occlusion, texture discontinuities and sharpness, and\nframespecific pose changes. Many methods employ deferred rendering, NeRFs and\nimplicit methods to represent clothed humans, on the premise that mesh-based\nrepresentations cannot capture complex clothing and textures from RGB,\nsilhouettes, and keypoints alone. We provide a counter viewpoint to this\nfundamental premise by optimizing a SMPL+D mesh and an efficient,\nmulti-resolution texture representation using only RGB images, binary\nsilhouettes and sparse 2D keypoints. Experimental results demonstrate that our\napproach is more capable of capturing geometric details compared to visual\nhull, mesh-based methods. We show competitive novel view synthesis and\nimprovements in novel pose synthesis compared to NeRF-based methods, which\nintroduce noticeable, unwanted artifacts. By restricting the solution space to\nthe SMPL+D model combined with differentiable rendering, we obtain dramatic\nspeedups in compute, training times (up to 24x) and inference times (up to\n192x). Our method therefore can be used as is or as a fast initialization to\nNeRF-based methods.\n","authors":["Rohit Jena","Pratik Chaudhari","James Gee","Ganesh Iyer","Siddharth Choudhary","Brandon M. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.08808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12242v2","updated":"2023-03-15T17:52:27Z","published":"2022-08-25T17:45:49Z","title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for\n  Subject-Driven Generation","summary":"  Large text-to-image models achieved a remarkable leap in the evolution of AI,\nenabling high-quality and diverse synthesis of images from a given text prompt.\nHowever, these models lack the ability to mimic the appearance of subjects in a\ngiven reference set and synthesize novel renditions of them in different\ncontexts. In this work, we present a new approach for \"personalization\" of\ntext-to-image diffusion models. Given as input just a few images of a subject,\nwe fine-tune a pretrained text-to-image model such that it learns to bind a\nunique identifier with that specific subject. Once the subject is embedded in\nthe output domain of the model, the unique identifier can be used to synthesize\nnovel photorealistic images of the subject contextualized in different scenes.\nBy leveraging the semantic prior embedded in the model with a new autogenous\nclass-specific prior preservation loss, our technique enables synthesizing the\nsubject in diverse scenes, poses, views and lighting conditions that do not\nappear in the reference images. We apply our technique to several\npreviously-unassailable tasks, including subject recontextualization,\ntext-guided view synthesis, and artistic rendering, all while preserving the\nsubject's key features. We also provide a new dataset and evaluation protocol\nfor this new task of subject-driven generation. Project page:\nhttps://dreambooth.github.io/\n","authors":["Nataniel Ruiz","Yuanzhen Li","Varun Jampani","Yael Pritch","Michael Rubinstein","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2208.12242v2.pdf","comment":"Published at CVPR 2023. Project page: https://dreambooth.github.io/"},{"id":"http://arxiv.org/abs/2303.08131v2","updated":"2023-03-15T17:51:55Z","published":"2023-03-14T17:58:34Z","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","summary":"  We present \\ourmodel{}, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, \\ourmodel{} beats the\nstate-of-the-art method for open-vocabulary instance and panoptic segmentation\nacross 5 datasets, and outperforms previous work for open-vocabulary detection\non LVIS and ODinW under similar settings. When transferred to specific tasks,\nour model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and\ninstance segmentation on ADE20K and Cityscapes.\n  Finally, we note that \\ourmodel{} is the first to explore the potential of\njoint training on segmentation and detection, and hope it can be received as a\nstrong baseline for developing a single model for both tasks in open world.\n","authors":["Hao Zhang","Feng Li","Xueyan Zou","Shilong Liu","Chunyuan Li","Jianfeng Gao","Jianwei Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08131v2.pdf","comment":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"id":"http://arxiv.org/abs/2210.06575v3","updated":"2023-03-15T17:35:57Z","published":"2022-10-12T20:31:23Z","title":"GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and\n  Specular Objects Using Generalizable NeRF","summary":"  In this work, we tackle 6-DoF grasp detection for transparent and specular\nobjects, which is an important yet challenging problem in vision-based robotic\nsystems, due to the failure of depth cameras in sensing their geometry. We, for\nthe first time, propose a multiview RGB-based 6-DoF grasp detection network,\nGraspNeRF, that leverages the generalizable neural radiance field (NeRF) to\nachieve material-agnostic object grasping in clutter. Compared to the existing\nNeRF-based 3-DoF grasp detection methods that rely on densely captured input\nimages and time-consuming per-scene optimization, our system can perform\nzero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF\ngrasps, both in real-time. The proposed framework jointly learns generalizable\nNeRF and grasp detection in an end-to-end manner, optimizing the scene\nrepresentation construction for the grasping. For training data, we generate a\nlarge-scale photorealistic domain-randomized synthetic dataset of grasping in\ncluttered tabletop scenes that enables direct transfer to the real world. Our\nextensive experiments in synthetic and real-world environments demonstrate that\nour method significantly outperforms all the baselines in all the experiments\nwhile remaining in real-time. Project page can be found at\nhttps://pku-epic.github.io/GraspNeRF\n","authors":["Qiyu Dai","Yan Zhu","Yiran Geng","Ciyu Ruan","Jiazhao Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2210.06575v3.pdf","comment":"IEEE International Conference on Robotics and Automation (ICRA), 2023"},{"id":"http://arxiv.org/abs/2303.07336v2","updated":"2023-03-15T17:30:03Z","published":"2023-03-13T17:57:59Z","title":"MP-Former: Mask-Piloted Transformer for Image Segmentation","summary":"  We present a mask-piloted Transformer which improves masked-attention in\nMask2Former for image segmentation. The improvement is based on our observation\nthat Mask2Former suffers from inconsistent mask predictions between consecutive\ndecoder layers, which leads to inconsistent optimization goals and low\nutilization of decoder queries. To address this problem, we propose a\nmask-piloted training approach, which additionally feeds noised ground-truth\nmasks in masked-attention and trains the model to reconstruct the original\nones. Compared with the predicted masks used in mask-attention, the\nground-truth masks serve as a pilot and effectively alleviate the negative\nimpact of inaccurate mask predictions in Mask2Former. Based on this technique,\nour \\M achieves a remarkable performance improvement on all three image\nsegmentation tasks (instance, panoptic, and semantic), yielding $+2.3$AP and\n$+1.6$mIoU on the Cityscapes instance and semantic segmentation tasks with a\nResNet-50 backbone. Our method also significantly speeds up the training,\noutperforming Mask2Former with half of the number of training epochs on ADE20K\nwith both a ResNet-50 and a Swin-L backbones. Moreover, our method only\nintroduces little computation during training and no extra computation during\ninference. Our code will be released at\n\\url{https://github.com/IDEA-Research/MP-Former}.\n","authors":["Hao Zhang","Feng Li","Huaizhe Xu","Shijia Huang","Shilong Liu","Lionel M. Ni","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07336v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08784v1","updated":"2023-03-15T17:26:17Z","published":"2023-03-15T17:26:17Z","title":"Query-guided Attention in Vision Transformers for Localizing Objects\n  Using a Single Sketch","summary":"  In this work, we investigate the problem of sketch-based object localization\non natural images, where given a crude hand-drawn sketch of an object, the goal\nis to localize all the instances of the same object on the target image. This\nproblem proves difficult due to the abstract nature of hand-drawn sketches,\nvariations in the style and quality of sketches, and the large domain gap\nexisting between the sketches and the natural images. To mitigate these\nchallenges, existing works proposed attention-based frameworks to incorporate\nquery information into the image features. However, in these works, the query\nfeatures are incorporated after the image features have already been\nindependently learned, leading to inadequate alignment. In contrast, we propose\na sketch-guided vision transformer encoder that uses cross-attention after each\nblock of the transformer-based image encoder to learn query-conditioned image\nfeatures leading to stronger alignment with the query sketch. Further, at the\noutput of the decoder, the object and the sketch features are refined to bring\nthe representation of relevant objects closer to the sketch query and thereby\nimprove the localization. The proposed model also generalizes to the object\ncategories not seen during training, as the target image features learned by\nour method are query-aware. Our localization framework can also utilize\nmultiple sketch queries via a trainable novel sketch fusion strategy. The model\nis evaluated on the images from the public object detection benchmark, namely\nMS-COCO, using the sketch queries from QuickDraw! and Sketchy datasets.\nCompared with existing localization methods, the proposed approach gives a\n$6.6\\%$ and $8.0\\%$ improvement in mAP for seen objects using sketch queries\nfrom QuickDraw! and Sketchy datasets, respectively, and a $12.2\\%$ improvement\nin AP@50 for large objects that are `unseen' during training.\n","authors":["Aditay Tripathi","Anand Mishra","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2303.08784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08778v1","updated":"2023-03-15T17:19:45Z","published":"2023-03-15T17:19:45Z","title":"Fully neuromorphic vision and control for autonomous drone flight","summary":"  Biological sensing and processing is asynchronous and sparse, leading to\nlow-latency and energy-efficient perception and action. In robotics,\nneuromorphic hardware for event-based vision and spiking neural networks\npromises to exhibit similar characteristics. However, robotic implementations\nhave been limited to basic tasks with low-dimensional sensory inputs and motor\nactions due to the restricted network size in current embedded neuromorphic\nprocessors and the difficulties of training spiking neural networks. Here, we\npresent the first fully neuromorphic vision-to-control pipeline for controlling\na freely flying drone. Specifically, we train a spiking neural network that\naccepts high-dimensional raw event-based camera data and outputs low-level\ncontrol actions for performing autonomous vision-based flight. The vision part\nof the network, consisting of five layers and 28.8k neurons, maps incoming raw\nevents to ego-motion estimates and is trained with self-supervised learning on\nreal event data. The control part consists of a single decoding layer and is\nlearned with an evolutionary algorithm in a drone simulator. Robotic\nexperiments show a successful sim-to-real transfer of the fully learned\nneuromorphic pipeline. The drone can accurately follow different ego-motion\nsetpoints, allowing for hovering, landing, and maneuvering\nsideways$\\unicode{x2014}$even while yawing at the same time. The neuromorphic\npipeline runs on board on Intel's Loihi neuromorphic processor with an\nexecution frequency of 200 Hz, spending only 27 $\\unicode{x00b5}$J per\ninference. These results illustrate the potential of neuromorphic sensing and\nprocessing for enabling smaller, more intelligent robots.\n","authors":["Federico Paredes-Vallés","Jesse Hagenaars","Julien Dupeyroux","Stein Stroobants","Yingfu Xu","Guido de Croon"],"pdf_url":"https://arxiv.org/pdf/2303.08778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08767v1","updated":"2023-03-15T17:07:45Z","published":"2023-03-15T17:07:45Z","title":"Highly Personalized Text Embedding for Image Manipulation by Stable\n  Diffusion","summary":"  Diffusion models have shown superior performance in image generation and\nmanipulation, but the inherent stochasticity presents challenges in preserving\nand manipulating image content and identity. While previous approaches like\nDreamBooth and Textual Inversion have proposed model or latent representation\npersonalization to maintain the content, their reliance on multiple reference\nimages and complex training limits their practicality. In this paper, we\npresent a simple yet highly effective approach to personalization using highly\npersonalized (HiPer) text embedding by decomposing the CLIP embedding space for\npersonalization and content manipulation. Our method does not require model\nfine-tuning or identifiers, yet still enables manipulation of background,\ntexture, and motion with just a single image and target text. Through\nexperiments on diverse target texts, we demonstrate that our approach produces\nhighly personalized and complex semantic image edits across a wide range of\ntasks. We believe that the novel understanding of the text embedding space\npresented in this work has the potential to inspire further research across\nvarious tasks.\n","authors":["Inhwa Han","Serin Yang","Taesung Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.08767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08757v1","updated":"2023-03-15T16:53:19Z","published":"2023-03-15T16:53:19Z","title":"Exploiting 4D CT Perfusion for segmenting infarcted areas in patients\n  with suspected acute ischemic stroke","summary":"  Precise and fast prediction methods for ischemic areas (core and penumbra) in\nacute ischemic stroke (AIS) patients are of significant clinical interest: they\nplay an essential role in improving diagnosis and treatment planning. Computed\nTomography (CT) scan is one of the primary modalities for early assessment in\npatients with suspected AIS. CT Perfusion (CTP) is often used as a primary\nassessment to determine stroke location, severity, and volume of ischemic\nlesions. Current automatic segmentation methods for CTP mostly use already\nprocessed 3D color maps conventionally used for visual assessment by\nradiologists as input. Alternatively, the raw CTP data is used on a\nslice-by-slice basis as 2D+time input, where the spatial information over the\nvolume is ignored. In this paper, we investigate different methods to utilize\nthe entire 4D CTP as input to fully exploit the spatio-temporal information.\nThis leads us to propose a novel 4D convolution layer. Our comprehensive\nexperiments on a local dataset comprised of 152 patients divided into three\ngroups show that our proposed models generate more precise results than other\nmethods explored. A Dice Coefficient of 0.70 and 0.45 is achieved for penumbra\nand core areas, respectively. The code is available on\nhttps://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.\n","authors":["Luca Tomasetti","Kjersti Engan","Liv Jorunn Høllesli","Kathinka Dæhli Kurz","Mahdieh Khanmohammadi"],"pdf_url":"https://arxiv.org/pdf/2303.08757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08747v1","updated":"2023-03-15T16:39:21Z","published":"2023-03-15T16:39:21Z","title":"Cascaded Zoom-in Detector for High Resolution Aerial Images","summary":"  Detecting objects in aerial images is challenging because they are typically\ncomposed of crowded small objects distributed non-uniformly over\nhigh-resolution images. Density cropping is a widely used method to improve\nthis small object detection where the crowded small object regions are\nextracted and processed in high resolution. However, this is typically\naccomplished by adding other learnable components, thus complicating the\ntraining and inference over a standard detection process. In this paper, we\npropose an efficient Cascaded Zoom-in (CZ) detector that re-purposes the\ndetector itself for density-guided training and inference. During training,\ndensity crops are located, labeled as a new class, and employed to augment the\ntraining dataset. During inference, the density crops are first detected along\nwith the base class objects, and then input for a second stage of inference.\nThis approach is easily integrated into any detector, and creates no\nsignificant change in the standard detection process, like the uniform cropping\napproach popular in aerial image detection. Experimental results on the aerial\nimages of the challenging VisDrone and DOTA datasets verify the benefits of the\nproposed approach. The proposed CZ detector also provides state-of-the-art\nresults over uniform cropping and other density cropping methods on the\nVisDrone dataset, increasing the detection mAP of small objects by more than 3\npoints.\n","authors":["Akhil Meethal","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2303.08747v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.08744v1","updated":"2023-03-15T16:35:58Z","published":"2023-03-15T16:35:58Z","title":"Towards Phytoplankton Parasite Detection Using Autoencoders","summary":"  Phytoplankton parasites are largely understudied microbial components with a\npotentially significant ecological impact on phytoplankton bloom dynamics. To\nbetter understand their impact, we need improved detection methods to integrate\nphytoplankton parasite interactions in monitoring aquatic ecosystems. Automated\nimaging devices usually produce high amount of phytoplankton image data, while\nthe occurrence of anomalous phytoplankton data is rare. Thus, we propose an\nunsupervised anomaly detection system based on the similarity of the original\nand autoencoder-reconstructed samples. With this approach, we were able to\nreach an overall F1 score of 0.75 in nine phytoplankton species, which could be\nfurther improved by species-specific fine-tuning. The proposed unsupervised\napproach was further compared with the supervised Faster R-CNN based object\ndetector. With this supervised approach and the model trained on plankton\nspecies and anomalies, we were able to reach the highest F1 score of 0.86.\nHowever, the unsupervised approach is expected to be more universal as it can\ndetect also unknown anomalies and it does not require any annotated anomalous\ndata that may not be always available in sufficient quantities. Although other\nstudies have dealt with plankton anomaly detection in terms of non-plankton\nparticles, or air bubble detection, our paper is according to our best\nknowledge the first one which focuses on automated anomaly detection\nconsidering putative phytoplankton parasites or infections.\n","authors":["Simon Bilik","Daniel Baktrakhanov","Tuomas Eerola","Lumi Haraguchi","Kaisa Kraft","Silke Van den Wyngaert","Jonna Kangas","Conny Sjöqvist","Karin Madsen","Lasse Lensu","Heikki Kälviäinen","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2303.08744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02978v2","updated":"2023-03-15T16:28:08Z","published":"2022-12-05T16:47:09Z","title":"Muscles in Action","summary":"  Small differences in a person's motion can engage drastically different\nmuscles. While most visual representations of human activity are trained from\nvideo, people learn from multimodal experiences, including from the\nproprioception of their own muscles. We present a new visual perception task\nand dataset to model muscle activation in human activities from monocular\nvideo. Our Muscles in Action (MIA) dataset consists of 2 hours of synchronized\nvideo and surface electromyography (sEMG) data of subjects performing various\nexercises. Using this dataset, we learn visual representations that are\npredictive of muscle activation from monocular video. We present several\nmodels, including a transformer model, and measure their ability to generalize\nto new exercises and subjects. Putting muscles into computer vision systems\nwill enable richer models of virtual humans, with applications in sports,\nfitness, and AR/VR.\n","authors":["Mia Chiquier","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2212.02978v2.pdf","comment":"The paper is incomplete and we have completely pivoted since"},{"id":"http://arxiv.org/abs/2303.08740v1","updated":"2023-03-15T16:27:49Z","published":"2023-03-15T16:27:49Z","title":"2D and 3D CNN-Based Fusion Approach for COVID-19 Severity Prediction\n  from 3D CT-Scans","summary":"  Since the appearance of Covid-19 in late 2019, Covid-19 has become an active\nresearch topic for the artificial intelligence (AI) community. One of the most\ninteresting AI topics is Covid-19 analysis of medical imaging. CT-scan imaging\nis the most informative tool about this disease. This work is part of the 3nd\nCOV19D competition for Covid-19 Severity Prediction. In order to deal with the\nbig gap between the validation and test results that were shown in the previous\nversion of this competition, we proposed to combine the prediction of 2D and 3D\nCNN predictions. For the 2D CNN approach, we propose 2B-InceptResnet\narchitecture which consists of two paths for segmented lungs and infection of\nall slices of the input CT-scan, respectively. Each path consists of ConvLayer\nand Inception-ResNet pretrained model on ImageNet. For the 3D CNN approach, we\npropose hybrid-DeCoVNet architecture which consists of four blocks: Stem, four\n3D-ResNet layers, Classification Head and Decision layer. Our proposed\napproaches outperformed the baseline approach in the validation data of the 3nd\nCOV19D competition for Covid-19 Severity Prediction by 36%.\n","authors":["Fares Bougourzi","Fadi Dornaika","Amir Nakib","Cosimo Distante","Abdelmalik Taleb-Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.08740v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.15431"},{"id":"http://arxiv.org/abs/2303.08730v1","updated":"2023-03-15T16:14:06Z","published":"2023-03-15T16:14:06Z","title":"DiffusionAD: Denoising Diffusion for Anomaly Detection","summary":"  Anomaly detection is widely applied due to its remarkable effectiveness and\nefficiency in meeting the needs of real-world industrial manufacturing. We\nintroduce a new pipeline, DiffusionAD, to anomaly detection. We frame anomaly\ndetection as a ``noise-to-norm'' paradigm, in which anomalies are identified as\ninconsistencies between a query image and its flawless approximation. Our\npipeline achieves this by restoring the anomalous regions from the noisy\ncorrupted query image while keeping the normal regions unchanged. DiffusionAD\nincludes a denoising sub-network and a segmentation sub-network, which work\ntogether to provide intuitive anomaly detection and localization in an\nend-to-end manner, without the need for complicated post-processing steps.\nRemarkably, during inference, this framework delivers satisfactory performance\nwith just one diffusion reverse process step, which is tens to hundreds of\ntimes faster than general diffusion methods. Extensive evaluations on standard\nand challenging benchmarks including VisA and DAGM show that DiffusionAD\noutperforms current state-of-the-art paradigms, demonstrating the effectiveness\nand generalizability of the proposed pipeline.\n","authors":["Hui Zhang","Zheng Wang","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.08730v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.08727v1","updated":"2023-03-15T16:12:14Z","published":"2023-03-15T16:12:14Z","title":"Background Matters: Enhancing Out-of-distribution Detection with Domain\n  Features","summary":"  Detecting out-of-distribution (OOD) inputs is a principal task for ensuring\nthe safety of deploying deep-neural-network classifiers in open-world\nscenarios. OOD samples can be drawn from arbitrary distributions and exhibit\ndeviations from in-distribution (ID) data in various dimensions, such as\nforeground semantic features (e.g., vehicle images vs. ID samples in fruit\nclassification) and background domain features (e.g., textural images vs. ID\nsamples in object recognition). Existing methods focus on detecting OOD samples\nbased on the semantic features, while neglecting the other dimensions such as\nthe domain features. This paper considers the importance of the domain features\nin OOD detection and proposes to leverage them to enhance the\nsemantic-feature-based OOD detection methods. To this end, we propose a novel\ngeneric framework that can learn the domain features from the ID training\nsamples by a dense prediction approach, with which different existing\nsemantic-feature-based OOD detection methods can be seamlessly combined to\njointly learn the in-distribution features from both the semantic and domain\ndimensions. Extensive experiments show that our approach 1) can substantially\nenhance the performance of four different state-of-the-art (SotA) OOD detection\nmethods on multiple widely-used OOD datasets with diverse domain features, and\n2) achieves new SotA performance on these benchmarks.\n","authors":["Choubo Ding","Guansong Pang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.08727v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2202.08192v2","updated":"2023-03-15T16:04:15Z","published":"2022-02-16T16:55:39Z","title":"Flexible-Modal Face Anti-Spoofing: A Benchmark","summary":"  Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems from presentation attacks. Benefitted from the maturing camera sensors,\nsingle-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in\nvarious scenarios with different configurations of sensors/modalities. Existing\nsingle- and multi-modal FAS methods usually separately train and deploy models\nfor each possible modality scenario, which might be redundant and inefficient.\nCan we train a unified model, and flexibly deploy it under various modality\nscenarios? In this paper, we establish the first flexible-modal FAS benchmark\nwith the principle `train one for all'. To be specific, with trained\nmulti-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings\nare conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and\nRGB+Depth+IR). We also investigate prevalent deep models and feature fusion\nstrategies for flexible-modal FAS. We hope this new benchmark will facilitate\nthe future research of the multi-modal FAS. The protocols and codes are\navailable at https://github.com/ZitongYu/Flex-Modal-FAS.\n","authors":["Zitong Yu","Ajian Liu","Chenxu Zhao","Kevin H. M. Cheng","Xu Cheng","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2202.08192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08717v1","updated":"2023-03-15T15:59:41Z","published":"2023-03-15T15:59:41Z","title":"Re-ReND: Real-time Rendering of NeRFs across Devices","summary":"  This paper proposes a novel approach for rendering a pre-trained Neural\nRadiance Field (NeRF) in real-time on resource-constrained devices. We\nintroduce Re-ReND, a method enabling Real-time Rendering of NeRFs across\nDevices. Re-ReND is designed to achieve real-time performance by converting the\nNeRF into a representation that can be efficiently processed by standard\ngraphics pipelines. The proposed method distills the NeRF by extracting the\nlearned density into a mesh, while the learned color information is factorized\ninto a set of matrices that represent the scene's light field. Factorization\nimplies the field is queried via inexpensive MLP-free matrix multiplications,\nwhile using a light field allows rendering a pixel by querying the field a\nsingle time-as opposed to hundreds of queries when employing a radiance field.\nSince the proposed representation can be implemented using a fragment shader,\nit can be directly integrated with standard rasterization frameworks. Our\nflexible implementation can render a NeRF in real-time with low memory\nrequirements and on a wide range of resource-constrained devices, including\nmobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a\n2.6-fold increase in rendering speed versus the state-of-the-art without\nperceptible losses in quality.\n","authors":["Sara Rojas","Jesus Zarzar","Juan Camilo Perez","Artsiom Sanakoyeu","Ali Thabet","Albert Pumarola","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.08717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08714v1","updated":"2023-03-15T15:50:11Z","published":"2023-03-15T15:50:11Z","title":"ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution","summary":"  Adapting the Diffusion Probabilistic Model (DPM) for direct image\nsuper-resolution is wasteful, given that a simple Convolutional Neural Network\n(CNN) can recover the main low-frequency content. Therefore, we present\nResDiff, a novel Diffusion Probabilistic Model based on Residual structure for\nSingle Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN,\nwhich restores primary low-frequency components, and a DPM, which predicts the\nresidual between the ground-truth image and the CNN-predicted image. In\ncontrast to the common diffusion-based methods that directly use LR images to\nguide the noise towards HR space, ResDiff utilizes the CNN's initial prediction\nto direct the noise towards the residual space between HR space and\nCNN-predicted space, which not only accelerates the generation process but also\nacquires superior sample quality. Additionally, a frequency-domain-based loss\nfunction for CNN is introduced to facilitate its restoration, and a\nfrequency-domain guided diffusion is designed for DPM on behalf of predicting\nhigh-frequency details. The extensive experiments on multiple benchmark\ndatasets demonstrate that ResDiff outperforms previous diffusion-based methods\nin terms of shorter model convergence time, superior generation quality, and\nmore diverse samples.\n","authors":["Shuyao Shang","Zhengyang Shan","Guangxing Liu","Jinglin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08714v1.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2301.10921v2","updated":"2023-03-15T15:49:07Z","published":"2023-01-26T03:53:25Z","title":"SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised\n  Learning","summary":"  The critical challenge of Semi-Supervised Learning (SSL) is how to\neffectively leverage the limited labeled data and massive unlabeled data to\nimprove the model's generalization performance. In this paper, we first revisit\nthe popular pseudo-labeling methods via a unified sample weighting formulation\nand demonstrate the inherent quantity-quality trade-off problem of\npseudo-labeling with thresholding, which may prohibit learning. To this end, we\npropose SoftMatch to overcome the trade-off by maintaining both high quantity\nand high quality of pseudo-labels during training, effectively exploiting the\nunlabeled data. We derive a truncated Gaussian function to weight samples based\non their confidence, which can be viewed as a soft version of the confidence\nthreshold. We further enhance the utilization of weakly-learned classes by\nproposing a uniform alignment approach. In experiments, SoftMatch shows\nsubstantial improvements across a wide variety of benchmarks, including image,\ntext, and imbalanced classification.\n","authors":["Hao Chen","Ran Tao","Yue Fan","Yidong Wang","Jindong Wang","Bernt Schiele","Xing Xie","Bhiksha Raj","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2301.10921v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2112.13309v2","updated":"2023-03-15T15:48:47Z","published":"2021-12-26T03:12:17Z","title":"Learning Cross-Scale Weighted Prediction for Efficient Neural Video\n  Compression","summary":"  Neural video codecs have demonstrated great potential in video transmission\nand storage applications. Existing neural hybrid video coding approaches rely\non optical flow or Gaussian-scale flow for prediction, which cannot support\nfine-grained adaptation to diverse motion content. Towards more\ncontent-adaptive prediction, we propose a novel cross-scale prediction module\nthat achieves more effective motion compensation. Specifically, on the one\nhand, we produce a reference feature pyramid as prediction sources and then\ntransmit cross-scale flows that leverage the feature scale to control the\nprecision of prediction. On the other hand, for the first time, a weighted\nprediction mechanism is introduced even if only a single reference frame is\navailable, which can help synthesize a fine prediction result by transmitting\ncross-scale weight maps. In addition to the cross-scale prediction module, we\nfurther propose a multi-stage quantization strategy, which improves the\nrate-distortion performance with no extra computational penalty during\ninference. We show the encouraging performance of our efficient neural video\ncodec (ENVC) on several benchmark datasets. In particular, the proposed ENVC\ncan compete with the latest coding standard H.266/VVC in terms of sRGB PSNR on\nUVG dataset for the low-latency mode. We also analyze in detail the\neffectiveness of the cross-scale prediction module in handling various video\ncontent, and provide a comprehensive ablation study to analyze those important\ncomponents. Test code is available at https://github.com/USTC-IMCL/ENVC .\n","authors":["Zongyu Guo","Runsen Feng","Zhizheng Zhang","Xin Jin","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2112.13309v2.pdf","comment":"Preprint. Revised after peer-reviewimg"},{"id":"http://arxiv.org/abs/2303.08063v2","updated":"2023-03-15T15:48:41Z","published":"2023-03-14T16:58:11Z","title":"Interpretable ODE-style Generative Diffusion Model via Force Field\n  Construction","summary":"  For a considerable time, researchers have focused on developing a method that\nestablishes a deep connection between the generative diffusion model and\nmathematical physics. Despite previous efforts, progress has been limited to\nthe pursuit of a single specialized method. In order to advance the\ninterpretability of diffusion models and explore new research directions, it is\nessential to establish a unified ODE-style generative diffusion model. Such a\nmodel should draw inspiration from physical models and possess a clear\ngeometric meaning. This paper aims to identify various physical models that are\nsuitable for constructing ODE-style generative diffusion models accurately from\na mathematical perspective. We then summarize these models into a unified\nmethod. Additionally, we perform a case study where we use the theoretical\nmodel identified by our method to develop a range of new diffusion model\nmethods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the\neffectiveness of our approach. We have constructed a computational framework\nthat attains highly proficient results with regards to image generation speed,\nalongside an additional model that demonstrates exceptional performance in both\nInception score and FID score. These results underscore the significance of our\nmethod in advancing the field of diffusion models.\n","authors":["Weiyang Jin","Yongpei Zhu","Yuxi Peng"],"pdf_url":"https://arxiv.org/pdf/2303.08063v2.pdf","comment":"find some mistake for the results"},{"id":"http://arxiv.org/abs/2302.10390v2","updated":"2023-03-15T15:44:37Z","published":"2023-02-21T01:32:27Z","title":"DrasCLR: A Self-supervised Framework of Learning Disease-related and\n  Anatomy-specific Representation for 3D Medical Images","summary":"  Large-scale volumetric medical images with annotation are rare, costly, and\ntime prohibitive to acquire. Self-supervised learning (SSL) offers a promising\npre-training and feature extraction solution for many downstream tasks, as it\nonly uses unlabeled data. Recently, SSL methods based on instance\ndiscrimination have gained popularity in the medical imaging domain. However,\nSSL pre-trained encoders may use many clues in the image to discriminate an\ninstance that are not necessarily disease-related. Moreover, pathological\npatterns are often subtle and heterogeneous, requiring the ability of the\ndesired method to represent anatomy-specific features that are sensitive to\nabnormal changes in different body parts. In this work, we present a novel SSL\nframework, named DrasCLR, for 3D medical imaging to overcome these challenges.\nWe propose two domain-specific contrastive learning strategies: one aims to\ncapture subtle disease patterns inside a local anatomical region, and the other\naims to represent severe disease patterns that span larger regions. We\nformulate the encoder using conditional hyper-parameterized network, in which\nthe parameters are dependant on the anatomical location, to extract\nanatomically sensitive features. Extensive experiments on large-scale computer\ntomography (CT) datasets of lung images show that our method improves the\nperformance of many downstream prediction and segmentation tasks. The\npatient-level representation improves the performance of the patient survival\nprediction task. We show how our method can detect emphysema subtypes via dense\nprediction. We demonstrate that fine-tuning the pre-trained model can\nsignificantly reduce annotation efforts without sacrificing emphysema detection\naccuracy. Our ablation study highlights the importance of incorporating\nanatomical context into the SSL framework.\n","authors":["Ke Yu","Li Sun","Junxiang Chen","Max Reynolds","Tigmanshu Chaudhary","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2302.10390v2.pdf","comment":"Added some recent references"},{"id":"http://arxiv.org/abs/2303.08704v1","updated":"2023-03-15T15:38:43Z","published":"2023-03-15T15:38:43Z","title":"Multi-Exposure HDR Composition by Gated Swin Transformer","summary":"  Fusing a sequence of perfectly aligned images captured at various exposures,\nhas shown great potential to approach High Dynamic Range (HDR) imaging by\nsensors with limited dynamic range. However, in the presence of large motion of\nscene objects or the camera, mis-alignment is almost inevitable and leads to\nthe notorious ``ghost'' artifacts. Besides, factors such as the noise in the\ndark region or color saturation in the over-bright region may also fail to fill\nlocal image details to the HDR image. This paper provides a novel\nmulti-exposure fusion model based on Swin Transformer. Particularly, we design\nfeature selection gates, which are integrated with the feature extraction\nlayers to detect outliers and block them from HDR image synthesis. To\nreconstruct the missing local details by well-aligned and properly-exposed\nregions, we exploit the long distance contextual dependency in the\nexposure-space pyramid by the self-attention mechanism. Extensive numerical and\nvisual evaluation has been conducted on a variety of benchmark datasets. The\nexperiments show that our model achieves the accuracy on par with current top\nperforming multi-exposure HDR imaging models, while gaining higher efficiency.\n","authors":["Rui Zhou","Yan Niu"],"pdf_url":"https://arxiv.org/pdf/2303.08704v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.08698v1","updated":"2023-03-15T15:32:59Z","published":"2023-03-15T15:32:59Z","title":"Bi-directional Distribution Alignment for Transductive Zero-Shot\n  Learning","summary":"  It is well-known that zero-shot learning (ZSL) can suffer severely from the\nproblem of domain shift, where the true and learned data distributions for the\nunseen classes do not match. Although transductive ZSL (TZSL) attempts to\nimprove this by allowing the use of unlabelled examples from the unseen\nclasses, there is still a high level of distribution shift. We propose a novel\nTZSL model (named as Bi-VAEGAN), which largely improves the shift by a\nstrengthened distribution alignment between the visual and auxiliary spaces.\nThe key proposal of the model design includes (1) a bi-directional distribution\nalignment, (2) a simple but effective L_2-norm based feature normalization\napproach, and (3) a more sophisticated unseen class prior estimation approach.\nIn benchmark evaluation using four datasets, Bi-VAEGAN achieves the new state\nof the arts under both the standard and generalized TZSL settings. Code could\nbe found at https://github.com/Zhicaiwww/Bi-VAEGAN\n","authors":["Zhicai Wang","Yanbin Hao","Tingting Mu","Ouxiang Li","Shuo Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2303.08698v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.08695v1","updated":"2023-03-15T15:27:18Z","published":"2023-03-15T15:27:18Z","title":"RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or\n  missing camera parameters","summary":"  Novel view synthesis (NVS) is a challenging task in computer vision that\ninvolves synthesizing new views of a scene from a limited set of input images.\nNeural Radiance Fields (NeRF) have emerged as a powerful approach to address\nthis problem, but they require accurate knowledge of camera \\textit{intrinsic}\nand \\textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)\nand multi-view stereo (MVS) approaches have been used to extract camera\nparameters, but these methods can be unreliable and may fail in certain cases.\nIn this paper, we propose a novel technique that leverages unposed images from\ndynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera\nparameters directly from data. Our approach is highly extensible and can be\nintegrated into existing NeRF architectures with minimal modifications. We\ndemonstrate the effectiveness of our method on a variety of static and dynamic\nscenes and show that it outperforms traditional SfM and MVS approaches. The\ncode for our method is publicly available at\n\\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.\nOur approach offers a promising new direction for improving the accuracy and\nrobustness of NVS using NeRF, and we anticipate that it will be a valuable tool\nfor a wide range of applications in computer vision and graphics.\n","authors":["Shuja Khalid","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2303.08695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00901v2","updated":"2023-03-15T15:25:02Z","published":"2023-02-02T06:38:00Z","title":"Longformer: Longitudinal Transformer for Alzheimer's Disease\n  Classification with Structural MRIs","summary":"  Structural magnetic resonance imaging (sMRI) is widely used for brain\nneurological disease diagnosis; while longitudinal MRIs are often collected to\nmonitor and capture disease progression, as clinically used in diagnosing\nAlzheimer's disease (AD). However, most current methods neglect AD's\nprogressive nature and only take a single sMRI for recognizing AD. In this\npaper, we consider the problem of leveraging the longitudinal MRIs of a subject\nfor AD identification. To capture longitudinal changes in sMRIs, we propose a\nnovel model Longformer, a spatiotemporal transformer network that performs\nattention mechanisms spatially on sMRIs at each time point and integrates brain\nregion features over time to obtain longitudinal embeddings for classification.\nOur Longformer achieves state-of-the-art performance on two binary\nclassification tasks of separating different stages of AD using the ADNI\ndataset. Our source code is available at https://github.com/Qybc/LongFormer.\n","authors":["Qiuhui Chen","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2302.00901v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.08692v1","updated":"2023-03-15T15:24:01Z","published":"2023-03-15T15:24:01Z","title":"SpiderMesh: Spatial-aware Demand-guided Recursive Meshing for RGB-T\n  Semantic Segmentation","summary":"  For semantic segmentation in urban scene understanding, RGB cameras alone\noften fail to capture a clear holistic topology, especially in challenging\nlighting conditions. Thermal signal is an informative additional channel that\ncan bring to light the contour and fine-grained texture of blurred regions in\nlow-quality RGB image. Aiming at RGB-T (thermal) segmentation, existing methods\neither use simple passive channel/spatial-wise fusion for cross-modal\ninteraction, or rely on heavy labeling of ambiguous boundaries for fine-grained\nsupervision. We propose a Spatial-aware Demand-guided Recursive Meshing\n(SpiderMesh) framework that: 1) proactively compensates inadequate contextual\nsemantics in optically-impaired regions via a demand-guided target masking\nalgorithm; 2) refines multimodal semantic features with recursive meshing to\nimprove pixel-level semantic analysis performance. We further introduce an\nasymmetric data augmentation technique M-CutOut, and enable semi-supervised\nlearning to fully utilize RGB-T labels only sparsely available in practical\nuse. Extensive experiments on MFNet and PST900 datasets demonstrate that\nSpiderMesh achieves new state-of-the-art performance on standard RGB-T\nsegmentation benchmarks.\n","authors":["Siqi Fan","Zhe Wang","Yan Wang","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08689v1","updated":"2023-03-15T15:20:36Z","published":"2023-03-15T15:20:36Z","title":"Panoptic One-Click Segmentation: Applied to Agricultural Data","summary":"  In weed control, precision agriculture can help to greatly reduce the use of\nherbicides, resulting in both economical and ecological benefits. A key element\nis the ability to locate and segment all the plants from image data. Modern\ninstance segmentation techniques can achieve this, however, training such\nsystems requires large amounts of hand-labelled data which is expensive and\nlaborious to obtain. Weakly supervised training can help to greatly reduce\nlabelling efforts and costs. We propose panoptic one-click segmentation, an\nefficient and accurate offline tool to produce pseudo-labels from click inputs\nwhich reduces labelling effort. Our approach jointly estimates the pixel-wise\nlocation of all N objects in the scene, compared to traditional approaches\nwhich iterate independently through all N objects; this greatly reduces\ntraining time. Using just 10% of the data to train our panoptic one-click\nsegmentation approach yields 68.1% and 68.8% mean object intersection over\nunion (IoU) on challenging sugar beet and corn image data respectively,\nproviding comparable performance to traditional one-click approaches while\nbeing approximately 12 times faster to train. We demonstrate the applicability\nof our system by generating pseudo-labels from clicks on the remaining 90% of\nthe data. These pseudo-labels are then used to train Mask R-CNN, in a\nsemi-supervised manner, improving the absolute performance (of mean foreground\nIoU) by 9.4 and 7.9 points for sugar beet and corn data respectively. Finally,\nwe show that our technique can recover missed clicks during annotation\noutlining a further benefit over traditional approaches.\n","authors":["Patrick Zimmer","Michael Halstead","Chris McCool"],"pdf_url":"https://arxiv.org/pdf/2303.08689v1.pdf","comment":"in IEEE Robotics and Automation Letters (2023)"},{"id":"http://arxiv.org/abs/2211.13202v2","updated":"2023-03-15T15:19:59Z","published":"2022-11-23T18:43:41Z","title":"Lite-Mono: A Lightweight CNN and Transformer Architecture for\n  Self-Supervised Monocular Depth Estimation","summary":"  Self-supervised monocular depth estimation that does not require ground truth\nfor training has attracted attention in recent years. It is of high interest to\ndesign lightweight but effective models so that they can be deployed on edge\ndevices. Many existing architectures benefit from using heavier backbones at\nthe expense of model sizes. This paper achieves comparable results with a\nlightweight architecture. Specifically, the efficient combination of CNNs and\nTransformers is investigated, and a hybrid architecture called Lite-Mono is\npresented. A Consecutive Dilated Convolutions (CDC) module and a Local-Global\nFeatures Interaction (LGFI) module are proposed. The former is used to extract\nrich multi-scale local features, and the latter takes advantage of the\nself-attention mechanism to encode long-range global information into the\nfeatures. Experiments demonstrate that Lite-Mono outperforms Monodepth2 by a\nlarge margin in accuracy, with about 80% fewer trainable parameters.\n","authors":["Ning Zhang","Francesco Nex","George Vosselman","Norman Kerle"],"pdf_url":"https://arxiv.org/pdf/2211.13202v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.08686v1","updated":"2023-03-15T15:14:00Z","published":"2023-03-15T15:14:00Z","title":"Weakly Supervised Monocular 3D Object Detection using Multi-View\n  Projection and Direction Consistency","summary":"  Monocular 3D object detection has become a mainstream approach in automatic\ndriving for its easy application. A prominent advantage is that it does not\nneed LiDAR point clouds during the inference. However, most current methods\nstill rely on 3D point cloud data for labeling the ground truths used in the\ntraining phase. This inconsistency between the training and inference makes it\nhard to utilize the large-scale feedback data and increases the data collection\nexpenses. To bridge this gap, we propose a new weakly supervised monocular 3D\nobjection detection method, which can train the model with only 2D labels\nmarked on images. To be specific, we explore three types of consistency in this\ntask, i.e. the projection, multi-view and direction consistency, and design a\nweakly-supervised architecture based on these consistencies. Moreover, we\npropose a new 2D direction labeling method in this task to guide the model for\naccurate rotation direction prediction. Experiments show that our\nweakly-supervised method achieves comparable performance with some fully\nsupervised methods. When used as a pre-training method, our model can\nsignificantly outperform the corresponding fully-supervised baseline with only\n1/3 3D labels. https://github.com/weakmono3d/weakmono3d\n","authors":["Runzhou Tao","Wencheng Han","Zhongying Qiu","Cheng-zhong Xu","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2303.08686v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08685v1","updated":"2023-03-15T15:12:36Z","published":"2023-03-15T15:12:36Z","title":"Making Vision Transformers Efficient from A Token Sparsification View","summary":"  The quadratic computational complexity to the number of tokens limits the\npractical applications of Vision Transformers (ViTs). Several works propose to\nprune redundant tokens to achieve efficient ViTs. However, these methods\ngenerally suffer from (i) dramatic accuracy drops, (ii) application difficulty\nin the local vision transformer, and (iii) non-general-purpose networks for\ndownstream tasks. In this work, we propose a novel Semantic Token ViT (STViT),\nfor efficient global and local vision transformers, which can also be revised\nto serve as backbone for downstream tasks. The semantic tokens represent\ncluster centers, and they are initialized by pooling image tokens in space and\nrecovered by attention, which can adaptively represent global or local semantic\ninformation. Due to the cluster properties, a few semantic tokens can attain\nthe same effect as vast image tokens, for both global and local vision\ntransformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base)\ncan achieve the same accuracy with more than 100% inference speed improvement\nand nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16\nsemantic tokens in each window to further speed it up by around 20% with slight\naccuracy increase. Besides great success in image classification, we also\nextend our method to video recognition. In addition, we design a\nSTViT-R(ecover) network to restore the detailed spatial information based on\nthe STViT, making it work for downstream tasks, which is powerless for previous\ntoken sparsification methods. Experiments demonstrate that our method can\nachieve competitive results compared to the original networks in object\ndetection and instance segmentation, with over 30% FLOPs reduction for\nbackbone.\n","authors":["Shuning Chang","Pichao Wang","Ming Lin","Fan Wang","David Junhao Zhang","Rong Jin","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.08685v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.08682v1","updated":"2023-03-15T15:11:31Z","published":"2023-03-15T15:11:31Z","title":"RSFNet: A White-Box Image Retouching Approach using Region-Specific\n  Color Filters","summary":"  Retouching images is an essential aspect of enhancing the visual appeal of\nphotos. Although users often share common aesthetic preferences, their\nretouching methods may vary based on their individual preferences. Therefore,\nthere is a need for white-box approaches that produce satisfying results and\nenable users to conveniently edit their images simultaneously. Recent white-box\nretouching methods rely on cascaded global filters that provide image-level\nfilter arguments but cannot perform fine-grained retouching. In contrast,\ncolorists typically use a divide-and-conquer approach, performing a series of\nregion-specific fine-grained enhancements when using traditional tools like\nDavinci Resolve. We draw on this insight to develop a white-box framework for\nphoto retouching using parallel region-specific filters, called RSFNet. Our\nmodel generates filter arguments (e.g., saturation, contrast, hue) and\nattention maps of regions for each filter simultaneously. Instead of cascading\nfilters, RSFNet employs linear summations of filters, allowing for a more\ndiverse range of filter classes that can be trained more easily. Our\nexperiments demonstrate that RSFNet achieves state-of-the-art results, offering\nsatisfying aesthetic appeal and greater user convenience for editable white-box\nretouching.\n","authors":["Wenqi Ouyang","Yi Dong","Peiran Ren","Xiaoyang Kang","Xin Xu","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2303.08682v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.08678v1","updated":"2023-03-15T15:02:15Z","published":"2023-03-15T15:02:15Z","title":"Visual Prompt Based Personalized Federated Learning","summary":"  As a popular paradigm of distributed learning, personalized federated\nlearning (PFL) allows personalized models to improve generalization ability and\nrobustness by utilizing knowledge from all distributed clients. Most existing\nPFL algorithms tackle personalization in a model-centric way, such as\npersonalized layer partition, model regularization, and model interpolation,\nwhich all fail to take into account the data characteristics of distributed\nclients. In this paper, we propose a novel PFL framework for image\nclassification tasks, dubbed pFedPT, that leverages personalized visual prompts\nto implicitly represent local data distribution information of clients and\nprovides that information to the aggregation model to help with classification\ntasks. Specifically, in each round of pFedPT training, each client generates a\nlocal personalized prompt related to local data distribution. Then, the local\nmodel is trained on the input composed of raw data and a visual prompt to learn\nthe distribution information contained in the prompt. During model testing, the\naggregated model obtains prior knowledge of the data distributions based on the\nprompts, which can be seen as an adaptive fine-tuning of the aggregation model\nto improve model performances on different clients. Furthermore, the visual\nprompt can be added as an orthogonal method to implement personalization on the\nclient for existing FL methods to boost their performance. Experiments on the\nCIFAR10 and CIFAR100 datasets show that pFedPT outperforms several\nstate-of-the-art (SOTA) PFL algorithms by a large margin in various settings.\n","authors":["Guanghao Li","Wansen Wu","Yan Sun","Li Shen","Baoyuan Wu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.08678v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2206.11736v2","updated":"2023-03-15T14:53:51Z","published":"2022-06-23T14:31:33Z","title":"NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds","summary":"  In order for artificial agents to successfully perform tasks in changing\nenvironments, they must be able to both detect and adapt to novelty. However,\nvisual novelty detection research often only evaluates on repurposed datasets\nsuch as CIFAR-10 originally intended for object classification, where images\nfocus on one distinct, well-centered object. New benchmarks are needed to\nrepresent the challenges of navigating the complex scenes of an open world. Our\nnew NovelCraft dataset contains multimodal episodic data of the images and\nsymbolic world-states seen by an agent completing a pogo stick assembly task\nwithin a modified Minecraft environment. In some episodes, we insert novel\nobjects within the complex 3D scene that may impact gameplay and appear in a\nvariety of sizes and positions. Our visual novelty detection benchmark finds\nthat methods that rank best on popular area-under-the-curve metrics may be\noutperformed by simpler alternatives when controlling false positives matters\nmost. Further multi-modal novelty detection experiments suggest that methods\nthat fuse both visual and symbolic information can improve time until detection\nas well as overall discrimination. Finally, our evaluation of recent\ngeneralized category discovery methods suggests that adapting to new imbalanced\ncategories in complex scenes remains an exciting open problem.\n","authors":["Patrick Feeney","Sarah Schneider","Panagiotis Lymperopoulos","Liping Liu","Matthias Scheutz","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2206.11736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08665v1","updated":"2023-03-15T14:52:46Z","published":"2023-03-15T14:52:46Z","title":"Identity-Preserving Knowledge Distillation for Low-resolution Face\n  Recognition","summary":"  Low-resolution face recognition (LRFR) has become a challenging problem for\nmodern deep face recognition systems. Existing methods mainly leverage prior\ninformation from high-resolution (HR) images by either reconstructing facial\ndetails with super-resolution techniques or learning a unified feature space.\nTo address this issue, this paper proposes a novel approach which enforces the\nnetwork to focus on the discriminative information stored in the low-frequency\ncomponents of a low-resolution (LR) image. A cross-resolution knowledge\ndistillation paradigm is first employed as the learning framework. An\nidentity-preserving network, WaveResNet, and a wavelet similarity loss are then\ndesigned to capture low-frequency details and boost performance. Finally, an\nimage degradation model is conceived to simulate more realistic LR training\ndata. Consequently, extensive experimental results show that the proposed\nmethod consistently outperforms the baseline model and other state-of-the-art\nmethods across a variety of image resolutions.\n","authors":["Yuhang Lu","Touradj Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2303.08665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16299v3","updated":"2023-03-15T14:46:49Z","published":"2022-11-29T15:33:02Z","title":"Transferability Estimation Based On Principal Gradient Expectation","summary":"  Transfer learning aims to improve the performance of target tasks by\ntransferring knowledge acquired in source tasks. The standard approach is\npre-training followed by fine-tuning or linear probing. Especially, selecting a\nproper source domain for a specific target domain under predefined tasks is\ncrucial for improving efficiency and effectiveness. It is conventional to solve\nthis problem via estimating transferability. However, existing methods can not\nreach a trade-off between performance and cost. To comprehensively evaluate\nestimation methods, we summarize three properties: stability, reliability and\nefficiency. Building upon them, we propose Principal Gradient Expectation(PGE),\na simple yet effective method for assessing transferability. Specifically, we\ncalculate the gradient over each weight unit multiple times with a restart\nscheme, and then we compute the expectation of all gradients. Finally, the\ntransferability between the source and target is estimated by computing the gap\nof normalized principal gradients. Extensive experiments show that the proposed\nmetric is superior to state-of-the-art methods on all properties.\n","authors":["Huiyan Qi","Lechao Cheng","Jingjing Chen","Yue Yu","Xue Song","Zunlei Feng","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2211.16299v3.pdf","comment":"11 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.06885v2","updated":"2023-03-15T14:45:40Z","published":"2023-03-13T06:05:18Z","title":"DR2: Diffusion-based Robust Degradation Remover for Blind Face\n  Restoration","summary":"  Blind face restoration usually synthesizes degraded low-quality data with a\npre-defined degradation model for training, while more complex cases could\nhappen in the real world. This gap between the assumed and actual degradation\nhurts the restoration performance where artifacts are often observed in the\noutput. However, it is expensive and infeasible to include every type of\ndegradation to cover real-world cases in the training data. To tackle this\nrobustness issue, we propose Diffusion-based Robust Degradation Remover (DR2)\nto first transform the degraded image to a coarse but degradation-invariant\nprediction, then employ an enhancement module to restore the coarse prediction\nto a high-quality image. By leveraging a well-performing denoising diffusion\nprobabilistic model, our DR2 diffuses input images to a noisy status where\nvarious types of degradation give way to Gaussian noise, and then captures\nsemantic information through iterative denoising steps. As a result, DR2 is\nrobust against common degradation (e.g. blur, resize, noise and compression)\nand compatible with different designs of enhancement modules. Experiments in\nvarious settings show that our framework outperforms state-of-the-art methods\non heavily degraded synthetic and real-world datasets.\n","authors":["Zhixin Wang","Xiaoyun Zhang","Ziying Zhang","Huangjie Zheng","Mingyuan Zhou","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06885v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08658v1","updated":"2023-03-15T14:41:26Z","published":"2023-03-15T14:41:26Z","title":"Skinned Motion Retargeting with Residual Perception of Motion Semantics\n  & Geometry","summary":"  A good motion retargeting cannot be reached without reasonable consideration\nof source-target differences on both the skeleton and shape geometry levels. In\nthis work, we propose a novel Residual RETargeting network (R2ET) structure,\nwhich relies on two neural modification modules, to adjust the source motions\nto fit the target skeletons and shapes progressively. In particular, a\nskeleton-aware module is introduced to preserve the source motion semantics. A\nshape-aware module is designed to perceive the geometries of target characters\nto reduce interpenetration and contact-missing. Driven by our explored\ndistance-based losses that explicitly model the motion semantics and geometry,\nthese two modules can learn residual motion modifications on the source motion\nto generate plausible retargeted motion in a single inference without\npost-processing. To balance these two modifications, we further present a\nbalancing gate to conduct linear interpolation between them. Extensive\nexperiments on the public dataset Mixamo demonstrate that our R2ET achieves the\nstate-of-the-art performance, and provides a good balance between the\npreservation of motion semantics as well as the attenuation of interpenetration\nand contact-missing. Code is available at https://github.com/Kebii/R2ET.\n","authors":["Jiaxu Zhang","Junwu Weng","Di Kang","Fang Zhao","Shaoli Huang","Xuefei Zhe","Linchao Bao","Ying Shan","Jue Wang","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2303.08658v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08657v1","updated":"2023-03-15T14:41:17Z","published":"2023-03-15T14:41:17Z","title":"Economical Quaternion Extraction from a Human Skeletal Pose Estimate\n  using 2-D Cameras","summary":"  In this paper, we present a novel algorithm to extract a quaternion from a\ntwo dimensional camera frame for estimating a contained human skeletal pose.\nThe problem of pose estimation is usually tackled through the usage of stereo\ncameras and intertial measurement units for obtaining depth and euclidean\ndistance for measurement of points in 3D space. However, the usage of these\ndevices comes with a high signal processing latency as well as a significant\nmonetary cost. By making use of MediaPipe, a framework for building perception\npipelines for human pose estimation, the proposed algorithm extracts a\nquaternion from a 2-D frame capturing an image of a human object at a sub-fifty\nmillisecond latency while also being capable of deployment at edges with a\nsingle camera frame and a generally low computational resource availability,\nespecially for use cases involving last-minute detection and reaction by\nautonomous robots. The algorithm seeks to bypass the funding barrier and\nimprove accessibility for robotics researchers involved in designing control\nsystems.\n","authors":["Sriram Radhakrishna","Adithya Balasubramanyam"],"pdf_url":"https://arxiv.org/pdf/2303.08657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14680v2","updated":"2023-03-15T14:38:21Z","published":"2023-02-28T15:45:20Z","title":"Which One Are You Referring To? Multimodal Object Identification in\n  Situated Dialogue","summary":"  The demand for multimodal dialogue systems has been rising in various\ndomains, emphasizing the importance of interpreting multimodal inputs from\nconversational and situational contexts. We explore three methods to tackle\nthis problem and evaluate them on the largest situated dialogue dataset, SIMMC\n2.1. Our best method, scene-dialogue alignment, improves the performance by\n~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and\ndiscussion regarding the limitation of our methods and the potential directions\nfor future works. Our code is publicly available at\nhttps://github.com/holylovenia/multimodal-object-identification.\n","authors":["Holy Lovenia","Samuel Cahyawijaya","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2302.14680v2.pdf","comment":"Accepted at EACL SRW 2023"},{"id":"http://arxiv.org/abs/2211.05231v2","updated":"2023-03-15T14:31:46Z","published":"2022-11-02T17:58:47Z","title":"Biologically-Inspired Continual Learning of Human Motion Sequences","summary":"  This work proposes a model for continual learning on tasks involving temporal\nsequences, specifically, human motions. It improves on a recently proposed\nbrain-inspired replay model (BI-R) by building a biologically-inspired\nconditional temporal variational autoencoder (BI-CTVAE), which instantiates a\nlatent mixture-of-Gaussians for class representation. We investigate a novel\ncontinual-learning-to-generate (CL2Gen) scenario where the model generates\nmotion sequences of different classes. The generative accuracy of the model is\ntested over a set of tasks. The final classification accuracy of BI-CTVAE on a\nhuman motion dataset after sequentially learning all action classes is 78%,\nwhich is 63% higher than using no-replay, and only 5.4% lower than a\nstate-of-the-art offline trained GRU model.\n","authors":["Joachim Ott","Shih-Chii Liu"],"pdf_url":"https://arxiv.org/pdf/2211.05231v2.pdf","comment":"5 pages, 2 figures, accepted at IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023"},{"id":"http://arxiv.org/abs/2303.08648v1","updated":"2023-03-15T14:24:01Z","published":"2023-03-15T14:24:01Z","title":"An End-to-End Multi-Task Learning Model for Image-based Table\n  Recognition","summary":"  Image-based table recognition is a challenging task due to the diversity of\ntable styles and the complexity of table structures. Most of the previous\nmethods focus on a non-end-to-end approach which divides the problem into two\nseparate sub-problems: table structure recognition; and cell-content\nrecognition and then attempts to solve each sub-problem independently using two\nseparate systems. In this paper, we propose an end-to-end multi-task learning\nmodel for image-based table recognition. The proposed model consists of one\nshared encoder, one shared decoder, and three separate decoders which are used\nfor learning three sub-tasks of table recognition: table structure recognition,\ncell detection, and cell-content recognition. The whole system can be easily\ntrained and inferred in an end-to-end approach. In the experiments, we evaluate\nthe performance of the proposed model on two large-scale datasets: FinTabNet\nand PubTabNet. The experiment results show that the proposed model outperforms\nthe state-of-the-art methods in all benchmark datasets.\n","authors":["Nam Tuan Ly","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2303.08648v1.pdf","comment":"10 pages, VISAPP2023. arXiv admin note: substantial text overlap with\n  arXiv:2303.07641"},{"id":"http://arxiv.org/abs/2303.08646v1","updated":"2023-03-15T14:23:07Z","published":"2023-03-15T14:23:07Z","title":"HFGD: High-level Feature Guided Decoder for Semantic Segmentation","summary":"  Commonly used backbones for semantic segmentation, such as ResNet and\nSwin-Transformer, have multiple stages for feature encoding. Simply using\nhigh-resolution low-level feature maps from the early stages of the backbone to\ndirectly refine the low-resolution high-level feature map is a common practice\nof low-resolution feature map upsampling. However, the representation power of\nthe low-level features is generally worse than high-level features, thus\nintroducing ``noise\" to the upsampling refinement. To address this issue, we\nproposed High-level Feature Guided Decoder (HFGD), which uses isolated\nhigh-level features to guide low-level features and upsampling process.\nSpecifically, the guidance is realized through carefully designed stop gradient\noperations and class kernels. Now the class kernels co-evolve only with the\nhigh-level features and are reused in the upsampling head to guide the training\nprocess of the upsampling head. HFGD is very efficient and effective that can\nalso upsample the feature maps to a previously unseen output stride (OS) of 2\nand still obtain accuracy gain. HFGD demonstrates state-of-the-art performance\non several benchmark datasets (e.g. Pascal Context, COCOStuff164k and\nCityscapes) with small FLOPs. The full code will be available at\nhttps://github.com/edwardyehuang/HFGD.git.\n","authors":["Ye Huang","Di Kang","Shenghua Gao","Wen Li","Lixin Duan"],"pdf_url":"https://arxiv.org/pdf/2303.08646v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.08639v1","updated":"2023-03-15T14:09:35Z","published":"2023-03-15T14:09:35Z","title":"Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images","summary":"  Cinemagraphs are short looping videos created by adding subtle motions to a\nstatic image. This kind of media is popular and engaging. However, automatic\ngeneration of cinemagraphs is an underexplored area and current solutions\nrequire tedious low-level manual authoring by artists. In this paper, we\npresent an automatic method that allows generating human cinemagraphs from\nsingle RGB images. We investigate the problem in the context of dressed humans\nunder the wind. At the core of our method is a novel cyclic neural network that\nproduces looping cinemagraphs for the target loop duration. To circumvent the\nproblem of collecting real data, we demonstrate that it is possible, by working\nin the image normal space, to learn garment motion dynamics on synthetic data\nand generalize to real data. We evaluate our method on both synthetic and real\ndata and demonstrate that it is possible to create compelling and plausible\ncinemagraphs from single RGB images.\n","authors":["Hugo Bertiche","Niloy J. Mitra","Kuldeep Kulkarni","Chun-Hao Paul Huang","Tuanfeng Y. Wang","Meysam Madadi","Sergio Escalera","Duygu Ceylan"],"pdf_url":"https://arxiv.org/pdf/2303.08639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10772v4","updated":"2023-03-15T14:03:07Z","published":"2022-11-19T19:06:22Z","title":"DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text\n  Spotting","summary":"  End-to-end text spotting aims to integrate scene text detection and\nrecognition into a unified framework. Dealing with the relationship between the\ntwo sub-tasks plays a pivotal role in designing effective spotters. Although\nTransformer-based methods eliminate the heuristic post-processing, they still\nsuffer from the synergy issue between the sub-tasks and low training\nefficiency. In this paper, we present DeepSolo, a simple DETR-like baseline\nthat lets a single Decoder with Explicit Points Solo for text detection and\nrecognition simultaneously. Technically, for each text instance, we represent\nthe character sequence as ordered points and model them with learnable explicit\npoint queries. After passing a single decoder, the point queries have encoded\nrequisite text semantics and locations, thus can be further decoded to the\ncenter line, boundary, script, and confidence of text via very simple\nprediction heads in parallel. Besides, we also introduce a text-matching\ncriterion to deliver more accurate supervisory signals, thus enabling more\nefficient training. Quantitative experiments on public benchmarks demonstrate\nthat DeepSolo outperforms previous state-of-the-art methods and achieves better\ntraining efficiency. In addition, DeepSolo is also compatible with line\nannotations, which require much less annotation cost than polygons. The code is\navailable at https://github.com/ViTAE-Transformer/DeepSolo.\n","authors":["Maoyuan Ye","Jing Zhang","Shanshan Zhao","Juhua Liu","Tongliang Liu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2211.10772v4.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08634v1","updated":"2023-03-15T14:01:12Z","published":"2023-03-15T14:01:12Z","title":"Quality evaluation of point clouds: a novel no-reference approach using\n  transformer-based architecture","summary":"  With the increased interest in immersive experiences, point cloud came to\nbirth and was widely adopted as the first choice to represent 3D media. Besides\nseveral distortions that could affect the 3D content spanning from acquisition\nto rendering, efficient transmission of such volumetric content over\ntraditional communication systems stands at the expense of the delivered\nperceptual quality. To estimate the magnitude of such degradation, employing\nquality metrics became an inevitable solution. In this work, we propose a novel\ndeep-based no-reference quality metric that operates directly on the whole\npoint cloud without requiring extensive pre-processing, enabling real-time\nevaluation over both transmission and rendering levels. To do so, we use a\nnovel model design consisting primarily of cross and self-attention layers, in\norder to learn the best set of local semantic affinities while keeping the best\ncombination of geometry and color information in multiple levels from basic\nfeatures extraction to deep representation modeling.\n","authors":["Marouane Tliba","Aladine Chetouani","Giuseppe Valenzise","Frederic Dufaux"],"pdf_url":"https://arxiv.org/pdf/2303.08634v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2211.02459"},{"id":"http://arxiv.org/abs/2303.08632v1","updated":"2023-03-15T14:00:11Z","published":"2023-03-15T14:00:11Z","title":"Pixel-Level Explanation of Multiple Instance Learning Models in\n  Biomedical Single Cell Images","summary":"  Explainability is a key requirement for computer-aided diagnosis systems in\nclinical decision-making. Multiple instance learning with attention pooling\nprovides instance-level explainability, however for many clinical applications\na deeper, pixel-level explanation is desirable, but missing so far. In this\nwork, we investigate the use of four attribution methods to explain a multiple\ninstance learning models: GradCAM, Layer-Wise Relevance Propagation (LRP),\nInformation Bottleneck Attribution (IBA), and InputIBA. With this collection of\nmethods, we can derive pixel-level explanations on for the task of diagnosing\nblood cancer from patients' blood smears. We study two datasets of acute\nmyeloid leukemia with over 100 000 single cell images and observe how each\nattribution method performs on the multiple instance learning architecture\nfocusing on different properties of the white blood single cells. Additionally,\nwe compare attribution maps with the annotations of a medical expert to see how\nthe model's decision-making differs from the human standard. Our study\naddresses the challenge of implementing pixel-level explainability in multiple\ninstance learning models and provides insights for clinicians to better\nunderstand and trust decisions from computer-aided diagnosis systems.\n","authors":["Ario Sadafi","Oleksandra Adonkina","Ashkan Khakzar","Peter Lienemann","Rudolf Matthias Hehr","Daniel Rueckert","Nassir Navab","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2303.08632v1.pdf","comment":"Accepted for publication at the international conference on\n  Information Processing in Medical Imaging (IPMI 2023)"},{"id":"http://arxiv.org/abs/2303.08622v1","updated":"2023-03-15T13:47:02Z","published":"2023-03-15T13:47:02Z","title":"Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style\n  Transfer","summary":"  Diffusion models have shown great promise in text-guided image style\ntransfer, but there is a trade-off between style transformation and content\npreservation due to their stochastic nature. Existing methods require\ncomputationally expensive fine-tuning of diffusion models or additional neural\nnetwork. To address this, here we propose a zero-shot contrastive loss for\ndiffusion models that doesn't require additional fine-tuning or auxiliary\nnetworks. By leveraging patch-wise contrastive loss between generated samples\nand original image embeddings in the pre-trained diffusion model, our method\ncan generate images with the same semantic content as the source image in a\nzero-shot manner. Our approach outperforms existing methods while preserving\ncontent and requiring no additional training, not only for image style transfer\nbut also for image-to-image translation and manipulation. Our experimental\nresults validate the effectiveness of our proposed method.\n","authors":["Serin Yang","Hyunmin Hwang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02489v3","updated":"2023-03-15T13:45:48Z","published":"2023-03-04T19:53:00Z","title":"CapDet: Unifying Dense Captioning and Open-World Detection Pretraining","summary":"  Benefiting from large-scale vision-language pre-training on image-text pairs,\nopen-world detection methods have shown superior generalization ability under\nthe zero-shot or few-shot detection settings. However, a pre-defined category\nspace is still required during the inference stage of existing methods and only\nthe objects belonging to that space will be predicted. To introduce a \"real\"\nopen-world detector, in this paper, we propose a novel method named CapDet to\neither predict under a given category list or directly generate the category of\npredicted bounding boxes. Specifically, we unify the open-world detection and\ndense caption tasks into a single yet effective framework by introducing an\nadditional dense captioning head to generate the region-grounded captions.\nBesides, adding the captioning task will in turn benefit the generalization of\ndetection performance since the captioning dataset covers more concepts.\nExperiment results show that by unifying the dense caption task, our CapDet has\nobtained significant performance improvements (e.g., +2.1% mAP on LVIS rare\nclasses) over the baseline method on LVIS (1203 classes). Besides, our CapDet\nalso achieves state-of-the-art performance on dense captioning tasks, e.g.,\n15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.\n","authors":["Yanxin Long","Youpeng Wen","Jianhua Han","Hang Xu","Pengzhen Ren","Wei Zhang","Shen Zhao","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2303.02489v3.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.08617v1","updated":"2023-03-15T13:43:06Z","published":"2023-03-15T13:43:06Z","title":"Exploring Large-scale Unlabeled Faces to Enhance Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) is an important task in computer vision\nand has wide applications in human-computer interaction, intelligent security,\nemotion analysis, and other fields. However, the limited size of FER datasets\nlimits the generalization ability of expression recognition models, resulting\nin ineffective model performance. To address this problem, we propose a\nsemi-supervised learning framework that utilizes unlabeled face data to train\nexpression recognition models effectively. Our method uses a dynamic threshold\nmodule (\\textbf{DTM}) that can adaptively adjust the confidence threshold to\nfully utilize the face recognition (FR) data to generate pseudo-labels, thus\nimproving the model's ability to model facial expressions. In the ABAW5 EXPR\ntask, our method achieved excellent results on the official validation set.\n","authors":["Jun Yu","Zhongpeng Cai","Renda Li","Gongpeng Zhao","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.08617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08611v1","updated":"2023-03-15T13:36:13Z","published":"2023-03-15T13:36:13Z","title":"Improving Fast Auto-Focus with Event Polarity","summary":"  Fast and accurate auto-focus in adverse conditions remains an arduous task.\nThe paper presents a polarity-based event camera auto-focus algorithm featuring\nhigh-speed, precise auto-focus in dark, dynamic scenes that conventional\nframe-based cameras cannot match. Specifically, the symmetrical relationship\nbetween the event polarities in focusing is investigated, and the event-based\nfocus evaluation function is proposed based on the principles of the event\ncameras and the imaging model in the focusing process. Comprehensive\nexperiments on the public EAD dataset show the robustness of the model.\nFurthermore, precise focus with less than one depth of focus is achieved within\n0.004 seconds on our self-built high-speed focusing platform. The dataset and\ncode will be made publicly available.\n","authors":["Yuhan Bao","Lei Sun","Yuqin Ma","Diyang Gu","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08611v1.pdf","comment":"18 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2211.10156v3","updated":"2023-03-15T13:30:47Z","published":"2022-11-17T13:35:11Z","title":"DETRDistill: A Universal Knowledge Distillation Framework for\n  DETR-families","summary":"  Transformer-based detectors (DETRs) are becoming popular for their simple\nframework, but the large model size and heavy time consumption hinder their\ndeployment in the real world. While knowledge distillation (KD) can be an\nappealing technique to compress giant detectors into small ones for comparable\ndetection performance and low inference cost. Since DETRs formulate object\ndetection as a set prediction problem, existing KD methods designed for classic\nconvolution-based detectors may not be directly applicable. In this paper, we\npropose DETRDistill, a novel knowledge distillation method dedicated to\nDETR-families. Specifically, we first design a Hungarian-matching logits\ndistillation to encourage the student model to have the exact predictions as\nthat of teacher DETRs. Next, we propose a target-aware feature distillation to\nhelp the student model learn from the object-centric features of the teacher\nmodel. Finally, in order to improve the convergence rate of the student DETR,\nwe introduce a query-prior assignment distillation to speed up the student\nmodel learning from well-trained queries and stable assignment of the teacher\nmodel. Extensive experimental results on the COCO dataset validate the\neffectiveness of our approach. Notably, DETRDistill consistently improves\nvarious DETRs by more than 2.0 mAP, even surpassing their teacher models.\n","authors":["Jiahao Chang","Shuo Wang","Haiming Xu","Zehui Chen","Chenhongyi Yang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2211.10156v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08605v1","updated":"2023-03-15T13:24:36Z","published":"2023-03-15T13:24:36Z","title":"RICO: Regularizing the Unobservable for Indoor Compositional\n  Reconstruction","summary":"  Recently, neural implicit surfaces have become popular for multi-view\nreconstruction. To facilitate practical applications like scene editing and\nmanipulation, some works extend the framework with semantic masks input for the\nobject-compositional reconstruction rather than the holistic perspective.\nThough achieving plausible disentanglement, the performance drops significantly\nwhen processing the indoor scenes where objects are usually partially observed.\nWe propose RICO to address this by regularizing the unobservable regions for\nindoor compositional reconstruction. Our key idea is to first regularize the\nsmoothness of the occluded background, which then in turn guides the foreground\nobject reconstruction in unobservable regions based on the object-background\nrelationship. Particularly, we regularize the geometry smoothness of occluded\nbackground patches. With the improved background surface, the signed distance\nfunction and the reversedly rendered depth of objects can be optimized to bound\nthem within the background range. Extensive experiments show our method\noutperforms other methods on synthetic and real-world indoor scenes and prove\nthe effectiveness of proposed regularizations.\n","authors":["Zizhang Li","Xiaoyang Lyu","Yuanyuan Ding","Mengmeng Wang","Yiyi Liao","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08600v1","updated":"2023-03-15T13:13:03Z","published":"2023-03-15T13:13:03Z","title":"MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving","summary":"  LiDAR and camera are two modalities available for 3D semantic segmentation in\nautonomous driving. The popular LiDAR-only methods severely suffer from\ninferior segmentation on small and distant objects due to insufficient laser\npoints, while the robust multi-modal solution is under-explored, where we\ninvestigate three crucial inherent difficulties: modality heterogeneity,\nlimited sensor field of view intersection, and multi-modal data augmentation.\nWe propose a multi-modal 3D semantic segmentation model (MSeg3D) with joint\nintra-modal feature extraction and inter-modal feature fusion to mitigate the\nmodality heterogeneity. The multi-modal fusion in MSeg3D consists of\ngeometry-based feature fusion GF-Phase, cross-modal feature completion, and\nsemantic-based feature fusion SF-Phase on all visible points. The multi-modal\ndata augmentation is reinvigorated by applying asymmetric transformations on\nLiDAR point cloud and multi-camera images individually, which benefits the\nmodel training with diversified augmentation transformations. MSeg3D achieves\nstate-of-the-art results on nuScenes, Waymo, and SemanticKITTI datasets. Under\nthe malfunctioning multi-camera input and the multi-frame point clouds input,\nMSeg3D still shows robustness and improves the LiDAR-only baseline. Our code is\npublicly available at \\url{https://github.com/jialeli1/lidarseg3d}.\n","authors":["Jiale Li","Hang Dai","Hao Han","Yong Ding"],"pdf_url":"https://arxiv.org/pdf/2303.08600v1.pdf","comment":"Accepted to CVPR 2023 (preprint)"},{"id":"http://arxiv.org/abs/2303.08597v1","updated":"2023-03-15T13:07:21Z","published":"2023-03-15T13:07:21Z","title":"Aerial-Ground Person Re-ID","summary":"  Person re-ID matches persons across multiple non-overlapping cameras. Despite\nthe increasing deployment of airborne platforms in surveillance, current\nexisting person re-ID benchmarks' focus is on ground-ground matching and very\nlimited efforts on aerial-aerial matching. We propose a new benchmark dataset -\nAG-ReID, which performs person re-ID matching in a new setting: across aerial\nand ground cameras. Our dataset contains 21,983 images of 388 identities and 15\nsoft attributes for each identity. The data was collected by a UAV flying at\naltitudes between 15 to 45 meters and a ground-based CCTV camera on a\nuniversity campus. Our dataset presents a novel elevated-viewpoint challenge\nfor person re-ID due to the significant difference in person appearance across\nthese cameras. We propose an explainable algorithm to guide the person re-ID\nmodel's training with soft attributes to address this challenge. Experiments\ndemonstrate the efficacy of our method on the aerial-ground person re-ID task.\nThe dataset will be published and the baseline codes will be open-sourced to\nfacilitate research in this area.\n","authors":["Huy Nguyen","Kien Nguyen","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2303.08597v1.pdf","comment":"6 pages, 5 Figures, 3 Tables, accepted at ICME 2023"},{"id":"http://arxiv.org/abs/2303.08594v1","updated":"2023-03-15T13:06:30Z","published":"2023-03-15T13:06:30Z","title":"FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation","summary":"  Recent attention in instance segmentation has focused on query-based models.\nDespite being non-maximum suppression (NMS)-free and end-to-end, the\nsuperiority of these models on high-accuracy real-time benchmarks has not been\nwell demonstrated. In this paper, we show the strong potential of query-based\nmodels on efficient instance segmentation algorithm designs. We present\nFastInst, a simple, effective query-based framework for real-time instance\nsegmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while\nyielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells\nand whistles. Specifically, FastInst follows the meta-architecture of recently\nintroduced Mask2Former. Its key designs include instance activation-guided\nqueries, dual-path update strategy, and ground truth mask-guided learning,\nwhich enable us to use lighter pixel decoders, fewer Transformer decoder\nlayers, while achieving better performance. The experiments show that FastInst\noutperforms most state-of-the-art real-time counterparts, including strong\nfully convolutional baselines, in both speed and accuracy. Code can be found at\nhttps://github.com/junjiehe96/FastInst .\n","authors":["Junjie He","Pengyu Li","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2303.08594v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08577v1","updated":"2023-03-15T12:51:16Z","published":"2023-03-15T12:51:16Z","title":"Investigating GANsformer: A Replication Study of a State-of-the-Art\n  Image Generation Model","summary":"  The field of image generation through generative modelling is abundantly\ndiscussed nowadays. It can be used for various applications, such as up-scaling\nexisting images, creating non-existing objects, such as interior design scenes,\nproducts or even human faces, and achieving transfer-learning processes. In\nthis context, Generative Adversarial Networks (GANs) are a class of widely\nstudied machine learning frameworks first appearing in the paper \"Generative\nadversarial nets\" by Goodfellow et al. that achieve the goal above. In our\nwork, we reproduce and evaluate a novel variation of the original GAN network,\nthe GANformer, proposed in \"Generative Adversarial Transformers\" by Hudson and\nZitnick. This project aimed to recreate the methods presented in this paper to\nreproduce the original results and comment on the authors' claims. Due to\nresources and time limitations, we had to constrain the network's training\ntimes, dataset types, and sizes. Our research successfully recreated both\nvariations of the proposed GANformer model and found differences between the\nauthors' and our results. Moreover, discrepancies between the publication\nmethodology and the one implemented, made available in the code, allowed us to\nstudy two undisclosed variations of the presented procedures.\n","authors":["Giorgia Adorni","Felix Boelter","Stefano Carlo Lambertenghi"],"pdf_url":"https://arxiv.org/pdf/2303.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02328v3","updated":"2023-03-15T12:39:19Z","published":"2023-03-04T05:23:11Z","title":"Decompose, Adjust, Compose: Effective Normalization by Playing with\n  Frequency for Domain Generalization","summary":"  Domain generalization (DG) is a principal task to evaluate the robustness of\ncomputer vision models. Many previous studies have used normalization for DG.\nIn normalization, statistics and normalized features are regarded as style and\ncontent, respectively. However, it has a content variation problem when\nremoving style because the boundary between content and style is unclear. This\nstudy addresses this problem from the frequency domain perspective, where\namplitude and phase are considered as style and content, respectively. First,\nwe verify the quantitative phase variation of normalization through the\nmathematical derivation of the Fourier transform formula. Then, based on this,\nwe propose a novel normalization method, PCNorm, which eliminates style only as\nthe preserving content through spectral decomposition. Furthermore, we propose\nadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of\nvariations in content and style, respectively. Thus, they can learn\ndomain-agnostic representations for DG. With the normalization methods, we\npropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain\ngap. The proposed models outperform other recent DG methods. The DAC-SC\nachieves an average state-of-the-art performance of 65.6% on five datasets:\nPACS, VLCS, Office-Home, DomainNet, and TerraIncognita.\n","authors":["Sangrok Lee","Jongseong Bae","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2303.02328v3.pdf","comment":"10 pages,6 figures, Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2109.09223v2","updated":"2023-03-15T12:35:46Z","published":"2021-09-19T21:00:44Z","title":"Baby Robot: Improving the Motor Skills of Toddlers","summary":"  This article introduces \"Baby Robot\", a robot aiming to improve motor skills\nof babies and toddlers. Authors developed a car-like toy that moves\nautonomously using reinforcement learning and computer vision techniques. The\nrobot behaviour is to escape from a target baby that has been previously\nrecognized, or at least detected, while avoiding obstacles, so that the\nsecurity of the baby is not compromised. A myriad of commercial toys with a\nsimilar mobility improvement purpose are into the market; however, there is no\none that bets for an intelligent autonomous movement, as they perform simple\nyet repetitive trajectories in the best of the cases. Two crawling toys -- one\nin representation of \"Baby Robot\" -- were tested in a real environment with\nrespect to regular toys in order to check how they improved the toddlers\nmobility. These real-life experiments were conducted with our proposed robot in\na kindergarten, where a group of children interacted with the toys. Significant\nimprovement in the motion skills of participants were detected.\n","authors":["Eric Canas","Alba M. G. Garcia","Anais Garrell","Cecilio Angulo"],"pdf_url":"https://arxiv.org/pdf/2109.09223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08566v1","updated":"2023-03-15T12:34:24Z","published":"2023-03-15T12:34:24Z","title":"Sensitivity-Aware Visual Parameter-Efficient Tuning","summary":"  Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative\nfor full fine-tuning so as to adapt pre-trained vision models to downstream\ntasks, which only tunes a small number of parameters while freezing the vast\nmajority ones to ease storage burden and optimization difficulty. However,\nexisting VPET methods introduce trainable parameters to the same positions\nacross different tasks depending solely on human heuristics and neglect the\ndomain gaps. To this end, we study where to introduce and how to allocate\ntrainable parameters by proposing a novel Sensitivity-aware visual\nParameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable\nparameters to task-specific important positions given a desired tunable\nparameter budget. Specifically, our SPT first quickly identifies the sensitive\nparameters that require tuning for a given task in a data-dependent way. Next,\nour SPT further boosts the representational capability for the weight matrices\nwhose number of sensitive parameters exceeds a pre-defined threshold by\nutilizing any of the existing structured tuning methods, e.g., LoRA or Adapter,\nto replace directly tuning the selected sensitive parameters (unstructured\ntuning) under the budget. Extensive experiments on a wide range of downstream\nrecognition tasks show that our SPT is complementary to the existing VPET\nmethods and largely boosts their performance, e.g., SPT improves Adapter with\nsupervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean Top-1 accuracy,\nreaching SOTA performance on FGVC and VTAB-1k benchmarks, respectively. Source\ncode is at https://github.com/ziplab/SPT\n","authors":["Haoyu He","Jianfei Cai","Jing Zhang","Dacheng Tao","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.08566v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.07937v2","updated":"2023-03-15T12:30:47Z","published":"2023-03-14T14:24:31Z","title":"Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D\n  Generation","summary":"  Text-to-3D generation has shown rapid progress in recent days with the advent\nof score distillation, a methodology of using pretrained text-to-2D diffusion\nmodels to optimize neural radiance field (NeRF) in the zero-shot setting.\nHowever, the lack of 3D awareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible 3D scene. To address\nthis issue, we propose 3DFuse, a novel framework that incorporates 3D awareness\ninto pretrained 2D diffusion models, enhancing the robustness and 3D\nconsistency of score distillation-based methods. We realize this by first\nconstructing a coarse 3D structure of a given text prompt and then utilizing\nprojected, view-specific depth map as a condition for the diffusion model.\nAdditionally, we introduce a training strategy that enables the 2D diffusion\nmodel learns to handle the errors and sparsity within the coarse 3D structure\nfor robust generation, as well as a method for ensuring semantic consistency\nthroughout all viewpoints of the scene. Our framework surpasses the limitations\nof prior arts, and has significant implications for 3D consistent generation of\n2D diffusion models.\n","authors":["Junyoung Seo","Wooseok Jang","Min-Seop Kwak","Jaehoon Ko","Hyeonsu Kim","Junho Kim","Jin-Hwa Kim","Jiyoung Lee","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07937v2.pdf","comment":"Project page https://ku-cvlab.github.io/3DFuse/"},{"id":"http://arxiv.org/abs/2303.08562v1","updated":"2023-03-15T12:28:31Z","published":"2023-03-15T12:28:31Z","title":"MGA: Medical generalist agent through text-guided knowledge\n  transformation","summary":"  Multi-modal representation methods have achieved advanced performance in\nmedical applications by extracting more robust features from multi-domain data.\nHowever, existing methods usually need to train additional branches for\ndownstream tasks, which may increase the model complexities in clinical\napplications as well as introduce additional human inductive bias. Besides,\nvery few studies exploit the rich clinical knowledge embedded in clinical daily\nreports. To this end, we propose a novel medical generalist agent, MGA, that\ncan address three kinds of common clinical tasks via clinical reports knowledge\ntransformation. Unlike the existing methods, MGA can easily adapt to different\ntasks without specific downstream branches when their corresponding annotations\nare missing. More importantly, we are the first attempt to use medical\nprofessional language guidance as a transmission medium to guide the agent's\nbehavior. The proposed method is implemented on four well-known X-ray\nopen-source datasets, MIMIC-CXR, CheXpert, MIMIC-CXR-JPG, and MIMIC-CXR-MS.\nPromising results are obtained, which validate the effectiveness of our\nproposed MGA. Code is available at: https://github.com/SZUHvern/MGA\n","authors":["Weijian Huang","Hao Yang","Cheng Li","Mingtong Dai","Rui Yang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12003v3","updated":"2023-03-15T12:28:22Z","published":"2021-07-26T07:36:02Z","title":"Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal\n  Latent Representations","summary":"  In this paper, we propose a multi-speaker face-to-speech waveform generation\nmodel that also works for unseen speaker conditions. Using a generative\nadversarial network (GAN) with linguistic and speaker characteristic features\nas auxiliary conditions, our method directly converts face images into speech\nwaveforms under an end-to-end training framework. The linguistic features are\nextracted from lip movements using a lip-reading model, and the speaker\ncharacteristic features are predicted from face images using cross-modal\nlearning with a pre-trained acoustic model. Since these two features are\nuncorrelated and controlled independently, we can flexibly synthesize speech\nwaveforms whose speaker characteristics vary depending on the input face\nimages. We show the superiority of our proposed model over conventional methods\nin terms of objective and subjective evaluation results. Specifically, we\nevaluate the performances of linguistic features by measuring their accuracy on\nan automatic speech recognition task. In addition, we estimate speaker and\ngender similarity for multi-speaker and unseen conditions, respectively. We\nalso evaluate the aturalness of the synthesized speech waveforms using a mean\nopinion score (MOS) test and non-intrusive objective speech quality assessment\n(NISQA).The demo samples of the proposed and other models are available at\nhttps://sam-0927.github.io/\n","authors":["Se-Yun Um","Jihyun Kim","Jihyun Lee","Hong-Goo Kang"],"pdf_url":"https://arxiv.org/pdf/2107.12003v3.pdf","comment":"5 pages (including references), 1 figure"},{"id":"http://arxiv.org/abs/2303.08557v1","updated":"2023-03-15T12:18:16Z","published":"2023-03-15T12:18:16Z","title":"Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey","summary":"  Deep learning has been highly successful in computer vision with large\namounts of labeled data, but struggles with limited labeled training data. To\naddress this, Few-shot learning (FSL) is proposed, but it assumes that all\nsamples (including source and target task data, where target tasks are\nperformed with prior knowledge from source ones) are from the same domain,\nwhich is a stringent assumption in the real world. To alleviate this\nlimitation, Cross-domain few-shot learning (CDFSL) has gained attention as it\nallows source and target data from different domains and label spaces. This\npaper provides a comprehensive review of CDFSL at the first time, which has\nreceived far less attention than FSL due to its unique setup and difficulties.\nWe expect this paper to serve as both a position paper and a tutorial for those\ndoing research in CDFSL. This review first introduces the definition of CDFSL\nand the issues involved, followed by the core scientific question and\nchallenge. A comprehensive review of validated CDFSL approaches from the\nexisting literature is then presented, along with their detailed descriptions\nbased on a rigorous taxonomy. Furthermore, this paper outlines and discusses\nseveral promising directions of CDFSL that deserve further scientific\ninvestigation, covering aspects of problem setups, applications and theories.\n","authors":["Huali Xu","Shuaifeng Zhi","Shuzhou Sun","Vishal M. Patel","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08557v1.pdf","comment":"35 pages, 14 figures, 8 tables"},{"id":"http://arxiv.org/abs/2303.08545v1","updated":"2023-03-15T11:59:24Z","published":"2023-03-15T11:59:24Z","title":"Local Region Perception and Relationship Learning Combined with Feature\n  Fusion for Facial Action Unit Detection","summary":"  Human affective behavior analysis plays a vital role in human-computer\ninteraction (HCI) systems. In this paper, we introduce our submission to the\nCVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). We\npropose a single-stage trained AU detection framework. Specifically, in order\nto effectively extract facial local region features related to AU detection, we\nuse a local region perception module to effectively extract features of\ndifferent AUs. Meanwhile, we use a graph neural network-based relational\nlearning module to capture the relationship between AUs. In addition,\nconsidering the role of the overall feature of the target face on AU detection,\nwe also use the feature fusion module to fuse the feature information extracted\nby the backbone network and the AU feature information extracted by the\nrelationship learning module. We also adopted some sampling methods, data\naugmentation techniques and post-processing strategies to further improve the\nperformance of the model.\n","authors":["Jun Yu","Renda Li","Zhongpeng Cai","Gongpeng Zhao","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.08545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08536v1","updated":"2023-03-15T11:29:36Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v1.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2302.14307v3","updated":"2023-03-15T11:15:41Z","published":"2023-02-28T04:45:31Z","title":"GradMA: A Gradient-Memory-based Accelerated Federated Learning with\n  Alleviated Catastrophic Forgetting","summary":"  Federated Learning (FL) has emerged as a de facto machine learning area and\nreceived rapid increasing research interests from the community. However,\ncatastrophic forgetting caused by data heterogeneity and partial participation\nposes distinctive challenges for FL, which are detrimental to the performance.\nTo tackle the problems, we propose a new FL approach (namely GradMA), which\ntakes inspiration from continual learning to simultaneously correct the\nserver-side and worker-side update directions as well as take full advantage of\nserver's rich computing and memory resources. Furthermore, we elaborate a\nmemory reduction strategy to enable GradMA to accommodate FL with a large scale\nof workers. We then analyze convergence of GradMA theoretically under the\nsmooth non-convex setting and show that its convergence rate achieves a linear\nspeed up w.r.t the increasing number of sampled active workers. At last, our\nextensive experiments on various image classification tasks show that GradMA\nachieves significant performance gains in accuracy and communication efficiency\ncompared to SOTA baselines.\n","authors":["Kangyang Luo","Xiang Li","Yunshi Lan","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2302.14307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08525v1","updated":"2023-03-15T11:15:03Z","published":"2023-03-15T11:15:03Z","title":"MRGAN360: Multi-stage Recurrent Generative Adversarial Network for 360\n  Degree Image Saliency Prediction","summary":"  Thanks to the ability of providing an immersive and interactive experience,\nthe uptake of 360 degree image content has been rapidly growing in consumer and\nindustrial applications. Compared to planar 2D images, saliency prediction for\n360 degree images is more challenging due to their high resolutions and\nspherical viewing ranges. Currently, most high-performance saliency prediction\nmodels for omnidirectional images (ODIs) rely on deeper or broader\nconvolutional neural networks (CNNs), which benefit from CNNs' superior feature\nrepresentation capabilities while suffering from their high computational\ncosts. In this paper, inspired by the human visual cognitive process, i.e.,\nhuman being's perception of a visual scene is always accomplished by multiple\nstages of analysis, we propose a novel multi-stage recurrent generative\nadversarial networks for ODIs dubbed MRGAN360, to predict the saliency maps\nstage by stage. At each stage, the prediction model takes as input the original\nimage and the output of the previous stage and outputs a more accurate saliency\nmap. We employ a recurrent neural network among adjacent prediction stages to\nmodel their correlations, and exploit a discriminator at the end of each stage\nto supervise the output saliency map. In addition, we share the weights among\nall the stages to obtain a lightweight architecture that is computationally\ncheap. Extensive experiments are conducted to demonstrate that our proposed\nmodel outperforms the state-of-the-art model in terms of both prediction\naccuracy and model size.\n","authors":["Pan Gao","Xinlang Chen","Rong Quan","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.08525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08524v1","updated":"2023-03-15T11:13:51Z","published":"2023-03-15T11:13:51Z","title":"CoordFill: Efficient High-Resolution Image Inpainting via Parameterized\n  Coordinate Querying","summary":"  Image inpainting aims to fill the missing hole of the input. It is hard to\nsolve this task efficiently when facing high-resolution images due to two\nreasons: (1) Large reception field needs to be handled for high-resolution\nimage inpainting. (2) The general encoder and decoder network synthesizes many\nbackground pixels synchronously due to the form of the image matrix. In this\npaper, we try to break the above limitations for the first time thanks to the\nrecent development of continuous implicit representation. In detail, we\ndown-sample and encode the degraded image to produce the spatial-adaptive\nparameters for each spatial patch via an attentional Fast Fourier\nConvolution(FFC)-based parameter generation network. Then, we take these\nparameters as the weights and biases of a series of multi-layer\nperceptron(MLP), where the input is the encoded continuous coordinates and the\noutput is the synthesized color value. Thanks to the proposed structure, we\nonly encode the high-resolution image in a relatively low resolution for larger\nreception field capturing. Then, the continuous position encoding will be\nhelpful to synthesize the photo-realistic high-frequency textures by\nre-sampling the coordinate in a higher resolution. Also, our framework enables\nus to query the coordinates of missing pixels only in parallel, yielding a more\nefficient solution than the previous methods. Experiments show that the\nproposed method achieves real-time performance on the 2048$\\times$2048 images\nusing a single GTX 2080 Ti GPU and can handle 4096$\\times$4096 images, with\nmuch better performance than existing state-of-the-art methods visually and\nnumerically. The code is available at:\nhttps://github.com/NiFangBaAGe/CoordFill.\n","authors":["Weihuang Liu","Xiaodong Cun","Chi-Man Pun","Menghan Xia","Yong Zhang","Jue Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08524v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08514v1","updated":"2023-03-15T10:45:21Z","published":"2023-03-15T10:45:21Z","title":"Deep Learning for Iris Recognition: A Review","summary":"  Iris recognition is a secure biometric technology known for its stability and\nprivacy. With no two irises being identical and little change throughout a\nperson's lifetime, iris recognition is considered more reliable and less\nsusceptible to external factors than other biometric recognition methods.\nUnlike traditional machine learning-based iris recognition methods, deep\nlearning technology does not rely on feature engineering and boasts excellent\nperformance. This paper collects 120 relevant papers to summarize the\ndevelopment of iris recognition based on deep learning. We first introduce the\nbackground of iris recognition and the motivation and contribution of this\nsurvey. Then, we present the common datasets widely used in iris recognition.\nAfter that, we summarize the key tasks involved in the process of iris\nrecognition based on deep learning technology, including identification,\nsegmentation, presentation attack detection, and localization. Finally, we\ndiscuss the challenges and potential development of iris recognition. This\nreview provides a comprehensive sight of the research of iris recognition based\non deep learning.\n","authors":["Yimin Yin","Siliang He","Renye Zhang","Hongli Chang","Xu Han","Jinghua Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08511v1","updated":"2023-03-15T10:39:31Z","published":"2023-03-15T10:39:31Z","title":"Mapping Urban Population Growth from Sentinel-2 MSI and Census Data\n  Using Deep Learning: A Case Study in Kigali, Rwanda","summary":"  To better understand current trends of urban population growth in Sub-Saharan\nAfrica, high-quality spatiotemporal population estimates are necessary. While\nthe joint use of remote sensing and deep learning has achieved promising\nresults for population distribution estimation, most of the current work\nfocuses on fine-scale spatial predictions derived from single date census,\nthereby neglecting temporal analyses. In this work, we focus on evaluating how\ndeep learning change detection techniques can unravel temporal population\ndynamics at short intervals. Since Post-Classification Comparison (PCC) methods\nfor change detection are known to propagate the error of the individual maps,\nwe propose an end-to-end population growth mapping method. Specifically, a\nResNet encoder, pretrained on a population mapping task with Sentinel-2 MSI\ndata, was incorporated into a Siamese network. The Siamese network was trained\nat the census level to accurately predict population change. The effectiveness\nof the proposed method is demonstrated in Kigali, Rwanda, for the time period\n2016-2020, using bi-temporal Sentinel-2 data. Compared to PCC, the Siamese\nnetwork greatly reduced errors in population change predictions at the census\nlevel. These results show promise for future remote sensing-based population\ngrowth mapping endeavors.\n","authors":["Sebastian Hafner","Stefanos Georganos","Theodomir Mugiraneza","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2303.08511v1.pdf","comment":"4 pages, 5 figures, accepted for publication in the JURSE 2023\n  Proceedings"},{"id":"http://arxiv.org/abs/2302.11217v2","updated":"2023-03-15T10:30:18Z","published":"2023-02-22T09:04:00Z","title":"Connecting Vision and Language with Video Localized Narratives","summary":"  We propose Video Localized Narratives, a new form of multimodal video\nannotations connecting vision and language. In the original Localized\nNarratives, annotators speak and move their mouse simultaneously on an image,\nthus grounding each word with a mouse trace segment. However, this is\nchallenging on a video. Our new protocol empowers annotators to tell the story\nof a video with Localized Narratives, capturing even complex events involving\nmultiple actors interacting with each other and with several passive objects.\nWe annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M\nwords. Based on this data, we also construct new benchmarks for the video\nnarrative grounding and video question answering tasks, and provide reference\nresults from strong baseline models. Our annotations are available at\nhttps://google.github.io/video-localized-narratives/.\n","authors":["Paul Voigtlaender","Soravit Changpinyo","Jordi Pont-Tuset","Radu Soricut","Vittorio Ferrari"],"pdf_url":"https://arxiv.org/pdf/2302.11217v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2202.13220v2","updated":"2023-03-15T10:26:32Z","published":"2022-02-26T20:02:47Z","title":"How Much Depth Information can Radar Contribute to a Depth Estimation\n  Model?","summary":"  Recently, several works have proposed fusing radar data as an additional\nperceptual signal into monocular depth estimation models because radar data is\nrobust against varying light and weather conditions. Although improved\nperformances were reported in prior works, it is still hard to tell how much\ndepth information radar can contribute to a depth estimation model. In this\npaper, we propose radar inference and supervision experiments to investigate\nthe intrinsic depth potential of radar data using state-of-the-art depth\nestimation models on the nuScenes dataset. In the inference experiment, the\nmodel predicts depth by taking only radar as input to demonstrate the inference\ncapability using radar data. In the supervision experiment, a monocular depth\nestimation model is trained under radar supervision to show the intrinsic depth\ninformation that radar can contribute. Our experiments demonstrate that the\nmodel using only sparse radar as input can detect the shape of surroundings to\na certain extent in the predicted depth. Furthermore, the monocular depth\nestimation model supervised by preprocessed radar achieves a good performance\ncompared to the baseline model trained with sparse lidar supervision.\n","authors":["Chen-Chou Lo","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2202.13220v2.pdf","comment":"published on EI2023, 7 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.08500v1","updated":"2023-03-15T10:20:49Z","published":"2023-03-15T10:20:49Z","title":"The Devil's Advocate: Shattering the Illusion of Unexploitable Data\n  using Diffusion Models","summary":"  Protecting personal data against the exploitation of machine learning models\nis of paramount importance. Recently, availability attacks have shown great\npromise to provide an extra layer of protection against the unauthorized use of\ndata to train neural networks. These methods aim to add imperceptible noise to\nclean data so that the neural networks cannot extract meaningful patterns from\nthe protected data, claiming that they can make personal data \"unexploitable.\"\nIn this paper, we provide a strong countermeasure against such approaches,\nshowing that unexploitable data might only be an illusion. In particular, we\nleverage the power of diffusion models and show that a carefully designed\ndenoising process can defuse the ramifications of the data-protecting\nperturbations. We rigorously analyze our algorithm, and theoretically prove\nthat the amount of required denoising is directly related to the magnitude of\nthe data-protecting perturbations. Our approach, called AVATAR, delivers\nstate-of-the-art performance against a suite of recent availability attacks in\nvarious scenarios, outperforming adversarial training. Our findings call for\nmore research into making personal data unexploitable, showing that this goal\nis far from over.\n","authors":["Hadi M. Dolatabadi","Sarah Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2303.08500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08498v1","updated":"2023-03-15T10:18:53Z","published":"2023-03-15T10:18:53Z","title":"BEVHeight: A Robust Framework for Vision-based Roadside 3D Object\n  Detection","summary":"  While most recent autonomous driving system focuses on developing perception\nmethods on ego-vehicle sensors, people tend to overlook an alternative approach\nto leverage intelligent roadside cameras to extend the perception ability\nbeyond the visual range. We discover that the state-of-the-art vision-centric\nbird's eye view detection methods have inferior performances on roadside\ncameras. This is because these methods mainly focus on recovering the depth\nregarding the camera center, where the depth difference between the car and the\nground quickly shrinks while the distance increases. In this paper, we propose\na simple yet effective approach, dubbed BEVHeight, to address this issue. In\nessence, instead of predicting the pixel-wise depth, we regress the height to\nthe ground to achieve a distance-agnostic formulation to ease the optimization\nprocess of camera-only perception methods. On popular 3D detection benchmarks\nof roadside cameras, our method surpasses all previous vision-centric methods\nby a significant margin. The code is available at\n{\\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.\n","authors":["Lei Yang","Kaicheng Yu","Tao Tang","Jun Li","Kun Yuan","Li Wang","Xinyu Zhang","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.08498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08490v1","updated":"2023-03-15T09:52:28Z","published":"2023-03-15T09:52:28Z","title":"Strong Baseline and Bag of Tricks for COVID-19 Detection of CT Scans","summary":"  This paper investigates the application of deep learning models for lung\nComputed Tomography (CT) image analysis. Traditional deep learning frameworks\nencounter compatibility issues due to variations in slice numbers and\nresolutions in CT images, which stem from the use of different machines.\nCommonly, individual slices are predicted and subsequently merged to obtain the\nfinal result; however, this approach lacks slice-wise feature learning and\nconsequently results in decreased performance. We propose a novel slice\nselection method for each CT dataset to address this limitation, effectively\nfiltering out uncertain slices and enhancing the model's performance.\nFurthermore, we introduce a spatial-slice feature learning (SSFL)\ntechnique\\cite{hsu2022} that employs a conventional and efficient backbone\nmodel for slice feature training, followed by extracting one-dimensional data\nfrom the trained model for COVID and non-COVID classification using a dedicated\nclassification model. Leveraging these experimental steps, we integrate\none-dimensional features with multiple slices for channel merging and employ a\n2D convolutional neural network (CNN) model for classification. In addition to\nthe aforementioned methods, we explore various high-performance classification\nmodels, ultimately achieving promising results.\n","authors":["Chih-Chung Hsu","Chih-Yu Jian","Chia-Ming Lee","Chi-Han Tsai","Sheng-Chieh Dai"],"pdf_url":"https://arxiv.org/pdf/2303.08490v1.pdf","comment":"technical report. Keywords: Spatial-Slice correlation, COVID-19\n  classification, convolutional neural networks, computed tomography"},{"id":"http://arxiv.org/abs/2303.08481v1","updated":"2023-03-15T09:36:58Z","published":"2023-03-15T09:36:58Z","title":"SeqCo-DETR: Sequence Consistency Training for Self-Supervised Object\n  Detection with Transformers","summary":"  Self-supervised pre-training and transformer-based networks have\nsignificantly improved the performance of object detection. However, most of\nthe current self-supervised object detection methods are built on\nconvolutional-based architectures. We believe that the transformers' sequence\ncharacteristics should be considered when designing a transformer-based\nself-supervised method for the object detection task. To this end, we propose\nSeqCo-DETR, a novel Sequence Consistency-based self-supervised method for\nobject DEtection with TRansformers. SeqCo-DETR defines a simple but effective\npretext by minimizes the discrepancy of the output sequences of transformers\nwith different image views as input and leverages bipartite matching to find\nthe most relevant sequence pairs to improve the sequence-level self-supervised\nrepresentation learning performance. Furthermore, we provide a mask-based\naugmentation strategy incorporated with the sequence consistency strategy to\nextract more representative contextual information about the object for the\nobject detection task. Our method achieves state-of-the-art results on MS COCO\n(45.8 AP) and PASCAL VOC (64.1 AP), demonstrating the effectiveness of our\napproach.\n","authors":["Guoqiang Jin","Fan Yang","Mingshan Sun","Ruyi Zhao","Yakun Liu","Wei Li","Tianpeng Bao","Liwei Wu","Xingyu Zeng","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.08481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.09213v4","updated":"2023-03-15T09:33:01Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v4.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2210.05669v2","updated":"2023-03-15T09:29:59Z","published":"2022-10-11T17:59:54Z","title":"A generic diffusion-based approach for 3D human pose prediction in the\n  wild","summary":"  Predicting 3D human poses in real-world scenarios, also known as human pose\nforecasting, is inevitably subject to noisy inputs arising from inaccurate 3D\npose estimations and occlusions. To address these challenges, we propose a\ndiffusion-based approach that can predict given noisy observations. We frame\nthe prediction task as a denoising problem, where both observation and\nprediction are considered as a single sequence containing missing elements\n(whether in the observation or prediction horizon). All missing elements are\ntreated as noise and denoised with our conditional diffusion model. To better\nhandle long-term forecasting horizon, we present a temporal cascaded diffusion\nmodel. We demonstrate the benefits of our approach on four publicly available\ndatasets (Human3.6M, HumanEva-I, AMASS, and 3DPW), outperforming the\nstate-of-the-art. Additionally, we show that our framework is generic enough to\nimprove any 3D pose prediction model as a pre-processing step to repair their\ninputs and a post-processing step to refine their outputs. The code is\navailable online: \\url{https://github.com/vita-epfl/DePOSit}.\n","authors":["Saeed Saadatnejad","Ali Rasekh","Mohammadreza Mofayezi","Yasamin Medghalchi","Sara Rajabzadeh","Taylor Mordan","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2210.05669v2.pdf","comment":"Accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.08475v1","updated":"2023-03-15T09:29:03Z","published":"2023-03-15T09:29:03Z","title":"Mutual Information-Based Temporal Difference Learning for Human Pose\n  Estimation in Video","summary":"  Temporal modeling is crucial for multi-frame human pose estimation. Most\nexisting methods directly employ optical flow or deformable convolution to\npredict full-spectrum motion fields, which might incur numerous irrelevant\ncues, such as a nearby person or background. Without further efforts to\nexcavate meaningful motion priors, their results are suboptimal, especially in\ncomplicated spatiotemporal interactions. On the other hand, the temporal\ndifference has the ability to encode representative motion information which\ncan potentially be valuable for pose estimation but has not been fully\nexploited. In this paper, we present a novel multi-frame human pose estimation\nframework, which employs temporal differences across frames to model dynamic\ncontexts and engages mutual information objectively to facilitate useful motion\ninformation disentanglement. To be specific, we design a multi-stage Temporal\nDifference Encoder that performs incremental cascaded learning conditioned on\nmulti-stage feature difference sequences to derive informative motion\nrepresentation. We further propose a Representation Disentanglement module from\nthe mutual information perspective, which can grasp discriminative\ntask-relevant motion signals by explicitly defining useful and noisy\nconstituents of the raw motion features and minimizing their mutual\ninformation. These place us to rank No.1 in the Crowd Pose Estimation in\nComplex Events Challenge on benchmark dataset HiEve, and achieve\nstate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,\nand PoseTrack21.\n","authors":["Runyang Feng","Yixing Gao","Xueqing Ma","Tze Ho Elden Tse","Hyung Jin Chang"],"pdf_url":"https://arxiv.org/pdf/2303.08475v1.pdf","comment":"This paper is accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08473v1","updated":"2023-03-15T09:26:29Z","published":"2023-03-15T09:26:29Z","title":"Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs","summary":"  Image synthesis driven by computer graphics achieved recently a remarkable\nrealism, yet synthetic image data generated this way reveals a significant\ndomain gap with respect to real-world data. This is especially true in\nautonomous driving scenarios, which represent a critical aspect for overcoming\nutilizing synthetic data for training neural networks. We propose a method\nbased on domain-invariant scene representation to directly synthesize traffic\nscene imagery without rendering. Specifically, we rely on synthetic scene\ngraphs as our internal representation and introduce an unsupervised neural\nnetwork architecture for realistic traffic scene synthesis. We enhance\nsynthetic scene graphs with spatial information about the scene and demonstrate\nthe effectiveness of our approach through scene manipulation.\n","authors":["Artem Savkin","Rachid Ellouze","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2303.08473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08466v1","updated":"2023-03-15T09:10:51Z","published":"2023-03-15T09:10:51Z","title":"Mining False Positive Examples for Text-Based Person Re-identification","summary":"  Text-based person re-identification (ReID) aims to identify images of the\ntargeted person from a large-scale person image database according to a given\ntextual description. However, due to significant inter-modal gaps, text-based\nperson ReID remains a challenging problem. Most existing methods generally rely\nheavily on the similarity contributed by matched word-region pairs, while\nneglecting mismatched word-region pairs which may play a decisive role.\nAccordingly, we propose to mine false positive examples (MFPE) via a jointly\noptimized multi-branch architecture to handle this problem. MFPE contains three\nbranches including a false positive mining (FPM) branch to highlight the role\nof mismatched word-region pairs. Besides, MFPE delicately designs a cross-relu\nloss to increase the gap of similarity scores between matched and mismatched\nword-region pairs. Extensive experiments on CUHK-PEDES demonstrate the superior\neffectiveness of MFPE. Our code is released at\nhttps://github.com/xx-adeline/MFPE.\n","authors":["Wenhao Xu","Zhiyin Shao","Changxing Ding"],"pdf_url":"https://arxiv.org/pdf/2303.08466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08463v1","updated":"2023-03-15T09:07:04Z","published":"2023-03-15T09:07:04Z","title":"Co-Occurrence Matters: Learning Action Relation for Temporal Action\n  Localization","summary":"  Temporal action localization (TAL) is a prevailing task due to its great\napplication potential. Existing works in this field mainly suffer from two\nweaknesses: (1) They often neglect the multi-label case and only focus on\ntemporal modeling. (2) They ignore the semantic information in class labels and\nonly use the visual information. To solve these problems, we propose a novel\nCo-Occurrence Relation Module (CORM) that explicitly models the co-occurrence\nrelationship between actions. Besides the visual information, it further\nutilizes the semantic embeddings of class labels to model the co-occurrence\nrelationship. The CORM works in a plug-and-play manner and can be easily\nincorporated with the existing sequence models. By considering both visual and\nsemantic co-occurrence, our method achieves high multi-label relationship\nmodeling capacity. Meanwhile, existing datasets in TAL always focus on\nlow-semantic atomic actions. Thus we construct a challenging multi-label\ndataset UCF-Crime-TAL that focuses on high-semantic actions by annotating the\nUCF-Crime dataset at frame level and considering the semantic overlap of\ndifferent events. Extensive experiments on two commonly used TAL datasets,\n\\textit{i.e.}, MultiTHUMOS and TSU, and our newly proposed UCF-Crime-TAL\ndemenstrate the effectiveness of the proposed CORM, which achieves\nstate-of-the-art performance on these datasets.\n","authors":["Congqi Cao","Yizhe Wang","Yue Lu","Xin Zhang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03194v2","updated":"2023-03-15T09:06:48Z","published":"2023-01-09T08:00:01Z","title":"Few-shot Semantic Segmentation with Support-induced Graph Convolutional\n  Network","summary":"  Few-shot semantic segmentation (FSS) aims to achieve novel objects\nsegmentation with only a few annotated samples and has made great progress\nrecently. Most of the existing FSS models focus on the feature matching between\nsupport and query to tackle FSS. However, the appearance variations between\nobjects from the same category could be extremely large, leading to unreliable\nfeature matching and query mask prediction. To this end, we propose a\nSupport-induced Graph Convolutional Network (SiGCN) to explicitly excavate\nlatent context structure in query images. Specifically, we propose a\nSupport-induced Graph Reasoning (SiGR) module to capture salient query object\nparts at different semantic levels with a Support-induced GCN. Furthermore, an\ninstance association (IA) module is designed to capture high-order instance\ncontext from both support and query instances. By integrating the proposed two\nmodules, SiGCN can learn rich query context representation, and thus being more\nrobust to appearance variations. Extensive experiments on PASCAL-5i and\nCOCO-20i demonstrate that our SiGCN achieves state-of-the-art performance.\n","authors":["Jie Liu","Yanqi Bao","Wenzhe Yin","Haochen Wang","Yang Gao","Jan-Jakob Sonke","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2301.03194v2.pdf","comment":"Accepted in BMVC2022 as oral presentation"},{"id":"http://arxiv.org/abs/2303.00575v2","updated":"2023-03-15T08:55:34Z","published":"2023-03-01T15:16:56Z","title":"IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint\n  Multi-Agent Trajectory Prediction","summary":"  Reliable multi-agent trajectory prediction is crucial for the safe planning\nand control of autonomous systems. Compared with single-agent cases, the major\nchallenge in simultaneously processing multiple agents lies in modeling complex\nsocial interactions caused by various driving intentions and road conditions.\nPrevious methods typically leverage graph-based message propagation or\nattention mechanism to encapsulate such interactions in the format of marginal\nprobabilistic distributions. However, it is inherently sub-optimal. In this\npaper, we propose IPCC-TP, a novel relevance-aware module based on Incremental\nPearson Correlation Coefficient to improve multi-agent interaction modeling.\nIPCC-TP learns pairwise joint Gaussian Distributions through the\ntightly-coupled estimation of the means and covariances according to\ninteractive incremental movements. Our module can be conveniently embedded into\nexisting multi-agent prediction methods to extend original motion distribution\ndecoders. Extensive experiments on nuScenes and Argoverse 2 datasets\ndemonstrate that IPCC-TP improves the performance of baselines by a large\nmargin.\n","authors":["Dekai Zhu","Guangyao Zhai","Yan Di","Fabian Manhardt","Hendrik Berkemeyer","Tuan Tran","Nassir Navab","Federico Tombari","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2303.00575v2.pdf","comment":"CVPR 2023 accepted"},{"id":"http://arxiv.org/abs/2303.08452v1","updated":"2023-03-15T08:54:20Z","published":"2023-03-15T08:54:20Z","title":"Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly\n  Detection","summary":"  Early and accurate disease detection is crucial for patient management and\nsuccessful treatment outcomes. However, the automatic identification of\nanomalies in medical images can be challenging. Conventional methods rely on\nlarge labeled datasets which are difficult to obtain. To overcome these\nlimitations, we introduce a novel unsupervised approach, called PHANES (Pseudo\nHealthy generative networks for ANomaly Segmentation). Our method has the\ncapability of reversing anomalies, i.e., preserving healthy tissue and\nreplacing anomalous regions with pseudo-healthy (PH) reconstructions. Unlike\nrecent diffusion models, our method does not rely on a learned noise\ndistribution nor does it introduce random alterations to the entire image.\nInstead, we use latent generative networks to create masks around possible\nanomalies, which are refined using inpainting generative networks. We\ndemonstrate the effectiveness of PHANES in detecting stroke lesions in T1w\nbrain MRI datasets and show significant improvements over state-of-the-art\n(SOTA) methods. We believe that our proposed framework will open new avenues\nfor interpretable, fast, and accurate anomaly segmentation with the potential\nto support various clinical-oriented downstream tasks.\n","authors":["Cosmin I Bercea","Benedikt Wiestler","Daniel Rueckert","Julia A Schnabel"],"pdf_url":"https://arxiv.org/pdf/2303.08452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08450v1","updated":"2023-03-15T08:51:17Z","published":"2023-03-15T08:51:17Z","title":"PoseRAC: Pose Saliency Transformer for Repetitive Action Counting","summary":"  This paper presents a significant contribution to the field of repetitive\naction counting through the introduction of a new approach called Pose Saliency\nRepresentation. The proposed method efficiently represents each action using\nonly two salient poses instead of redundant frames, which significantly reduces\nthe computational cost while improving the performance. Moreover, we introduce\na pose-level method, PoseRAC, which is based on this representation and\nachieves state-of-the-art performance on two new version datasets by using Pose\nSaliency Annotation to annotate salient poses for training. Our lightweight\nmodel is highly efficient, requiring only 15 minutes for training on a GPU, and\ninfers nearly 10x faster compared to previous methods. In addition, our\napproach achieves a substantial improvement over the previous state-of-the-art\nTransRAC, achieving an OBO metric of 0.56 compared to 0.29 of TransRAC. The\ncode and new dataset are available at https://github.com/MiracleDance/PoseRAC\nfor further research and experimentation, making our proposed approach highly\naccessible to the research community.\n","authors":["Ziyu Yao","Xuxin Cheng","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2303.08450v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.08446v1","updated":"2023-03-15T08:41:57Z","published":"2023-03-15T08:41:57Z","title":"Task-specific Fine-tuning via Variational Information Bottleneck for\n  Weakly-supervised Pathology Whole Slide Image Classification","summary":"  While Multiple Instance Learning (MIL) has shown promising results in digital\nPathology Whole Slide Image (WSI) classification, such a paradigm still faces\nperformance and generalization problems due to challenges in high computational\ncosts on Gigapixel WSIs and limited sample size for model training. To deal\nwith the computation problem, most MIL methods utilize a frozen pretrained\nmodel from ImageNet to obtain representations first. This process may lose\nessential information owing to the large domain gap and hinder the\ngeneralization of model due to the lack of image-level training-time\naugmentations. Though Self-supervised Learning (SSL) proposes viable\nrepresentation learning schemes, the improvement of the downstream task still\nneeds to be further explored in the conversion from the task-agnostic features\nof SSL to the task-specifics under the partial label supervised learning. To\nalleviate the dilemma of computation cost and performance, we propose an\nefficient WSI fine-tuning framework motivated by the Information Bottleneck\ntheory. The theory enables the framework to find the minimal sufficient\nstatistics of WSI, thus supporting us to fine-tune the backbone into a\ntask-specific representation only depending on WSI-level weak labels. The\nWSI-MIL problem is further analyzed to theoretically deduce our fine-tuning\nmethod. Our framework is evaluated on five pathology WSI datasets on various\nWSI heads. The experimental results of our fine-tuned representations show\nsignificant improvements in both accuracy and generalization compared with\nprevious works. Source code will be available at\nhttps://github.com/invoker-LL/WSI-finetuning.\n","authors":["Honglin Li","Chenglu Zhu","Yunlong Zhang","Yuxuan Sun","Zhongyi Shui","Wenwei Kuang","Sunyi Zheng","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08446v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08085v2","updated":"2023-03-15T08:40:18Z","published":"2023-03-14T17:16:16Z","title":"Alias-Free Convnets: Fractional Shift Invariance via Polynomial\n  Activations","summary":"  Although CNNs are believed to be invariant to translations, recent works have\nshown this is not the case, due to aliasing effects that stem from downsampling\nlayers. The existing architectural solutions to prevent aliasing are partial\nsince they do not solve these effects, that originate in non-linearities. We\npropose an extended anti-aliasing method that tackles both downsampling and\nnon-linear layers, thus creating truly alias-free, shift-invariant CNNs. We\nshow that the presented model is invariant to integer as well as fractional\n(i.e., sub-pixel) translations, thus outperforming other shift-invariant\nmethods in terms of robustness to adversarial translations.\n","authors":["Hagay Michaeli","Tomer Michaeli","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2303.08085v2.pdf","comment":"The paper was accepted to CVPR 2023. Our code is available at\n  https://github.com/hmichaeli/alias_free_convnets/"},{"id":"http://arxiv.org/abs/2303.08444v1","updated":"2023-03-15T08:38:08Z","published":"2023-03-15T08:38:08Z","title":"Real-time Multi-Object Tracking Based on Bi-directional Matching","summary":"  In recent years, anchor-free object detection models combined with matching\nalgorithms are used to achieve real-time muti-object tracking and also ensure\nhigh tracking accuracy. However, there are still great challenges in\nmulti-object tracking. For example, when most part of a target is occluded or\nthe target just disappears from images temporarily, it often leads to tracking\ninterruptions for most of the existing tracking algorithms. Therefore, this\nstudy offers a bi-directional matching algorithm for multi-object tracking that\nmakes advantage of bi-directional motion prediction information to improve\nocclusion handling. A stranded area is used in the matching algorithm to\ntemporarily store the objects that fail to be tracked. When objects recover\nfrom occlusions, our method will first try to match them with objects in the\nstranded area to avoid erroneously generating new identities, thus forming a\nmore continuous trajectory. Experiments show that our approach can improve the\nmulti-object tracking performance in the presence of occlusions. In addition,\nthis study provides an attentional up-sampling module that not only assures\ntracking accuracy but also accelerates training speed. In the MOT17 challenge,\nthe proposed algorithm achieves 63.4% MOTA, 55.3% IDF1, and 20.1 FPS tracking\nspeed.\n","authors":["Huilan Luo","Zehua Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.08444v1.pdf","comment":"19 pages,6 figures"},{"id":"http://arxiv.org/abs/2303.08440v1","updated":"2023-03-15T08:28:06Z","published":"2023-03-15T08:28:06Z","title":"Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models","summary":"  Diffusion models have become a popular approach for image generation and\nreconstruction due to their numerous advantages. However, most diffusion-based\ninverse problem-solving methods only deal with 2D images, and even recently\npublished 3D methods do not fully exploit the 3D distribution prior. To address\nthis, we propose a novel approach using two perpendicular pre-trained 2D\ndiffusion models to solve the 3D inverse problem. By modeling the 3D data\ndistribution as a product of 2D distributions sliced in different directions,\nour method effectively addresses the curse of dimensionality. Our experimental\nresults demonstrate that our method is highly effective for 3D medical image\nreconstruction tasks, including MRI Z-axis super-resolution, compressed sensing\nMRI, and sparse-view CT. Our method can generate high-quality voxel volumes\nsuitable for medical applications.\n","authors":["Suhyeon Lee","Hyungjin Chung","Minyoung Park","Jonghyuk Park","Wi-Sun Ryu","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.08440v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.08439v1","updated":"2023-03-15T08:27:56Z","published":"2023-03-15T08:27:56Z","title":"Real Face Foundation Representation Learning for Generalized Deepfake\n  Detection","summary":"  The emergence of deepfake technologies has become a matter of social concern\nas they pose threats to individual privacy and public security. It is now of\ngreat significance to develop reliable deepfake detectors. However, with\nnumerous face manipulation algorithms present, it is almost impossible to\ncollect sufficient representative fake faces, and it is hard for existing\ndetectors to generalize to all types of manipulation. Therefore, we turn to\nlearn the distribution of real faces, and indirectly identify fake images that\ndeviate from the real face distribution. In this study, we propose Real Face\nFoundation Representation Learning (RFFR), which aims to learn a general\nrepresentation from large-scale real face datasets and detect potential\nartifacts outside the distribution of RFFR. Specifically, we train a model on\nreal face datasets by masked image modeling (MIM), which results in a\ndiscrepancy between input faces and the reconstructed ones when applying the\nmodel on fake samples. This discrepancy reveals the low-level artifacts not\ncontained in RFFR, making it easier to build a deepfake detector sensitive to\nall kinds of potential artifacts outside the distribution of RFFR. Extensive\nexperiments demonstrate that our method brings about better generalization\nperformance, as it significantly outperforms the state-of-the-art methods in\ncross-manipulation evaluations, and has the potential to further improve by\nintroducing extra real faces for training RFFR.\n","authors":["Liang Shi","Jie Zhang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2303.08439v1.pdf","comment":"12 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.08438v1","updated":"2023-03-15T08:24:10Z","published":"2023-03-15T08:24:10Z","title":"Learning Accurate Template Matching with Differentiable Coarse-to-Fine\n  Correspondence Refinement","summary":"  Template matching is a fundamental task in computer vision and has been\nstudied for decades. It plays an essential role in manufacturing industry for\nestimating the poses of different parts, facilitating downstream tasks such as\nrobotic grasping. Existing methods fail when the template and source images\nhave different modalities, cluttered backgrounds or weak textures. They also\nrarely consider geometric transformations via homographies, which commonly\nexist even for planar industrial parts. To tackle the challenges, we propose an\naccurate template matching method based on differentiable coarse-to-fine\ncorrespondence refinement. We use an edge-aware module to overcome the domain\ngap between the mask template and the grayscale image, allowing robust\nmatching. An initial warp is estimated using coarse correspondences based on\nnovel structure-aware information provided by transformers. This initial\nalignment is passed to a refinement network using references and aligned images\nto obtain sub-pixel level correspondences which are used to give the final\ngeometric transformation. Extensive evaluation shows that our method is\nsignificantly better than state-of-the-art methods and baselines, providing\ngood generalization ability and visually plausible results even on unseen real\ndata.\n","authors":["Zhirui Gao","Renjiao Yi","Zheng Qin","Yunfan Ye","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08435v1","updated":"2023-03-15T08:17:07Z","published":"2023-03-15T08:17:07Z","title":"Physics-Informed Optical Kernel Regression Using Complex-valued Neural\n  Fields","summary":"  Lithography is fundamental to integrated circuit fabrication, necessitating\nlarge computation overhead. The advancement of machine learning (ML)-based\nlithography models alleviates the trade-offs between manufacturing process\nexpense and capability. However, all previous methods regard the lithography\nsystem as an image-to-image black box mapping, utilizing network parameters to\nlearn by rote mappings from massive mask-to-aerial or mask-to-resist image\npairs, resulting in poor generalization capability. In this paper, we propose a\nnew ML-based paradigm disassembling the rigorous lithographic model into\nnon-parametric mask operations and learned optical kernels containing\ndeterminant source, pupil, and lithography information. By optimizing\ncomplex-valued neural fields to perform optical kernel regression from\ncoordinates, our method can accurately restore lithography system using a\nsmall-scale training dataset with fewer parameters, demonstrating superior\ngeneralization capability as well. Experiments show that our framework can use\n31\\% of parameters while achieving 69$\\times$ smaller mean squared error with\n1.3$\\times$ higher throughput than the state-of-the-art.\n","authors":["Guojin Chen","Zehua Pei","Haoyu Yang","Yuzhe Ma","Bei Yu","Martin D. F. Wong"],"pdf_url":"https://arxiv.org/pdf/2303.08435v1.pdf","comment":"Accepted by DAC23"},{"id":"http://arxiv.org/abs/2303.08434v1","updated":"2023-03-15T08:12:28Z","published":"2023-03-15T08:12:28Z","title":"DeDA: Deep Directed Accumulator","summary":"  Chronic active multiple sclerosis lesions, also termed as rim+ lesions, can\nbe characterized by a hyperintense rim at the edge of the lesion on\nquantitative susceptibility maps. These rim+ lesions exhibit a geometrically\nsimple structure, where gradients at the lesion edge are radially oriented and\na greater magnitude of gradients is observed in contrast to rim- (non rim+)\nlesions. However, recent studies have shown that the identification performance\nof such lesions remains unsatisfied due to the limited amount of data and high\nclass imbalance. In this paper, we propose a simple yet effective image\nprocessing operation, deep directed accumulator (DeDA), that provides a new\nperspective for injecting domain-specific inductive biases (priors) into neural\nnetworks for rim+ lesion identification. Given a feature map and a set of\nsampling grids, DeDA creates and quantizes an accumulator space into finite\nintervals, and accumulates feature values accordingly. This DeDA operation is a\ngeneralized discrete Radon transform and can also be regarded as a symmetric\noperation to the grid sampling within the forward-backward neural network\nframework, the process of which is order-agnostic, and can be efficiently\nimplemented with the native CUDA programming. Experimental results on a dataset\nwith 177 rim+ and 3986 rim- lesions show that 10.1% of improvement in a partial\n(false positive rate<0.1) area under the receiver operating characteristic\ncurve (pROC AUC) and 10.2% of improvement in an area under the precision recall\ncurve (PR AUC) can be achieved respectively comparing to other state-of-the-art\nmethods. The source code is available online at\nhttps://github.com/tinymilky/DeDA\n","authors":["Hang Zhang","Rongguang Wang","Renjiu Hu","Jinwei Zhang","Jiahao Li"],"pdf_url":"https://arxiv.org/pdf/2303.08434v1.pdf","comment":"18 pages, 3 Tables and 4 figures"},{"id":"http://arxiv.org/abs/2302.02871v2","updated":"2023-03-15T08:12:02Z","published":"2023-02-06T15:38:21Z","title":"Top-Down Beats Bottom-Up in 3D Instance Segmentation","summary":"  Most 3D instance segmentation methods exploit a bottom-up strategy, typically\nincluding resource-exhaustive post-processing. For point grouping, bottom-up\nmethods rely on prior assumptions about the objects in the form of\nhyperparameters, which are domain-specific and need to be carefully tuned. On\nthe contrary, we address 3D instance segmentation with a TD3D: top-down, fully\ndata-driven, simple approach trained in an end-to-end manner. With its\nstraightforward fully-convolutional pipeline, it performs surprisingly well on\nthe standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS.\nBesides, our method is much faster on inference than the current\nstate-of-the-art grouping-based approaches. Code is available at\nhttps://github.com/SamsungLabs/td3d .\n","authors":["Maksim Kolodiazhnyi","Danila Rukhovich","Anna Vorontsova","Anton Konushin"],"pdf_url":"https://arxiv.org/pdf/2302.02871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07333v2","updated":"2023-03-15T08:07:56Z","published":"2022-07-15T08:05:41Z","title":"Rainfall Estimation with SAR using NEXRAD collocations with\n  Convolutional Neural Networks","summary":"  Remote sensing of rainfall events is critical for both operational and\nscientific needs, including for example weather forecasting, extreme flood\nmitigation, water cycle monitoring, etc. Ground-based weather radars, such as\nNOAA's Next-Generation Radar (NEXRAD), provide reflectivity and precipitation\nestimates of rainfall events. However, their observation range is limited to a\nfew hundred kilometers, prompting the exploration of other remote sensing\nmethods, particularly over the open ocean, that represents large areas not\ncovered by land-based radars. Here we propose a deep learning approach to\nextract rainfall information from SAR imagery. SAR has the advantage of\nproviding global coverage and a very high resolution. We demonstrate that a\nconvolutional neural network trained on a collocated Sentinel-1/NEXRAD dataset\nclearly outperforms state-of-the-art filtering schemes such as the Koch's\nfilters, which has been implemented here as a neural network in a machine\nlearning framework. Our results indicate high performance in segmenting\nprecipitation regimes, delineated by thresholds at 24.7, 31.5, and 38.8 dBZ.\nCompared to current methods that rely on Koch's filters to draw binary rainfall\nmaps, these multi-threshold learning-based models can provide rainfall\nestimation. They may be of great interest for improving the qualification of\nSAR-derived wind field data.\n","authors":["Aurélien Colin","Pierre Tandeo","Charles Peureux","Romain Husson","Nicolas Longépé","Ronan Fablet"],"pdf_url":"https://arxiv.org/pdf/2207.07333v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2210.11407v2","updated":"2023-03-15T08:06:48Z","published":"2022-10-20T16:56:47Z","title":"Similarity of Neural Architectures Based on Input Gradient\n  Transferability","summary":"  In recent years, a huge amount of deep neural architectures have been\ndeveloped for image classification. It remains curious whether these models are\nsimilar or different and what factors contribute to their similarities or\ndifferences. To address this question, we aim to design a quantitative and\nscalable similarity function between neural architectures. We utilize\nadversarial attack transferability, which has information related to input\ngradients and decision boundaries that are widely used to understand model\nbehaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet\nclassifiers using our proposed similarity function to answer the question.\nMoreover, we observe neural architecture-related phenomena using model\nsimilarity that model diversity can lead to better performance on model\nensembles and knowledge distillation under specific conditions. Our results\nprovide insights into why the development of diverse neural architectures with\ndistinct components is necessary.\n","authors":["Jaehui Hwang","Dongyoon Han","Byeongho Heo","Song Park","Sanghyuk Chun","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2210.11407v2.pdf","comment":"21pages, 10 figures, 1.5MB"},{"id":"http://arxiv.org/abs/2206.13386v2","updated":"2023-03-15T07:43:31Z","published":"2022-06-17T06:59:51Z","title":"Uncovering variability in human driving behavior through automatic\n  extraction of similar traffic scenes from large naturalistic datasets","summary":"  Recently, multiple naturalistic traffic datasets of human-driven trajectories\nhave been published (e.g., highD, NGSim, and pNEUMA). These datasets have been\nused in studies that investigate variability in human driving behavior, for\nexample for scenario-based validation of autonomous vehicle (AV) behavior,\nmodeling driver behavior, or validating driver models. Thus far, these studies\nfocused on the variability on an operational level (e.g., velocity profiles\nduring a lane change), not on a tactical level (i.e., to change lanes or not).\nInvestigating the variability on both levels is necessary to develop driver\nmodels and AVs that include multiple tactical behaviors. To expose multi-level\nvariability, the human responses to the same traffic scene could be\ninvestigated. However, no method exists to automatically extract similar scenes\nfrom datasets. Here, we present a four-step extraction method that uses the\nHausdorff distance, a mathematical distance metric for sets. We performed a\ncase study on the highD dataset that showed that the method is practically\napplicable. The human responses to the selected scenes exposed the variability\non both the tactical and operational levels. With this new method, the\nvariability in operational and tactical human behavior can be investigated,\nwithout the need for costly and time-consuming driving-simulator experiments.\n","authors":["Olger Siebinga","Arkady Zgonnikov","David Abbink"],"pdf_url":"https://arxiv.org/pdf/2206.13386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07898v2","updated":"2023-03-15T07:41:12Z","published":"2023-03-14T13:36:36Z","title":"AutoEnsemble: Automated Ensemble Search Framework for Semantic\n  Segmentation Using Image Labels","summary":"  A key bottleneck of employing state-of-the-art semantic segmentation networks\nin the real world is the availability of training labels. Standard semantic\nsegmentation networks require massive pixel-wise annotated labels to reach\nstate-of-the-art prediction quality. Hence, several works focus on semantic\nsegmentation networks trained with only image-level annotations. However, when\nscrutinizing the state-of-the-art results in more detail, we notice that\nalthough they are very close to each other on average prediction quality,\ndifferent approaches perform better in different classes while providing low\nquality in others. To address this problem, we propose a novel framework,\nAutoEnsemble, which employs an ensemble of the \"pseudo-labels\" for a given set\nof different segmentation techniques on a class-wise level. Pseudo-labels are\nthe pixel-wise predictions of the image-level semantic segmentation frameworks\nused to train the final segmentation model. Our pseudo-labels seamlessly\ncombine the strong points of multiple segmentation techniques approaches to\nreach superior prediction quality. We reach up to 2.4% improvement over\nAutoEnsemble's components. An exhaustive analysis was performed to demonstrate\nAutoEnsemble's effectiveness over state-of-the-art frameworks for image-level\nsemantic segmentation.\n","authors":["Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07898v2.pdf","comment":"This paper is submitted to a IEEE conference for peer review\n  publication"},{"id":"http://arxiv.org/abs/2303.08419v1","updated":"2023-03-15T07:40:28Z","published":"2023-03-15T07:40:28Z","title":"Multi-Modal Facial Expression Recognition with Transformer-Based Fusion\n  Networks and Dynamic Sampling","summary":"  Facial expression recognition is important for various purpose such as\nemotion detection, mental health analysis, and human-machine interaction. In\nfacial expression recognition, incorporating audio information along with still\nimages can provide a more comprehensive understanding of an expression state.\nThis paper presents the Multi-modal facial expression recognition methods for\nAffective Behavior in-the-wild (ABAW) challenge at CVPR 2023. We propose a\nModal Fusion Module (MFM) to fuse audio-visual information. The modalities used\nare image and audio, and features are extracted based on Swin Transformer to\nforward the MFM. Our approach also addresses imbalances in the dataset through\ndata resampling in training dataset and leverages the rich modal in a single\nframe using dynmaic data sampling, leading to improved performance.\n","authors":["Jun-Hwa Kim","Namho Kim","Chee Sun Won"],"pdf_url":"https://arxiv.org/pdf/2303.08419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08418v1","updated":"2023-03-15T07:39:23Z","published":"2023-03-15T07:39:23Z","title":"SymBa: Symmetric Backpropagation-Free Contrastive Learning with\n  Forward-Forward Algorithm for Optimizing Convergence","summary":"  The paper proposes a new algorithm called SymBa that aims to achieve more\nbiologically plausible learning than Back-Propagation (BP). The algorithm is\nbased on the Forward-Forward (FF) algorithm, which is a BP-free method for\ntraining neural networks. SymBa improves the FF algorithm's convergence\nbehavior by addressing the problem of asymmetric gradients caused by\nconflicting converging directions for positive and negative samples. The\nalgorithm balances positive and negative losses to enhance performance and\nconvergence speed. Furthermore, it modifies the FF algorithm by adding\nIntrinsic Class Pattern (ICP) containing class information to prevent the loss\nof class information during training. The proposed algorithm has the potential\nto improve our understanding of how the brain learns and processes information\nand to develop more effective and efficient artificial intelligence systems.\nThe paper presents experimental results that demonstrate the effectiveness of\nSymBa algorithm compared to the FF algorithm and BP.\n","authors":["Heung-Chang Lee","Jeonggeun Song"],"pdf_url":"https://arxiv.org/pdf/2303.08418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15115v2","updated":"2023-03-15T07:35:09Z","published":"2022-11-28T08:05:45Z","title":"Generalized Category Discovery with Decoupled Prototypical Network","summary":"  Generalized Category Discovery (GCD) aims to recognize both known and novel\ncategories from a set of unlabeled data, based on another dataset labeled with\nonly known categories. Without considering differences between known and novel\ncategories, current methods learn about them in a coupled manner, which can\nhurt model's generalization and discriminative ability. Furthermore, the\ncoupled training approach prevents these models transferring category-specific\nknowledge explicitly from labeled data to unlabeled data, which can lose\nhigh-level semantic information and impair model performance. To mitigate above\nlimitations, we present a novel model called Decoupled Prototypical Network\n(DPN). By formulating a bipartite matching problem for category prototypes, DPN\ncan not only decouple known and novel categories to achieve different training\ntargets effectively, but also align known categories in labeled and unlabeled\ndata to transfer category-specific knowledge explicitly and capture high-level\nsemantics. Furthermore, DPN can learn more discriminative features for both\nknown and novel categories through our proposed Semantic-aware Prototypical\nLearning (SPL). Besides capturing meaningful semantic information, SPL can also\nalleviate the noise of hard pseudo labels through semantic-weighted soft\nassignment. Extensive experiments show that DPN outperforms state-of-the-art\nmodels by a large margin on all evaluation metrics across multiple benchmark\ndatasets. Code and data are available at https://github.com/Lackel/DPN.\n","authors":["Wenbin An","Feng Tian","Qinghua Zheng","Wei Ding","QianYing Wang","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15115v2.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08416v1","updated":"2023-03-15T07:31:55Z","published":"2023-03-15T07:31:55Z","title":"Lung Nodule Segmentation and Low-Confidence Region Prediction with\n  Uncertainty-Aware Attention Mechanism","summary":"  Radiologists have different training and clinical experiences, so they may\nprovide various segmentation annotations for a lung nodule, which causes\nsegmentation uncertainty among multiple annotations. Conventional methods\nusually chose a single annotation as the learning target or tried to learn a\nlatent space of various annotations. Still, they wasted the valuable\ninformation of consensus or disagreements ingrained in the multiple\nannotations. This paper proposes an Uncertainty-Aware Attention Mechanism\n(UAAM), which utilizes consensus or disagreements among annotations to produce\na better segmentation. In UAAM, we propose a Multi-Confidence Mask (MCM), which\nis a combination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask.\nLC mask indicates regions with low segmentation confidence, which may cause\ndifferent segmentation options among radiologists. Following UAAM, we further\ndesign an Uncertainty-Guide Segmentation Network (UGS-Net), which contains\nthree modules:Feature Extracting Module captures a general feature of a lung\nnodule. Uncertainty-Aware Module produce three features for the annotations'\nunion, intersection, and annotation set. Finally, Intersection-Union\nConstraining Module use distances between three features to balance the\npredictions of final segmentation, LC mask, and HC mask. To fully demonstrate\nthe performance of our method, we propose a Complex Nodule Challenge on\nLIDC-IDRI, which tests UGS-Net's segmentation performance on the lung nodules\nthat are difficult to segment by U-Net. Experimental results demonstrate that\nour method can significantly improve the segmentation performance on nodules\nwith poor segmentation by U-Net.\n","authors":["Han Yang","Qiuli Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08416v1.pdf","comment":"10 pages, 10 figures. We have reported a preliminary version of this\n  work in MICCAI 2022"},{"id":"http://arxiv.org/abs/2303.08415v1","updated":"2023-03-15T07:31:06Z","published":"2023-03-15T07:31:06Z","title":"Rice paddy disease classifications using CNNs","summary":"  Rice is a staple food in the world's diet, and yet huge percentages of crop\nyields are lost each year to disease. To combat this problem, people have been\nsearching for ways to automate disease diagnosis. Here, we extend on previous\nmodelling work by analysing how disease-classification accuracy is sensitive to\nboth model architecture and common computer vision techniques. In doing so, we\nmaximise accuracy whilst working in the constraints of smaller model sizes,\nminimum GPUs and shorter training times. Whilst previous state-of-the-art\nmodels had 93% accuracy only predicting 5 diseases, we improve this to 98.7%\nusing 10 disease classes.\n","authors":["Charles O'Neill"],"pdf_url":"https://arxiv.org/pdf/2303.08415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08414v1","updated":"2023-03-15T07:28:46Z","published":"2023-03-15T07:28:46Z","title":"From Local Binary Patterns to Pixel Difference Networks for Efficient\n  Visual Representation Learning","summary":"  LBP is a successful hand-crafted feature descriptor in computer vision.\nHowever, in the deep learning era, deep neural networks, especially\nconvolutional neural networks (CNNs) can automatically learn powerful\ntask-aware features that are more discriminative and of higher representational\ncapacity. To some extent, such hand-crafted features can be safely ignored when\ndesigning deep computer vision models. Nevertheless, due to LBP's preferable\nproperties in visual representation learning, an interesting topic has arisen\nto explore the value of LBP in enhancing modern deep models in terms of\nefficiency, memory consumption, and predictive performance. In this paper, we\nprovide a comprehensive review on such efforts which aims to incorporate the\nLBP mechanism into the design of CNN modules to make deep models stronger. In\nretrospect of what has been achieved so far, the paper discusses open\nchallenges and directions for future research.\n","authors":["Zhuo Su","Matti Pietikäinen","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08414v1.pdf","comment":"A small survey paper on Local Binary Pattern (LBP) inspired Deep\n  Learning networks, accepted in SCIA 2023 (Scandinavian Conference on Image\n  Analysis)"},{"id":"http://arxiv.org/abs/2303.08409v1","updated":"2023-03-15T07:21:28Z","published":"2023-03-15T07:21:28Z","title":"Lana: A Language-Capable Navigator for Instruction Following and\n  Generation","summary":"  Recently, visual-language navigation (VLN) -- entailing robot agents to\nfollow navigation instructions -- has shown great advance. However, existing\nliterature put most emphasis on interpreting instructions into actions, only\ndelivering \"dumb\" wayfinding agents. In this article, we devise LANA, a\nlanguage-capable navigation agent which is able to not only execute\nhuman-written navigation commands, but also provide route descriptions to\nhumans. This is achieved by simultaneously learning instruction following and\ngeneration with only one single model. More specifically, two encoders,\nrespectively for route and language encoding, are built and shared by two\ndecoders, respectively, for action prediction and instruction generation, so as\nto exploit cross-task knowledge and capture task-specific characteristics.\nThroughout pretraining and fine-tuning, both instruction following and\ngeneration are set as optimization objectives. We empirically verify that,\ncompared with recent advanced task-specific solutions, LANA attains better\nperformances on both instruction following and route description, with nearly\nhalf complexity. In addition, endowed with language generation capability, LANA\ncan explain to humans its behaviors and assist human's wayfinding. This work is\nexpected to foster future efforts towards building more trustworthy and\nsocially-intelligent navigation robots.\n","authors":["Xiaohan Wang","Wenguan Wang","Jiayi Shao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08409v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12036v2","updated":"2023-03-15T07:11:13Z","published":"2022-11-22T06:19:17Z","title":"Dual Prototype Attention for Unsupervised Video Object Segmentation","summary":"  Unsupervised video object segmentation (VOS) aims to detect and segment the\nmost salient object in videos. The primary techniques used in unsupervised VOS\nare 1) the collaboration of appearance and motion information and 2) temporal\nfusion between different frames. This paper proposes two novel prototype-based\nattention mechanisms, inter-modality attention (IMA) and inter-frame attention\n(IFA), to incorporate these techniques via dense propagation across different\nmodalities and frames. IMA densely integrates context information from\ndifferent modalities based on a mutual refinement. IFA injects global context\nof a video to the query frame, enabling a full utilization of useful properties\nfrom multiple frames. Experimental results on public benchmark datasets\ndemonstrate that our proposed approach outperforms all existing methods by a\nsubstantial margin. The proposed two components are also thoroughly validated\nvia ablative study.\n","authors":["Suhwan Cho","Minhyeok Lee","Seunghoon Lee","Dogyoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2211.12036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07662v2","updated":"2023-03-15T07:07:48Z","published":"2023-03-14T07:07:34Z","title":"One scalar is all you need -- absolute depth estimation using monocular\n  self-supervision","summary":"  Self-supervised monocular depth estimators can be trained or fine-tuned on\nnew scenes using only images and no ground-truth depth data, achieving good\naccuracy. However, these estimators suffer from the inherent ambiguity of the\ndepth scale, significantly limiting their applicability. In this work, we\npresent a method for transferring the depth-scale from existing source datasets\ncollected with ground-truth depths to depth estimators that are trained using\nself-supervision on a newly collected target dataset consisting of images only,\nsolving a significant limiting factor. We show that self-supervision based on\nprojective geometry results in predicted depths that are linearly correlated\nwith their ground-truth depths. Moreover, the linearity of this relationship\nalso holds when jointly training on images from two different (real or\nsynthetic) source and target domains. We utilize this observed property and\nmodel the relationship between the ground-truth and the predicted up-to-scale\ndepths of images from the source domain using a single global scalar. Then, we\nscale the predicted up-to-scale depths of images from the target domain using\nthe estimated global scaling factor, performing depth-scale transfer between\nthe two domains. This suggested method was evaluated on the target KITTI and\nDDAD datasets, while using other real or synthetic source datasets, that have a\nlarger field-of-view, other image style or structural content. Our approach\nachieves competitive accuracy on KITTI, even without using the specially\ntailored vKITTI or vKITTI2 datasets, and higher accuracy on DDAD, when using\nboth real or synthetic source datasets.\n","authors":["Alexandra Dana","Nadav Carmel","Amit Shomer","Ofer Manela","Tomer Peleg"],"pdf_url":"https://arxiv.org/pdf/2303.07662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08401v1","updated":"2023-03-15T07:05:07Z","published":"2023-03-15T07:05:07Z","title":"Implicit Ray-Transformers for Multi-view Remote Sensing Image\n  Segmentation","summary":"  The mainstream CNN-based remote sensing (RS) image semantic segmentation\napproaches typically rely on massive labeled training data. Such a paradigm\nstruggles with the problem of RS multi-view scene segmentation with limited\nlabeled views due to the lack of considering 3D information within the scene.\nIn this paper, we propose ''Implicit Ray-Transformer (IRT)'' based on Implicit\nNeural Representation (INR), for RS scene semantic segmentation with sparse\nlabels (such as 4-6 labels per 100 images). We explore a new way of introducing\nmulti-view 3D structure priors to the task for accurate and view-consistent\nsemantic segmentation. The proposed method includes a two-stage learning\nprocess. In the first stage, we optimize a neural field to encode the color and\n3D structure of the remote sensing scene based on multi-view images. In the\nsecond stage, we design a Ray Transformer to leverage the relations between the\nneural field 3D features and 2D texture features for learning better semantic\nrepresentations. Different from previous methods that only consider 3D prior or\n2D features, we incorporate additional 2D texture information and 3D prior by\nbroadcasting CNN features to different point features along the sampled ray. To\nverify the effectiveness of the proposed method, we construct a challenging\ndataset containing six synthetic sub-datasets collected from the Carla platform\nand three real sub-datasets from Google Maps. Experiments show that the\nproposed method outperforms the CNN-based methods and the state-of-the-art\nINR-based segmentation methods in quantitative and qualitative metrics.\n","authors":["Zipeng Qi","Hao Chen","Chenyang Liu","Zhenwei Shi","Zhengxia Zou"],"pdf_url":"https://arxiv.org/pdf/2303.08401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08398v1","updated":"2023-03-15T07:01:44Z","published":"2023-03-15T07:01:44Z","title":"A Triplet-loss Dilated Residual Network for High-Resolution\n  Representation Learning in Image Retrieval","summary":"  Content-based image retrieval is the process of retrieving a subset of images\nfrom an extensive image gallery based on visual contents, such as color, shape\nor spatial relations, and texture. In some applications, such as localization,\nimage retrieval is employed as the initial step. In such cases, the accuracy of\nthe top-retrieved images significantly affects the overall system accuracy. The\ncurrent paper introduces a simple yet efficient image retrieval system with a\nfewer trainable parameters, which offers acceptable accuracy in top-retrieved\nimages. The proposed method benefits from a dilated residual convolutional\nneural network with triplet loss. Experimental evaluations show that this model\ncan extract richer information (i.e., high-resolution representations) by\nenlarging the receptive field, thus improving image retrieval accuracy without\nincreasing the depth or complexity of the model. To enhance the extracted\nrepresentations' robustness, the current research obtains candidate regions of\ninterest from each feature map and applies Generalized-Mean pooling to the\nregions. As the choice of triplets in a triplet-based network affects the model\ntraining, we employ a triplet online mining method. We test the performance of\nthe proposed method under various configurations on two of the challenging\nimage-retrieval datasets, namely Revisited Paris6k (RPar) and UKBench. The\nexperimental results show an accuracy of 94.54 and 80.23 (mean precision at\nrank 10) in the RPar medium and hard modes and 3.86 (recall at rank 4) in the\nUKBench dataset, respectively.\n","authors":["Saeideh Yousefzadeh","Hamidreza Pourreza","Hamidreza Mahyar"],"pdf_url":"https://arxiv.org/pdf/2303.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06796v2","updated":"2023-03-15T06:31:49Z","published":"2023-03-13T01:02:28Z","title":"Ins-ATP: Deep Estimation of ATP for Organoid Based on High Throughput\n  Microscopic Images","summary":"  Adenosine triphosphate (ATP) is a high-energy phosphate compound and the most\ndirect energy source in organisms. ATP is an essential biomarker for evaluating\ncell viability in biology. Researchers often use ATP bioluminescence to measure\nthe ATP of organoid after drug to evaluate the drug efficacy. However, ATP\nbioluminescence has some limitations, leading to unreliable drug screening\nresults. Performing ATP bioluminescence causes cell lysis of organoids, so it\nis impossible to observe organoids' long-term viability changes after\nmedication continually. To overcome the disadvantages of ATP bioluminescence,\nwe propose Ins-ATP, a non-invasive strategy, the first organoid ATP estimation\nmodel based on the high-throughput microscopic image. Ins-ATP directly\nestimates the ATP of organoids from high-throughput microscopic images, so that\nit does not influence the drug reactions of organoids. Therefore, the ATP\nchange of organoids can be observed for a long time to obtain more stable\nresults. Experimental results show that the ATP estimation by Ins-ATP is in\ngood agreement with those determined by ATP bioluminescence. Specifically, the\npredictions of Ins-ATP are consistent with the results measured by ATP\nbioluminescence in the efficacy evaluation experiments of different drugs.\n","authors":["Xuesheng Bian","Cheng Wang","Shuting Chen","Weiquan Liu","Sen Xu","Jinxin Zhu","Rugang Wang","Zexin Chen","Min Huang","Gang Li"],"pdf_url":"https://arxiv.org/pdf/2303.06796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08384v1","updated":"2023-03-15T06:00:38Z","published":"2023-03-15T06:00:38Z","title":"Rethinking Optical Flow from Geometric Matching Consistent Perspective","summary":"  Optical flow estimation is a challenging problem remaining unsolved. Recent\ndeep learning based optical flow models have achieved considerable success.\nHowever, these models often train networks from the scratch on standard optical\nflow data, which restricts their ability to robustly and geometrically match\nimage features. In this paper, we propose a rethinking to previous optical flow\nestimation. We particularly leverage Geometric Image Matching (GIM) as a\npre-training task for the optical flow estimation (MatchFlow) with better\nfeature representations, as GIM shares some common challenges as optical flow\nestimation, and with massive labeled real-world data. Thus, matching static\nscenes helps to learn more fundamental feature correlations of objects and\nscenes with consistent displacements. Specifically, the proposed MatchFlow\nmodel employs a QuadTree attention-based network pre-trained on MegaDepth to\nextract coarse features for further flow regression. Extensive experiments show\nthat our model has great cross-dataset generalization. Our method achieves\n11.5% and 10.1% error reduction from GMA on Sintel clean pass and KITTI test\nset. At the time of anonymous submission, our MatchFlow(G) enjoys\nstate-of-the-art performance on Sintel clean and final pass compared to\npublished approaches with comparable computation and memory footprint. Codes\nand models will be released in https://github.com/DQiaole/MatchFlow.\n","authors":["Qiaole Dong","Chenjie Cao","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2303.08384v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08370v1","updated":"2023-03-15T05:15:21Z","published":"2023-03-15T05:15:21Z","title":"Harnessing Low-Frequency Neural Fields for Few-Shot View Synthesis","summary":"  Neural Radiance Fields (NeRF) have led to breakthroughs in the novel view\nsynthesis problem. Positional Encoding (P.E.) is a critical factor that brings\nthe impressive performance of NeRF, where low-dimensional coordinates are\nmapped to high-dimensional space to better recover scene details. However,\nblindly increasing the frequency of P.E. leads to overfitting when the\nreconstruction problem is highly underconstrained, \\eg, few-shot images for\ntraining. We harness low-frequency neural fields to regularize high-frequency\nneural fields from overfitting to better address the problem of few-shot view\nsynthesis. We propose reconstructing with a low-frequency only field and then\nfinishing details with a high-frequency equipped field. Unlike most existing\nsolutions that regularize the output space (\\ie, rendered images), our\nregularization is conducted in the input space (\\ie, signal frequency). We\nfurther propose a simple-yet-effective strategy for tuning the frequency to\navoid overfitting few-shot inputs: enforcing consistency among the frequency\ndomain of rendered 2D images. Thanks to the input space regularizing scheme,\nour method readily applies to inputs beyond spatial locations, such as the time\ndimension in dynamic scenes. Comparisons with state-of-the-art on both\nsynthetic and natural datasets validate the effectiveness of our proposed\nsolution for few-shot view synthesis. Code is available at\n\\href{https://github.com/lsongx/halo}{https://github.com/lsongx/halo}.\n","authors":["Liangchen Song","Zhong Li","Xuan Gong","Lele Chen","Zhang Chen","Yi Xu","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08367v1","updated":"2023-03-15T04:58:43Z","published":"2023-03-15T04:58:43Z","title":"Uncertainty-Aware Pedestrian Trajectory Prediction via Distributional\n  Diffusion","summary":"  Tremendous efforts have been devoted to pedestrian trajectory prediction\nusing generative modeling for accommodating uncertainty and multi-modality in\nhuman behaviors. An individual's inherent uncertainty, e.g., change of\ndestination, can be masked by complex patterns resulting from the movements of\ninteracting pedestrians. However, latent variable-based generative models often\nentangle such uncertainty with complexity, leading to either limited\nexpressivity or overconfident predictions. In this work, we propose to\nseparately model these two factors by implicitly deriving a flexible\ndistribution that describes complex pedestrians' movements, whereas\nincorporating predictive uncertainty of individuals with explicit density\nfunctions over their future locations. More specifically, we present an\nuncertainty-aware pedestrian trajectory prediction framework, parameterizing\nsufficient statistics for the distributions of locations that jointly comprise\nthe multi-modal trajectories. We further estimate these parameters of interest\nby approximating a denoising process that progressively recovers pedestrian\nmovements from noise. Unlike prior studies, we translate the predictive\nstochasticity to the explicit distribution, making it readily used to generate\nplausible future trajectories indicating individuals' self-uncertainty.\nMoreover, our framework is model-agnostic for compatibility with different\nneural network architectures. We empirically show the performance advantages of\nour framework on widely-used benchmarks, outperforming state-of-the-art in most\nscenes even with lighter backbones.\n","authors":["Yao Liu","Zesheng Ye","Binghao Li","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2303.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15158v3","updated":"2023-03-15T04:49:40Z","published":"2022-11-28T09:14:36Z","title":"Heterogeneous Graph Learning for Multi-modal Medical Data Analysis","summary":"  Routine clinical visits of a patient produce not only image data, but also\nnon-image data containing clinical information regarding the patient, i.e.,\nmedical data is multi-modal in nature. Such heterogeneous modalities offer\ndifferent and complementary perspectives on the same patient, resulting in more\naccurate clinical decisions when they are properly combined. However, despite\nits significance, how to effectively fuse the multi-modal medical data into a\nunified framework has received relatively little attention. In this paper, we\npropose an effective graph-based framework called HetMed (Heterogeneous Graph\nLearning for Multi-modal Medical Data Analysis) for fusing the multi-modal\nmedical data. Specifically, we construct a multiplex network that incorporates\nmultiple types of non-image features of patients to capture the complex\nrelationship between patients in a systematic way, which leads to more accurate\nclinical decisions. Extensive experiments on various real-world datasets\ndemonstrate the superiority and practicality of HetMed. The source code for\nHetMed is available at https://github.com/Sein-Kim/Multimodal-Medical.\n","authors":["Sein Kim","Namkyeong Lee","Junseok Lee","Dongmin Hyun","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2211.15158v3.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08364v1","updated":"2023-03-15T04:48:19Z","published":"2023-03-15T04:48:19Z","title":"Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle\n  Consistency Losses","summary":"  Analyzing the dynamic changes of cellular morphology is important for\nunderstanding the various functions and characteristics of live cells,\nincluding stem cells and metastatic cancer cells. To this end, we need to track\nall points on the highly deformable cellular contour in every frame of live\ncell video. Local shapes and textures on the contour are not evident, and their\nmotions are complex, often with expansion and contraction of local contour\nfeatures. The prior arts for optical flow or deep point set tracking are\nunsuited due to the fluidity of cells, and previous deep contour tracking does\nnot consider point correspondence. We propose the first deep learning-based\ntracking of cellular (or more generally viscoelastic materials) contours with\npoint correspondence by fusing dense representation between two contours with\ncross attention. Since it is impractical to manually label dense tracking\npoints on the contour, unsupervised learning comprised of the mechanical and\ncyclical consistency losses is proposed to train our contour tracker. The\nmechanical loss forcing the points to move perpendicular to the contour\neffectively helps out. For quantitative evaluation, we labeled sparse tracking\npoints along the contour of live cells from two live cell datasets taken with\nphase contrast and confocal fluorescence microscopes. Our contour tracker\nquantitatively outperforms compared methods and produces qualitatively more\nfavorable results. Our code and data are publicly available at\nhttps://github.com/JunbongJang/contour-tracking/\n","authors":["Junbong Jang","Kwonmoo Lee","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2303.08364v1.pdf","comment":"12 pages, 9 figures, Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2211.09288v2","updated":"2023-03-15T04:40:51Z","published":"2022-11-17T01:43:53Z","title":"Longitudinal thermal imaging for scalable non-residential HVAC and\n  occupant behaviour characterization","summary":"  This work presents a study on the characterization of the air-conditioning\n(AC) usage pattern of non-residential buildings from thermal images collected\nfrom an urban-scale infrared (IR) observatory. To achieve this first, an image\nprocessing scheme, for cleaning and extraction of the temperature time series\nfrom the thermal images is implemented. To test the accuracy of the thermal\nmeasurements using IR camera, the extracted temperature is compared against the\nground truth surface temperature measurements. It is observed that the\ndetrended thermal measurements match well with the ground truth surface\ntemperature measurements. Subsequently, the operational pattern of the\nwater-cooled systems and window AC units are extracted from the analysis of the\nthermal signature. It is observed that for the water-cooled system, the\ndifference between the rate of change of the window and wall can be used to\nextract the operational pattern. While, in the case of the window AC units,\nwavelet transform of the AC unit temperature is used to extract the frequency\nand time domain information of the AC unit operation. The results of the\nanalysis are compared against the indoor temperature sensors installed in the\noffice spaces of the building. It is realized that the accuracy in the\nprediction of the operational pattern is highest between 8 pm to 10 am, and it\nreduces during the day because of solar radiation and high daytime temperature.\nSubsequently, a characterization study is conducted for eight window/split AC\nunits from the thermal image collected during the nighttime. This forms one of\nthe first studies on the operational behavior of HVAC systems for\nnon-residential buildings using the longitudinal thermal imaging technique. The\noutput from this study can be used to better understand the operational and\noccupant behavior, without requiring to deploy a large array of sensors in the\nbuilding space.\n","authors":["Vasantha Ramani","Miguel Martin","Pandarasamy Arjunan","Adrian Chong","Kameshwar Poolla","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2211.09288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08360v1","updated":"2023-03-15T04:39:01Z","published":"2023-03-15T04:39:01Z","title":"Knowledge Distillation from Single to Multi Labels: an Empirical Study","summary":"  Knowledge distillation (KD) has been extensively studied in single-label\nimage classification. However, its efficacy for multi-label classification\nremains relatively unexplored. In this study, we firstly investigate the\neffectiveness of classical KD techniques, including logit-based and\nfeature-based methods, for multi-label classification. Our findings indicate\nthat the logit-based method is not well-suited for multi-label classification,\nas the teacher fails to provide inter-category similarity information or\nregularization effect on student model's training. Moreover, we observe that\nfeature-based methods struggle to convey compact information of multiple labels\nsimultaneously. Given these limitations, we propose that a suitable dark\nknowledge should incorporate class-wise information and be highly correlated\nwith the final classification results. To address these issues, we introduce a\nnovel distillation method based on Class Activation Maps (CAMs), which is both\neffective and straightforward to implement. Across a wide range of settings,\nCAMs-based distillation consistently outperforms other methods.\n","authors":["Youcai Zhang","Yuzhuo Qin","Hengwei Liu","Yanhao Zhang","Yaqian Li","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2303.08360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08358v1","updated":"2023-03-15T04:24:01Z","published":"2023-03-15T04:24:01Z","title":"DICNet: Deep Instance-Level Contrastive Network for Double Incomplete\n  Multi-View Multi-Label Classification","summary":"  In recent years, multi-view multi-label learning has aroused extensive\nresearch enthusiasm. However, multi-view multi-label data in the real world is\ncommonly incomplete due to the uncertain factors of data collection and manual\nannotation, which means that not only multi-view features are often missing,\nand label completeness is also difficult to be satisfied. To deal with the\ndouble incomplete multi-view multi-label classification problem, we propose a\ndeep instance-level contrastive network, namely DICNet. Different from\nconventional methods, our DICNet focuses on leveraging deep neural network to\nexploit the high-level semantic representations of samples rather than\nshallow-level features. First, we utilize the stacked autoencoders to build an\nend-to-end multi-view feature extraction framework to learn the view-specific\nrepresentations of samples. Furthermore, in order to improve the consensus\nrepresentation ability, we introduce an incomplete instance-level contrastive\nlearning scheme to guide the encoders to better extract the consensus\ninformation of multiple views and use a multi-view weighted fusion module to\nenhance the discrimination of semantic features. Overall, our DICNet is adept\nin capturing consistent discriminative representations of multi-view\nmulti-label data and avoiding the negative effects of missing views and missing\nlabels. Extensive experiments performed on five datasets validate that our\nmethod outperforms other state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Xiaoling Luo","Chao Huang","Zhihao Wu","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08358v1.pdf","comment":"Accepted to AAAi-2023"},{"id":"http://arxiv.org/abs/2303.08356v1","updated":"2023-03-15T04:15:57Z","published":"2023-03-15T04:15:57Z","title":"Continuous emotion recognition based on TCN and Transformer","summary":"  Human emotion recognition plays an important role in human-computer\ninteraction. In this paper, we present our approach to the Valence-Arousal (VA)\nEstimation Challenge, Expression (Expr) Classification Challenge, and Action\nUnit (AU) Detection Challenge of the 5th Workshop and Competition on Affective\nBehavior Analysis in-the-wild (ABAW). Specifically, we propose a novel\nmulti-modal fusion model that leverages Temporal Convolutional Networks (TCN)\nand Transformer to enhance the performance of continuous emotion recognition.\nOur model aims to effectively integrate visual and audio information for\nimproved accuracy in recognizing emotions. The model is evaluate with\nConcordance Correlation Coefficient (CCC)\n","authors":["Weiwei Zhou","Jiada Lu","Zhaolong Xiong","Weifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04681v2","updated":"2023-03-15T04:06:46Z","published":"2022-12-09T06:06:47Z","title":"Dynamic Test-Time Augmentation via Differentiable Functions","summary":"  Distribution shifts, which often occur in the real world, degrade the\naccuracy of deep learning systems, and thus improving robustness is essential\nfor practical applications. To improve robustness, we study an image\nenhancement method that generates recognition-friendly images without\nretraining the recognition model. We propose a novel image enhancement method,\nDynTTA, which is based on differentiable data augmentation techniques and\ngenerates a blended image from many augmented images to improve the recognition\naccuracy under distribution shifts. In addition to standard data augmentations,\nDynTTA also incorporates deep neural network-based image transformation, which\nfurther improves the robustness. Because DynTTA is composed of differentiable\nfunctions, it is directly trained with the classification loss of the\nrecognition model. We experiment with widely used image recognition datasets\nusing various classification models, including Vision Transformer and\nMLP-Mixer. DynTTA improves the robustness with almost no reduction in\nclassification accuracy for clean images, which is a better result than the\nexisting methods. Furthermore, we show that estimating the training time\naugmentation for distribution-shifted datasets using DynTTA and retraining the\nrecognition model with the estimated augmentation significantly improves\nrobustness.\n","authors":["Shohei Enomoto","Monikka Roslianna Busto","Takeharu Eda"],"pdf_url":"https://arxiv.org/pdf/2212.04681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10705v4","updated":"2023-03-15T04:00:10Z","published":"2022-11-19T14:06:58Z","title":"TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer","summary":"  In this paper, we introduce a set of simple yet effective TOken REduction\n(TORE) strategies for Transformer-based Human Mesh Recovery from monocular\nimages. Current SOTA performance is achieved by Transformer-based structures.\nHowever, they suffer from high model complexity and computation cost caused by\nredundant tokens. We propose token reduction strategies based on two important\naspects, i.e., the 3D geometry structure and 2D image feature, where we\nhierarchically recover the mesh geometry with priors from body structure and\nconduct token clustering to pass fewer but more discriminative image feature\ntokens to the Transformer. Our method massively reduces the number of tokens\ninvolved in high-complexity interactions in the Transformer. This leads to a\nsignificantly reduced computational cost while still achieving competitive or\neven higher accuracy in shape recovery. Extensive experiments across a wide\nrange of benchmarks validate the superior effectiveness of the proposed method.\nWe further demonstrate the generalizability of our method on hand mesh\nrecovery. Our code will be publicly available once the paper is published.\n","authors":["Zhiyang Dou","Qingxuan Wu","Cheng Lin","Zeyu Cao","Qiangqiang Wu","Weilin Wan","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10705v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08348v1","updated":"2023-03-15T03:59:27Z","published":"2023-03-15T03:59:27Z","title":"Active Teacher for Semi-Supervised Object Detection","summary":"  In this paper, we study teacher-student learning from the perspective of data\ninitialization and propose a novel algorithm called Active Teacher(Source code\nare available at: \\url{https://github.com/HunterJ-Lin/ActiveTeacher}) for\nsemi-supervised object detection (SSOD). Active Teacher extends the\nteacher-student framework to an iterative version, where the label set is\npartially initialized and gradually augmented by evaluating three key factors\nof unlabeled examples, including difficulty, information and diversity. With\nthis design, Active Teacher can maximize the effect of limited label\ninformation while improving the quality of pseudo-labels. To validate our\napproach, we conduct extensive experiments on the MS-COCO benchmark and compare\nActive Teacher with a set of recently proposed SSOD methods. The experimental\nresults not only validate the superior performance gain of Active Teacher over\nthe compared methods, but also show that it enables the baseline network, ie,\nFaster-RCNN, to achieve 100% supervised performance with much less label\nexpenditure, ie 40% labeled examples on MS-COCO. More importantly, we believe\nthat the experimental analyses in this paper can provide useful empirical\nknowledge for data annotation in practical applications.\n","authors":["Peng Mi","Jianghang Lin","Yiyi Zhou","Yunhang Shen","Gen Luo","Xiaoshuai Sun","Liujuan Cao","Rongrong Fu","Qiang Xu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.08348v1.pdf","comment":"8 pages, 7 figures, CVPR2022"},{"id":"http://arxiv.org/abs/2303.08345v1","updated":"2023-03-15T03:54:43Z","published":"2023-03-15T03:54:43Z","title":"Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding\n  in Long Videos","summary":"  Video temporal grounding aims to pinpoint a video segment that matches the\nquery description. Despite the recent advance in short-form videos\n(\\textit{e.g.}, in minutes), temporal grounding in long videos (\\textit{e.g.},\nin hours) is still at its early stage. To address this challenge, a common\npractice is to employ a sliding window, yet can be inefficient and inflexible\ndue to the limited number of frames within the window. In this work, we propose\nan end-to-end framework for fast temporal grounding, which is able to model an\nhours-long video with \\textbf{one-time} network execution. Our pipeline is\nformulated in a coarse-to-fine manner, where we first extract context knowledge\nfrom non-overlapped video clips (\\textit{i.e.}, anchors), and then supplement\nthe anchors that highly response to the query with detailed content knowledge.\nBesides the remarkably high pipeline efficiency, another advantage of our\napproach is the capability of capturing long-range temporal correlation, thanks\nto modeling the entire video as a whole, and hence facilitates more accurate\ngrounding. Experimental results suggest that, on the long-form video datasets\nMAD and Ego4d, our method significantly outperforms state-of-the-arts, and\nachieves \\textbf{14.6$\\times$} / \\textbf{102.8$\\times$} higher efficiency\nrespectively. The code will be released at\n\\url{https://github.com/afcedf/SOONet.git}\n","authors":["Yulin Pan","Xiangteng He","Biao Gong","Yiliang Lv","Yujun Shen","Yuxin Peng","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.08345v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2210.10959v5","updated":"2023-03-15T03:42:31Z","published":"2022-10-20T02:00:58Z","title":"Geo6D: Geometric Constraints Learning for 6D Pose Estimation","summary":"  Existing direct 6D pose estimation methods regress target 6D poses without\nthe need for post-processing, making them effective and easy to develop.\nHowever, due to the lack of geometric constraints, the precision of these\napproaches remains limited, and more training data are required to achieve\nbetter performance. To this end, we propose a geometric constraint-based 6D\npose estimation method (Geo6D), which establishes an explicit geometric\nconstraint between the input and the regression target. Specifically, a Geo\nhead is proposed to build a point-to-point relationship between the camera\nframe and the object frame. And additional inputs are introduced as\nsupplementary information to facilitate the learning of the geometric\ncorrelation. The proposed Geo6D can be used as a plugin in mainstream direct 6D\npose estimation methods. Extensive experimental results show that when equipped\nwith Geo6D, the direct 6D method achieves state-of-the-art performance on\nmultiple datasets and shows significant effectiveness with limited data volume.\n","authors":["Jianqiu Chen","Mingshan Sun","Ye Zheng","Tianpeng Bao","Zhenyu He","Donghai Li","Guoqiang Jin","Rui Zhao","Liwei Wu","Xiaoke Jiang"],"pdf_url":"https://arxiv.org/pdf/2210.10959v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13428v2","updated":"2023-03-15T03:38:08Z","published":"2023-01-31T05:51:05Z","title":"Contrast and Clustering: Learning Neighborhood Pair Representation for\n  Source-free Domain Adaptation","summary":"  Unsupervised domain adaptation uses source data from different distributions\nto solve the problem of classifying data from unlabeled target domains.\nHowever, conventional methods require access to source data, which often raise\nconcerns about data privacy. In this paper, we consider a more practical but\nchallenging setting where the source domain data is unavailable and the target\ndomain data is unlabeled. Specifically, we address the domain discrepancy\nproblem from the perspective of contrastive learning. The key idea of our work\nis to learn a domain-invariant feature by 1) performing clustering directly in\nthe original feature space with nearest neighbors; 2) constructing truly hard\nnegative pairs by extended neighbors without introducing additional\ncomputational complexity; and 3) combining noise-contrastive estimation theory\nto gain computational advantage. We conduct careful ablation studies and\nextensive experiments on three common benchmarks: VisDA, Office-Home, and\nOffice-31. The results demonstrate the superiority of our methods compared with\nother state-of-the-art works.\n","authors":["Yuqi Chen","Xiangbin Zhu","Yonggang Li","Yingjian Li","Yuanwang Wei","Haojie Fang"],"pdf_url":"https://arxiv.org/pdf/2301.13428v2.pdf","comment":"Journal articles"},{"id":"http://arxiv.org/abs/2303.08340v1","updated":"2023-03-15T03:14:30Z","published":"2023-03-15T03:14:30Z","title":"VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow\n  Estimation","summary":"  We introduce VideoFlow, a novel optical flow estimation framework for videos.\nIn contrast to previous methods that learn to estimate optical flow from two\nframes, VideoFlow concurrently estimates bi-directional optical flows for\nmultiple frames that are available in videos by sufficiently exploiting\ntemporal cues. We first propose a TRi-frame Optical Flow (TROF) module that\nestimates bi-directional optical flows for the center frame in a three-frame\nmanner. The information of the frame triplet is iteratively fused onto the\ncenter frame. To extend TROF for handling more frames, we further propose a\nMOtion Propagation (MOP) module that bridges multiple TROFs and propagates\nmotion features between adjacent TROFs. With the iterative flow estimation\nrefinement, the information fused in individual TROFs can be propagated into\nthe whole sequence via MOP. By effectively exploiting video information,\nVideoFlow presents extraordinary performance, ranking 1st on all public\nbenchmarks. On the Sintel benchmark, VideoFlow achieves 1.649 and 0.991 average\nend-point-error (AEPE) on the final and clean passes, a 15.1% and 7.6% error\nreduction from the best published results (1.943 and 1.073 from FlowFormer++).\nOn the KITTI-2015 benchmark, VideoFlow achieves an F1-all error of 3.65%, a\n19.2% error reduction from the best published result (4.52% from FlowFormer++).\n","authors":["Xiaoyu Shi","Zhaoyang Huang","Weikang Bian","Dasong Li","Manyuan Zhang","Ka Chun Cheung","Simon See","Hongwei Qin","Jifeng Dai","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.08340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07938v2","updated":"2023-03-15T03:13:08Z","published":"2023-03-14T14:25:29Z","title":"Controllable Mesh Generation Through Sparse Latent Point Diffusion\n  Models","summary":"  Mesh generation is of great value in various applications involving computer\ngraphics and virtual content, yet designing generative models for meshes is\nchallenging due to their irregular data structure and inconsistent topology of\nmeshes in the same category. In this work, we design a novel sparse latent\npoint diffusion model for mesh generation. Our key insight is to regard point\nclouds as an intermediate representation of meshes, and model the distribution\nof point clouds instead. While meshes can be generated from point clouds via\ntechniques like Shape as Points (SAP), the challenges of directly generating\nmeshes can be effectively avoided. To boost the efficiency and controllability\nof our mesh generation method, we propose to further encode point clouds to a\nset of sparse latent points with point-wise semantic meaningful features, where\ntwo DDPMs are trained in the space of sparse latent points to respectively\nmodel the distribution of the latent point positions and features at these\nlatent points. We find that sampling in this latent space is faster than\ndirectly sampling dense point clouds. Moreover, the sparse latent points also\nenable us to explicitly control both the overall structures and local details\nof the generated meshes. Extensive experiments are conducted on the ShapeNet\ndataset, where our proposed sparse latent point diffusion model achieves\nsuperior performance in terms of generation quality and controllability when\ncompared to existing methods.\n","authors":["Zhaoyang Lyu","Jinyi Wang","Yuwei An","Ya Zhang","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2303.07938v2.pdf","comment":"Accepted to CVPR 2023. Project page is at https://slide-3d.github.io"},{"id":"http://arxiv.org/abs/2303.01046v2","updated":"2023-03-15T03:10:39Z","published":"2023-03-02T08:00:22Z","title":"Jointly Visual- and Semantic-Aware Graph Memory Networks for Temporal\n  Sentence Localization in Videos","summary":"  Temporal sentence localization in videos (TSLV) aims to retrieve the most\ninterested segment in an untrimmed video according to a given sentence query.\nHowever, almost of existing TSLV approaches suffer from the same limitations:\n(1) They only focus on either frame-level or object-level visual representation\nlearning and corresponding correlation reasoning, but fail to integrate them\nboth; (2) They neglect to leverage the rich semantic contexts to further\nbenefit the query reasoning. To address these issues, in this paper, we propose\na novel Hierarchical Visual- and Semantic-Aware Reasoning Network (HVSARN),\nwhich enables both visual- and semantic-aware query reasoning from object-level\nto frame-level. Specifically, we present a new graph memory mechanism to\nperform visual-semantic query reasoning: For visual reasoning, we design a\nvisual graph memory to leverage visual information of video; For semantic\nreasoning, a semantic graph memory is also introduced to explicitly leverage\nsemantic knowledge contained in the classes and attributes of video objects,\nand perform correlation reasoning in the semantic space. Experiments on three\ndatasets demonstrate that our HVSARN achieves a new state-of-the-art\nperformance.\n","authors":["Daizong Liu","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.01046v2.pdf","comment":"Accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2209.10150v2","updated":"2023-03-15T03:05:12Z","published":"2022-09-21T07:06:46Z","title":"RNGDet++: Road Network Graph Detection by Transformer with Instance\n  Segmentation and Multi-scale Features Enhancement","summary":"  The road network graph is a critical component for downstream tasks in\nautonomous driving, such as global route planning and navigation. In the past\nyears, road network graphs are usually annotated by human experts manually,\nwhich is time-consuming and labor-intensive. To annotate road network graphs\neffectively and efficiently, automatic algorithms for road network graph\ndetection are demanded. Most existing methods either adopt a post-processing\nstep on semantic segmentation maps to produce road network graphs, or propose\ngraph-based algorithms to directly predict the graphs. However, these works\nsuffer from hard-coded algorithms and inferior performance. To enhance the\nprevious state-of-the-art (SOTA) method RNGDet, we add an instance segmentation\nhead to better supervise the training, and enable the network to leverage\nmulti-scale features of the backbone. Since the new proposed approach is\nimproved from RNGDet, we name it RNGDet++. Experimental results show that our\nRNGDet++ outperforms baseline methods in terms of almost all evaluation metrics\non two large-scale public datasets. Our code and supplementary materials are\navailable at \\url{https://tonyxuqaq.github.io/projects/RNGDetPlusPlus/}.\n","authors":["Zhenhua Xu","Yuxuan Liu","Yuxiang Sun","Ming Liu","Lujia Wang"],"pdf_url":"https://arxiv.org/pdf/2209.10150v2.pdf","comment":"Accepted by IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2301.01970v5","updated":"2023-03-15T03:02:19Z","published":"2023-01-05T09:11:16Z","title":"CAT: LoCalization and IdentificAtion Cascade Detection Transformer for\n  Open-World Object Detection","summary":"  Open-world object detection (OWOD), as a more general and challenging goal,\nrequires the model trained from data on known objects to detect both known and\nunknown objects and incrementally learn to identify these unknown objects. The\nexisting works which employ standard detection framework and fixed\npseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion\nof detecting unknown objects substantially reduces the model's ability to\ndetect known ones. (ii) The PLM does not adequately utilize the priori\nknowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee\nthat the model is trained in the right direction. We observe that humans\nsubconsciously prefer to focus on all foreground objects and then identify each\none in detail, rather than localize and identify a single object\nsimultaneously, for alleviating the confusion. This motivates us to propose a\nnovel solution called CAT: LoCalization and IdentificAtion Cascade Detection\nTransformer which decouples the detection process via the shared decoder in the\ncascade decoding way. In the meanwhile, we propose the self-adaptive\npseudo-labelling mechanism which combines the model-driven with input-driven\nPLM and self-adaptively generates robust pseudo-labels for unknown objects,\nsignificantly improving the ability of CAT to retrieve unknown objects.\nComprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL\nVOC, show that our model outperforms the state-of-the-art in terms of all\nmetrics in the task of OWOD, incremental object detection (IOD) and open-set\ndetection.\n","authors":["Shuailei Ma","Yuefeng Wang","Jiaqi Fan","Ying Wei","Thomas H. Li","Hongli Liu","Fanbing Lv"],"pdf_url":"https://arxiv.org/pdf/2301.01970v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04253v2","updated":"2023-03-15T02:52:14Z","published":"2023-03-07T21:52:10Z","title":"SKGHOI: Spatial-Semantic Knowledge Graph for Human-Object Interaction\n  Detection","summary":"  Detecting human-object interactions (HOIs) is a challenging problem in\ncomputer vision. Existing techniques for HOI detection heavily rely on\nappearance-based features, which may not capture other essential\ncharacteristics for accurate detection. Furthermore, the use of\ntransformer-based models for sentiment representation of human-object pairs can\nbe computationally expensive. To address these challenges, we propose a novel\ngraph-based approach, SKGHOI (Spatial-Semantic Knowledge Graph for Human-Object\nInteraction Detection), that effectively captures the sentiment representation\nof HOIs by integrating both spatial and semantic knowledge. In a graph, SKGHOI\ntakes the components of interaction as nodes, and the spatial relationships\nbetween them as edges. Our approach employs a spatial encoder and a semantic\nencoder to extract spatial and semantic information, respectively, and then\ncombines these encodings to create a knowledge graph that captures the\nsentiment representation of HOIs. Compared to existing techniques, SKGHOI is\ncomputationally efficient and allows for the incorporation of prior knowledge,\nmaking it practical for use in real-world applications. We demonstrate the\neffectiveness of our proposed method on the widely-used HICO-DET datasets,\nwhere it outperforms existing state-of-the-art graph-based methods by a\nsignificant margin. Our results indicate that the SKGHOI approach has the\npotential to significantly improve the accuracy and efficiency of HOI\ndetection, and we anticipate that it will be of great interest to researchers\nand practitioners working on this challenging task.\n","authors":["Lijing Zhu","Qizhen Lan","Alvaro Velasquez","Houbing Song","Acharya Kamal","Qing Tian","Shuteng Niu"],"pdf_url":"https://arxiv.org/pdf/2303.04253v2.pdf","comment":"10 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.08333v1","updated":"2023-03-15T02:42:48Z","published":"2023-03-15T02:42:48Z","title":"DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception","summary":"  BEV perception is of great importance in the field of autonomous driving,\nserving as the cornerstone of planning, controlling, and motion prediction. The\nquality of the BEV feature highly affects the performance of BEV perception.\nHowever, taking the noises in camera parameters and LiDAR scans into\nconsideration, we usually obtain BEV representation with harmful noises.\nDiffusion models naturally have the ability to denoise noisy samples to the\nideal data, which motivates us to utilize the diffusion model to get a better\nBEV representation. In this work, we propose an end-to-end framework, named\nDiffBEV, to exploit the potential of diffusion model to generate a more\ncomprehensive BEV representation. To the best of our knowledge, we are the\nfirst to apply diffusion model to BEV perception. In practice, we design three\ntypes of conditions to guide the training of the diffusion model which denoises\nthe coarse samples and refines the semantic feature in a progressive way.\nWhat's more, a cross-attention module is leveraged to fuse the context of BEV\nfeature and the semantic content of conditional diffusion model. DiffBEV\nachieves a 25.9% mIoU on the nuScenes dataset, which is 6.2% higher than the\nbest-performing existing approach. Quantitative and qualitative results on\nmultiple benchmarks demonstrate the effectiveness of DiffBEV in BEV semantic\nsegmentation and 3D object detection tasks. The code will be available soon.\n","authors":["Jiayu Zou","Zheng Zhu","Yun Ye","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08331v1","updated":"2023-03-15T02:40:02Z","published":"2023-03-15T02:40:02Z","title":"Towards High-Quality and Efficient Video Super-Resolution via\n  Spatial-Temporal Data Overfitting","summary":"  As deep convolutional neural networks (DNNs) are widely used in various\nfields of computer vision, leveraging the overfitting ability of the DNN to\nachieve video resolution upscaling has become a new trend in the modern video\ndelivery system. By dividing videos into chunks and overfitting each chunk with\na super-resolution model, the server encodes videos before transmitting them to\nthe clients, thus achieving better video quality and transmission efficiency.\nHowever, a large number of chunks are expected to ensure good overfitting\nquality, which substantially increases the storage and consumes more bandwidth\nresources for data transmission. On the other hand, decreasing the number of\nchunks through training optimization techniques usually requires high model\ncapacity, which significantly slows down execution speed. To reconcile such, we\npropose a novel method for high-quality and efficient video resolution\nupscaling tasks, which leverages the spatial-temporal information to accurately\ndivide video into chunks, thus keeping the number of chunks as well as the\nmodel size to minimum. Additionally, we advance our method into a single\noverfitting model by a data-aware joint training technique, which further\nreduces the storage requirement with negligible quality drop. We deploy our\nmodels on an off-the-shelf mobile phone, and experimental results show that our\nmethod achieves real-time video super-resolution with high video quality.\nCompared with the state-of-the-art, our method achieves 28 fps streaming speed\nwith 41.6 PSNR, which is 14$\\times$ faster and 2.29 dB better in the live video\nresolution upscaling tasks. Our codes are available at:\nhttps://github.com/coulsonlee/STDO-CVPR2023.git\n","authors":["Gen Li","Jie Ji","Minghai Qin","Wei Niu","Bin Ren","Fatemeh Afghah","Linke Guo","Xiaolong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.08331v1.pdf","comment":"CVPR 2023 Paper"},{"id":"http://arxiv.org/abs/2302.14325v2","updated":"2023-03-15T02:38:54Z","published":"2023-02-28T05:37:45Z","title":"BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View\n  Images","summary":"  Place recognition is a key module for long-term SLAM systems. Current\nLiDAR-based place recognition methods are usually based on representations of\npoint clouds such as unordered points or range images. These methods achieve\nhigh recall rates of retrieval, but their performance may degrade in the case\nof view variation or scene changes. In this work, we explore the potential of a\ndifferent representation in place recognition, i.e. bird's eye view (BEV)\nimages. We observe that the structural contents of BEV images are less\ninfluenced by rotations and translations of point clouds. We validate that,\nwithout any delicate design, a simple VGGNet trained on BEV images achieves\ncomparable performance with the state-of-the-art place recognition methods in\nscenes of slight viewpoint changes. For more robust place recognition, we\ndesign a rotation-invariant network called BEVPlace. We use group convolution\nto extract rotation-equivariant local features from the images and NetVLAD for\nglobal feature aggregation. In addition, we observe that the distance between\nBEV features is correlated with the geometry distance of point clouds. Based on\nthe observation, we develop a method to estimate the position of the query\ncloud, extending the usage of place recognition. The experiments conducted on\nlarge-scale public datasets show that our method 1) achieves state-of-the-art\nperformance in terms of recall rates, 2) is robust to view changes, 3) shows\nstrong generalization ability, and 4) can estimate the positions of query point\nclouds. Source code will be made publicly available at\nhttps://github.com/zjuluolun/BEVPlace.\n","authors":["Lun Luo","Shuhang Zheng","Yixuan Li","Yongzhi Fan","Beinan Yu","Siyuan Cao","Huiliang Shen"],"pdf_url":"https://arxiv.org/pdf/2302.14325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08325v1","updated":"2023-03-15T02:22:07Z","published":"2023-03-15T02:22:07Z","title":"FairAdaBN: Mitigating unfairness with adaptive batch normalization and\n  its application to dermatological disease classification","summary":"  Deep learning is becoming increasingly ubiquitous in medical research and\napplications while involving sensitive information and even critical diagnosis\ndecisions. Researchers observe a significant performance disparity among\nsubgroups with different demographic attributes, which is called model\nunfairness, and put lots of effort into carefully designing elegant\narchitectures to address unfairness, which poses heavy training burden, brings\npoor generalization, and reveals the trade-off between model performance and\nfairness. To tackle these issues, we propose FairAdaBN by making batch\nnormalization adaptive to sensitive attribute. This simple but effective design\ncan be adopted to several classification backbones that are originally unaware\nof fairness. Additionally, we derive a novel loss function that restrains\nstatistical parity between subgroups on mini-batches, encouraging the model to\nconverge with considerable fairness. In order to evaluate the trade-off between\nmodel performance and fairness, we propose a new metric, named\nFairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness\nimprovement over accuracy drop. Experiments on two dermatological datasets show\nthat our proposed method outperforms other methods on fairness criteria and\nFATE.\n","authors":["Zikang Xu","Shang Zhao","Quan Quan","Qingsong Yao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.08325v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2206.02405v5","updated":"2023-03-15T02:21:30Z","published":"2022-06-06T07:26:29Z","title":"Image Protection for Robust Cropping Localization and Recovery","summary":"  Existing image cropping detection schemes ignore that recovering the\ncropped-out contents can unveil the purpose of the behaved cropping attack.\nThis paper presents \\textbf{CLR}-Net, a novel image protection scheme\naddressing the combined challenge of image \\textbf{C}ropping\n\\textbf{L}ocalization and \\textbf{R}ecovery. We first protect the original\nimage by introducing imperceptible perturbations. Then, typical image\npost-processing attacks are simulated to erode the protected image. On the\nrecipient's side, we predict the cropping mask and recover the original image.\nBesides, we propose a novel \\textbf{F}ine-\\textbf{G}rained generative\n\\textbf{JPEG} simulator (FG-JPEG) as well as a feature alignment network to\nimprove the real-world robustness. Comprehensive experiments prove that the\nquality of the recovered image and the accuracy of crop localization are both\nsatisfactory.\n","authors":["Qichao Ying","Hang Zhou","Xiaoxiao Hu","Zhenxing Qian","Sheng Li","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2206.02405v5.pdf","comment":"Accepted by IEEE ICME 2023"},{"id":"http://arxiv.org/abs/2303.08320v1","updated":"2023-03-15T02:16:39Z","published":"2023-03-15T02:16:39Z","title":"Decomposed Diffusion Models for High-Quality Video Generation","summary":"  A diffusion probabilistic model (DPM), which constructs a forward diffusion\nprocess by gradually adding noise to data points and learns the reverse\ndenoising process to generate new samples, has been shown to handle complex\ndata distribution. Despite its recent success in image synthesis, applying DPMs\nto video generation is still challenging due to the high dimensional data\nspace. Previous methods usually adopt a standard diffusion process, where\nframes in the same video clip are destroyed with independent noises, ignoring\nthe content redundancy and temporal correlation. This work presents a\ndecomposed diffusion process via resolving the per-frame noise into a base\nnoise that is shared among all frames and a residual noise that varies along\nthe time axis. The denoising pipeline employs two jointly-learned networks to\nmatch the noise decomposition accordingly. Experiments on various datasets\nconfirm that our approach, termed as VideoFusion, surpasses both GAN-based and\ndiffusion-based alternatives in high-quality video generation. We further show\nthat our decomposed formulation can benefit from pre-trained image diffusion\nmodels and well-support text-conditioned video creation.\n","authors":["Zhengxiong Luo","Dayou Chen","Yingya Zhang","Yan Huang","Liang Wang","Yujun Shen","Deli Zhao","Jinren Zhou","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.08320v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.08319v1","updated":"2023-03-15T02:14:56Z","published":"2023-03-15T02:14:56Z","title":"FAQ: Feature Aggregated Queries for Transformer-based Video Object\n  Detectors","summary":"  Video object detection needs to solve feature degradation situations that\nrarely happen in the image domain. One solution is to use the temporal\ninformation and fuse the features from the neighboring frames. With\nTransformerbased object detectors getting a better performance on the image\ndomain tasks, recent works began to extend those methods to video object\ndetection. However, those existing Transformer-based video object detectors\nstill follow the same pipeline as those used for classical object detectors,\nlike enhancing the object feature representations by aggregation. In this work,\nwe take a different perspective on video object detection. In detail, we\nimprove the qualities of queries for the Transformer-based models by\naggregation. To achieve this goal, we first propose a vanilla query aggregation\nmodule that weighted averages the queries according to the features of the\nneighboring frames. Then, we extend the vanilla module to a more practical\nversion, which generates and aggregates queries according to the features of\nthe input frames. Extensive experimental results validate the effectiveness of\nour proposed methods: On the challenging ImageNet VID benchmark, when\nintegrated with our proposed modules, the current state-of-the-art\nTransformer-based object detectors can be improved by more than 2.4% on mAP and\n4.2% on AP50.\n","authors":["Yiming Cui"],"pdf_url":"https://arxiv.org/pdf/2303.08319v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.08318v1","updated":"2023-03-15T02:13:34Z","published":"2023-03-15T02:13:34Z","title":"Micro-video Tagging via Jointly Modeling Social Influence and Tag\n  Relation","summary":"  The last decade has witnessed the proliferation of micro-videos on various\nuser-generated content platforms. According to our statistics, around 85.7\\% of\nmicro-videos lack annotation. In this paper, we focus on annotating\nmicro-videos with tags. Existing methods mostly focus on analyzing video\ncontent, neglecting users' social influence and tag relation. Meanwhile,\nexisting tag relation construction methods suffer from either deficient\nperformance or low tag coverage. To jointly model social influence and tag\nrelation, we formulate micro-video tagging as a link prediction problem in a\nconstructed heterogeneous network. Specifically, the tag relation (represented\nby tag ontology) is constructed in a semi-supervised manner. Then, we combine\ntag relation, video-tag annotation, and user-follow relation to build the\nnetwork. Afterward, a better video and tag representation are derived through\nBehavior Spread modeling and visual and linguistic knowledge aggregation.\nFinally, the semantic similarity between each micro-video and all candidate\ntags is calculated in this video-tag network. Extensive experiments on\nindustrial datasets of three verticals verify the superiority of our model\ncompared with several state-of-the-art baselines.\n","authors":["Xiao Wang","Tian Gan","Yinwei Wei","Jianlong Wu","Dai Meng","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2303.08318v1.pdf","comment":"Accepted by Proceedings of the 30th ACM International Conference on\n  Multimedia (2022)"},{"id":"http://arxiv.org/abs/2303.08316v1","updated":"2023-03-15T02:10:27Z","published":"2023-03-15T02:10:27Z","title":"MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection\n  from Point Cloud Sequences","summary":"  Point cloud sequences are commonly used to accurately detect 3D objects in\napplications such as autonomous driving. Current top-performing multi-frame\ndetectors mostly follow a Detect-and-Fuse framework, which extracts features\nfrom each frame of the sequence and fuses them to detect the objects in the\ncurrent frame. However, this inevitably leads to redundant computation since\nadjacent frames are highly correlated. In this paper, we propose an efficient\nMotion-guided Sequential Fusion (MSF) method, which exploits the continuity of\nobject motion to mine useful sequential contexts for object detection in the\ncurrent frame. We first generate 3D proposals on the current frame and\npropagate them to preceding frames based on the estimated velocities. The\npoints-of-interest are then pooled from the sequence and encoded as proposal\nfeatures. A novel Bidirectional Feature Aggregation (BiFA) module is further\nproposed to facilitate the interactions of proposal features across frames.\nBesides, we optimize the point cloud pooling by a voxel-based sampling\ntechnique so that millions of points can be processed in several milliseconds.\nThe proposed MSF method achieves not only better efficiency than other\nmulti-frame detectors but also leading accuracy, with 83.12% and 78.30% mAP on\nthe LEVEL1 and LEVEL2 test sets of Waymo Open Dataset, respectively. Codes can\nbe found at \\url{https://github.com/skyhehe123/MSF}.\n","authors":["Chenhang He","Ruihuang Li","Yabin Zhang","Shuai Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08316v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.08314v1","updated":"2023-03-15T02:08:20Z","published":"2023-03-15T02:08:20Z","title":"Guided Slot Attention for Unsupervised Video Object Segmentation","summary":"  Unsupervised video object segmentation aims to segment the most prominent\nobject in a video sequence. However, the existence of complex backgrounds and\nmultiple foreground objects make this task challenging. To address this issue,\nwe propose a guided slot attention network to reinforce spatial structural\ninformation and obtain better foreground--background separation. The foreground\nand background slots, which are initialized with query guidance, are\niteratively refined based on interactions with template information.\nFurthermore, to improve slot--template interaction and effectively fuse global\nand local features in the target and reference frames, K-nearest neighbors\nfiltering and a feature aggregation transformer are introduced. The proposed\nmodel achieves state-of-the-art performance on two popular datasets.\nAdditionally, we demonstrate the robustness of the proposed model in\nchallenging scenes through various comparative experiments.\n","authors":["Minhyeok Lee","Suhwan Cho","Dogyoon Lee","Chaewon Park","Jungho Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2303.08314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08308v1","updated":"2023-03-15T01:41:21Z","published":"2023-03-15T01:41:21Z","title":"SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8\n  Inference","summary":"  The combination of Neural Architecture Search (NAS) and quantization has\nproven successful in automatically designing low-FLOPs INT8 quantized neural\nnetworks (QNN). However, directly applying NAS to design accurate QNN models\nthat achieve low latency on real-world devices leads to inferior performance.\nIn this work, we find that the poor INT8 latency is due to the\nquantization-unfriendly issue: the operator and configuration (e.g., channel\nwidth) choices in prior art search spaces lead to diverse quantization\nefficiency and can slow down the INT8 inference speed. To address this\nchallenge, we propose SpaceEvo, an automatic method for designing a dedicated,\nquantization-friendly search space for each target hardware. The key idea of\nSpaceEvo is to automatically search hardware-preferred operators and\nconfigurations to construct the search space, guided by a metric called Q-T\nscore to quantify how quantization-friendly a candidate search space is. We\nfurther train a quantized-for-all supernet over our discovered search space,\nenabling the searched models to be directly deployed without extra retraining\nor quantization. Our discovered models establish new SOTA INT8 quantized\naccuracy under various latency constraints, achieving up to 10.1% accuracy\nimprovement on ImageNet than prior art CNNs under the same latency. Extensive\nexperiments on diverse edge devices demonstrate that SpaceEvo consistently\noutperforms existing manually-designed search spaces with up to 2.5x faster\nspeed while achieving the same accuracy.\n","authors":["Li Lyna Zhang","Xudong Wang","Jiahang Xu","Quanlu Zhang","Yujing Wang","Yuqing Yang","Ningxin Zheng","Ting Cao","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08303v1","updated":"2023-03-15T01:30:48Z","published":"2023-03-15T01:30:48Z","title":"SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep\n  Models for Kidney Stone Classification","summary":"  Recently, deep learning has produced encouraging results for kidney stone\nclassification using endoscope images. However, the shortage of annotated\ntraining data poses a severe problem in improving the performance and\ngeneralization ability of the trained model. It is thus crucial to fully\nexploit the limited data at hand. In this paper, we propose SegPrompt to\nalleviate the data shortage problems by exploiting segmentation maps from two\naspects. First, SegPrompt integrates segmentation maps to facilitate\nclassification training so that the classification model is aware of the\nregions of interest. The proposed method allows the image and segmentation\ntokens to interact with each other to fully utilize the segmentation map\ninformation. Second, we use the segmentation maps as prompts to tune the\npretrained deep model, resulting in much fewer trainable parameters than\nvanilla finetuning. We perform extensive experiments on the collected kidney\nstone dataset. The results show that SegPrompt can achieve an advantageous\nbalance between the model fitting ability and the generalization ability,\neventually leading to an effective model with limited training data.\n","authors":["Wei Zhu","Runtao Zhou","Yao Yuan","Campbell Timothy","Rajat Jain","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.08303v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.08722v1","updated":"2023-03-15T16:08:40Z","published":"2023-03-15T16:08:40Z","title":"Dually Enhanced Propensity Score Estimation in Sequential Recommendation","summary":"  Sequential recommender systems train their models based on a large amount of\nimplicit user feedback data and may be subject to biases when users are\nsystematically under/over-exposed to certain items. Unbiased learning based on\ninverse propensity scores (IPS), which estimate the probability of observing a\nuser-item pair given the historical information, has been proposed to address\nthe issue. In these methods, propensity score estimation is usually limited to\nthe view of item, that is, treating the feedback data as sequences of items\nthat interacted with the users. However, the feedback data can also be treated\nfrom the view of user, as the sequences of users that interact with the items.\nMoreover, the two views can jointly enhance the propensity score estimation.\nInspired by the observation, we propose to estimate the propensity scores from\nthe views of user and item, called Dually Enhanced Propensity Score Estimation\n(DEPS). Specifically, given a target user-item pair and the corresponding item\nand user interaction sequences, DEPS firstly constructs a time-aware causal\ngraph to represent the user-item observational probability. According to the\ngraph, two complementary propensity scores are estimated from the views of item\nand user, respectively, based on the same set of user feedback data. Finally,\ntwo transformers are designed to make the final preference prediction.\nTheoretical analysis showed the unbiasedness and variance of DEPS. Experimental\nresults on three publicly available and an industrial datasets demonstrated\nthat DEPS can significantly outperform the state-of-the-art baselines.\n","authors":["Chen Xu","Jun Xu","Xu Chen","Zhenghua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2303.08722v1.pdf","comment":"Accepted in CIKM2022"},{"id":"http://arxiv.org/abs/2303.00386v2","updated":"2023-03-15T15:41:37Z","published":"2023-03-01T10:11:50Z","title":"Authorship Conflicts in Academia: an International Cross-Discipline\n  Survey","summary":"  Collaboration among scholars has emerged as a significant characteristic of\ncontemporary science. As a result, the number of authors listed in publications\ncontinues to rise steadily. Unfortunately, determining the authors to be\nincluded in the byline and their respective order entails multiple difficulties\nwhich often lead to conflicts. Despite the large volume of literature about\nconflicts in academia, it remains unclear how exactly it is distributed over\nthe main socio-demographic properties, as well as the different types of\ninteractions academics experience. To address this gap, we conducted an\ninternational and cross-disciplinary survey answered by 752 academics from 41\nfields of research and 93 countries that statistically well-represent the\noverall academic workforce. Our findings are concerning and suggest that\nauthorship credit conflicts arise very early in one's academic career, even at\nthe level of Master and Ph.D., and become increasingly common over time.\n","authors":["Elizaveta Savchenko","Ariel Rosenfeld"],"pdf_url":"https://arxiv.org/pdf/2303.00386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08652v1","updated":"2023-03-15T14:32:00Z","published":"2023-03-15T14:32:00Z","title":"Automated Query Generation for Evidence Collection from Web Search\n  Engines","summary":"  It is widely accepted that so-called facts can be checked by searching for\ninformation on the Internet. This process requires a fact-checker to formulate\na search query based on the fact and to present it to a search engine. Then,\nrelevant and believable passages need to be identified in the search results\nbefore a decision is made. This process is carried out by sub-editors at many\nnews and media organisations on a daily basis. Here, we ask the question as to\nwhether it is possible to automate the first step, that of query generation.\nCan we automatically formulate search queries based on factual statements which\nare similar to those formulated by human experts? Here, we consider similarity\nboth in terms of textual similarity and with respect to relevant documents\nbeing returned by a search engine. First, we introduce a moderate-sized\nevidence collection dataset which includes 390 factual statements together with\nassociated human-generated search queries and search results. Then, we\ninvestigate generating queries using a number of rule-based and automatic text\ngeneration methods based on pre-trained large language models (LLMs). We show\nthat these methods have different merits and propose a hybrid approach which\nhas superior performance in practice.\n","authors":["Nestor Prieto-Chavana","Julie Weeds","David Weir"],"pdf_url":"https://arxiv.org/pdf/2303.08652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08537v1","updated":"2023-03-15T11:30:16Z","published":"2023-03-15T11:30:16Z","title":"Graph-less Collaborative Filtering","summary":"  Graph neural networks (GNNs) have shown the power in representation learning\nover graph-structured user-item interaction data for collaborative filtering\n(CF) task. However, with their inherently recursive message propagation among\nneighboring nodes, existing GNN-based CF models may generate indistinguishable\nand inaccurate user (item) representations due to the over-smoothing and noise\neffect with low-pass Laplacian smoothing operators. In addition, the recursive\ninformation propagation with the stacked aggregators in the entire graph\nstructures may result in poor scalability in practical applications. Motivated\nby these limitations, we propose a simple and effective collaborative filtering\nmodel (SimRec) that marries the power of knowledge distillation and contrastive\nlearning. In SimRec, adaptive transferring knowledge is enabled between the\nteacher GNN model and a lightweight student network, to not only preserve the\nglobal collaborative signals, but also address the over-smoothing issue with\nrepresentation recalibration. Empirical results on public datasets show that\nSimRec archives better efficiency while maintaining superior recommendation\nperformance compared with various strong baselines. Our implementations are\npublicly available at: https://github.com/HKUDS/SimRec.\n","authors":["Lianghao Xia","Chao Huang","Jiao Shi","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08537v1.pdf","comment":"Accepted by ACM WWW 2023"},{"id":"http://arxiv.org/abs/2303.08448v1","updated":"2023-03-15T08:44:07Z","published":"2023-03-15T08:44:07Z","title":"A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP\n  Algorithms on Electronic Health Records","summary":"  Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.\n","authors":["Sicheng Zhou","Nan Wang","Liwei Wang","Ju Sun","Anne Blaes","Hongfang Liu","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08448v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.08346v1","updated":"2023-03-15T03:57:09Z","published":"2023-03-15T03:57:09Z","title":"Robust Preference-Guided Denoising for Graph based Social Recommendation","summary":"  Graph Neural Network(GNN) based social recommendation models improve the\nprediction accuracy of user preference by leveraging GNN in exploiting\npreference similarity contained in social relations. However, in terms of both\neffectiveness and efficiency of recommendation, a large portion of social\nrelations can be redundant or even noisy, e.g., it is quite normal that friends\nshare no preference in a certain domain. Existing models do not fully solve\nthis problem of relation redundancy and noise, as they directly characterize\nsocial influence over the full social network. In this paper, we instead\npropose to improve graph based social recommendation by only retaining the\ninformative social relations to ensure an efficient and effective influence\ndiffusion, i.e., graph denoising. Our designed denoising method is\npreference-guided to model social relation confidence and benefits user\npreference learning in return by providing a denoised but more informative\nsocial graph for recommendation models. Moreover, to avoid interference of\nnoisy social relations, it designs a self-correcting curriculum learning module\nand an adaptive denoising strategy, both favoring highly-confident samples.\nExperimental results on three public datasets demonstrate its consistent\ncapability of improving two state-of-the-art social recommendation models by\nrobustly removing 10-40% of original relations. We release the source code at\nhttps://github.com/tsinghua-fib-lab/Graph-Denoising-SocialRec.\n","authors":["Yuhan Quan","Jingtao Ding","Chen Gao","Lingling Yi","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2303.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03735v2","updated":"2023-03-15T23:34:33Z","published":"2023-02-07T20:12:59Z","title":"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language\n  Modelling Paradigm Adaptations in Recommender Systems","summary":"  The emergency of Pre-trained Language Models (PLMs) has achieved tremendous\nsuccess in the field of Natural Language Processing (NLP) by learning universal\nrepresentations on large corpora in a self-supervised manner. The pre-trained\nmodels and the learned representations can be beneficial to a series of\ndownstream NLP tasks. This training paradigm has recently been adapted to the\nrecommendation domain and is considered a promising approach by both academia\nand industry. In this paper, we systematically investigate how to extract and\ntransfer knowledge from pre-trained models learned by different PLM-related\ntraining paradigms to improve recommendation performance from various\nperspectives, such as generality, sparsity, efficiency and effectiveness.\nSpecifically, we propose an orthogonal taxonomy to divide existing PLM-based\nrecommender systems w.r.t. their training strategies and objectives. Then, we\nanalyze and summarize the connection between PLM-based training paradigms and\ndifferent input data types for recommender systems. Finally, we elaborate on\nopen issues and future research directions in this vibrant field.\n","authors":["Peng Liu","Lemei Zhang","Jon Atle Gulla"],"pdf_url":"https://arxiv.org/pdf/2302.03735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11163v1","updated":"2023-03-15T01:40:32Z","published":"2023-03-15T01:40:32Z","title":"Finding Similar Exercises in Retrieval Manner","summary":"  When students make a mistake in an exercise, they can consolidate it by\n``similar exercises'' which have the same concepts, purposes and methods.\nCommonly, for a certain subject and study stage, the size of the exercise bank\nis in the range of millions to even tens of millions, how to find similar\nexercises for a given exercise becomes a crucial technical problem. Generally,\nwe can assign a variety of explicit labels to the exercise, and then query\nthrough the labels, but the label annotation is time-consuming, laborious and\ncostly, with limited precision and granularity, so it is not feasible. In\npractice, we define ``similar exercises'' as a retrieval process of finding a\nset of similar exercises based on recall, ranking and re-rank procedures,\ncalled the \\textbf{FSE} problem (Finding similar exercises). Furthermore,\ncomprehensive representation of the semantic information of exercises was\nobtained through representation learning. In addition to the reasonable\narchitecture, we also explore what kind of tasks are more conducive to the\nlearning of exercise semantic information from pre-training and supervised\nlearning. It is difficult to annotate similar exercises and the annotation\nconsistency among experts is low. Therefore this paper also provides solutions\nto solve the problem of low-quality annotated data. Compared with other\nmethods, this paper has obvious advantages in both architecture rationality and\nalgorithm precision, which now serves the daily teaching of hundreds of\nschools.\n","authors":["Tongwen Huang","Xihua Li","Chao Yi","Xuemin Zhao","Yunbo Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11163v1.pdf","comment":"37th Conference on AAAI 2023 Artificial Intelligence for\n  Education(AI4Edu)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2212.07398v3","updated":"2023-03-15T17:59:29Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v3.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2303.08816v1","updated":"2023-03-15T17:59:27Z","published":"2023-03-15T17:59:27Z","title":"Borda Regret Minimization for Generalized Linear Dueling Bandits","summary":"  Dueling bandits are widely used to model preferential feedback that is\nprevalent in machine learning applications such as recommendation systems and\nranking. In this paper, we study the Borda regret minimization problem for\ndueling bandits, which aims to identify the item with the highest Borda score\nwhile minimizing the cumulative regret. We propose a new and highly expressive\ngeneralized linear dueling bandits model, which covers many existing models.\nSurprisingly, the Borda regret minimization problem turns out to be difficult,\nas we prove a regret lower bound of order $\\Omega(d^{2/3} T^{2/3})$, where $d$\nis the dimension of contextual vectors and $T$ is the time horizon. To attain\nthe lower bound, we propose an explore-then-commit type algorithm, which has a\nnearly matching regret upper bound $\\tilde{O}(d^{2/3} T^{2/3})$. When the\nnumber of items/arms $K$ is small, our algorithm can achieve a smaller regret\n$\\tilde{O}( (d \\log K)^{1/3} T^{2/3})$ with proper choices of hyperparameters.\nWe also conduct empirical experiments on both synthetic data and a simulated\nreal-world environment, which corroborate our theoretical analysis.\n","authors":["Yue Wu","Tao Jin","Hao Lou","Farzad Farnoud","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2303.08816v1.pdf","comment":"28 pages, 3 figure"},{"id":"http://arxiv.org/abs/2303.08812v1","updated":"2023-03-15T17:59:01Z","published":"2023-03-15T17:59:01Z","title":"Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse\n  Submanifold Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) have seen extensive applications in\nscientific data analysis, including in neutrino telescopes. However, the data\nfrom these experiments present numerous challenges to CNNs, such as non-regular\ngeometry, sparsity, and high dimensionality. Consequently, CNNs are highly\ninefficient on neutrino telescope data, and require significant pre-processing\nthat results in information loss. We propose sparse submanifold convolutions\n(SSCNNs) as a solution to these issues and show that the SSCNN event\nreconstruction performance is comparable to or better than traditional and\nmachine learning algorithms. Additionally, our SSCNN runs approximately 16\ntimes faster than a traditional CNN on a GPU. As a result of this speedup, it\nis expected to be capable of handling the trigger-level event rate of\nIceCube-scale neutrino telescopes. These networks could be used to improve the\nfirst estimation of the neutrino energy and direction to seed more advanced\nreconstructions, or to provide this information to an alert-sending system to\nquickly follow-up interesting events.\n","authors":["Felix J. Yu","Jeffrey Lazar","Carlos A. Argüelles"],"pdf_url":"https://arxiv.org/pdf/2303.08812v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08811v1","updated":"2023-03-15T17:58:48Z","published":"2023-03-15T17:58:48Z","title":"Relax, it doesn't matter how you get there: A new self-supervised\n  approach for multi-timescale behavior analysis","summary":"  Natural behavior consists of dynamics that are complex and unpredictable,\nespecially when trying to predict many steps into the future. While some\nsuccess has been found in building representations of behavior under\nconstrained or simplified task-based conditions, many of these models cannot be\napplied to free and naturalistic settings where behavior becomes increasingly\nhard to model. In this work, we develop a multi-task representation learning\nmodel for behavior that combines two novel components: (i) An action prediction\nobjective that aims to predict the distribution of actions over future\ntimesteps, and (ii) A multi-scale architecture that builds separate latent\nspaces to accommodate short- and long-term dynamics. After demonstrating the\nability of the method to build representations of both local and global\ndynamics in realistic robots in varying environments and terrains, we apply our\nmethod to the MABe 2022 Multi-agent behavior challenge, where our model ranks\n1st overall and on all global tasks, and 1st or 2nd on 7 out of 9 frame-level\ntasks. In all of these cases, we show that our model can build representations\nthat capture the many different factors that drive behavior and solve a wide\nrange of downstream tasks.\n","authors":["Mehdi Azabou","Michael Mendelson","Nauman Ahad","Maks Sorokin","Shantanu Thakoor","Carolina Urzay","Eva L. Dyer"],"pdf_url":"https://arxiv.org/pdf/2303.08811v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.07041"},{"id":"http://arxiv.org/abs/2303.08806v1","updated":"2023-03-15T17:56:34Z","published":"2023-03-15T17:56:34Z","title":"Understanding Post-hoc Explainers: The Case of Anchors","summary":"  In many scenarios, the interpretability of machine learning models is a\nhighly required but difficult task. To explain the individual predictions of\nsuch models, local model-agnostic approaches have been proposed. However, the\nprocess generating the explanations can be, for a user, as mysterious as the\nprediction to be explained. Furthermore, interpretability methods frequently\nlack theoretical guarantees, and their behavior on simple models is frequently\nunknown. While it is difficult, if not impossible, to ensure that an explainer\nbehaves as expected on a cutting-edge model, we can at least ensure that\neverything works on simple, already interpretable models. In this paper, we\npresent a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular\nrule-based interpretability method that highlights a small set of words to\nexplain a text classifier's decision. After formalizing its algorithm and\nproviding useful insights, we demonstrate mathematically that Anchors produces\nmeaningful results when used with linear text classifiers on top of a TF-IDF\nvectorization. We believe that our analysis framework can aid in the\ndevelopment of new explainability methods based on solid theoretical\nfoundations.\n","authors":["Gianluigi Lopardo","Frederic Precioso","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2303.08806v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2205.13789"},{"id":"http://arxiv.org/abs/2208.12242v2","updated":"2023-03-15T17:52:27Z","published":"2022-08-25T17:45:49Z","title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for\n  Subject-Driven Generation","summary":"  Large text-to-image models achieved a remarkable leap in the evolution of AI,\nenabling high-quality and diverse synthesis of images from a given text prompt.\nHowever, these models lack the ability to mimic the appearance of subjects in a\ngiven reference set and synthesize novel renditions of them in different\ncontexts. In this work, we present a new approach for \"personalization\" of\ntext-to-image diffusion models. Given as input just a few images of a subject,\nwe fine-tune a pretrained text-to-image model such that it learns to bind a\nunique identifier with that specific subject. Once the subject is embedded in\nthe output domain of the model, the unique identifier can be used to synthesize\nnovel photorealistic images of the subject contextualized in different scenes.\nBy leveraging the semantic prior embedded in the model with a new autogenous\nclass-specific prior preservation loss, our technique enables synthesizing the\nsubject in diverse scenes, poses, views and lighting conditions that do not\nappear in the reference images. We apply our technique to several\npreviously-unassailable tasks, including subject recontextualization,\ntext-guided view synthesis, and artistic rendering, all while preserving the\nsubject's key features. We also provide a new dataset and evaluation protocol\nfor this new task of subject-driven generation. Project page:\nhttps://dreambooth.github.io/\n","authors":["Nataniel Ruiz","Yuanzhen Li","Varun Jampani","Yael Pritch","Michael Rubinstein","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2208.12242v2.pdf","comment":"Published at CVPR 2023. Project page: https://dreambooth.github.io/"},{"id":"http://arxiv.org/abs/2302.02477v3","updated":"2023-03-15T17:50:56Z","published":"2023-02-05T20:29:53Z","title":"Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for\n  Parkinson Disease Treatment","summary":"  Deep brain stimulation (DBS) has shown great promise toward treating motor\nsymptoms caused by Parkinson's disease (PD), by delivering electrical pulses to\nthe Basal Ganglia (BG) region of the brain. However, DBS devices approved by\nthe U.S. Food and Drug Administration (FDA) can only deliver continuous DBS\n(cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces\nbattery lifetime of the device, cannot adapt treatment dynamically for\nactivity, and may cause significant side-effects (e.g., gait impairment). In\nthis work, we introduce an offline reinforcement learning (RL) framework,\nallowing the use of past clinical data to train an RL policy to adjust the\nstimulation amplitude in real time, with the goal of reducing energy use while\nmaintaining the same level of treatment (i.e., control) efficacy as cDBS.\nMoreover, clinical protocols require the safety and performance of such RL\ncontrollers to be demonstrated ahead of deployments in patients. Thus, we also\nintroduce an offline policy evaluation (OPE) method to estimate the performance\nof RL policies using historical data, before deploying them on patients. We\nevaluated our framework on four PD patients equipped with the RC+S DBS system,\nemploying the RL controllers during monthly clinical visits, with the overall\ncontrol efficacy evaluated by severity of symptoms (i.e., bradykinesia and\ntremor), changes in PD biomakers (i.e., local field potentials), and patient\nratings. The results from clinical experiments show that our RL-based\ncontroller maintains the same level of control efficacy as cDBS, but with\nsignificantly reduced stimulation energy. Further, the OPE method is shown\neffective in accurately estimating and ranking the expected returns of RL\ncontrollers.\n","authors":["Qitong Gao","Stephen L. Schimdt","Afsana Chowdhury","Guangyu Feng","Jennifer J. Peters","Katherine Genty","Warren M. Grill","Dennis A. Turner","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2302.02477v3.pdf","comment":"Accepted to International Conference on Cyber Physical Systems\n  (ICCPS) 2023"},{"id":"http://arxiv.org/abs/2303.08102v2","updated":"2023-03-15T17:48:14Z","published":"2023-03-14T17:41:31Z","title":"Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice","summary":"  We investigate the problem of bandits with expert advice when the experts are\nfixed and known distributions over the actions. Improving on previous analyses,\nwe show that the regret in this setting is controlled by information-theoretic\nquantities that measure the similarity between experts. In some natural special\ncases, this allows us to obtain the first regret bound for EXP4 that can get\narbitrarily close to zero if the experts are similar enough. While for a\ndifferent algorithm, we provide another bound that describes the similarity\nbetween the experts in terms of the KL-divergence, and we show that this bound\ncan be smaller than the one of EXP4 in some cases. Additionally, we provide\nlower bounds for certain classes of experts showing that the algorithms we\nanalyzed are nearly optimal in some cases.\n","authors":["Khaled Eldowa","Nicolò Cesa-Bianchi","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2303.08102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08797v1","updated":"2023-03-15T17:43:42Z","published":"2023-03-15T17:43:42Z","title":"Stochastic Interpolants: A Unifying Framework for Flows and Diffusions","summary":"  We introduce a class of generative models based on the stochastic interpolant\nframework proposed in Albergo & Vanden-Eijnden (2023) that unifies flow-based\nand diffusion-based methods. We first show how to construct a broad class of\ncontinuous-time stochastic processes whose time-dependent probability density\nfunction bridges two arbitrary densities exactly in finite time. These\n`stochastic interpolants' are built by combining data from the two densities\nwith an additional latent variable, and the specific details of the\nconstruction can be leveraged to shape the resulting time-dependent density in\na flexible way. We then show that the time-dependent density of the stochastic\ninterpolant satisfies a first-order transport equation as well as a family of\nforward and backward Fokker-Planck equations with tunable diffusion; upon\nconsideration of the time evolution of an individual sample, this viewpoint\nimmediately leads to both deterministic and stochastic generative models based\non probability flow equations or stochastic differential equations with a\ntunable level of noise. The drift coefficients entering these models are\ntime-dependent velocity fields characterized as the unique minimizers of simple\nquadratic objective functions, one of which is a new objective for the score of\nthe interpolant density. Remarkably, we show that minimization of these\nquadratic objectives leads to control of the likelihood for generative models\nbuilt upon stochastic dynamics; by contrast, we show that generative models\nbased upon a deterministic dynamics must, in addition, control the Fisher\ndivergence between the target and the model. Finally, we construct estimators\nfor the likelihood and the cross-entropy of interpolant-based generative\nmodels, and demonstrate that such models recover the Schr\\\"odinger bridge\nbetween the two target densities when explicitly optimizing over the\ninterpolant.\n","authors":["Michael S. Albergo","Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2303.08797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08789v1","updated":"2023-03-15T17:31:37Z","published":"2023-03-15T17:31:37Z","title":"PLEX: Making the Most of the Available Data for Robotic Manipulation\n  Pretraining","summary":"  A rich representation is key to general robotic manipulation, but existing\nmodel architectures require a lot of data to learn it. Unfortunately, ideal\nrobotic manipulation training data, which comes in the form of expert\nvisuomotor demonstrations for a variety of annotated tasks, is scarce. In this\nwork we propose PLEX, a transformer-based architecture that learns from\ntask-agnostic visuomotor trajectories accompanied by a much larger amount of\ntask-conditioned object manipulation videos -- a type of robotics-relevant data\navailable in quantity. The key insight behind PLEX is that the trajectories\nwith observations and actions help induce a latent feature space and train a\nrobot to execute task-agnostic manipulation routines, while a diverse set of\nvideo-only demonstrations can efficiently teach the robot how to plan in this\nfeature space for a wide variety of tasks. In contrast to most works on robotic\nmanipulation pretraining, PLEX learns a generalizable sensorimotor multi-task\npolicy, not just an observational representation. We also show that using\nrelative positional encoding in PLEX's transformers further increases its data\nefficiency when learning from human-collected demonstrations. Experiments\nshowcase \\appr's generalization on Meta-World-v2 benchmark and establish\nstate-of-the-art performance in challenging Robosuite environments.\n","authors":["Garrett Thomas","Ching-An Cheng","Ricky Loynd","Vibhav Vineet","Mihai Jalobeanu","Andrey Kolobov"],"pdf_url":"https://arxiv.org/pdf/2303.08789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06314v2","updated":"2023-03-15T17:30:20Z","published":"2023-03-11T05:17:59Z","title":"Stabilizing and Improving Federated Learning with Non-IID Data and\n  Client Dropout","summary":"  The label distribution skew induced data heterogeniety has been shown to be a\nsignificant obstacle that limits the model performance in federated learning,\nwhich is particularly developed for collaborative model training over\ndecentralized data sources while preserving user privacy. This challenge could\nbe more serious when the participating clients are in unstable circumstances\nand dropout frequently. Previous work and our empirical observations\ndemonstrate that the classifier head for classification task is more sensitive\nto label skew and the unstable performance of FedAvg mainly lies in the\nimbalanced training samples across different classes. The biased classifier\nhead will also impact the learning of feature representations. Therefore,\nmaintaining a balanced classifier head is of significant importance for\nbuilding a better global model. To this end, we propose a simple yet effective\nframework by introducing a prior-calibrated softmax function for computing the\ncross-entropy loss and a prototype-based feature augmentation scheme to\nre-balance the local training, which are lightweight for edge devices and can\nfacilitate the global model aggregation. The improved model performance over\nexisting baselines in the presence of non-IID data and client dropout is\ndemonstrated by conducting extensive experiments on benchmark classification\ntasks.\n","authors":["Jian Xu","Meiling Yang","Wenbo Ding","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2303.06314v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2211.01001v2","updated":"2023-03-15T17:24:42Z","published":"2022-11-02T10:02:13Z","title":"Thunderstorm nowcasting with deep learning: a multi-hazard data fusion\n  model","summary":"  Predictions of thunderstorm-related hazards are needed in several sectors,\nincluding first responders, infrastructure management and aviation. To address\nthis need, we present a deep learning model that can be adapted to different\nhazard types. The model can utilize multiple data sources; we use data from\nweather radar, lightning detection, satellite visible/infrared imagery,\nnumerical weather prediction and digital elevation models. We demonstrate the\nability of the model to predict lightning, hail and heavy precipitation\nprobabilistically on a 1 km resolution grid, with a temporal resolution of 5\nmin and lead times up to 60 min. Shapley values quantify the importance of the\ndifferent data sources, showing that the weather radar products are the most\nimportant predictors for all three hazard types.\n","authors":["Jussi Leinonen","Ulrich Hamann","Ioannis V. Sideris","Urs Germann"],"pdf_url":"https://arxiv.org/pdf/2211.01001v2.pdf","comment":"17 pages, 3 figures (main text); 13 pages, 10 figures, 1 table\n  (supplement). Accepted for publication in Geophysical Research Letters"},{"id":"http://arxiv.org/abs/2303.08778v1","updated":"2023-03-15T17:19:45Z","published":"2023-03-15T17:19:45Z","title":"Fully neuromorphic vision and control for autonomous drone flight","summary":"  Biological sensing and processing is asynchronous and sparse, leading to\nlow-latency and energy-efficient perception and action. In robotics,\nneuromorphic hardware for event-based vision and spiking neural networks\npromises to exhibit similar characteristics. However, robotic implementations\nhave been limited to basic tasks with low-dimensional sensory inputs and motor\nactions due to the restricted network size in current embedded neuromorphic\nprocessors and the difficulties of training spiking neural networks. Here, we\npresent the first fully neuromorphic vision-to-control pipeline for controlling\na freely flying drone. Specifically, we train a spiking neural network that\naccepts high-dimensional raw event-based camera data and outputs low-level\ncontrol actions for performing autonomous vision-based flight. The vision part\nof the network, consisting of five layers and 28.8k neurons, maps incoming raw\nevents to ego-motion estimates and is trained with self-supervised learning on\nreal event data. The control part consists of a single decoding layer and is\nlearned with an evolutionary algorithm in a drone simulator. Robotic\nexperiments show a successful sim-to-real transfer of the fully learned\nneuromorphic pipeline. The drone can accurately follow different ego-motion\nsetpoints, allowing for hovering, landing, and maneuvering\nsideways$\\unicode{x2014}$even while yawing at the same time. The neuromorphic\npipeline runs on board on Intel's Loihi neuromorphic processor with an\nexecution frequency of 200 Hz, spending only 27 $\\unicode{x00b5}$J per\ninference. These results illustrate the potential of neuromorphic sensing and\nprocessing for enabling smaller, more intelligent robots.\n","authors":["Federico Paredes-Vallés","Jesse Hagenaars","Julien Dupeyroux","Stein Stroobants","Yingfu Xu","Guido de Croon"],"pdf_url":"https://arxiv.org/pdf/2303.08778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08777v1","updated":"2023-03-15T17:18:31Z","published":"2023-03-15T17:18:31Z","title":"Distribution-free Deviation Bounds of Learning via Model Selection with\n  Cross-validation Risk Estimation","summary":"  Cross-validation techniques for risk estimation and model selection are\nwidely used in statistics and machine learning. However, the understanding of\nthe theoretical properties of learning via model selection with\ncross-validation risk estimation is quite low in face of its widespread use. In\nthis context, this paper presents learning via model selection with\ncross-validation risk estimation as a general systematic learning framework\nwithin classical statistical learning theory and establishes distribution-free\ndeviation bounds in terms of VC dimension, giving detailed proofs of the\nresults and considering both bounded and unbounded loss functions. We also\ndeduce conditions under which the deviation bounds of learning via model\nselection are tighter than that of learning via empirical risk minimization in\nthe whole hypotheses space, supporting the better performance of model\nselection frameworks observed empirically in some instances.\n","authors":["Diego Marcondes","Cláudia Peixoto"],"pdf_url":"https://arxiv.org/pdf/2303.08777v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2109.03866"},{"id":"http://arxiv.org/abs/2303.07647v2","updated":"2023-03-15T17:10:56Z","published":"2023-03-14T06:15:17Z","title":"Recent Advances and Applications of Machine Learning in Experimental\n  Solid Mechanics: A Review","summary":"  For many decades, experimental solid mechanics has played a crucial role in\ncharacterizing and understanding the mechanical properties of natural and novel\nmaterials. Recent advances in machine learning (ML) provide new opportunities\nfor the field, including experimental design, data analysis, uncertainty\nquantification, and inverse problems. As the number of papers published in\nrecent years in this emerging field is exploding, it is timely to conduct a\ncomprehensive and up-to-date review of recent ML applications in experimental\nsolid mechanics. Here, we first provide an overview of common ML algorithms and\nterminologies that are pertinent to this review, with emphasis placed on\nphysics-informed and physics-based ML methods. Then, we provide thorough\ncoverage of recent ML applications in traditional and emerging areas of\nexperimental mechanics, including fracture mechanics, biomechanics, nano- and\nmicro-mechanics, architected materials, and 2D material. Finally, we highlight\nsome current challenges of applying ML to multi-modality and multi-fidelity\nexperimental datasets and propose several future research directions. This\nreview aims to provide valuable insights into the use of ML methods as well as\na variety of examples for researchers in solid mechanics to integrate into\ntheir experiments.\n","authors":["Hanxun Jin","Enrui Zhang","Horacio D. Espinosa"],"pdf_url":"https://arxiv.org/pdf/2303.07647v2.pdf","comment":"76 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08760v1","updated":"2023-03-15T16:57:10Z","published":"2023-03-15T16:57:10Z","title":"Deep Calibration With Artificial Neural Network: A Performance\n  Comparison on Option Pricing Models","summary":"  This paper explores Artificial Neural Network (ANN) as a model-free solution\nfor a calibration algorithm of option pricing models. We construct ANNs to\ncalibrate parameters for two well-known GARCH-type option pricing models:\nDuan's GARCH and the classical tempered stable GARCH that significantly improve\nupon the limitation of the Black-Scholes model but have suffered from\ncomputation complexity. To mitigate this technical difficulty, we train ANNs\nwith a dataset generated by Monte Carlo Simulation (MCS) method and apply them\nto calibrate optimal parameters. The performance results indicate that the ANN\napproach consistently outperforms MCS and takes advantage of faster computation\ntimes once trained. The Greeks of options are also discussed.\n","authors":["Young Shin Kim","Hyangju Kim","Jaehyung Choi"],"pdf_url":"https://arxiv.org/pdf/2303.08760v1.pdf","comment":"26 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.00513v2","updated":"2023-03-15T16:54:00Z","published":"2022-10-02T13:19:48Z","title":"Gradient Gating for Deep Multi-Rate Learning on Graphs","summary":"  We present Gradient Gating (G$^2$), a novel framework for improving the\nperformance of Graph Neural Networks (GNNs). Our framework is based on gating\nthe output of GNN layers with a mechanism for multi-rate flow of message\npassing information across nodes of the underlying graph. Local gradients are\nharnessed to further modulate message passing updates. Our framework flexibly\nallows one to use any basic GNN layer as a wrapper around which the multi-rate\ngradient gating mechanism is built. We rigorously prove that G$^2$ alleviates\nthe oversmoothing problem and allows the design of deep GNNs. Empirical results\nare presented to demonstrate that the proposed framework achieves\nstate-of-the-art performance on a variety of graph learning tasks, including on\nlarge-scale heterophilic graphs.\n","authors":["T. Konstantin Rusch","Benjamin P. Chamberlain","Michael W. Mahoney","Michael M. Bronstein","Siddhartha Mishra"],"pdf_url":"https://arxiv.org/pdf/2210.00513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08757v1","updated":"2023-03-15T16:53:19Z","published":"2023-03-15T16:53:19Z","title":"Exploiting 4D CT Perfusion for segmenting infarcted areas in patients\n  with suspected acute ischemic stroke","summary":"  Precise and fast prediction methods for ischemic areas (core and penumbra) in\nacute ischemic stroke (AIS) patients are of significant clinical interest: they\nplay an essential role in improving diagnosis and treatment planning. Computed\nTomography (CT) scan is one of the primary modalities for early assessment in\npatients with suspected AIS. CT Perfusion (CTP) is often used as a primary\nassessment to determine stroke location, severity, and volume of ischemic\nlesions. Current automatic segmentation methods for CTP mostly use already\nprocessed 3D color maps conventionally used for visual assessment by\nradiologists as input. Alternatively, the raw CTP data is used on a\nslice-by-slice basis as 2D+time input, where the spatial information over the\nvolume is ignored. In this paper, we investigate different methods to utilize\nthe entire 4D CTP as input to fully exploit the spatio-temporal information.\nThis leads us to propose a novel 4D convolution layer. Our comprehensive\nexperiments on a local dataset comprised of 152 patients divided into three\ngroups show that our proposed models generate more precise results than other\nmethods explored. A Dice Coefficient of 0.70 and 0.45 is achieved for penumbra\nand core areas, respectively. The code is available on\nhttps://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.\n","authors":["Luca Tomasetti","Kjersti Engan","Liv Jorunn Høllesli","Kathinka Dæhli Kurz","Mahdieh Khanmohammadi"],"pdf_url":"https://arxiv.org/pdf/2303.08757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08737v1","updated":"2023-03-15T16:21:50Z","published":"2023-03-15T16:21:50Z","title":"Evaluating gesture-generation in a large-scale open challenge: The GENEA\n  Challenge 2022","summary":"  This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems. The\ndataset was based on 18 hours of full-body motion capture, including fingers,\nof different persons engaging in a dyadic conversation. Ten teams participated\nin the challenge across two tiers: full-body and upper-body gesticulation. For\neach tier, we evaluated both the human-likeness of the gesture motion and its\nappropriateness for the specific speech signal. Our evaluations decouple\nhuman-likeness from gesture appropriateness, which has been a difficult problem\nin the field.\n  The evaluation results are a revolution, and a revelation. Some synthetic\nconditions are rated as significantly more human-like than human motion\ncapture. To the best of our knowledge, this has never been shown before on a\nhigh-fidelity avatar. On the other hand, all synthetic motion is found to be\nvastly less appropriate for the speech than the original motion-capture\nrecordings. We also find that conventional objective metrics do not correlate\nwell with subjective human-likeness ratings in this large evaluation. The one\nexception is the Fr\\'echet gesture distance (FGD), which achieves a Kendall's\ntau rank correlation of around -0.5. Based on the challenge results we\nformulate numerous recommendations for system building and evaluation.\n","authors":["Taras Kucherenko","Pieter Wolfert","Youngwoo Yoon","Carla Viegas","Teodor Nikolov","Mihail Tsakov","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2303.08737v1.pdf","comment":"The first three authors made equal contributions and share joint\n  first authorship. arXiv admin note: substantial text overlap with\n  arXiv:2208.10441"},{"id":"http://arxiv.org/abs/2303.08736v1","updated":"2023-03-15T16:21:15Z","published":"2023-03-15T16:21:15Z","title":"A machine-learning approach to thunderstorm forecasting through\n  post-processing of simulation data","summary":"  Thunderstorms pose a major hazard to society and economy, which calls for\nreliable thunderstorm forecasts. In this work, we introduce SALAMA, a\nfeedforward neural network model for identifying thunderstorm occurrence in\nnumerical weather prediction (NWP) data. The model is trained on\nconvection-resolving ensemble forecasts over Central Europe and lightning\nobservations. Given only a set of pixel-wise input parameters that are\nextracted from NWP data and related to thunderstorm development, SALAMA infers\nthe probability of thunderstorm occurrence in a reliably calibrated manner. For\nlead times up to eleven hours, we find a forecast skill superior to\nclassification based only on convective available potential energy. Varying the\nspatiotemporal criteria by which we associate lightning observations with NWP\ndata, we show that the time scale for skillful thunderstorm predictions\nincreases linearly with the spatial scale of the forecast.\n","authors":["Kianusch Vahid Yousefnia","Tobias Bölle","Isabella Zöbisch","Thomas Gerz"],"pdf_url":"https://arxiv.org/pdf/2303.08736v1.pdf","comment":"15 pages, 9 figures, 2 tables. Submitted to Quarterly Journal of the\n  Royal Meteorological Society"},{"id":"http://arxiv.org/abs/2112.00038v2","updated":"2023-03-15T16:16:56Z","published":"2021-11-30T19:01:32Z","title":"Robust and Provably Monotonic Networks","summary":"  The Lipschitz constant of the map between the input and output space\nrepresented by a neural network is a natural metric for assessing the\nrobustness of the model. We present a new method to constrain the Lipschitz\nconstant of dense deep learning models that can also be generalized to other\narchitectures. The method relies on a simple weight normalization scheme during\ntraining that ensures the Lipschitz constant of every layer is below an upper\nlimit specified by the analyst. A simple monotonic residual connection can then\nbe used to make the model monotonic in any subset of its inputs, which is\nuseful in scenarios where domain knowledge dictates such dependence. Examples\ncan be found in algorithmic fairness requirements or, as presented here, in the\nclassification of the decays of subatomic particles produced at the CERN Large\nHadron Collider. Our normalization is minimally constraining and allows the\nunderlying architecture to maintain higher expressiveness compared to other\ntechniques which aim to either control the Lipschitz constant of the model or\nensure its monotonicity. We show how the algorithm was used to train a\npowerful, robust, and interpretable discriminator for heavy-flavor-quark\ndecays, which has been adopted for use as the primary data-selection algorithm\nin the LHCb real-time data-processing system in the current LHC data-taking\nperiod known as Run 3. In addition, our algorithm has also achieved\nstate-of-the-art performance on benchmarks in medicine, finance, and other\napplications.\n","authors":["Ouail Kitouni","Niklas Nolte","Mike Williams"],"pdf_url":"https://arxiv.org/pdf/2112.00038v2.pdf","comment":"15 pages, 7 figures, v2 extended journal version, v1 presented at the\n  Machine Learning and the Physical Sciences Workshop at the 35th Conference on\n  Neural Information Processing Systems (NeurIPS) December 13, 2021"},{"id":"http://arxiv.org/abs/2301.04298v2","updated":"2023-03-15T16:16:18Z","published":"2023-01-11T04:15:51Z","title":"Age of Information in Deep Learning-Driven Task-Oriented Communications","summary":"  This paper studies the notion of age in task-oriented communications that\naims to execute a task at a receiver utilizing the data at its transmitter. The\ntransmitter-receiver operations are modeled as an encoder-decoder pair that is\njointly trained while considering channel effects. The encoder converts data\nsamples into feature vectors of small dimension and transmits them with a small\nnumber of channel uses thereby reducing the number of transmissions and\nlatency. Instead of reconstructing input samples, the decoder performs a task,\ne.g., classification, on the received signals. Applying different deep neural\nnetworks of encoder-decoder pairs on MNIST and CIFAR-10 image datasets, the\nclassifier accuracy is shown to increase with the number of channel uses at the\nexpense of longer service time. The peak age of task information (PAoTI) is\nintroduced to analyze this accuracy-latency tradeoff when the age grows unless\na received signal is classified correctly. By incorporating channel and traffic\neffects, design guidelines are obtained for task-oriented communications by\ncharacterizing how the PAoTI first decreases and then increases with the number\nof channel uses. A dynamic update mechanism is presented to adapt the number of\nchannel uses to channel and traffic conditions, and reduce the PAoTI in\ntask-oriented communications.\n","authors":["Yalin E. Sagduyu","Sennur Ulukus","Aylin Yener"],"pdf_url":"https://arxiv.org/pdf/2301.04298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08729v1","updated":"2023-03-15T16:13:40Z","published":"2023-03-15T16:13:40Z","title":"DACOS-A Manually Annotated Dataset of Code Smells","summary":"  Researchers apply machine-learning techniques for code smell detection to\ncounter the subjectivity of many code smells. Such approaches need a large,\nmanually annotated dataset for training and benchmarking. Existing literature\noffers a few datasets; however, they are small in size and, more importantly,\ndo not focus on the subjective code snippets. In this paper, we present DACOS,\na manually annotated dataset containing 10,267 annotations for 5,192 code\nsnippets. The dataset targets three kinds of code smells at different\ngranularity: multifaceted abstraction, complex method, and long parameter list.\nThe dataset is created in two phases. The first phase helps us identify the\ncode snippets that are potentially subjective by determining the thresholds of\nmetrics used to detect a smell. The second phase collects annotations for\npotentially subjective snippets. We also offer an extended dataset DACOSX that\nincludes definitely benign and definitely smelly snippets by using the\nthresholds identified in the first phase. We have developed TagMan, a web\napplication to help annotators view and mark the snippets one-by-one and record\nthe provided annotations. We make the datasets and the web application\naccessible publicly. This dataset will help researchers working on smell\ndetection techniques to build relevant and context-aware machine-learning\nmodels.\n","authors":["Himesh Nandani","Mootez Saad","Tushar Sharma"],"pdf_url":"https://arxiv.org/pdf/2303.08729v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2303.07519v2","updated":"2023-03-15T16:07:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100% rate.\nAccuracy shows great improvement when scaling the models, with the largest\nmodel (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for\ndifferent prompt categories. We open source the finetuned Architext models and\nour synthetic dataset, hoping to inspire experimentation in this exciting area\nof design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01063v3","updated":"2023-03-15T16:06:39Z","published":"2022-10-03T16:22:57Z","title":"On Stability and Generalization of Bilevel Optimization Problem","summary":"  (Stochastic) bilevel optimization is a frequently encountered problem in\nmachine learning with a wide range of applications such as meta-learning,\nhyper-parameter optimization, and reinforcement learning. Most of the existing\nstudies on this problem only focused on analyzing the convergence or improving\nthe convergence rate, while little effort has been devoted to understanding its\ngeneralization behaviors. In this paper, we conduct a thorough analysis on the\ngeneralization of first-order (gradient-based) methods for the bilevel\noptimization problem. We first establish a fundamental connection between\nalgorithmic stability and generalization error in different forms and give a\nhigh probability generalization bound which improves the previous best one from\n$\\bigO(\\sqrt{n})$ to $\\bigO(\\log n)$, where $n$ is the sample size. We then\nprovide the first stability bounds for the general case where both inner and\nouter level parameters are subject to continuous update, while existing work\nallows only the outer level parameter to be updated. Our analysis can be\napplied in various standard settings such as strongly-convex-strongly-convex\n(SC-SC), convex-convex (C-C), and nonconvex-nonconvex (NC-NC). Our analysis for\nthe NC-NC setting can also be extended to a particular\nnonconvex-strongly-convex (NC-SC) setting that is commonly encountered in\npractice. Finally, we corroborate our theoretical analysis and demonstrate how\niterations can affect the generalization error by experiments on meta-learning\nand hyper-parameter optimization.\n","authors":["Meng Ding","Mingxi Lei","Yunwen Lei","Di Wang","Jinhui Xu"],"pdf_url":"https://arxiv.org/pdf/2210.01063v3.pdf","comment":"This paper currently contains unresolved technical flaws that have\n  the potential to mislead readers. However, we are committed to addressing\n  these issues and improving the quality of the paper in the future"},{"id":"http://arxiv.org/abs/2207.09845v2","updated":"2023-03-15T16:06:29Z","published":"2022-07-20T12:17:02Z","title":"Quantifying the Effect of Feedback Frequency in Interactive\n  Reinforcement Learning for Robotic Tasks","summary":"  Reinforcement learning (RL) has become widely adopted in robot control.\nDespite many successes, one major persisting problem can be very low data\nefficiency. One solution is interactive feedback, which has been shown to speed\nup RL considerably. As a result, there is an abundance of different strategies,\nwhich are, however, primarily tested on discrete grid-world and small scale\noptimal control scenarios. In the literature, there is no consensus about which\nfeedback frequency is optimal or at which time the feedback is most beneficial.\nTo resolve these discrepancies we isolate and quantify the effect of feedback\nfrequency in robotic tasks with continuous state and action spaces. The\nexperiments encompass inverse kinematics learning for robotic manipulator arms\nof different complexity. We show that seemingly contradictory reported\nphenomena occur at different complexity levels. Furthermore, our results\nsuggest that no single ideal feedback frequency exists. Rather that feedback\nfrequency should be changed as the agent's proficiency in the task increases.\n","authors":["Daniel Harnack","Julie Pivin-Bachler","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2207.09845v2.pdf","comment":"Neural Computing and Applications (2022). Special Issue on\n  Human-aligned Reinforcement Learning for Autonomous Agents and Robots"},{"id":"http://arxiv.org/abs/2303.08721v1","updated":"2023-03-15T16:05:11Z","published":"2023-03-15T16:05:11Z","title":"Artificial Influence: An Analysis Of AI-Driven Persuasion","summary":"  Persuasion is a key aspect of what it means to be human, and is central to\nbusiness, politics, and other endeavors. Advancements in artificial\nintelligence (AI) have produced AI systems that are capable of persuading\nhumans to buy products, watch videos, click on search results, and more. Even\nsystems that are not explicitly designed to persuade may do so in practice. In\nthe future, increasingly anthropomorphic AI systems may form ongoing\nrelationships with users, increasing their persuasive power. This paper\ninvestigates the uncertain future of persuasive AI systems. We examine ways\nthat AI could qualitatively alter our relationship to and views regarding\npersuasion by shifting the balance of persuasive power, allowing personalized\npersuasion to be deployed at scale, powering misinformation campaigns, and\nchanging the way humans can shape their own discourse. We consider ways\nAI-driven persuasion could differ from human-driven persuasion. We warn that\nubiquitous highlypersuasive AI systems could alter our information environment\nso significantly so as to contribute to a loss of human control of our own\nfuture. In response, we examine several potential responses to AI-driven\npersuasion: prohibition, identification of AI agents, truthful AI, and legal\nremedies. We conclude that none of these solutions will be airtight, and that\nindividuals and governments will need to take active steps to guard against the\nmost pernicious effects of persuasive AI.\n","authors":["Matthew Burtell","Thomas Woodside"],"pdf_url":"https://arxiv.org/pdf/2303.08721v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.08720v1","updated":"2023-03-15T16:05:05Z","published":"2023-03-15T16:05:05Z","title":"Practicality of generalization guarantees for unsupervised domain\n  adaptation with neural networks","summary":"  Understanding generalization is crucial to confidently engineer and deploy\nmachine learning models, especially when deployment implies a shift in the data\ndomain. For such domain adaptation problems, we seek generalization bounds\nwhich are tractably computable and tight. If these desiderata can be reached,\nthe bounds can serve as guarantees for adequate performance in deployment.\nHowever, in applications where deep neural networks are the models of choice,\nderiving results which fulfill these remains an unresolved challenge; most\nexisting bounds are either vacuous or has non-estimable terms, even in\nfavorable conditions. In this work, we evaluate existing bounds from the\nliterature with potential to satisfy our desiderata on domain adaptation image\nclassification tasks, where deep neural networks are preferred. We find that\nall bounds are vacuous and that sample generalization terms account for much of\nthe observed looseness, especially when these terms interact with measures of\ndomain shift. To overcome this and arrive at the tightest possible results, we\ncombine each bound with recent data-dependent PAC-Bayes analysis, greatly\nimproving the guarantees. We find that, when domain overlap can be assumed, a\nsimple importance weighting extension of previous work provides the tightest\nestimable bound. Finally, we study which terms dominate the bounds and identify\npossible directions for further improvement.\n","authors":["Adam Breitholtz","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.08720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06950v3","updated":"2023-03-15T15:59:22Z","published":"2022-07-14T14:23:14Z","title":"Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA\n  Models","summary":"  Low-order functional ANOVA (fANOVA) models have been rediscovered in the\nmachine learning (ML) community under the guise of inherently interpretable\nmachine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and\nGAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting\nfunctional main effects and second-order interactions. We propose a new\nalgorithm, called GAMI-Tree, that is similar to EBM, but has a number of\nfeatures that lead to better performance. It uses model-based trees as base\nlearners and incorporates a new interaction filtering method that is better at\ncapturing the underlying interactions. In addition, our iterative training\nmethod converges to a model with better predictive performance, and the\nembedded purification ensures that interactions are hierarchically orthogonal\nto main effects. The algorithm does not need extensive tuning, and our\nimplementation is fast and efficient. We use simulated and real datasets to\ncompare the performance and interpretability of GAMI-Tree with EBM and\nGAMI-Net.\n","authors":["Linwei Hu","Jie Chen","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2207.06950v3.pdf","comment":"25 pages plus appendix"},{"id":"http://arxiv.org/abs/2106.07452v3","updated":"2023-03-15T15:57:38Z","published":"2021-06-14T14:23:34Z","title":"Marginalising over Stationary Kernels with Bayesian Quadrature","summary":"  Marginalising over families of Gaussian Process kernels produces flexible\nmodel classes with well-calibrated uncertainty estimates. Existing approaches\nrequire likelihood evaluations of many kernels, rendering them prohibitively\nexpensive for larger datasets. We propose a Bayesian Quadrature scheme to make\nthis marginalisation more efficient and thereby more practical. Through use of\nthe maximum mean discrepancies between distributions, we define a kernel over\nkernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel\nsamples are selected by generalising an information-theoretic acquisition\nfunction for warped Bayesian Quadrature. We show that our framework achieves\nmore accurate predictions with better calibrated uncertainty than\nstate-of-the-art baselines, especially when given limited (wall-clock) time\nbudgets.\n","authors":["Saad Hamid","Sebastian Schulze","Michael A. Osborne","Stephen J. Roberts"],"pdf_url":"https://arxiv.org/pdf/2106.07452v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10921v2","updated":"2023-03-15T15:49:07Z","published":"2023-01-26T03:53:25Z","title":"SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised\n  Learning","summary":"  The critical challenge of Semi-Supervised Learning (SSL) is how to\neffectively leverage the limited labeled data and massive unlabeled data to\nimprove the model's generalization performance. In this paper, we first revisit\nthe popular pseudo-labeling methods via a unified sample weighting formulation\nand demonstrate the inherent quantity-quality trade-off problem of\npseudo-labeling with thresholding, which may prohibit learning. To this end, we\npropose SoftMatch to overcome the trade-off by maintaining both high quantity\nand high quality of pseudo-labels during training, effectively exploiting the\nunlabeled data. We derive a truncated Gaussian function to weight samples based\non their confidence, which can be viewed as a soft version of the confidence\nthreshold. We further enhance the utilization of weakly-learned classes by\nproposing a uniform alignment approach. In experiments, SoftMatch shows\nsubstantial improvements across a wide variety of benchmarks, including image,\ntext, and imbalanced classification.\n","authors":["Hao Chen","Ran Tao","Yue Fan","Yidong Wang","Jindong Wang","Bernt Schiele","Xing Xie","Bhiksha Raj","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2301.10921v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2203.11012v2","updated":"2023-03-15T15:46:06Z","published":"2022-03-07T19:40:39Z","title":"Learning Resilient Radio Resource Management Policies with Graph Neural\n  Networks","summary":"  We consider the problems of user selection and power control in wireless\ninterference networks, comprising multiple access points (APs) communicating\nwith a group of user equipment devices (UEs) over a shared wireless medium. To\nachieve a high aggregate rate, while ensuring fairness across all users, we\nformulate a resilient radio resource management (RRM) policy optimization\nproblem with per-user minimum-capacity constraints that adapt to the underlying\nnetwork conditions via learnable slack variables. We reformulate the problem in\nthe Lagrangian dual domain, and show that we can parameterize the RRM policies\nusing a finite set of parameters, which can be trained alongside the slack and\ndual variables via an unsupervised primal-dual approach thanks to a provably\nsmall duality gap. We use a scalable and permutation-equivariant graph neural\nnetwork (GNN) architecture to parameterize the RRM policies based on a graph\ntopology derived from the instantaneous channel conditions. Through\nexperimental results, we verify that the minimum-capacity constraints adapt to\nthe underlying network configurations and channel conditions. We further\ndemonstrate that, thanks to such adaptation, our proposed method achieves a\nsuperior tradeoff between the average rate and the 5th percentile rate -- a\nmetric that quantifies the level of fairness in the resource allocation\ndecisions -- as compared to baseline algorithms.\n","authors":["Navid NaderiAlizadeh","Mark Eisen","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2203.11012v2.pdf","comment":"Accepted by IEEE Transactions on Signal Processing. Code available at\n  https://github.com/navid-naderi/Resilient_RRM_GNN"},{"id":"http://arxiv.org/abs/2302.14732v2","updated":"2023-03-15T15:45:16Z","published":"2023-02-28T16:36:26Z","title":"Constrained Bayesian Optimization for Automatic Underwater Vehicle Hull\n  Design","summary":"  Automatic underwater vehicle hull Design optimization is a complex\nengineering process for generating a UUV hull with optimized properties on a\ngiven requirement. First, it involves the integration of involved\ncomputationally complex engineering simulation tools. Second, it needs\nintegration of a sample efficient optimization framework with the integrated\ntoolchain. To this end, we integrated the CAD tool called FreeCAD with CFD tool\nopenFoam for automatic design evaluation. For optimization, we chose Bayesian\noptimization (BO), which is a well-known technique developed for optimizing\ntime-consuming expensive engineering simulations and has proven to be very\nsample efficient in a variety of problems, including hyper-parameter tuning and\nexperimental design. During the optimization process, we can handle infeasible\ndesign as constraints integrated into the optimization process. By integrating\ndomain-specific toolchain with AI-based optimization, we executed the automatic\ndesign optimization of underwater vehicle hull design. For empirical\nevaluation, we took two different use cases of real-world underwater vehicle\ndesign to validate the execution of our tool.\n","authors":["Harsh Vardhan","Peter Volgyesi","Will Hedgecock","Janos Sztipanovits"],"pdf_url":"https://arxiv.org/pdf/2302.14732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10390v2","updated":"2023-03-15T15:44:37Z","published":"2023-02-21T01:32:27Z","title":"DrasCLR: A Self-supervised Framework of Learning Disease-related and\n  Anatomy-specific Representation for 3D Medical Images","summary":"  Large-scale volumetric medical images with annotation are rare, costly, and\ntime prohibitive to acquire. Self-supervised learning (SSL) offers a promising\npre-training and feature extraction solution for many downstream tasks, as it\nonly uses unlabeled data. Recently, SSL methods based on instance\ndiscrimination have gained popularity in the medical imaging domain. However,\nSSL pre-trained encoders may use many clues in the image to discriminate an\ninstance that are not necessarily disease-related. Moreover, pathological\npatterns are often subtle and heterogeneous, requiring the ability of the\ndesired method to represent anatomy-specific features that are sensitive to\nabnormal changes in different body parts. In this work, we present a novel SSL\nframework, named DrasCLR, for 3D medical imaging to overcome these challenges.\nWe propose two domain-specific contrastive learning strategies: one aims to\ncapture subtle disease patterns inside a local anatomical region, and the other\naims to represent severe disease patterns that span larger regions. We\nformulate the encoder using conditional hyper-parameterized network, in which\nthe parameters are dependant on the anatomical location, to extract\nanatomically sensitive features. Extensive experiments on large-scale computer\ntomography (CT) datasets of lung images show that our method improves the\nperformance of many downstream prediction and segmentation tasks. The\npatient-level representation improves the performance of the patient survival\nprediction task. We show how our method can detect emphysema subtypes via dense\nprediction. We demonstrate that fine-tuning the pre-trained model can\nsignificantly reduce annotation efforts without sacrificing emphysema detection\naccuracy. Our ablation study highlights the importance of incorporating\nanatomical context into the SSL framework.\n","authors":["Ke Yu","Li Sun","Junxiang Chen","Max Reynolds","Tigmanshu Chaudhary","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2302.10390v2.pdf","comment":"Added some recent references"},{"id":"http://arxiv.org/abs/2303.08691v1","updated":"2023-03-15T15:21:41Z","published":"2023-03-15T15:21:41Z","title":"Learning to Reconstruct Signals From Binary Measurements","summary":"  Recent advances in unsupervised learning have highlighted the possibility of\nlearning to reconstruct signals from noisy and incomplete linear measurements\nalone. These methods play a key role in medical and scientific imaging and\nsensing, where ground truth data is often scarce or difficult to obtain.\nHowever, in practice, measurements are not only noisy and incomplete but also\nquantized. Here we explore the extreme case of learning from binary\nobservations and provide necessary and sufficient conditions on the number of\nmeasurements required for identifying a set of signals from incomplete binary\ndata. Our results are complementary to existing bounds on signal recovery from\nbinary measurements. Furthermore, we introduce a novel self-supervised learning\napproach, which we name SSBM, that only requires binary data for training. We\ndemonstrate in a series of experiments with real datasets that SSBM performs on\npar with supervised learning and outperforms sparse reconstruction methods with\na fixed wavelet basis by a large margin.\n","authors":["Julián Tachella","Laurent Jacques"],"pdf_url":"https://arxiv.org/pdf/2303.08691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08690v1","updated":"2023-03-15T15:21:26Z","published":"2023-03-15T15:21:26Z","title":"Replay Buffer With Local Forgetting for Adaptive Deep Model-Based\n  Reinforcement Learning","summary":"  One of the key behavioral characteristics used in neuroscience to determine\nwhether the subject of study -- be it a rodent or a human -- exhibits\nmodel-based learning is effective adaptation to local changes in the\nenvironment. In reinforcement learning, however, recent work has shown that\nmodern deep model-based reinforcement-learning (MBRL) methods adapt poorly to\nsuch changes. An explanation for this mismatch is that MBRL methods are\ntypically designed with sample-efficiency on a single task in mind and the\nrequirements for effective adaptation are substantially higher, both in terms\nof the learned world model and the planning routine. One particularly\nchallenging requirement is that the learned world model has to be sufficiently\naccurate throughout relevant parts of the state-space. This is challenging for\ndeep-learning-based world models due to catastrophic forgetting. And while a\nreplay buffer can mitigate the effects of catastrophic forgetting, the\ntraditional first-in-first-out replay buffer precludes effective adaptation due\nto maintaining stale data. In this work, we show that a conceptually simple\nvariation of this traditional replay buffer is able to overcome this\nlimitation. By removing only samples from the buffer from the local\nneighbourhood of the newly observed samples, deep world models can be built\nthat maintain their accuracy across the state-space, while also being able to\neffectively adapt to changes in the reward function. We demonstrate this by\napplying our replay-buffer variation to a deep version of the classical Dyna\nmethod, as well as to recent methods such as PlaNet and DreamerV2,\ndemonstrating that deep model-based methods can adapt effectively as well to\nlocal changes in the environment.\n","authors":["Ali Rahimi-Kalahroudi","Janarthanan Rajendran","Ida Momennejad","Harm van Seijen","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2303.08690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08685v1","updated":"2023-03-15T15:12:36Z","published":"2023-03-15T15:12:36Z","title":"Making Vision Transformers Efficient from A Token Sparsification View","summary":"  The quadratic computational complexity to the number of tokens limits the\npractical applications of Vision Transformers (ViTs). Several works propose to\nprune redundant tokens to achieve efficient ViTs. However, these methods\ngenerally suffer from (i) dramatic accuracy drops, (ii) application difficulty\nin the local vision transformer, and (iii) non-general-purpose networks for\ndownstream tasks. In this work, we propose a novel Semantic Token ViT (STViT),\nfor efficient global and local vision transformers, which can also be revised\nto serve as backbone for downstream tasks. The semantic tokens represent\ncluster centers, and they are initialized by pooling image tokens in space and\nrecovered by attention, which can adaptively represent global or local semantic\ninformation. Due to the cluster properties, a few semantic tokens can attain\nthe same effect as vast image tokens, for both global and local vision\ntransformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base)\ncan achieve the same accuracy with more than 100% inference speed improvement\nand nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16\nsemantic tokens in each window to further speed it up by around 20% with slight\naccuracy increase. Besides great success in image classification, we also\nextend our method to video recognition. In addition, we design a\nSTViT-R(ecover) network to restore the detailed spatial information based on\nthe STViT, making it work for downstream tasks, which is powerless for previous\ntoken sparsification methods. Experiments demonstrate that our method can\nachieve competitive results compared to the original networks in object\ndetection and instance segmentation, with over 30% FLOPs reduction for\nbackbone.\n","authors":["Shuning Chang","Pichao Wang","Ming Lin","Fan Wang","David Junhao Zhang","Rong Jin","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.08685v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2101.07415v6","updated":"2023-03-15T15:05:53Z","published":"2021-01-19T02:19:05Z","title":"ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search\n  Spaces","summary":"  In this paper, we approach the problem of optimizing blackbox functions over\nlarge hybrid search spaces consisting of both combinatorial and continuous\nparameters. We demonstrate that previous evolutionary algorithms which rely on\nmutation-based approaches, while flexible over combinatorial spaces, suffer\nfrom a curse of dimensionality in high dimensional continuous spaces both\ntheoretically and empirically, which thus limits their scope over hybrid search\nspaces as well. In order to combat this curse, we propose ES-ENAS, a simple and\nmodular joint optimization procedure combining the class of sample-efficient\nsmoothed gradient techniques, commonly known as Evolutionary Strategies (ES),\nwith combinatorial optimizers in a highly scalable and intuitive way, inspired\nby the one-shot or supernet paradigm introduced in Efficient Neural\nArchitecture Search (ENAS). By doing so, we achieve significantly more sample\nefficiency, which we empirically demonstrate over synthetic benchmarks, and are\nfurther able to apply ES-ENAS for architecture search over popular RL\nbenchmarks.\n","authors":["Xingyou Song","Krzysztof Choromanski","Jack Parker-Holder","Yunhao Tang","Qiuyi Zhang","Daiyi Peng","Deepali Jain","Wenbo Gao","Aldo Pacchiano","Tamas Sarlos","Yuxiang Yang"],"pdf_url":"https://arxiv.org/pdf/2101.07415v6.pdf","comment":"Previously published at ICLR 2020 NAS Workshop. See\n  https://github.com/google-research/google-research/tree/master/es_enas for\n  associated code"},{"id":"http://arxiv.org/abs/2303.08680v1","updated":"2023-03-15T15:03:09Z","published":"2023-03-15T15:03:09Z","title":"Muti-Agent Proximal Policy Optimization For Data Freshness in\n  UAV-assisted Networks","summary":"  Unmanned aerial vehicles (UAVs) are seen as a promising technology to perform\na wide range of tasks in wireless communication networks. In this work, we\nconsider the deployment of a group of UAVs to collect the data generated by IoT\ndevices. Specifically, we focus on the case where the collected data is\ntime-sensitive, and it is critical to maintain its timeliness. Our objective is\nto optimally design the UAVs' trajectories and the subsets of visited IoT\ndevices such as the global Age-of-Updates (AoU) is minimized. To this end, we\nformulate the studied problem as a mixed-integer nonlinear programming (MINLP)\nunder time and quality of service constraints. To efficiently solve the\nresulting optimization problem, we investigate the cooperative Multi-Agent\nReinforcement Learning (MARL) framework and propose an RL approach based on the\npopular on-policy Reinforcement Learning (RL) algorithm: Policy Proximal\nOptimization (PPO). Our approach leverages the centralized training\ndecentralized execution (CTDE) framework where the UAVs learn their optimal\npolicies while training a centralized value function. Our simulation results\nshow that the proposed MAPPO approach reduces the global AoU by at least a\nfactor of 1/2 compared to conventional off-policy reinforcement learning\napproaches.\n","authors":["Mouhamed Naby Ndiaye","El Houcine Bergou","Hajar El Hammouti"],"pdf_url":"https://arxiv.org/pdf/2303.08680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08678v1","updated":"2023-03-15T15:02:15Z","published":"2023-03-15T15:02:15Z","title":"Visual Prompt Based Personalized Federated Learning","summary":"  As a popular paradigm of distributed learning, personalized federated\nlearning (PFL) allows personalized models to improve generalization ability and\nrobustness by utilizing knowledge from all distributed clients. Most existing\nPFL algorithms tackle personalization in a model-centric way, such as\npersonalized layer partition, model regularization, and model interpolation,\nwhich all fail to take into account the data characteristics of distributed\nclients. In this paper, we propose a novel PFL framework for image\nclassification tasks, dubbed pFedPT, that leverages personalized visual prompts\nto implicitly represent local data distribution information of clients and\nprovides that information to the aggregation model to help with classification\ntasks. Specifically, in each round of pFedPT training, each client generates a\nlocal personalized prompt related to local data distribution. Then, the local\nmodel is trained on the input composed of raw data and a visual prompt to learn\nthe distribution information contained in the prompt. During model testing, the\naggregated model obtains prior knowledge of the data distributions based on the\nprompts, which can be seen as an adaptive fine-tuning of the aggregation model\nto improve model performances on different clients. Furthermore, the visual\nprompt can be added as an orthogonal method to implement personalization on the\nclient for existing FL methods to boost their performance. Experiments on the\nCIFAR10 and CIFAR100 datasets show that pFedPT outperforms several\nstate-of-the-art (SOTA) PFL algorithms by a large margin in various settings.\n","authors":["Guanghao Li","Wansen Wu","Yan Sun","Li Shen","Baoyuan Wu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.08678v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2206.11736v2","updated":"2023-03-15T14:53:51Z","published":"2022-06-23T14:31:33Z","title":"NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds","summary":"  In order for artificial agents to successfully perform tasks in changing\nenvironments, they must be able to both detect and adapt to novelty. However,\nvisual novelty detection research often only evaluates on repurposed datasets\nsuch as CIFAR-10 originally intended for object classification, where images\nfocus on one distinct, well-centered object. New benchmarks are needed to\nrepresent the challenges of navigating the complex scenes of an open world. Our\nnew NovelCraft dataset contains multimodal episodic data of the images and\nsymbolic world-states seen by an agent completing a pogo stick assembly task\nwithin a modified Minecraft environment. In some episodes, we insert novel\nobjects within the complex 3D scene that may impact gameplay and appear in a\nvariety of sizes and positions. Our visual novelty detection benchmark finds\nthat methods that rank best on popular area-under-the-curve metrics may be\noutperformed by simpler alternatives when controlling false positives matters\nmost. Further multi-modal novelty detection experiments suggest that methods\nthat fuse both visual and symbolic information can improve time until detection\nas well as overall discrimination. Finally, our evaluation of recent\ngeneralized category discovery methods suggests that adapting to new imbalanced\ncategories in complex scenes remains an exciting open problem.\n","authors":["Patrick Feeney","Sarah Schneider","Panagiotis Lymperopoulos","Liping Liu","Matthias Scheutz","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2206.11736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01973v2","updated":"2023-03-15T14:36:49Z","published":"2023-02-03T19:47:22Z","title":"Measuring The Impact Of Programming Language Distribution","summary":"  Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.\n","authors":["Gabriel Orlanski","Kefan Xiao","Xavier Garcia","Jeffrey Hui","Joshua Howland","Jonathan Malmaud","Jacob Austin","Rishah Singh","Michele Catasta"],"pdf_url":"https://arxiv.org/pdf/2302.01973v2.pdf","comment":"Code and data release: https://github.com/google-research/babelcode"},{"id":"http://arxiv.org/abs/2302.11814v2","updated":"2023-03-15T14:35:50Z","published":"2023-02-23T06:53:16Z","title":"FTM: A Frame-level Timeline Modeling Method for Temporal Graph\n  Representation Learning","summary":"  Learning representations for graph-structured data is essential for graph\nanalytical tasks. While remarkable progress has been made on static graphs,\nresearches on temporal graphs are still in its beginning stage. The bottleneck\nof the temporal graph representation learning approach is the neighborhood\naggregation strategy, based on which graph attributes share and gather\ninformation explicitly. Existing neighborhood aggregation strategies fail to\ncapture either the short-term features or the long-term features of temporal\ngraph attributes, leading to unsatisfactory model performance and even poor\nrobustness and domain generality of the representation learning method. To\naddress this problem, we propose a Frame-level Timeline Modeling (FTM) method\nthat helps to capture both short-term and long-term features and thus learns\nmore informative representations on temporal graphs. In particular, we present\na novel link-based framing technique to preserve the short-term features and\nthen incorporate a timeline aggregator module to capture the intrinsic dynamics\nof graph evolution as long-term features. Our method can be easily assembled\nwith most temporal GNNs. Extensive experiments on common datasets show that our\nmethod brings great improvements to the capability, robustness, and domain\ngenerality of backbone methods in downstream tasks. Our code can be found at\nhttps://github.com/yeeeqichen/FTM.\n","authors":["Bowen Cao","Qichen Ye","Weiyuan Xu","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2302.11814v2.pdf","comment":"Accepted in AAAI 2023, oral"},{"id":"http://arxiv.org/abs/2302.11799v2","updated":"2023-03-15T14:31:56Z","published":"2023-02-23T06:25:51Z","title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question\n  Answering","summary":"  Knowledge-aware question answering (KAQA) requires the model to answer\nquestions over a knowledge base, which is essential for both open-domain QA and\ndomain-specific QA, especially when language models alone cannot provide all\nthe knowledge needed. Despite the promising result of recent KAQA systems which\ntend to integrate linguistic knowledge from pre-trained language models (PLM)\nand factual knowledge from knowledge graphs (KG) to answer complex questions, a\nbottleneck exists in effectively fusing the representations from PLMs and KGs\nbecause of (i) the semantic and distributional gaps between them, and (ii) the\ndifficulties in joint reasoning over the provided knowledge from both\nmodalities. To address the above two problems, we propose a Fine-grained\nTwo-stage training framework (FiTs) to boost the KAQA system performance: The\nfirst stage aims at aligning representations from the PLM and the KG, thus\nbridging the modality gaps between them, named knowledge adaptive\npost-training. The second stage, called knowledge-aware fine-tuning, aims to\nimprove the model's joint reasoning ability based on the aligned\nrepresentations. In detail, we fine-tune the post-trained model via two\nauxiliary self-supervised tasks in addition to the QA supervision. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,\nOpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.\n","authors":["Qichen Ye","Bowen Cao","Nuo Chen","Weiyuan Xu","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2302.11799v2.pdf","comment":"Accepted in AAAI 2023, oral"},{"id":"http://arxiv.org/abs/2211.05231v2","updated":"2023-03-15T14:31:46Z","published":"2022-11-02T17:58:47Z","title":"Biologically-Inspired Continual Learning of Human Motion Sequences","summary":"  This work proposes a model for continual learning on tasks involving temporal\nsequences, specifically, human motions. It improves on a recently proposed\nbrain-inspired replay model (BI-R) by building a biologically-inspired\nconditional temporal variational autoencoder (BI-CTVAE), which instantiates a\nlatent mixture-of-Gaussians for class representation. We investigate a novel\ncontinual-learning-to-generate (CL2Gen) scenario where the model generates\nmotion sequences of different classes. The generative accuracy of the model is\ntested over a set of tasks. The final classification accuracy of BI-CTVAE on a\nhuman motion dataset after sequentially learning all action classes is 78%,\nwhich is 63% higher than using no-replay, and only 5.4% lower than a\nstate-of-the-art offline trained GRU model.\n","authors":["Joachim Ott","Shih-Chii Liu"],"pdf_url":"https://arxiv.org/pdf/2211.05231v2.pdf","comment":"5 pages, 2 figures, accepted at IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023"},{"id":"http://arxiv.org/abs/2303.08644v1","updated":"2023-03-15T14:20:06Z","published":"2023-03-15T14:20:06Z","title":"RGI : Regularized Graph Infomax for self-supervised learning on graphs","summary":"  Self-supervised learning is gaining considerable attention as a solution to\navoid the requirement of extensive annotations in representation learning on\ngraphs. We introduce \\textit{Regularized Graph Infomax (RGI)}, a simple yet\neffective framework for node level self-supervised learning on graphs that\ntrains a graph neural network encoder by maximizing the mutual information\nbetween node level local and global views, in contrast to previous works that\nemploy graph level global views. The method promotes the predictability between\nviews while regularizing the covariance matrices of the representations.\nTherefore, RGI is non-contrastive, does not depend on complex asymmetric\narchitectures nor training tricks, is augmentation-free and does not rely on a\ntwo branch architecture. We run RGI on both transductive and inductive settings\nwith popular graph benchmarks and show that it can achieve state-of-the-art\nperformance regardless of its simplicity.\n","authors":["Oscar Pina","Verónica Vilaplana"],"pdf_url":"https://arxiv.org/pdf/2303.08644v1.pdf","comment":"11 pages, 1 figure, preprint"},{"id":"http://arxiv.org/abs/2207.02849v2","updated":"2023-03-15T14:10:55Z","published":"2022-07-05T14:01:15Z","title":"Betty: An Automatic Differentiation Library for Multilevel Optimization","summary":"  Gradient-based multilevel optimization (MLO) has gained attention as a\nframework for studying numerous problems, ranging from hyperparameter\noptimization and meta-learning to neural architecture search and reinforcement\nlearning. However, gradients in MLO, which are obtained by composing\nbest-response Jacobians via the chain rule, are notoriously difficult to\nimplement and memory/compute intensive. We take an initial step towards closing\nthis gap by introducing Betty, a software library for large-scale MLO. At its\ncore, we devise a novel dataflow graph for MLO, which allows us to (1) develop\nefficient automatic differentiation for MLO that reduces the computational\ncomplexity from O(d^3) to O(d^2), (2) incorporate systems support such as\nmixed-precision and data-parallel training for scalability, and (3) facilitate\nimplementation of MLO programs of arbitrary complexity while allowing a modular\ninterface for diverse algorithmic and systems design choices. We empirically\ndemonstrate that Betty can be used to implement an array of MLO programs, while\nalso observing up to 11% increase in test accuracy, 14% decrease in GPU memory\nusage, and 20% decrease in training wall time over existing implementations on\nmultiple benchmarks. We also showcase that Betty enables scaling MLO to models\nwith hundreds of millions of parameters. We open-source the code at\nhttps://github.com/leopard-ai/betty.\n","authors":["Sang Keun Choe","Willie Neiswanger","Pengtao Xie","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2207.02849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.06242v2","updated":"2023-03-15T14:06:40Z","published":"2022-04-13T08:22:31Z","title":"Encoding Domain Knowledge in Multi-view Latent Variable Models: A\n  Bayesian Approach with Structured Sparsity","summary":"  Many real-world systems are described not only by data from a single source\nbut via multiple data views. In genomic medicine, for instance, patients can be\ncharacterized by data from different molecular layers. Latent variable models\nwith structured sparsity are a commonly used tool for disentangling variation\nwithin and across data views. However, their interpretability is cumbersome\nsince it requires a direct inspection and interpretation of each factor from\ndomain experts. Here, we propose MuVI, a novel multi-view latent variable model\nbased on a modified horseshoe prior for modeling structured sparsity. This\nfacilitates the incorporation of limited and noisy domain knowledge, thereby\nallowing for an analysis of multi-view data in an inherently explainable\nmanner. We demonstrate that our model (i) outperforms state-of-the-art\napproaches for modeling structured sparsity in terms of the reconstruction\nerror and the precision/recall, (ii) robustly integrates noisy domain expertise\nin the form of feature sets, (iii) promotes the identifiability of factors and\n(iv) infers interpretable and biologically meaningful axes of variation in a\nreal-world multi-view dataset of cancer patients.\n","authors":["Arber Qoku","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2204.06242v2.pdf","comment":"9 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.08632v1","updated":"2023-03-15T14:00:11Z","published":"2023-03-15T14:00:11Z","title":"Pixel-Level Explanation of Multiple Instance Learning Models in\n  Biomedical Single Cell Images","summary":"  Explainability is a key requirement for computer-aided diagnosis systems in\nclinical decision-making. Multiple instance learning with attention pooling\nprovides instance-level explainability, however for many clinical applications\na deeper, pixel-level explanation is desirable, but missing so far. In this\nwork, we investigate the use of four attribution methods to explain a multiple\ninstance learning models: GradCAM, Layer-Wise Relevance Propagation (LRP),\nInformation Bottleneck Attribution (IBA), and InputIBA. With this collection of\nmethods, we can derive pixel-level explanations on for the task of diagnosing\nblood cancer from patients' blood smears. We study two datasets of acute\nmyeloid leukemia with over 100 000 single cell images and observe how each\nattribution method performs on the multiple instance learning architecture\nfocusing on different properties of the white blood single cells. Additionally,\nwe compare attribution maps with the annotations of a medical expert to see how\nthe model's decision-making differs from the human standard. Our study\naddresses the challenge of implementing pixel-level explainability in multiple\ninstance learning models and provides insights for clinicians to better\nunderstand and trust decisions from computer-aided diagnosis systems.\n","authors":["Ario Sadafi","Oleksandra Adonkina","Ashkan Khakzar","Peter Lienemann","Rudolf Matthias Hehr","Daniel Rueckert","Nassir Navab","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2303.08632v1.pdf","comment":"Accepted for publication at the international conference on\n  Information Processing in Medical Imaging (IPMI 2023)"},{"id":"http://arxiv.org/abs/2303.08631v1","updated":"2023-03-15T13:58:07Z","published":"2023-03-15T13:58:07Z","title":"Smoothed Q-learning","summary":"  In Reinforcement Learning the Q-learning algorithm provably converges to the\noptimal solution. However, as others have demonstrated, Q-learning can also\noverestimate the values and thereby spend too long exploring unhelpful states.\nDouble Q-learning is a provably convergent alternative that mitigates some of\nthe overestimation issues, though sometimes at the expense of slower\nconvergence. We introduce an alternative algorithm that replaces the max\noperation with an average, resulting also in a provably convergent off-policy\nalgorithm which can mitigate overestimation yet retain similar convergence as\nstandard Q-learning.\n","authors":["David Barber"],"pdf_url":"https://arxiv.org/pdf/2303.08631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08627v1","updated":"2023-03-15T13:54:11Z","published":"2023-03-15T13:54:11Z","title":"From Images to Features: Unbiased Morphology Classification via\n  Variational Auto-Encoders and Domain Adaptation","summary":"  We present a novel approach for the dimensionality reduction of galaxy images\nby leveraging a combination of variational auto-encoders (VAE) and domain\nadaptation (DA). We demonstrate the effectiveness of this approach using a\nsample of low redshift galaxies with detailed morphological type labels from\nthe Galaxy-Zoo DECaLS project. We show that 40-dimensional latent variables can\neffectively reproduce most morphological features in galaxy images. To further\nvalidate the effectiveness of our approach, we utilised a classical random\nforest (RF) classifier on the 40-dimensional latent variables to make detailed\nmorphology feature classifications. This approach performs similarly to a\ndirect neural network application on galaxy images. We further enhance our\nmodel by tuning the VAE network via DA using galaxies in the overlapping\nfootprint of DECaLS and BASS+MzLS, enabling the unbiased application of our\nmodel to galaxy images in both surveys. We observed that noise suppression\nduring DA led to even better morphological feature extraction and\nclassification performance. Overall, this combination of VAE and DA can be\napplied to achieve image dimensionality reduction, defect image identification,\nand morphology classification in large optical surveys.\n","authors":["Quanfeng Xu","Shiyin Shen","Rafael S. de Souza","Mi Chen","Renhao Ye","Yumei She","Zhu Chen","Emille E. O. Ishida","Alberto Krone-Martins","Rupesh Durgesh"],"pdf_url":"https://arxiv.org/pdf/2303.08627v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.08625v1","updated":"2023-03-15T13:50:36Z","published":"2023-03-15T13:50:36Z","title":"Interpretable Ensembles of Hyper-Rectangles as Base Models","summary":"  A new extremely simple ensemble-based model with the uniformly generated\naxis-parallel hyper-rectangles as base models (HRBM) is proposed. Two types of\nHRBMs are studied: closed rectangles and corners. The main idea behind HRBM is\nto consider and count training examples inside and outside each rectangle. It\nis proposed to incorporate HRBMs into the gradient boosting machine (GBM).\nDespite simplicity of HRBMs, it turns out that these simple base models allow\nus to construct effective ensemble-based models and avoid overfitting. A simple\nmethod for calculating optimal regularization parameters of the ensemble-based\nmodel, which can be modified in the explicit way at each iteration of GBM, is\nconsidered. Moreover, a new regularization called the \"step height penalty\" is\nstudied in addition to the standard L1 and L2 regularizations. An extremely\nsimple approach to the proposed ensemble-based model prediction interpretation\nby using the well-known method SHAP is proposed. It is shown that GBM with HRBM\ncan be regarded as a model extending a set of interpretable models for\nexplaining black-box models. Numerical experiments with real datasets\nillustrate the proposed GBM with HRBMs for regression and classification\nproblems. Experiments also illustrate computational efficiency of the proposed\nSHAP modifications. The code of proposed algorithms implementing GBM with HRBM\nis publicly available.\n","authors":["Andrei V. Konstantinov","Lev V. Utkin"],"pdf_url":"https://arxiv.org/pdf/2303.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08622v1","updated":"2023-03-15T13:47:02Z","published":"2023-03-15T13:47:02Z","title":"Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style\n  Transfer","summary":"  Diffusion models have shown great promise in text-guided image style\ntransfer, but there is a trade-off between style transformation and content\npreservation due to their stochastic nature. Existing methods require\ncomputationally expensive fine-tuning of diffusion models or additional neural\nnetwork. To address this, here we propose a zero-shot contrastive loss for\ndiffusion models that doesn't require additional fine-tuning or auxiliary\nnetworks. By leveraging patch-wise contrastive loss between generated samples\nand original image embeddings in the pre-trained diffusion model, our method\ncan generate images with the same semantic content as the source image in a\nzero-shot manner. Our approach outperforms existing methods while preserving\ncontent and requiring no additional training, not only for image style transfer\nbut also for image-to-image translation and manipulation. Our experimental\nresults validate the effectiveness of our proposed method.\n","authors":["Serin Yang","Hyunmin Hwang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08613v1","updated":"2023-03-15T13:40:16Z","published":"2023-03-15T13:40:16Z","title":"Learning to Incentivize Information Acquisition: Proper Scoring Rules\n  Meet Principal-Agent Model","summary":"  We study the incentivized information acquisition problem, where a principal\nhires an agent to gather information on her behalf. Such a problem is modeled\nas a Stackelberg game between the principal and the agent, where the principal\nannounces a scoring rule that specifies the payment, and then the agent then\nchooses an effort level that maximizes her own profit and reports the\ninformation. We study the online setting of such a problem from the principal's\nperspective, i.e., designing the optimal scoring rule by repeatedly interacting\nwith the strategic agent. We design a provably sample efficient algorithm that\ntailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a\nsublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a\ndelicate estimation procedure for the optimal profit of the principal, and a\nconservative correction scheme that ensures the desired agent's actions are\nincentivized. Furthermore, a key feature of our regret bound is that it is\nindependent of the number of states of the environment.\n","authors":["Siyu Chen","Jibang Wu","Yifan Wu","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08613v1.pdf","comment":"34 pages, Optimal information acquisition via proper scoring rule"},{"id":"http://arxiv.org/abs/2103.06484v2","updated":"2023-03-15T13:22:11Z","published":"2021-03-11T06:13:09Z","title":"Robust High-speed Running for Quadruped Robots via Deep Reinforcement\n  Learning","summary":"  Deep reinforcement learning has emerged as a popular and powerful way to\ndevelop locomotion controllers for quadruped robots. Common approaches have\nlargely focused on learning actions directly in joint space, or learning to\nmodify and offset foot positions produced by trajectory generators. Both\napproaches typically require careful reward shaping and training for millions\nof time steps, and with trajectory generators introduce human bias into the\nresulting control policies. In this paper, we present a learning framework that\nleads to the natural emergence of fast and robust bounding policies for\nquadruped robots. The agent both selects and controls actions directly in task\nspace to track desired velocity commands subject to environmental noise\nincluding model uncertainty and rough terrain. We observe that this framework\nimproves sample efficiency, necessitates little reward shaping, leads to the\nemergence of natural gaits such as galloping and bounding, and eases the\nsim-to-real transfer at running speeds. Policies can be learned in only a few\nmillion time steps, even for challenging tasks of running over rough terrain\nwith loads of over 100% of the nominal quadruped mass. Training occurs in\nPyBullet, and we perform a sim-to-sim transfer to Gazebo and sim-to-real\ntransfer to the Unitree A1 hardware. For sim-to-sim, our results show the\nquadruped is able to run at over 4 m/s without a load, and 3.5 m/s with a 10 kg\nload, which is over 83% of the nominal quadruped mass. For sim-to-real, the\nUnitree A1 is able to bound at 2 m/s with a 5 kg load, representing 42% of the\nnominal quadruped mass.\n","authors":["Guillaume Bellegarda","Yiyu Chen","Zhuochen Liu","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2103.06484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12117v3","updated":"2023-03-15T13:10:32Z","published":"2022-05-24T14:46:39Z","title":"Phased Progressive Learning with Coupling-Regulation-Imbalance Loss for\n  Imbalanced Data Classification","summary":"  Deep convolutional neural networks often perform poorly when faced with\ndatasets that suffer from quantity imbalances and classification difficulties.\nDespite advances in the field, existing two-stage approaches still exhibit\ndataset bias or domain shift. To counter this, a phased progressive learning\nschedule has been proposed that gradually shifts the emphasis from\nrepresentation learning to training the upper classifier. This approach is\nparticularly beneficial for datasets with larger imbalances or fewer samples.\nAnother new method a coupling-regulation-imbalance loss function is proposed,\nwhich combines three parts: a correction term, Focal loss, and LDAM loss. This\nloss is effective in addressing quantity imbalances and outliers, while\nregulating the focus of attention on samples with varying classification\ndifficulties. These approaches have yielded satisfactory results on several\nbenchmark datasets, including Imbalanced CIFAR10, Imbalanced CIFAR100,\nImageNet-LT, and iNaturalist 2018, and can be easily generalized to other\nimbalanced classification models.\n","authors":["Liang Xu","Yi Cheng","Fan Zhang","Bingxuan Wu","Pengfei Shao","Peng Liu","Shuwei Shen","Peng Yao","Ronald X. Xu"],"pdf_url":"https://arxiv.org/pdf/2205.12117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14355v3","updated":"2023-03-15T13:03:05Z","published":"2022-09-28T18:36:56Z","title":"Generalized Kernel Regularized Least Squares","summary":"  Kernel Regularized Least Squares (KRLS) is a popular method for flexibly\nestimating models that may have complex relationships between variables.\nHowever, its usefulness to many researchers is limited for two reasons. First,\nexisting approaches are inflexible and do not allow KRLS to be combined with\ntheoretically-motivated extensions such as random effects, unregularized fixed\neffects, or non-Gaussian outcomes. Second, estimation is extremely\ncomputationally intensive for even modestly sized datasets. Our paper addresses\nboth concerns by introducing generalized KRLS (gKRLS). We note that KRLS can be\nre-formulated as a hierarchical model thereby allowing easy inference and\nmodular model construction where KRLS can be used alongside random effects,\nsplines, and unregularized fixed effects. Computationally, we also implement\nrandom sketching to dramatically accelerate estimation while incurring a\nlimited penalty in estimation quality. We demonstrate that gKRLS can be fit on\ndatasets with tens of thousands of observations in under one minute. Further,\nstate-of-the-art techniques that require fitting the model over a dozen times\n(e.g. meta-learners) can be estimated quickly.\n","authors":["Qing Chang","Max Goplerud"],"pdf_url":"https://arxiv.org/pdf/2209.14355v3.pdf","comment":"Revised Manuscript"},{"id":"http://arxiv.org/abs/2303.08577v1","updated":"2023-03-15T12:51:16Z","published":"2023-03-15T12:51:16Z","title":"Investigating GANsformer: A Replication Study of a State-of-the-Art\n  Image Generation Model","summary":"  The field of image generation through generative modelling is abundantly\ndiscussed nowadays. It can be used for various applications, such as up-scaling\nexisting images, creating non-existing objects, such as interior design scenes,\nproducts or even human faces, and achieving transfer-learning processes. In\nthis context, Generative Adversarial Networks (GANs) are a class of widely\nstudied machine learning frameworks first appearing in the paper \"Generative\nadversarial nets\" by Goodfellow et al. that achieve the goal above. In our\nwork, we reproduce and evaluate a novel variation of the original GAN network,\nthe GANformer, proposed in \"Generative Adversarial Transformers\" by Hudson and\nZitnick. This project aimed to recreate the methods presented in this paper to\nreproduce the original results and comment on the authors' claims. Due to\nresources and time limitations, we had to constrain the network's training\ntimes, dataset types, and sizes. Our research successfully recreated both\nvariations of the proposed GANformer model and found differences between the\nauthors' and our results. Moreover, discrepancies between the publication\nmethodology and the one implemented, made available in the code, allowed us to\nstudy two undisclosed variations of the presented procedures.\n","authors":["Giorgia Adorni","Felix Boelter","Stefano Carlo Lambertenghi"],"pdf_url":"https://arxiv.org/pdf/2303.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08574v1","updated":"2023-03-15T12:50:54Z","published":"2023-03-15T12:50:54Z","title":"WikiCoder: Learning to Write Knowledge-Powered Code","summary":"  We tackle the problem of automatic generation of computer programs from a few\npairs of input-output examples. The starting point of this work is the\nobservation that in many applications a solution program must use external\nknowledge not present in the examples: we call such programs knowledge-powered\nsince they can refer to information collected from a knowledge graph such as\nWikipedia. This paper makes a first step towards knowledge-powered program\nsynthesis. We present WikiCoder, a system building upon state of the art\nmachine-learned program synthesizers and integrating knowledge graphs. We\nevaluate it to show its wide applicability over different domains and discuss\nits limitations. WikiCoder solves tasks that no program synthesizers were able\nto solve before thanks to the use of knowledge graphs, while integrating with\nrecent developments in the field to operate at scale.\n","authors":["Théo Matricon","Nathanaël Fijalkow","Gaëtan Margueritte"],"pdf_url":"https://arxiv.org/pdf/2303.08574v1.pdf","comment":"Published in the proceedings of SPIN 2023"},{"id":"http://arxiv.org/abs/2302.00422v2","updated":"2023-03-15T12:46:10Z","published":"2023-02-01T13:14:26Z","title":"Robust online active learning","summary":"  In many industrial applications, obtaining labeled observations is not\nstraightforward as it often requires the intervention of human experts or the\nuse of expensive testing equipment. In these circumstances, active learning can\nbe highly beneficial in suggesting the most informative data points to be used\nwhen fitting a model. Reducing the number of observations needed for model\ndevelopment alleviates both the computational burden required for training and\nthe operational expenses related to labeling. Online active learning, in\nparticular, is useful in high-volume production processes where the decision\nabout the acquisition of the label for a data point needs to be taken within an\nextremely short time frame. However, despite the recent efforts to develop\nonline active learning strategies, the behavior of these methods in the\npresence of outliers has not been thoroughly examined. In this work, we\ninvestigate the performance of online active linear regression in contaminated\ndata streams. Our study shows that the currently available query strategies are\nprone to sample outliers, whose inclusion in the training set eventually\ndegrades the predictive performance of the models. To address this issue, we\npropose a solution that bounds the search area of a conditional D-optimal\nalgorithm and uses a robust estimator. Our approach strikes a balance between\nexploring unseen regions of the input space and protecting against outliers.\nThrough numerical simulations, we show that the proposed method is effective in\nimproving the performance of online active learning in the presence of\noutliers, thus expanding the potential applications of this powerful tool.\n","authors":["Davide Cacciarelli","Murat Kulahci","John Sølve Tyssedal"],"pdf_url":"https://arxiv.org/pdf/2302.00422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04756v2","updated":"2023-03-15T12:45:36Z","published":"2023-03-08T17:45:48Z","title":"Meta-learning Control Variates: Variance Reduction with Limited Data","summary":"  Control variates can be a powerful tool to reduce the variance of Monte Carlo\nestimators, but constructing effective control variates can be challenging when\nthe number of samples is small. In this paper, we show that when a large number\nof related integrals need to be computed, it is possible to leverage the\nsimilarity between these integration tasks to improve performance even when the\nnumber of samples per task is very small. Our approach, called meta learning\nCVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our\nempirical assessment indicates that Meta-CVs can lead to significant variance\nreduction in such settings, and our theoretical analysis establishes general\nconditions under which Meta-CVs can be successfully trained.\n","authors":["Zhuo Sun","Chris J. Oates","François-Xavier Briol"],"pdf_url":"https://arxiv.org/pdf/2303.04756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.13067v2","updated":"2023-03-15T12:43:43Z","published":"2022-12-26T09:45:41Z","title":"Online Active Learning for Soft Sensor Development using Semi-Supervised\n  Autoencoders","summary":"  Data-driven soft sensors are extensively used in industrial and chemical\nprocesses to predict hard-to-measure process variables whose real value is\ndifficult to track during routine operations. The regression models used by\nthese sensors often require a large number of labeled examples, yet obtaining\nthe label information can be very expensive given the high time and cost\nrequired by quality inspections. In this context, active learning methods can\nbe highly beneficial as they can suggest the most informative labels to query.\nHowever, most of the active learning strategies proposed for regression focus\non the offline setting. In this work, we adapt some of these approaches to the\nstream-based scenario and show how they can be used to select the most\ninformative data points. We also demonstrate how to use a semi-supervised\narchitecture based on orthogonal autoencoders to learn salient features in a\nlower dimensional space. The Tennessee Eastman Process is used to compare the\npredictive performance of the proposed approaches.\n","authors":["Davide Cacciarelli","Murat Kulahci","John Tyssedal"],"pdf_url":"https://arxiv.org/pdf/2212.13067v2.pdf","comment":"ICML 2022 Workshop on Adaptive Experimental Design and Active\n  Learning in the Real World"},{"id":"http://arxiv.org/abs/2303.07900v2","updated":"2023-03-15T12:37:15Z","published":"2023-03-14T13:41:28Z","title":"Generalised Scale-Space Properties for Probabilistic Diffusion Models","summary":"  Probabilistic diffusion models enjoy increasing popularity in the deep\nlearning community. They generate convincing samples from a learned\ndistribution of input images with a wide field of practical applications.\nOriginally, these approaches were motivated from drift-diffusion processes, but\nthese origins find less attention in recent, practice-oriented publications.\n  We investigate probabilistic diffusion models from the viewpoint of\nscale-space research and show that they fulfil generalised scale-space\nproperties on evolving probability distributions. Moreover, we discuss\nsimilarities and differences between interpretations of the physical core\nconcept of drift-diffusion in the deep learning and model-based world. To this\nend, we examine relations of probabilistic diffusion to osmosis filters.\n","authors":["Pascal Peter"],"pdf_url":"https://arxiv.org/pdf/2303.07900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08566v1","updated":"2023-03-15T12:34:24Z","published":"2023-03-15T12:34:24Z","title":"Sensitivity-Aware Visual Parameter-Efficient Tuning","summary":"  Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative\nfor full fine-tuning so as to adapt pre-trained vision models to downstream\ntasks, which only tunes a small number of parameters while freezing the vast\nmajority ones to ease storage burden and optimization difficulty. However,\nexisting VPET methods introduce trainable parameters to the same positions\nacross different tasks depending solely on human heuristics and neglect the\ndomain gaps. To this end, we study where to introduce and how to allocate\ntrainable parameters by proposing a novel Sensitivity-aware visual\nParameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable\nparameters to task-specific important positions given a desired tunable\nparameter budget. Specifically, our SPT first quickly identifies the sensitive\nparameters that require tuning for a given task in a data-dependent way. Next,\nour SPT further boosts the representational capability for the weight matrices\nwhose number of sensitive parameters exceeds a pre-defined threshold by\nutilizing any of the existing structured tuning methods, e.g., LoRA or Adapter,\nto replace directly tuning the selected sensitive parameters (unstructured\ntuning) under the budget. Extensive experiments on a wide range of downstream\nrecognition tasks show that our SPT is complementary to the existing VPET\nmethods and largely boosts their performance, e.g., SPT improves Adapter with\nsupervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean Top-1 accuracy,\nreaching SOTA performance on FGVC and VTAB-1k benchmarks, respectively. Source\ncode is at https://github.com/ziplab/SPT\n","authors":["Haoyu He","Jianfei Cai","Jing Zhang","Dacheng Tao","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.08566v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2107.12003v3","updated":"2023-03-15T12:28:22Z","published":"2021-07-26T07:36:02Z","title":"Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal\n  Latent Representations","summary":"  In this paper, we propose a multi-speaker face-to-speech waveform generation\nmodel that also works for unseen speaker conditions. Using a generative\nadversarial network (GAN) with linguistic and speaker characteristic features\nas auxiliary conditions, our method directly converts face images into speech\nwaveforms under an end-to-end training framework. The linguistic features are\nextracted from lip movements using a lip-reading model, and the speaker\ncharacteristic features are predicted from face images using cross-modal\nlearning with a pre-trained acoustic model. Since these two features are\nuncorrelated and controlled independently, we can flexibly synthesize speech\nwaveforms whose speaker characteristics vary depending on the input face\nimages. We show the superiority of our proposed model over conventional methods\nin terms of objective and subjective evaluation results. Specifically, we\nevaluate the performances of linguistic features by measuring their accuracy on\nan automatic speech recognition task. In addition, we estimate speaker and\ngender similarity for multi-speaker and unseen conditions, respectively. We\nalso evaluate the aturalness of the synthesized speech waveforms using a mean\nopinion score (MOS) test and non-intrusive objective speech quality assessment\n(NISQA).The demo samples of the proposed and other models are available at\nhttps://sam-0927.github.io/\n","authors":["Se-Yun Um","Jihyun Kim","Jihyun Lee","Hong-Goo Kang"],"pdf_url":"https://arxiv.org/pdf/2107.12003v3.pdf","comment":"5 pages (including references), 1 figure"},{"id":"http://arxiv.org/abs/2008.06302v2","updated":"2023-03-15T12:22:53Z","published":"2020-08-14T11:48:13Z","title":"Computation Offloading in Heterogeneous Vehicular Edge Networks: On-line\n  and Off-policy Bandit Solutions","summary":"  With the rapid advancement of Intelligent Transportation Systems (ITS) and\nvehicular communications, Vehicular Edge Computing (VEC) is emerging as a\npromising technology to support low-latency ITS applications and services. In\nthis paper, we consider the computation offloading problem from mobile\nvehicles/users in a heterogeneous VEC scenario, and focus on the network- and\nbase station selection problems, where different networks have different\ntraffic loads. In a fast-varying vehicular environment, computation offloading\nexperience of users is strongly affected by the latency due to the congestion\nat the edge computing servers co-located with the base stations. However, as a\nresult of the non-stationary property of such an environment and also\ninformation shortage, predicting this congestion is an involved task. To\naddress this challenge, we propose an on-line learning algorithm and an\noff-policy learning algorithm based on multi-armed bandit theory. To\ndynamically select the least congested network in a piece-wise stationary\nenvironment, these algorithms predict the latency that the offloaded tasks\nexperience using the offloading history. In addition, to minimize the task loss\ndue to the mobility of the vehicles, we develop a method for base station\nselection. Moreover, we propose a relaying mechanism for the selected network,\nwhich operates based on the sojourn time of the vehicles. Through intensive\nnumerical analysis, we demonstrate that the proposed learning-based solutions\nadapt to the traffic changes of the network by selecting the least congested\nnetwork, thereby reducing the latency of offloaded tasks. Moreover, we\ndemonstrate that the proposed joint base station selection and the relaying\nmechanism minimize the task loss in a vehicular environment.\n","authors":["Arash Bozorgchenani","Setareh Maghsudi","Daniele Tarchi","Ekram Hossain"],"pdf_url":"https://arxiv.org/pdf/2008.06302v2.pdf","comment":"Published in IEEE Transactions on Mobile Computing, Vol 21, Issue 12,\n  Dec 2022"},{"id":"http://arxiv.org/abs/2210.02226v3","updated":"2023-03-15T12:17:54Z","published":"2022-10-05T13:03:55Z","title":"Null Hypothesis Test for Anomaly Detection","summary":"  We extend the use of Classification Without Labels for anomaly detection with\na hypothesis test designed to exclude the background-only hypothesis. By\ntesting for statistical independence of the two discriminating dataset regions,\nwe are able to exclude the background-only hypothesis without relying on fixed\nanomaly score cuts or extrapolations of background estimates between regions.\nThe method relies on the assumption of conditional independence of anomaly\nscore features and dataset regions, which can be ensured using existing\ndecorrelation techniques. As a benchmark example, we consider the LHC Olympics\ndataset where we show that mutual information represents a suitable test for\nstatistical independence and our method exhibits excellent and robust\nperformance at different signal fractions even in presence of realistic feature\ncorrelations.\n","authors":["Jernej F. Kamenik","Manuel Szewc"],"pdf_url":"https://arxiv.org/pdf/2210.02226v3.pdf","comment":"10 pages, 3 figures, 1 Table. Matches published version at Physics\n  Letters B. All code is available at\n  https://github.com/ManuelSzewc/Null_Hypothesis_Test_for_Anomaly_Detection.\n  Comments welcome!"},{"id":"http://arxiv.org/abs/2303.08552v1","updated":"2023-03-15T12:12:13Z","published":"2023-03-15T12:12:13Z","title":"Joint Graph and Vertex Importance Learning","summary":"  In this paper, we explore the topic of graph learning from the perspective of\nthe Irregularity-Aware Graph Fourier Transform, with the goal of learning the\ngraph signal space inner product to better model data. We propose a novel\nmethod to learn a graph with smaller edge weight upper bounds compared to\ncombinatorial Laplacian approaches. Experimentally, our approach yields much\nsparser graphs compared to a combinatorial Laplacian approach, with a more\ninterpretable model.\n","authors":["Benjamin Girault","Eduardo Pavez","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2303.08552v1.pdf","comment":"submitted to 2023 31st European Signal Processing Conference\n  (EUSIPCO)"},{"id":"http://arxiv.org/abs/2303.04743v2","updated":"2023-03-15T11:50:41Z","published":"2023-03-08T17:27:39Z","title":"Vector Quantized Time Series Generation with a Bidirectional Prior Model","summary":"  Time series generation (TSG) studies have mainly focused on the use of\nGenerative Adversarial Networks (GANs) combined with recurrent neural network\n(RNN) variants. However, the fundamental limitations and challenges of training\nGANs still remain. In addition, the RNN-family typically has difficulties with\ntemporal consistency between distant timesteps. Motivated by the successes in\nthe image generation (IMG) domain, we propose TimeVQVAE, the first work, to our\nknowledge, that uses vector quantization (VQ) techniques to address the TSG\nproblem. Moreover, the priors of the discrete latent spaces are learned with\nbidirectional transformer models that can better capture global temporal\nconsistency. We also propose VQ modeling in a time-frequency domain, separated\ninto low-frequency (LF) and high-frequency (HF). This allows us to retain\nimportant characteristics of the time series and, in turn, generate new\nsynthetic signals that are of better quality, with sharper changes in\nmodularity, than its competing TSG methods. Our experimental evaluation is\nconducted on all datasets from the UCR archive, using well-established metrics\nin the IMG literature, such as Fr\\'echet inception distance and inception\nscores. Our implementation on GitHub:\n\\url{https://github.com/ML4ITS/TimeVQVAE}.\n","authors":["Daesoo Lee","Sara Malacarne","Erlend Aune"],"pdf_url":"https://arxiv.org/pdf/2303.04743v2.pdf","comment":"accepted at AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.08541v1","updated":"2023-03-15T11:36:27Z","published":"2023-03-15T11:36:27Z","title":"Adapting U-Net for linear elastic stress estimation in polycrystal Zr\n  microstructures","summary":"  A variant of the U-Net convolutional neural network architecture is proposed\nto estimate linear elastic compatibility stresses in a-Zr (hcp) polycrystalline\ngrain structures. Training data was generated using VGrain software with a\nregularity alpha of 0.73 and uniform random orientation for the grain\nstructures and ABAQUS to evaluate the stress welds using the finite element\nmethod. The initial dataset contains 200 samples with 20 held from training for\nvalidation. The network gives speedups of around 200x to 6000x using a CPU or\nGPU, with signifcant memory savings, compared to finite element analysis with a\nmodest reduction in accuracy of up to 10%. Network performance is not\ncorrelated with grain structure regularity or texture, showing generalisation\nof the network beyond the training set to arbitrary Zr crystal structures.\nPerformance when trained with 200 and 400 samples was measured, finding an\nimprovement in accuracy of approximately 10% when the size of the dataset was\ndoubled.\n","authors":["J. D. Langcaster","D. S. Balint","M. R. Wenman"],"pdf_url":"https://arxiv.org/pdf/2303.08541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08538v1","updated":"2023-03-15T11:34:06Z","published":"2023-03-15T11:34:06Z","title":"Health Monitoring of Movement Disorder Subject based on Diamond Stacked\n  Sparse Autoencoder Ensemble Model","summary":"  The health monitoring of chronic diseases is very important for people with\nmovement disorders because of their limited mobility and long duration of\nchronic diseases. Machine learning-based processing of data collected from the\nhuman with movement disorders using wearable sensors is an effective method\ncurrently available for health monitoring. However, wearable sensor systems are\ndifficult to obtain high-quality and large amounts of data, which cannot meet\nthe requirement for diagnostic accuracy. Moreover, existing machine learning\nmethods do not handle this problem well. Feature learning is key to machine\nlearning. To solve this problem, a health monitoring of movement disorder\nsubject based on diamond stacked sparse autoencoder ensemble model (DsaeEM) is\nproposed in this paper. This algorithm has two major components. First, feature\nexpansion is designed using feature-embedded stacked sparse autoencoder\n(FSSAE). Second, a feature reduction mechanism is designed to remove the\nredundancy among the expanded features. This mechanism includes L1 regularized\nfeature-reduction algorithm and the improved manifold dimensionality reduction\nalgorithm. This paper refers to the combined feature expansion and feature\nreduction mechanism as the diamond-like feature learning mechanism. The method\nis experimentally verified with several state of art algorithms and on two\ndatasets. The results show that the proposed algorithm has higher accuracy\napparently. In conclusion, this study developed an effective and feasible\nfeature-learning algorithm for the recognition of chronic diseases.\n","authors":["Likun Tang","Jie Ma","Yongming Li"],"pdf_url":"https://arxiv.org/pdf/2303.08538v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08536v1","updated":"2023-03-15T11:29:36Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v1.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2303.08535v1","updated":"2023-03-15T11:29:02Z","published":"2023-03-15T11:29:02Z","title":"Singular relaxation of a random walk in a box with a Metropolis Monte\n  Carlo dynamics","summary":"  We study analytically the relaxation eigenmodes of a simple Monte Carlo\nalgorithm, corresponding to a particle in a box which moves by uniform random\njumps. Moves outside of the box are rejected. At long times, the system\napproaches the equilibrium probability density, which is uniform inside the\nbox. We show that the relaxation towards this equilibrium is unusual: for a\njump length comparable to the size of the box, the number of relaxation\neigenmodes can be surprisingly small, one or two. We provide a complete\nanalytic description of the transition between these two regimes. When only a\nsingle relaxation eigenmode is present, a suitable choice of the symmetry of\nthe initial conditions gives a localizing decay to equilibrium. In this case,\nthe deviation from equilibrium concentrates at the edges of the box where the\nrejection probability is maximal. Finally, in addition to the relaxation\nanalysis of the master equation, we also describe the full eigen-spectrum of\nthe master equation including its sub-leading eigen-modes.\n","authors":["Alexei D. Chepelianskii","Satya N. Majumdar","Hendrik Schawe","Emmanuel Trizac"],"pdf_url":"https://arxiv.org/pdf/2303.08535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00335v2","updated":"2023-03-15T11:24:12Z","published":"2022-11-01T09:01:44Z","title":"Recurrent Neural Networks and Universal Approximation of Bayesian\n  Filters","summary":"  We consider the Bayesian optimal filtering problem: i.e. estimating some\nconditional statistics of a latent time-series signal from an observation\nsequence. Classical approaches often rely on the use of assumed or estimated\ntransition and observation models. Instead, we formulate a generic recurrent\nneural network framework and seek to learn directly a recursive mapping from\nobservational inputs to the desired estimator statistics. The main focus of\nthis article is the approximation capabilities of this framework. We provide\napproximation error bounds for filtering in general non-compact domains. We\nalso consider strong time-uniform approximation error bounds that guarantee\ngood long-time performance. We discuss and illustrate a number of practical\nconcerns and implications of these results.\n","authors":["Adrian N. Bishop","Edwin V. Bonilla"],"pdf_url":"https://arxiv.org/pdf/2211.00335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14307v3","updated":"2023-03-15T11:15:41Z","published":"2023-02-28T04:45:31Z","title":"GradMA: A Gradient-Memory-based Accelerated Federated Learning with\n  Alleviated Catastrophic Forgetting","summary":"  Federated Learning (FL) has emerged as a de facto machine learning area and\nreceived rapid increasing research interests from the community. However,\ncatastrophic forgetting caused by data heterogeneity and partial participation\nposes distinctive challenges for FL, which are detrimental to the performance.\nTo tackle the problems, we propose a new FL approach (namely GradMA), which\ntakes inspiration from continual learning to simultaneously correct the\nserver-side and worker-side update directions as well as take full advantage of\nserver's rich computing and memory resources. Furthermore, we elaborate a\nmemory reduction strategy to enable GradMA to accommodate FL with a large scale\nof workers. We then analyze convergence of GradMA theoretically under the\nsmooth non-convex setting and show that its convergence rate achieves a linear\nspeed up w.r.t the increasing number of sampled active workers. At last, our\nextensive experiments on various image classification tasks show that GradMA\nachieves significant performance gains in accuracy and communication efficiency\ncompared to SOTA baselines.\n","authors":["Kangyang Luo","Xiang Li","Yunshi Lan","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2302.14307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04284v2","updated":"2023-03-15T10:48:36Z","published":"2022-11-08T14:46:03Z","title":"Efficient Compressed Ratio Estimation using Online Sequential Learning\n  for Edge Computing","summary":"  Owing to the widespread adoption of the Internet of Things, a vast amount of\nsensor information is being acquired in real time. Accordingly, the\ncommunication cost of data from edge devices is increasing. Compressed sensing\n(CS), a data compression method that can be used on edge devices, has been\nattracting attention as a method to reduce communication costs. In CS,\nestimating the appropriate compression ratio is important. There is a method to\nadaptively estimate the compression ratio for the acquired data using\nreinforcement learning (RL). However, the computational costs associated with\nexisting RL methods that can be utilized on edges are often high. In this\nstudy, we developed an efficient RL method for edge devices, referred to as the\nactor--critic online sequential extreme learning machine (AC-OSELM), and a\nsystem to compress data by estimating an appropriate compression ratio on the\nedge using AC-OSELM. The performance of the proposed method in estimating the\ncompression ratio is evaluated by comparing it with other RL methods for edge\ndevices. The experimental results indicate that AC-OSELM demonstrated the same\nor better compression performance and faster compression ratio estimation than\nthe existing methods.\n","authors":["Hiroki Oikawa","Hangli Ge","Noboru Koshizuka"],"pdf_url":"https://arxiv.org/pdf/2211.04284v2.pdf","comment":"7 pages, 7 figures, Submitted to IEEE PIMRC 2023"},{"id":"http://arxiv.org/abs/2303.08516v1","updated":"2023-03-15T10:47:48Z","published":"2023-03-15T10:47:48Z","title":"Fair Off-Policy Learning from Observational Data","summary":"  Businesses and organizations must ensure that their algorithmic\ndecision-making is fair in order to meet legislative, ethical, and societal\ndemands. For example, decision-making in automated hiring must not discriminate\nwith respect to gender or race. To achieve this, prior research has contributed\napproaches that ensure algorithmic fairness in machine learning predictions,\nwhile comparatively little effort has focused on algorithmic fairness in\ndecision models, specifically off-policy learning. In this paper, we propose a\nnovel framework for fair off-policy learning: we learn decision rules from\nobservational data under different notions of fairness, where we explicitly\nassume that observational data were collected under a different -- potentially\nbiased -- behavioral policy. For this, we first formalize different fairness\nnotions for off-policy learning. We then propose a machine learning approach to\nlearn optimal policies under these fairness notions. Specifically, we\nreformulate the fairness notions into unconstrained learning objectives that\ncan be estimated from finite samples. Here, we leverage machine learning to\nminimize the objective constrained on a fair representation of the data, so\nthat the resulting policies satisfy our fairness notions. We further provide\ntheoretical guarantees in form of generalization bounds for the finite-sample\nversion of our framework. We demonstrate the effectiveness of our framework\nthrough extensive numerical experiments using both simulated and real-world\ndata. As a result, our work enables algorithmic decision-making in a wide array\nof practical applications where fairness must ensured.\n","authors":["Dennis Frauen","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2303.08516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08500v1","updated":"2023-03-15T10:20:49Z","published":"2023-03-15T10:20:49Z","title":"The Devil's Advocate: Shattering the Illusion of Unexploitable Data\n  using Diffusion Models","summary":"  Protecting personal data against the exploitation of machine learning models\nis of paramount importance. Recently, availability attacks have shown great\npromise to provide an extra layer of protection against the unauthorized use of\ndata to train neural networks. These methods aim to add imperceptible noise to\nclean data so that the neural networks cannot extract meaningful patterns from\nthe protected data, claiming that they can make personal data \"unexploitable.\"\nIn this paper, we provide a strong countermeasure against such approaches,\nshowing that unexploitable data might only be an illusion. In particular, we\nleverage the power of diffusion models and show that a carefully designed\ndenoising process can defuse the ramifications of the data-protecting\nperturbations. We rigorously analyze our algorithm, and theoretically prove\nthat the amount of required denoising is directly related to the magnitude of\nthe data-protecting perturbations. Our approach, called AVATAR, delivers\nstate-of-the-art performance against a suite of recent availability attacks in\nvarious scenarios, outperforming adversarial training. Our findings call for\nmore research into making personal data unexploitable, showing that this goal\nis far from over.\n","authors":["Hadi M. Dolatabadi","Sarah Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2303.08500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.09213v4","updated":"2023-03-15T09:33:01Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v4.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.08473v1","updated":"2023-03-15T09:26:29Z","published":"2023-03-15T09:26:29Z","title":"Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs","summary":"  Image synthesis driven by computer graphics achieved recently a remarkable\nrealism, yet synthetic image data generated this way reveals a significant\ndomain gap with respect to real-world data. This is especially true in\nautonomous driving scenarios, which represent a critical aspect for overcoming\nutilizing synthetic data for training neural networks. We propose a method\nbased on domain-invariant scene representation to directly synthesize traffic\nscene imagery without rendering. Specifically, we rely on synthetic scene\ngraphs as our internal representation and introduce an unsupervised neural\nnetwork architecture for realistic traffic scene synthesis. We enhance\nsynthetic scene graphs with spatial information about the scene and demonstrate\nthe effectiveness of our approach through scene manipulation.\n","authors":["Artem Savkin","Rachid Ellouze","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2303.08473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01178v3","updated":"2023-03-15T09:25:41Z","published":"2022-03-02T15:25:27Z","title":"DCT-Former: Efficient Self-Attention with Discrete Cosine Transform","summary":"  Since their introduction the Trasformer architectures emerged as the\ndominating architectures for both natural language processing and, more\nrecently, computer vision applications. An intrinsic limitation of this family\nof \"fully-attentive\" architectures arises from the computation of the\ndot-product attention, which grows both in memory consumption and number of\noperations as $O(n^2)$ where $n$ stands for the input sequence length, thus\nlimiting the applications that require modeling very long sequences. Several\napproaches have been proposed so far in the literature to mitigate this issue,\nwith varying degrees of success. Our idea takes inspiration from the world of\nlossy data compression (such as the JPEG algorithm) to derive an approximation\nof the attention module by leveraging the properties of the Discrete Cosine\nTransform. An extensive section of experiments shows that our method takes up\nless memory for the same performance, while also drastically reducing inference\ntime. This makes it particularly suitable in real-time contexts on embedded\nplatforms. Moreover, we assume that the results of our research might serve as\na starting point for a broader family of deep neural models with reduced memory\nfootprint. The implementation will be made publicly available at\nhttps://github.com/cscribano/DCT-Former-Public\n","authors":["Carmelo Scribano","Giorgia Franchini","Marco Prato","Marko Bertogna"],"pdf_url":"https://arxiv.org/pdf/2203.01178v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09992v3","updated":"2023-03-15T09:24:44Z","published":"2021-12-18T20:14:11Z","title":"Weisfeiler and Leman go Machine Learning: The Story so far","summary":"  In recent years, algorithms and neural architectures based on the\nWeisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism\nproblem, have emerged as a powerful tool for machine learning with graphs and\nrelational data. Here, we give a comprehensive overview of the algorithm's use\nin a machine-learning setting, focusing on the supervised regime. We discuss\nthe theoretical background, show how to use it for supervised graph and node\nrepresentation learning, discuss recent extensions, and outline the algorithm's\nconnection to (permutation-)equivariant neural architectures. Moreover, we give\nan overview of current applications and future directions to stimulate further\nresearch.\n","authors":["Christopher Morris","Yaron Lipman","Haggai Maron","Bastian Rieck","Nils M. Kriege","Martin Grohe","Matthias Fey","Karsten Borgwardt"],"pdf_url":"https://arxiv.org/pdf/2112.09992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08459v1","updated":"2023-03-15T09:03:58Z","published":"2023-03-15T09:03:58Z","title":"Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic\n  Systems using Recurrent Neural Networks","summary":"  Accurate intra-day forecasts of the power output by PhotoVoltaic (PV) systems\nare critical to improve the operation of energy distribution grids. We describe\na hybrid-physical model, which aims at improving deterministic intra-day\nforecasts, issued by a PV performance model fed by Numerical Weather\nPredictions (NWP), by using them as covariates in the context of an\nautoregressive recurrent neural model. Our proposal repurposes a neural model\ninitially used in the retail sector, and discloses a novel truncated Gaussian\noutput distribution. We experimentally compare many model variants to\nalternatives from the literature, and an ablation study shows that the\ncomponents in the best performing variant work synergistically to reach a skill\nscore of 7.54% with respect to the NWP-driven PV performance model baseline.\n","authors":["Pierrick Bruneau","David Fiorelli","Christian Braun","Daniel Koster"],"pdf_url":"https://arxiv.org/pdf/2303.08459v1.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.08455v1","updated":"2023-03-15T08:59:03Z","published":"2023-03-15T08:59:03Z","title":"On the uncertainty analysis of the data-enabled physics-informed neural\n  network for solving neutron diffusion eigenvalue problem","summary":"  In practical engineering experiments, the data obtained through detectors are\ninevitably noisy. For the already proposed data-enabled physics-informed neural\nnetwork (DEPINN) \\citep{DEPINN}, we investigate the performance of DEPINN in\ncalculating the neutron diffusion eigenvalue problem from several perspectives\nwhen the prior data contain different scales of noise. Further, in order to\nreduce the effect of noise and improve the utilization of the noisy prior data,\nwe propose innovative interval loss functions and give some rigorous\nmathematical proofs. The robustness of DEPINN is examined on two typical\nbenchmark problems through a large number of numerical results, and the\neffectiveness of the proposed interval loss function is demonstrated by\ncomparison. This paper confirms the feasibility of the improved DEPINN for\npractical engineering applications in nuclear reactor physics.\n","authors":["Yu Yang","Helin Gong","Qihong Yang","Yangtao Deng","Qiaolin He","Shiquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08455v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2208.13483"},{"id":"http://arxiv.org/abs/2303.08452v1","updated":"2023-03-15T08:54:20Z","published":"2023-03-15T08:54:20Z","title":"Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly\n  Detection","summary":"  Early and accurate disease detection is crucial for patient management and\nsuccessful treatment outcomes. However, the automatic identification of\nanomalies in medical images can be challenging. Conventional methods rely on\nlarge labeled datasets which are difficult to obtain. To overcome these\nlimitations, we introduce a novel unsupervised approach, called PHANES (Pseudo\nHealthy generative networks for ANomaly Segmentation). Our method has the\ncapability of reversing anomalies, i.e., preserving healthy tissue and\nreplacing anomalous regions with pseudo-healthy (PH) reconstructions. Unlike\nrecent diffusion models, our method does not rely on a learned noise\ndistribution nor does it introduce random alterations to the entire image.\nInstead, we use latent generative networks to create masks around possible\nanomalies, which are refined using inpainting generative networks. We\ndemonstrate the effectiveness of PHANES in detecting stroke lesions in T1w\nbrain MRI datasets and show significant improvements over state-of-the-art\n(SOTA) methods. We believe that our proposed framework will open new avenues\nfor interpretable, fast, and accurate anomaly segmentation with the potential\nto support various clinical-oriented downstream tasks.\n","authors":["Cosmin I Bercea","Benedikt Wiestler","Daniel Rueckert","Julia A Schnabel"],"pdf_url":"https://arxiv.org/pdf/2303.08452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08448v1","updated":"2023-03-15T08:44:07Z","published":"2023-03-15T08:44:07Z","title":"A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP\n  Algorithms on Electronic Health Records","summary":"  Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.\n","authors":["Sicheng Zhou","Nan Wang","Liwei Wang","Ju Sun","Anne Blaes","Hongfang Liu","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08448v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.08447v1","updated":"2023-03-15T08:42:48Z","published":"2023-03-15T08:42:48Z","title":"MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids","summary":"  Integrating variable renewable energy into the grid has posed challenges to\nsystem operators in achieving optimal trade-offs among energy availability,\ncost affordability, and pollution controllability. This paper proposes a\nmulti-agent reinforcement learning framework for managing energy transactions\nin microgrids. The framework addresses the challenges above: it seeks to\noptimize the usage of available resources by minimizing the carbon footprint\nwhile benefiting all stakeholders. The proposed architecture consists of three\nlayers of agents, each pursuing different objectives. The first layer,\ncomprised of prosumers and consumers, minimizes the total energy cost. The\nother two layers control the energy price to decrease the carbon impact while\nbalancing the consumption and production of both renewable and conventional\nenergy. This framework also takes into account fluctuations in energy demand\nand supply.\n","authors":["Nicolas Cuadrado","Roberto Gutierrez","Yongli Zhu","Martin Takac"],"pdf_url":"https://arxiv.org/pdf/2303.08447v1.pdf","comment":"ICLR 2023 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2303.08440v1","updated":"2023-03-15T08:28:06Z","published":"2023-03-15T08:28:06Z","title":"Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models","summary":"  Diffusion models have become a popular approach for image generation and\nreconstruction due to their numerous advantages. However, most diffusion-based\ninverse problem-solving methods only deal with 2D images, and even recently\npublished 3D methods do not fully exploit the 3D distribution prior. To address\nthis, we propose a novel approach using two perpendicular pre-trained 2D\ndiffusion models to solve the 3D inverse problem. By modeling the 3D data\ndistribution as a product of 2D distributions sliced in different directions,\nour method effectively addresses the curse of dimensionality. Our experimental\nresults demonstrate that our method is highly effective for 3D medical image\nreconstruction tasks, including MRI Z-axis super-resolution, compressed sensing\nMRI, and sparse-view CT. Our method can generate high-quality voxel volumes\nsuitable for medical applications.\n","authors":["Suhyeon Lee","Hyungjin Chung","Minyoung Park","Jonghyuk Park","Wi-Sun Ryu","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2303.08440v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2210.07241v2","updated":"2023-03-15T08:21:03Z","published":"2022-10-13T17:59:55Z","title":"Visual Reinforcement Learning with Self-Supervised 3D Representations","summary":"  A prominent approach to visual Reinforcement Learning (RL) is to learn an\ninternal state representation using self-supervised methods, which has the\npotential benefit of improved sample-efficiency and generalization through\nadditional learning signal and inductive biases. However, while the real world\nis inherently 3D, prior efforts have largely been focused on leveraging 2D\ncomputer vision techniques as auxiliary self-supervision. In this work, we\npresent a unified framework for self-supervised learning of 3D representations\nfor motor control. Our proposed framework consists of two phases: a pretraining\nphase where a deep voxel-based 3D autoencoder is pretrained on a large\nobject-centric dataset, and a finetuning phase where the representation is\njointly finetuned together with RL on in-domain data. We empirically show that\nour method enjoys improved sample efficiency in simulated manipulation tasks\ncompared to 2D representation learning methods. Additionally, our learned\npolicies transfer zero-shot to a real robot setup with only approximate\ngeometric correspondence, and successfully solve motor control tasks that\ninvolve grasping and lifting from a single, uncalibrated RGB camera. Code and\nvideos are available at https://yanjieze.com/3d4rl/ .\n","authors":["Yanjie Ze","Nicklas Hansen","Yinbo Chen","Mohit Jain","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2210.07241v2.pdf","comment":"Accepted in RA-L 2023 and IROS 2023. Project page:\n  https://yanjieze.com/3d4rl/"},{"id":"http://arxiv.org/abs/2303.08435v1","updated":"2023-03-15T08:17:07Z","published":"2023-03-15T08:17:07Z","title":"Physics-Informed Optical Kernel Regression Using Complex-valued Neural\n  Fields","summary":"  Lithography is fundamental to integrated circuit fabrication, necessitating\nlarge computation overhead. The advancement of machine learning (ML)-based\nlithography models alleviates the trade-offs between manufacturing process\nexpense and capability. However, all previous methods regard the lithography\nsystem as an image-to-image black box mapping, utilizing network parameters to\nlearn by rote mappings from massive mask-to-aerial or mask-to-resist image\npairs, resulting in poor generalization capability. In this paper, we propose a\nnew ML-based paradigm disassembling the rigorous lithographic model into\nnon-parametric mask operations and learned optical kernels containing\ndeterminant source, pupil, and lithography information. By optimizing\ncomplex-valued neural fields to perform optical kernel regression from\ncoordinates, our method can accurately restore lithography system using a\nsmall-scale training dataset with fewer parameters, demonstrating superior\ngeneralization capability as well. Experiments show that our framework can use\n31\\% of parameters while achieving 69$\\times$ smaller mean squared error with\n1.3$\\times$ higher throughput than the state-of-the-art.\n","authors":["Guojin Chen","Zehua Pei","Haoyu Yang","Yuzhe Ma","Bei Yu","Martin D. F. Wong"],"pdf_url":"https://arxiv.org/pdf/2303.08435v1.pdf","comment":"Accepted by DAC23"},{"id":"http://arxiv.org/abs/2303.08434v1","updated":"2023-03-15T08:12:28Z","published":"2023-03-15T08:12:28Z","title":"DeDA: Deep Directed Accumulator","summary":"  Chronic active multiple sclerosis lesions, also termed as rim+ lesions, can\nbe characterized by a hyperintense rim at the edge of the lesion on\nquantitative susceptibility maps. These rim+ lesions exhibit a geometrically\nsimple structure, where gradients at the lesion edge are radially oriented and\na greater magnitude of gradients is observed in contrast to rim- (non rim+)\nlesions. However, recent studies have shown that the identification performance\nof such lesions remains unsatisfied due to the limited amount of data and high\nclass imbalance. In this paper, we propose a simple yet effective image\nprocessing operation, deep directed accumulator (DeDA), that provides a new\nperspective for injecting domain-specific inductive biases (priors) into neural\nnetworks for rim+ lesion identification. Given a feature map and a set of\nsampling grids, DeDA creates and quantizes an accumulator space into finite\nintervals, and accumulates feature values accordingly. This DeDA operation is a\ngeneralized discrete Radon transform and can also be regarded as a symmetric\noperation to the grid sampling within the forward-backward neural network\nframework, the process of which is order-agnostic, and can be efficiently\nimplemented with the native CUDA programming. Experimental results on a dataset\nwith 177 rim+ and 3986 rim- lesions show that 10.1% of improvement in a partial\n(false positive rate<0.1) area under the receiver operating characteristic\ncurve (pROC AUC) and 10.2% of improvement in an area under the precision recall\ncurve (PR AUC) can be achieved respectively comparing to other state-of-the-art\nmethods. The source code is available online at\nhttps://github.com/tinymilky/DeDA\n","authors":["Hang Zhang","Rongguang Wang","Renjiu Hu","Jinwei Zhang","Jiahao Li"],"pdf_url":"https://arxiv.org/pdf/2303.08434v1.pdf","comment":"18 pages, 3 Tables and 4 figures"},{"id":"http://arxiv.org/abs/2303.08433v1","updated":"2023-03-15T08:11:47Z","published":"2023-03-15T08:11:47Z","title":"The Benefits of Mixup for Feature Learning","summary":"  Mixup, a simple data augmentation method that randomly mixes two data points\nvia linear interpolation, has been extensively applied in various deep learning\napplications to gain better generalization. However, the theoretical\nunderpinnings of its efficacy are not yet fully understood. In this paper, we\naim to seek a fundamental understanding of the benefits of Mixup. We first show\nthat Mixup using different linear interpolation parameters for features and\nlabels can still achieve similar performance to the standard Mixup. This\nindicates that the intuitive linearity explanation in Zhang et al., (2018) may\nnot fully explain the success of Mixup. Then we perform a theoretical study of\nMixup from the feature learning perspective. We consider a feature-noise data\nmodel and show that Mixup training can effectively learn the rare features\n(appearing in a small fraction of data) from its mixture with the common\nfeatures (appearing in a large fraction of data). In contrast, standard\ntraining can only learn the common features but fails to learn the rare\nfeatures, thus suffering from bad generalization performance. Moreover, our\ntheoretical analysis also shows that the benefits of Mixup for feature learning\nare mostly gained in the early training phase, based on which we propose to\napply early stopping in Mixup. Experimental results verify our theoretical\nfindings and demonstrate the effectiveness of the early-stopped Mixup training.\n","authors":["Difan Zou","Yuan Cao","Yuanzhi Li","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2303.08433v1.pdf","comment":"72 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.08431v1","updated":"2023-03-15T08:08:02Z","published":"2023-03-15T08:08:02Z","title":"Policy Gradient Converges to the Globally Optimal Policy for Nearly\n  Linear-Quadratic Regulators","summary":"  Nonlinear control systems with partial information to the decision maker are\nprevalent in a variety of applications. As a step toward studying such\nnonlinear systems, this work explores reinforcement learning methods for\nfinding the optimal policy in the nearly linear-quadratic regulator systems. In\nparticular, we consider a dynamic system that combines linear and nonlinear\ncomponents, and is governed by a policy with the same structure. Assuming that\nthe nonlinear component comprises kernels with small Lipschitz coefficients, we\ncharacterize the optimization landscape of the cost function. Although the cost\nfunction is nonconvex in general, we establish the local strong convexity and\nsmoothness in the vicinity of the global optimizer. Additionally, we propose an\ninitialization mechanism to leverage these properties. Building on the\ndevelopments, we design a policy gradient algorithm that is guaranteed to\nconverge to the globally optimal policy with a linear rate.\n","authors":["Yinbin Han","Meisam Razaviyayn","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08431v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2210.11407v2","updated":"2023-03-15T08:06:48Z","published":"2022-10-20T16:56:47Z","title":"Similarity of Neural Architectures Based on Input Gradient\n  Transferability","summary":"  In recent years, a huge amount of deep neural architectures have been\ndeveloped for image classification. It remains curious whether these models are\nsimilar or different and what factors contribute to their similarities or\ndifferences. To address this question, we aim to design a quantitative and\nscalable similarity function between neural architectures. We utilize\nadversarial attack transferability, which has information related to input\ngradients and decision boundaries that are widely used to understand model\nbehaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet\nclassifiers using our proposed similarity function to answer the question.\nMoreover, we observe neural architecture-related phenomena using model\nsimilarity that model diversity can lead to better performance on model\nensembles and knowledge distillation under specific conditions. Our results\nprovide insights into why the development of diverse neural architectures with\ndistinct components is necessary.\n","authors":["Jaehui Hwang","Dongyoon Han","Byeongho Heo","Song Park","Sanghyuk Chun","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2210.11407v2.pdf","comment":"21pages, 10 figures, 1.5MB"},{"id":"http://arxiv.org/abs/2210.17312v2","updated":"2023-03-15T07:23:03Z","published":"2022-10-31T16:47:11Z","title":"Training Neural Networks for Sequential Change-point Detection","summary":"  Detecting an abrupt distributional shift of a data stream, known as\nchange-point detection, is a fundamental problem in statistics and machine\nlearning. We introduce a novel approach for online change-point detection using\nneural networks. To be specific, our approach is training neural networks to\ncompute the cumulative sum of a detection statistic sequentially, which\nexhibits a significant change when a change-point occurs. We demonstrated the\nsuperiority and potential of the proposed method in detecting change-point\nusing both synthetic and real-world data.\n","authors":["Junghwan Lee","Yao Xie","Xiuyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2210.17312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08403v1","updated":"2023-03-15T07:13:54Z","published":"2023-03-15T07:13:54Z","title":"DualFair: Fair Representation Learning at Both Group and Individual\n  Levels via Contrastive Self-supervision","summary":"  Algorithmic fairness has become an important machine learning problem,\nespecially for mission-critical Web applications. This work presents a\nself-supervised model, called DualFair, that can debias sensitive attributes\nlike gender and race from learned representations. Unlike existing models that\ntarget a single type of fairness, our model jointly optimizes for two fairness\ncriteria - group fairness and counterfactual fairness - and hence makes fairer\npredictions at both the group and individual levels. Our model uses contrastive\nloss to generate embeddings that are indistinguishable for each protected\ngroup, while forcing the embeddings of counterfactual pairs to be similar. It\nthen uses a self-knowledge distillation method to maintain the quality of\nrepresentation for the downstream tasks. Extensive analysis over multiple\ndatasets confirms the model's validity and further shows the synergy of jointly\naddressing two fairness criteria, suggesting the model's potential value in\nfair intelligent Web applications.\n","authors":["Sungwon Han","Seungeon Lee","Fangzhao Wu","Sundong Kim","Chuhan Wu","Xiting Wang","Xing Xie","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2303.08403v1.pdf","comment":"Accepted and will be published at TheWebConf2023 (WWW2023)"},{"id":"http://arxiv.org/abs/2210.01162v4","updated":"2023-03-15T07:12:34Z","published":"2022-10-03T18:32:20Z","title":"Learning Minimally-Violating Continuous Control for Infeasible Linear\n  Temporal Logic Specifications","summary":"  This paper explores continuous-time control synthesis for target-driven\nnavigation to satisfy complex high-level tasks expressed as linear temporal\nlogic (LTL). We propose a model-free framework using deep reinforcement\nlearning (DRL) where the underlying dynamic system is unknown (an opaque box).\nUnlike prior work, this paper considers scenarios where the given LTL\nspecification might be infeasible and therefore cannot be accomplished\nglobally. Instead of modifying the given LTL formula, we provide a general\nDRL-based approach to satisfy it with minimal violation. To do this, we\ntransform a previously multi-objective DRL problem, which requires simultaneous\nautomata satisfaction and minimum violation cost, into a single objective. By\nguiding the DRL agent with a sampling-based path planning algorithm for the\npotentially infeasible LTL task, the proposed approach mitigates the myopic\ntendencies of DRL, which are often an issue when learning general LTL tasks\nthat can have long or infinite horizons. This is achieved by decomposing an\ninfeasible LTL formula into several reach-avoid sub-tasks with shorter\nhorizons, which can be trained in a modular DRL architecture. Furthermore, we\novercome the challenge of the exploration process for DRL in complex and\ncluttered environments by using path planners to design rewards that are dense\nin the configuration space. The benefits of the presented approach are\ndemonstrated through testing on various complex nonlinear systems and compared\nwith state-of-the-art baselines. The Video demonstration can be found\nhere:https://youtu.be/jBhx6Nv224E.\n","authors":["Mingyu Cai","Makai Mann","Zachary Serlin","Kevin Leahy","Cristian-Ioan Vasile"],"pdf_url":"https://arxiv.org/pdf/2210.01162v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08385v1","updated":"2023-03-15T06:01:02Z","published":"2023-03-15T06:01:02Z","title":"Generating symbolic music using diffusion models","summary":"  Probabilistic Denoising Diffusion models have emerged as simple yet very\npowerful generative models. Diffusion models unlike other generative models do\nnot suffer from mode collapse nor require a discriminator to generate high\nquality samples. In this paper, we propose a diffusion model that uses a\nbinomial prior distribution to generate piano-rolls. The paper also proposes an\nefficient method to train the model and generate samples. The generated music\nhas coherence at time scales up to the length of the training piano-roll\nsegments. We show how such a model is conditioned on the input and can be used\nto harmonize a given melody, complete an incomplete piano-roll or generate a\nvariation of a given piece. The code is shared publicly to encourage the use\nand development of the method by the community.\n","authors":["Lilac Atassi"],"pdf_url":"https://arxiv.org/pdf/2303.08385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08374v1","updated":"2023-03-15T05:23:42Z","published":"2023-03-15T05:23:42Z","title":"MCR-DL: Mix-and-Match Communication Runtime for Deep Learning","summary":"  In recent years, the training requirements of many state-of-the-art Deep\nLearning (DL) models have scaled beyond the compute and memory capabilities of\na single processor, and necessitated distribution among processors. Training\nsuch massive models necessitates advanced parallelism strategies to maintain\nefficiency. However, such distributed DL parallelism strategies require a\nvaried mixture of collective and point-to-point communication operations across\na broad range of message sizes and scales. Examples of models using advanced\nparallelism strategies include Deep Learning Recommendation Models (DLRM) and\nMixture-of-Experts (MoE). Communication libraries' performance varies wildly\nacross different communication operations, scales, and message sizes. We\npropose MCR-DL: an extensible DL communication framework that supports all\npoint-to-point and collective operations while enabling users to dynamically\nmix-and-match communication backends for a given operation without deadlocks.\nMCR-DL also comes packaged with a tuning suite for dynamically selecting the\nbest communication backend for a given input tensor. We select DeepSpeed-MoE\nand DLRM as candidate DL models and demonstrate a 31% improvement in DS-MoE\nthroughput on 256 V100 GPUs on the Lassen HPC system. Further, we achieve a 20%\nthroughput improvement in a dense Megatron-DeepSpeed model and a 25% throughput\nimprovement in DLRM on 32 A100 GPUs with the Theta-GPU HPC system.\n","authors":["Quentin Anthony","Ammar Ahmad Awan","Jeff Rasley","Yuxiong He","Aamir Shafi","Mustafa Abduljabbar","Hari Subramoni","Dhabaleswar Panda"],"pdf_url":"https://arxiv.org/pdf/2303.08374v1.pdf","comment":"Accepted, to be presented at IPDPS 2023"},{"id":"http://arxiv.org/abs/2211.15158v3","updated":"2023-03-15T04:49:40Z","published":"2022-11-28T09:14:36Z","title":"Heterogeneous Graph Learning for Multi-modal Medical Data Analysis","summary":"  Routine clinical visits of a patient produce not only image data, but also\nnon-image data containing clinical information regarding the patient, i.e.,\nmedical data is multi-modal in nature. Such heterogeneous modalities offer\ndifferent and complementary perspectives on the same patient, resulting in more\naccurate clinical decisions when they are properly combined. However, despite\nits significance, how to effectively fuse the multi-modal medical data into a\nunified framework has received relatively little attention. In this paper, we\npropose an effective graph-based framework called HetMed (Heterogeneous Graph\nLearning for Multi-modal Medical Data Analysis) for fusing the multi-modal\nmedical data. Specifically, we construct a multiplex network that incorporates\nmultiple types of non-image features of patients to capture the complex\nrelationship between patients in a systematic way, which leads to more accurate\nclinical decisions. Extensive experiments on various real-world datasets\ndemonstrate the superiority and practicality of HetMed. The source code for\nHetMed is available at https://github.com/Sein-Kim/Multimodal-Medical.\n","authors":["Sein Kim","Namkyeong Lee","Junseok Lee","Dongmin Hyun","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2211.15158v3.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08362v1","updated":"2023-03-15T04:46:57Z","published":"2023-03-15T04:46:57Z","title":"Transfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations","summary":"  With the development of computer -systems that can collect and analyze\nenormous volumes of data, the medical profession is establishing several\nnon-invasive tools. This work attempts to develop a non-invasive technique for\nidentifying respiratory sounds acquired by a stethoscope and voice recording\nsoftware via machine learning techniques. This study suggests a trained and\nproven CNN-based approach for categorizing respiratory sounds. A visual\nrepresentation of each audio sample is constructed, allowing resource\nidentification for classification using methods like those used to effectively\ndescribe visuals. We used a technique called Mel Frequency Cepstral\nCoefficients (MFCCs). Here, features are retrieved and categorized via VGG16\n(transfer learning) and prediction is accomplished using 5-fold\ncross-validation. Employing various data splitting techniques, Respiratory\nSound Database obtained cutting-edge results, including accuracy of 95%,\nprecision of 88%, recall score of 86%, and F1 score of 81%. The ICBHI dataset\nis used to train and test the model.\n","authors":["Hafsa Gulzar","Jiyun Li","Arslan Manzoor","Sadaf Rehmat","Usman Amjad","Hadiqa Jalil Khan"],"pdf_url":"https://arxiv.org/pdf/2303.08362v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.08361v1","updated":"2023-03-15T04:41:36Z","published":"2023-03-15T04:41:36Z","title":"Towards Cooperative Federated Learning over Heterogeneous Edge/Fog\n  Networks","summary":"  Federated learning (FL) has been promoted as a popular technique for training\nmachine learning (ML) models over edge/fog networks. Traditional\nimplementations of FL have largely neglected the potential for inter-network\ncooperation, treating edge/fog devices and other infrastructure participating\nin ML as separate processing elements. Consequently, FL has been vulnerable to\nseveral dimensions of network heterogeneity, such as varying computation\ncapabilities, communication resources, data qualities, and privacy demands. We\nadvocate for cooperative federated learning (CFL), a cooperative edge/fog ML\nparadigm built on device-to-device (D2D) and device-to-server (D2S)\ninteractions. Through D2D and D2S cooperation, CFL counteracts network\nheterogeneity in edge/fog networks through enabling a model/data/resource\npooling mechanism, which will yield substantial improvements in ML model\ntraining quality and network resource consumption. We propose a set of core\nmethodologies that form the foundation of D2D and D2S cooperation and present\npreliminary experiments that demonstrate their benefits. We also discuss new FL\nfunctionalities enabled by this cooperative framework such as the integration\nof unlabeled data and heterogeneous device privacy into ML model training.\nFinally, we describe some open research directions at the intersection of\ncooperative edge/fog and FL.\n","authors":["Su Wang","Seyyedali Hosseinalipour","Vaneet Aggarwal","Christopher G. Brinton","David J. Love","Weifeng Su","Mung Chiang"],"pdf_url":"https://arxiv.org/pdf/2303.08361v1.pdf","comment":"This paper has been accepted for publication in IEEE Communications\n  Magazine"},{"id":"http://arxiv.org/abs/2301.11166v2","updated":"2023-03-15T04:22:26Z","published":"2023-01-20T12:49:21Z","title":"Flex-Net: A Graph Neural Network Approach to Resource Management in\n  Flexible Duplex Networks","summary":"  Flexible duplex networks allow users to dynamically employ uplink and\ndownlink channels without static time scheduling, thereby utilizing the network\nresources efficiently. This work investigates the sum-rate maximization of\nflexible duplex networks. In particular, we consider a network with\npairwise-fixed communication links. Corresponding combinatorial optimization is\na non-deterministic polynomial (NP)-hard without a closed-form solution. In\nthis respect, the existing heuristics entail high computational complexity,\nraising a scalability issue in large networks. Motivated by the recent success\nof Graph Neural Networks (GNNs) in solving NP-hard wireless resource management\nproblems, we propose a novel GNN architecture, named Flex-Net, to jointly\noptimize the communication direction and transmission power. The proposed GNN\nproduces near-optimal performance meanwhile maintaining a low computational\ncomplexity compared to the most commonly used techniques. Furthermore, our\nnumerical results shed light on the advantages of using GNNs in terms of sample\ncomplexity, scalability, and generalization capability.\n","authors":["Tharaka Perera","Saman Atapattu","Yuting Fang","Prathapasinghe Dharmawansa","Jamie Evans"],"pdf_url":"https://arxiv.org/pdf/2301.11166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08355v1","updated":"2023-03-15T04:15:51Z","published":"2023-03-15T04:15:51Z","title":"Efficient and Secure Federated Learning for Financial Applications","summary":"  The conventional machine learning (ML) and deep learning approaches need to\nshare customers' sensitive information with an external credit bureau to\ngenerate a prediction model that opens the door to privacy leakage. This\nleakage risk makes financial companies face an enormous challenge in their\ncooperation. Federated learning is a machine learning setting that can protect\ndata privacy, but the high communication cost is often the bottleneck of the\nfederated systems, especially for large neural networks. Limiting the number\nand size of communications is necessary for the practical training of large\nneural structures. Gradient sparsification has received increasing attention as\na method to reduce communication cost, which only updates significant gradients\nand accumulates insignificant gradients locally. However, the secure\naggregation framework cannot directly use gradient sparsification. This article\nproposes two sparsification methods to reduce communication cost in federated\nlearning. One is a time-varying hierarchical sparsification method for model\nparameter update, which solves the problem of maintaining model accuracy after\nhigh ratio sparsity. It can significantly reduce the cost of a single\ncommunication. The other is to apply the sparsification method to the secure\naggregation framework. We sparse the encryption mask matrix to reduce the cost\nof communication while protecting privacy. Experiments show that under\ndifferent Non-IID experiment settings, our method can reduce the upload\ncommunication cost to about 2.9% to 18.9% of the conventional federated\nlearning algorithm when the sparse rate is 0.01.\n","authors":["Tao Liu","Zhi Wang","Hui He","Wei Shi","Liangliang Lin","Wei Shi","Ran An","Chenhao Li"],"pdf_url":"https://arxiv.org/pdf/2303.08355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06625v3","updated":"2023-03-15T04:13:03Z","published":"2023-01-16T22:22:04Z","title":"TDSTF: Transformer-based Diffusion probabilistic model for Sparse Time\n  series Forecasting","summary":"  Background and objective: In the intensive care unit (ICU), vital sign\nmonitoring is critical, and an accurate predictive system is required. This\nstudy will create a novel model to forecast Heart Rate (HR), Systolic Blood\nPressure (SBP), and Diastolic Blood Pressure (DBP) in ICU. These vital signs\nare crucial for prompt interventions for patients. We extracted $24,886$ ICU\nstays from the MIMIC-III database, which contains data from over $46$ thousand\npatients, to train and test the model. Methods: The model proposed in this\nstudy, areansformerin intensive careabilistic Model for Sparse Time Series\nForecasting (TDSTF), uses a deep learning technique called the Transformer. The\nTDSTF model showed state-of-the-art performance in predicting vital signs in\nthe ICU, outperforming other models' ability to predict distributions of vital\nsigns and being more computationally efficient. The code is available at\nhttps://github.com/PingChang818/TDSTF. Results: The results of the study showed\nthat TDSTF achieved a Normalized Average Continuous Ranked Probability Score\n(NACRPS) of $0.4438$ and a Mean Squared Error (MSE) of $0.4168$, an improvement\nof $18.9\\%$ and $34.3\\%$ over the best baseline model, respectively.\nConclusion: In conclusion, TDSTF is an effective and efficient solution for\nforecasting vital signs in the ICU, and it shows a significant improvement\ncompared to other models in the field.\n","authors":["Ping Chang","Huayu Li","Stuart F. Quan","Shuyang Lu","Shu-Fen Wung","Janet Roveda","Ao Li"],"pdf_url":"https://arxiv.org/pdf/2301.06625v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02462v2","updated":"2023-03-15T03:47:48Z","published":"2023-03-04T17:36:09Z","title":"Towards Improved Illicit Node Detection with Positive-Unlabelled\n  Learning","summary":"  Detecting illicit nodes on blockchain networks is a valuable task for\nstrengthening future regulation. Recent machine learning-based methods proposed\nto tackle the tasks are using some blockchain transaction datasets with a small\nportion of samples labeled positive and the rest unlabelled (PU). Albeit the\nassumption that a random sample of unlabeled nodes are normal nodes is used in\nsome works, we discuss that the label mechanism assumption for the hidden\npositive labels and its effect on the evaluation metrics is worth considering.\nWe further explore that PU classifiers dealing with potential hidden positive\nlabels can have improved performance compared to regular machine learning\nmodels. We test the PU classifiers with a list of graph representation learning\nmethods for obtaining different feature distributions for the same data to have\nmore reliable results.\n","authors":["Junliang Luo","Farimah Poursafaei","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2303.02462v2.pdf","comment":"accepted at the 5th edition of the IEEE International Conference on\n  Blockchain and Cryptocurrency (ICBC 2023)"},{"id":"http://arxiv.org/abs/2301.13428v2","updated":"2023-03-15T03:38:08Z","published":"2023-01-31T05:51:05Z","title":"Contrast and Clustering: Learning Neighborhood Pair Representation for\n  Source-free Domain Adaptation","summary":"  Unsupervised domain adaptation uses source data from different distributions\nto solve the problem of classifying data from unlabeled target domains.\nHowever, conventional methods require access to source data, which often raise\nconcerns about data privacy. In this paper, we consider a more practical but\nchallenging setting where the source domain data is unavailable and the target\ndomain data is unlabeled. Specifically, we address the domain discrepancy\nproblem from the perspective of contrastive learning. The key idea of our work\nis to learn a domain-invariant feature by 1) performing clustering directly in\nthe original feature space with nearest neighbors; 2) constructing truly hard\nnegative pairs by extended neighbors without introducing additional\ncomputational complexity; and 3) combining noise-contrastive estimation theory\nto gain computational advantage. We conduct careful ablation studies and\nextensive experiments on three common benchmarks: VisDA, Office-Home, and\nOffice-31. The results demonstrate the superiority of our methods compared with\nother state-of-the-art works.\n","authors":["Yuqi Chen","Xiangbin Zhu","Yonggang Li","Yingjian Li","Yuanwang Wei","Haojie Fang"],"pdf_url":"https://arxiv.org/pdf/2301.13428v2.pdf","comment":"Journal articles"},{"id":"http://arxiv.org/abs/2303.08343v1","updated":"2023-03-15T03:21:38Z","published":"2023-03-15T03:21:38Z","title":"Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech\n  Recognition Models","summary":"  Continued improvements in machine learning techniques offer exciting new\nopportunities through the use of larger models and larger training datasets.\nHowever, there is a growing need to offer these new capabilities on-board\nlow-powered devices such as smartphones, wearables and other embedded\nenvironments where only low memory is available. Towards this, we consider\nmethods to reduce the model size of Conformer-based speech recognition models\nwhich typically require models with greater than 100M parameters down to just\n$5$M parameters while minimizing impact on model quality. Such a model allows\nus to achieve always-on ambient speech recognition on edge devices with\nlow-memory neural processors. We propose model weight reuse at different levels\nwithin our model architecture: (i) repeating full conformer block layers, (ii)\nsharing specific conformer modules across layers, (iii) sharing sub-components\nper conformer module, and (iv) sharing decomposed sub-component weights after\nlow-rank decomposition. By sharing weights at different levels of our model, we\ncan retain the full model in-memory while increasing the number of virtual\ntransformations applied to the input. Through a series of ablation studies and\nevaluations, we find that with weight sharing and a low-rank architecture, we\ncan achieve a WER of 2.84 and 2.94 for Librispeech dev-clean and test-clean\nrespectively with a $5$M parameter model.\n","authors":["Steven M. Hernandez","Ding Zhao","Shaojin Ding","Antoine Bruguier","Rohit Prabhavalkar","Tara N. Sainath","Yanzhang He","Ian McGraw"],"pdf_url":"https://arxiv.org/pdf/2303.08343v1.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.09900v2","updated":"2023-03-15T03:08:18Z","published":"2022-12-19T22:43:08Z","title":"Policy learning \"without'' overlap: Pessimism and generalized empirical\n  Bernstein's inequality","summary":"  This paper studies offline policy learning, which aims at utilizing\nobservations collected a priori (from either fixed or adaptively evolving\nbehavior policies) to learn the optimal individualized decision rule in a given\nclass. Existing policy learning methods rely on a uniform overlap assumption,\ni.e., the propensities of exploring all actions for all individual\ncharacteristics are lower bounded in the offline dataset. In other words, the\nperformance of these methods depends on the worst-case propensity in the\noffline dataset. As one has no control over the data collection process, this\nassumption can be unrealistic in many situations, especially when the behavior\npolicies are allowed to evolve over time with diminishing propensities.\n  In this paper, we propose a new algorithm that optimizes lower confidence\nbounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs\nare constructed by quantifying the estimation uncertainty of the augmented\ninverse propensity weighted (AIPW)-type estimators using knowledge of the\nbehavior policies for collecting the offline data. Without assuming any uniform\noverlap condition, we establish a data-dependent upper bound for the\nsuboptimality of our algorithm, which depends only on (i) the overlap for the\noptimal policy, and (ii) the complexity of the policy class. As an implication,\nfor adaptively collected data, we ensure efficient policy learning as long as\nthe propensities for optimal actions are lower bounded over time, while those\nfor suboptimal ones are allowed to diminish arbitrarily fast. In our\ntheoretical analysis, we develop a new self-normalized concentration inequality\nfor IPW estimators, generalizing the well-known empirical Bernstein's\ninequality to unbounded and non-i.i.d. data.\n","authors":["Ying Jin","Zhimei Ren","Zhuoran Yang","Zhaoran Wang"],"pdf_url":"https://arxiv.org/pdf/2212.09900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06980v2","updated":"2023-03-15T03:05:52Z","published":"2023-03-13T10:30:02Z","title":"Self-supervised based general laboratory progress pretrained model for\n  cardiovascular event detection","summary":"  Regular surveillance is an indispensable aspect of managing cardiovascular\ndisorders. Patient recruitment for rare or specific diseases is often limited\ndue to their small patient size and episodic observations, whereas prevalent\ncases accumulate longitudinal data easily due to regular follow-ups. These\ndata, however, are notorious for their irregularity, temporality, absenteeism,\nand sparsity. In this study, we leveraged self-supervised learning (SSL) and\ntransfer learning to overcome the above-mentioned barriers, transferring\npatient progress trends in cardiovascular laboratory parameters from prevalent\ncases to rare or specific cardiovascular events detection. We pretrained a\ngeneral laboratory progress (GLP) pretrain model using hypertension patients\n(who were yet to be diabetic), and transferred their laboratory progress trend\nto assist in detecting target vessel revascularization (TVR) in percutaneous\ncoronary intervention patients. GLP adopted a two-stage training process that\nutilized interpolated data, enhancing the performance of SSL. After pretraining\nGLP, we fine-tuned it for TVR prediction. The proposed two-stage training\nprocess outperformed SSL. Upon processing by GLP, the classification\ndemonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged\naccuracy. All metrics were significantly superior (p < 0.01) to the performance\nof prior GLP processing. The representation displayed distinct separability\nindependent of algorithmic mechanisms, and diverse data distribution trend. Our\napproach effectively transferred the progression trends of cardiovascular\nlaboratory parameters from prevalent cases to small-numbered cases, thereby\ndemonstrating its efficacy in aiding the risk assessment of cardiovascular\nevents without limiting to episodic observation. The potential for extending\nthis approach to other laboratory tests and diseases is promising.\n","authors":["Li-Chin Chen","Kuo-Hsuan Hung","Yi-Ju Tseng","Hsin-Yao Wang","Tse-Min Lu","Wei-Chieh Huang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2303.06980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08331v1","updated":"2023-03-15T02:40:02Z","published":"2023-03-15T02:40:02Z","title":"Towards High-Quality and Efficient Video Super-Resolution via\n  Spatial-Temporal Data Overfitting","summary":"  As deep convolutional neural networks (DNNs) are widely used in various\nfields of computer vision, leveraging the overfitting ability of the DNN to\nachieve video resolution upscaling has become a new trend in the modern video\ndelivery system. By dividing videos into chunks and overfitting each chunk with\na super-resolution model, the server encodes videos before transmitting them to\nthe clients, thus achieving better video quality and transmission efficiency.\nHowever, a large number of chunks are expected to ensure good overfitting\nquality, which substantially increases the storage and consumes more bandwidth\nresources for data transmission. On the other hand, decreasing the number of\nchunks through training optimization techniques usually requires high model\ncapacity, which significantly slows down execution speed. To reconcile such, we\npropose a novel method for high-quality and efficient video resolution\nupscaling tasks, which leverages the spatial-temporal information to accurately\ndivide video into chunks, thus keeping the number of chunks as well as the\nmodel size to minimum. Additionally, we advance our method into a single\noverfitting model by a data-aware joint training technique, which further\nreduces the storage requirement with negligible quality drop. We deploy our\nmodels on an off-the-shelf mobile phone, and experimental results show that our\nmethod achieves real-time video super-resolution with high video quality.\nCompared with the state-of-the-art, our method achieves 28 fps streaming speed\nwith 41.6 PSNR, which is 14$\\times$ faster and 2.29 dB better in the live video\nresolution upscaling tasks. Our codes are available at:\nhttps://github.com/coulsonlee/STDO-CVPR2023.git\n","authors":["Gen Li","Jie Ji","Minghai Qin","Wei Niu","Bin Ren","Fatemeh Afghah","Linke Guo","Xiaolong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.08331v1.pdf","comment":"CVPR 2023 Paper"},{"id":"http://arxiv.org/abs/2303.06261v2","updated":"2023-03-15T02:31:18Z","published":"2023-03-11T00:53:49Z","title":"Interpretable Outlier Summarization","summary":"  Outlier detection is critical in real applications to prevent financial\nfraud, defend network intrusions, or detecting imminent device failures. To\nreduce the human effort in evaluating outlier detection results and effectively\nturn the outliers into actionable insights, the users often expect a system to\nautomatically produce interpretable summarizations of subgroups of outlier\ndetection results. Unfortunately, to date no such systems exist. To fill this\ngap, we propose STAIR which learns a compact set of human understandable rules\nto summarize and explain the anomaly detection results. Rather than use the\nclassical decision tree algorithms to produce these rules, STAIR proposes a new\noptimization objective to produce a small number of rules with least\ncomplexity, hence strong interpretability, to accurately summarize the\ndetection results. The learning algorithm of STAIR produces a rule set by\niteratively splitting the large rules and is optimal in maximizing this\nobjective in each iteration. Moreover, to effectively handle high dimensional,\nhighly complex data sets which are hard to summarize with simple rules, we\npropose a localized STAIR approach, called L-STAIR. Taking data locality into\nconsideration, it simultaneously partitions data and learns a set of localized\nrules for each partition. Our experimental study on many outlier benchmark\ndatasets shows that STAIR significantly reduces the complexity of the rules\nrequired to summarize the outlier detection results, thus more amenable for\nhumans to understand and evaluate, compared to the decision tree methods.\n","authors":["Yu Wang","Lei Cao","Yizhou Yan","Samuel Madden"],"pdf_url":"https://arxiv.org/pdf/2303.06261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08054v2","updated":"2023-03-15T02:30:04Z","published":"2023-03-14T16:37:38Z","title":"Statistical Hardware Design With Multi-model Active Learning","summary":"  With the rising complexity of numerous novel applications that serve our\nmodern society comes the strong need to design efficient computing platforms.\nDesigning efficient hardware is, however, a complex multi-objective problem\nthat deals with multiple parameters and their interactions. Given that there\nare a large number of parameters and objectives involved in hardware design,\nsynthesizing all possible combinations is not a feasible method to find the\noptimal solution. One promising approach to tackle this problem is statistical\nmodeling of a desired hardware performance. Here, we propose a model-based\nactive learning approach to solve this problem. Our proposed method uses\nBayesian models to characterize various aspects of hardware performance. We\nalso use transfer learning and Gaussian regression bootstrapping techniques in\nconjunction with active learning to create more accurate models. Our proposed\nstatistical modeling method provides hardware models that are sufficiently\naccurate to perform design space exploration as well as performance prediction\nsimultaneously. We use our proposed method to perform design space exploration\nand performance prediction for various hardware setups, such as\nmicro-architecture design and OpenCL kernels for FPGA targets. Our experiments\nshow that the number of samples required to create performance models\nsignificantly reduces while maintaining the predictive power of our proposed\nstatistical models. For instance, in our performance prediction setting, the\nproposed method needs 65% fewer samples to create the model, and in the design\nspace exploration setting, our proposed method can find the best parameter\nsettings by exploring less than 50 samples.\n","authors":["Alireza Ghaffari","Masoud Asgharian","Yvon Savaria"],"pdf_url":"https://arxiv.org/pdf/2303.08054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08325v1","updated":"2023-03-15T02:22:07Z","published":"2023-03-15T02:22:07Z","title":"FairAdaBN: Mitigating unfairness with adaptive batch normalization and\n  its application to dermatological disease classification","summary":"  Deep learning is becoming increasingly ubiquitous in medical research and\napplications while involving sensitive information and even critical diagnosis\ndecisions. Researchers observe a significant performance disparity among\nsubgroups with different demographic attributes, which is called model\nunfairness, and put lots of effort into carefully designing elegant\narchitectures to address unfairness, which poses heavy training burden, brings\npoor generalization, and reveals the trade-off between model performance and\nfairness. To tackle these issues, we propose FairAdaBN by making batch\nnormalization adaptive to sensitive attribute. This simple but effective design\ncan be adopted to several classification backbones that are originally unaware\nof fairness. Additionally, we derive a novel loss function that restrains\nstatistical parity between subgroups on mini-batches, encouraging the model to\nconverge with considerable fairness. In order to evaluate the trade-off between\nmodel performance and fairness, we propose a new metric, named\nFairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness\nimprovement over accuracy drop. Experiments on two dermatological datasets show\nthat our proposed method outperforms other methods on fairness criteria and\nFATE.\n","authors":["Zikang Xu","Shang Zhao","Quan Quan","Qingsong Yao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.08325v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.08322v1","updated":"2023-03-15T02:18:21Z","published":"2023-03-15T02:18:21Z","title":"Optimization Design for Federated Learning in Heterogeneous 6G Networks","summary":"  With the rapid advancement of 5G networks, billions of smart Internet of\nThings (IoT) devices along with an enormous amount of data are generated at the\nnetwork edge. While still at an early age, it is expected that the evolving 6G\nnetwork will adopt advanced artificial intelligence (AI) technologies to\ncollect, transmit, and learn this valuable data for innovative applications and\nintelligent services. However, traditional machine learning (ML) approaches\nrequire centralizing the training data in the data center or cloud, raising\nserious user-privacy concerns. Federated learning, as an emerging distributed\nAI paradigm with privacy-preserving nature, is anticipated to be a key enabler\nfor achieving ubiquitous AI in 6G networks. However, there are several system\nand statistical heterogeneity challenges for effective and efficient FL\nimplementation in 6G networks. In this article, we investigate the optimization\napproaches that can effectively address the challenging heterogeneity issues\nfrom three aspects: incentive mechanism design, network resource management,\nand personalized model optimization. We also present some open problems and\npromising directions for future research.\n","authors":["Bing Luo","Xiaomin Ouyang","Peng Sun","Pengchao Han","Ningning Ding","Jianwei Huang"],"pdf_url":"https://arxiv.org/pdf/2303.08322v1.pdf","comment":"Accepted in IEEE Nework"},{"id":"http://arxiv.org/abs/2302.06232v3","updated":"2023-03-15T01:38:19Z","published":"2023-02-13T10:11:05Z","title":"Understanding Multimodal Contrastive Learning and Incorporating Unpaired\n  Data","summary":"  Language-supervised vision models have recently attracted great attention in\ncomputer vision. A common approach to build such models is to use contrastive\nlearning on paired data across the two modalities, as exemplified by\nContrastive Language-Image Pre-Training (CLIP). In this paper, under linear\nrepresentation settings, (i) we initiate the investigation of a general class\nof nonlinear loss functions for multimodal contrastive learning (MMCL)\nincluding CLIP loss and show its connection to singular value decomposition\n(SVD). Namely, we show that each step of loss minimization by gradient descent\ncan be seen as performing SVD on a contrastive cross-covariance matrix. Based\non this insight, (ii) we analyze the performance of MMCL. We quantitatively\nshow that the feature learning ability of MMCL can be better than that of\nunimodal contrastive learning applied to each modality even under the presence\nof wrongly matched pairs. This characterizes the robustness of MMCL to noisy\ndata. Furthermore, when we have access to additional unpaired data, (iii) we\npropose a new MMCL loss that incorporates additional unpaired datasets. We show\nthat the algorithm can detect the ground-truth pairs and improve performance by\nfully exploiting unpaired datasets. The performance of the proposed algorithm\nwas verified by numerical experiments.\n","authors":["Ryumei Nakada","Halil Ibrahim Gulluk","Zhun Deng","Wenlong Ji","James Zou","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.06232v3.pdf","comment":"42 pages, 3 figures, accepted by AISTATS 2023; a link to GitHub\n  repository added, style corrected, acknowledgements section added"},{"id":"http://arxiv.org/abs/2303.08303v1","updated":"2023-03-15T01:30:48Z","published":"2023-03-15T01:30:48Z","title":"SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep\n  Models for Kidney Stone Classification","summary":"  Recently, deep learning has produced encouraging results for kidney stone\nclassification using endoscope images. However, the shortage of annotated\ntraining data poses a severe problem in improving the performance and\ngeneralization ability of the trained model. It is thus crucial to fully\nexploit the limited data at hand. In this paper, we propose SegPrompt to\nalleviate the data shortage problems by exploiting segmentation maps from two\naspects. First, SegPrompt integrates segmentation maps to facilitate\nclassification training so that the classification model is aware of the\nregions of interest. The proposed method allows the image and segmentation\ntokens to interact with each other to fully utilize the segmentation map\ninformation. Second, we use the segmentation maps as prompts to tune the\npretrained deep model, resulting in much fewer trainable parameters than\nvanilla finetuning. We perform extensive experiments on the collected kidney\nstone dataset. The results show that SegPrompt can achieve an advantageous\nbalance between the model fitting ability and the generalization ability,\neventually leading to an effective model with limited training data.\n","authors":["Wei Zhu","Runtao Zhou","Yao Yuan","Campbell Timothy","Rajat Jain","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08302v1","updated":"2023-03-15T01:27:15Z","published":"2023-03-15T01:27:15Z","title":"A Comprehensive Study on Post-Training Quantization for Large Language\n  Models","summary":"  Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce the memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study on those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bits is similar to 5-bits). We also\npresent recommendations about how to utilize quantization for \\llms with\ndifferent sizes, and leave suggestions of future opportunities and system work\nthat are not resolved in this work.\n","authors":["Zhewei Yao","Cheng Li","Xiaoxia Wu","Stephen Youn","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.08302v1.pdf","comment":"25 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.08301v1","updated":"2023-03-15T01:26:39Z","published":"2023-03-15T01:26:39Z","title":"Dataset Management Platform for Machine Learning","summary":"  The quality of the data in a dataset can have a substantial impact on the\nperformance of a machine learning model that is trained and/or evaluated using\nthe dataset. Effective dataset management, including tasks such as data\ncleanup, versioning, access control, dataset transformation, automation,\nintegrity and security, etc., can help improve the efficiency and speed of the\nmachine learning process. Currently, engineers spend a substantial amount of\nmanual effort and time to manage dataset versions or to prepare datasets for\nmachine learning tasks. This disclosure describes a platform to manage and use\ndatasets effectively. The techniques integrate dataset management and dataset\ntransformation mechanisms. A storage engine is described that acts as a source\nof truth for all data and handles versioning, access control etc. The dataset\ntransformation mechanism is a key part to generate a dataset (snapshot) to\nserve different purposes. The described techniques can support different\nworkflows, pipelines, or data orchestration needs, e.g., for training and/or\nevaluation of machine learning models.\n","authors":["Ze Mao","Yang Xu","Erick Suarez"],"pdf_url":"https://arxiv.org/pdf/2303.08301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08300v1","updated":"2023-03-15T01:21:50Z","published":"2023-03-15T01:21:50Z","title":"Learning From High-Dimensional Cyber-Physical Data Streams for\n  Diagnosing Faults in Smart Grids","summary":"  The performance of fault diagnosis systems is highly affected by data quality\nin cyber-physical power systems. These systems generate massive amounts of data\nthat overburden the system with excessive computational costs. Another issue is\nthe presence of noise in recorded measurements, which prevents building a\nprecise decision model. Furthermore, the diagnostic model is often provided\nwith a mixture of redundant measurements that may deviate it from learning\nnormal and fault distributions. This paper presents the effect of feature\nengineering on mitigating the aforementioned challenges in cyber-physical\nsystems. Feature selection and dimensionality reduction methods are combined\nwith decision models to simulate data-driven fault diagnosis in a 118-bus power\nsystem. A comparative study is enabled accordingly to compare several advanced\ntechniques in both domains. Dimensionality reduction and feature selection\nmethods are compared both jointly and separately. Finally, experiments are\nconcluded, and a setting is suggested that enhances data quality for fault\ndiagnosis.\n","authors":["Hossein Hassani","Ehsan Hallaji","Roozbeh Razavi-Far","Mehrdad Saif"],"pdf_url":"https://arxiv.org/pdf/2303.08300v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.02732v2","updated":"2023-03-15T01:12:22Z","published":"2023-01-06T22:24:53Z","title":"Multimodal Lyrics-Rhythm Matching","summary":"  Despite the recent increase in research on artificial intelligence for music,\nprominent correlations between key components of lyrics and rhythm such as\nkeywords, stressed syllables, and strong beats are not frequently studied. This\nis likely due to challenges such as audio misalignment, inaccuracies in\nsyllabic identification, and most importantly, the need for cross-disciplinary\nknowledge. To address this lack of research, we propose a novel multimodal\nlyrics-rhythm matching approach in this paper that specifically matches key\ncomponents of lyrics and music with each other without any language\nlimitations. We use audio instead of sheet music with readily available\nmetadata, which creates more challenges yet increases the application\nflexibility of our method. Furthermore, our approach creatively generates\nseveral patterns involving various multimodalities, including music strong\nbeats, lyrical syllables, auditory changes in a singer's pronunciation, and\nespecially lyrical keywords, which are utilized for matching key lyrical\nelements with key rhythmic elements. This advantageous approach not only\nprovides a unique way to study auditory lyrics-rhythm correlations including\nefficient rhythm-based audio alignment algorithms, but also bridges\ncomputational linguistics with music as well as music cognition. Our\nexperimental results reveal an 0.81 probability of matching on average, and\naround 30% of the songs have a probability of 0.9 or higher of keywords landing\non strong beats, including 12% of the songs with a perfect landing. Also, the\nsimilarity metrics are used to evaluate the correlation between lyrics and\nrhythm. It shows that nearly 50% of the songs have 0.70 similarity or higher.\nIn conclusion, our approach contributes significantly to the lyrics-rhythm\nrelationship by computationally unveiling insightful correlations.\n","authors":["Callie C. Liao","Duoduo Liao","Jesse Guessford"],"pdf_url":"https://arxiv.org/pdf/2301.02732v2.pdf","comment":"Accepted by 2022 IEEE International Conference on Big Data (IEEE Big\n  Data 2022)"},{"id":"http://arxiv.org/abs/2303.08291v1","updated":"2023-03-15T00:39:31Z","published":"2023-03-15T00:39:31Z","title":"Machine Learning Approaches in Agile Manufacturing with Recycled\n  Materials for Sustainability","summary":"  It is important to develop sustainable processes in materials science and\nmanufacturing that are environmentally friendly. AI can play a significant role\nin decision support here as evident from our earlier research leading to tools\ndeveloped using our proposed machine learning based approaches. Such tools\nserved the purpose of computational estimation and expert systems. This\nresearch addresses environmental sustainability in materials science via\ndecision support in agile manufacturing using recycled and reclaimed materials.\nIt is a safe and responsible way to turn a specific waste stream to value-added\nproducts. We propose to use data-driven methods in AI by applying machine\nlearning models for predictive analysis to guide decision support in\nmanufacturing. This includes harnessing artificial neural networks to study\nparameters affecting heat treatment of materials and impacts on their\nproperties; deep learning via advances such as convolutional neural networks to\nexplore grain size detection; and other classifiers such as Random Forests to\nanalyze phrase fraction detection. Results with all these methods seem\npromising to embark on further work, e.g. ANN yields accuracy around 90\\% for\npredicting micro-structure development as per quench tempering, a heat\ntreatment process. Future work entails several challenges: investigating\nvarious computer vision models (VGG, ResNet etc.) to find optimal accuracy,\nefficiency and robustness adequate for sustainable processes; creating\ndomain-specific tools using machine learning for decision support in agile\nmanufacturing; and assessing impacts on sustainability with metrics\nincorporating the appropriate use of recycled materials as well as the\neffectiveness of developed products. Our work makes impacts on green technology\nfor smart manufacturing, and is motivated by related work in the highly\ninteresting realm of AI for materials science.\n","authors":["Aparna S. Varde","Jianyu Liang"],"pdf_url":"https://arxiv.org/pdf/2303.08291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08290v1","updated":"2023-03-15T00:37:18Z","published":"2023-03-15T00:37:18Z","title":"Rediscovery of CNN's Versatility for Text-based Encoding of Raw\n  Electronic Health Records","summary":"  Making the most use of abundant information in electronic health records\n(EHR) is rapidly becoming an important topic in the medical domain. Recent work\npresented a promising framework that embeds entire features in raw EHR data\nregardless of its form and medical code standards. The framework, however, only\nfocuses on encoding EHR with minimal preprocessing and fails to consider how to\nlearn efficient EHR representation in terms of computation and memory usage. In\nthis paper, we search for a versatile encoder not only reducing the large data\ninto a manageable size but also well preserving the core information of\npatients to perform diverse clinical tasks. We found that hierarchically\nstructured Convolutional Neural Network (CNN) often outperforms the\nstate-of-the-art model on diverse tasks such as reconstruction, prediction, and\ngeneration, even with fewer parameters and less training time. Moreover, it\nturns out that making use of the inherent hierarchy of EHR data can boost the\nperformance of any kind of backbone models and clinical tasks performed.\nThrough extensive experiments, we present concrete evidence to generalize our\nresearch findings into real-world practice. We give a clear guideline on\nbuilding the encoder based on the research findings captured while exploring\nnumerous settings.\n","authors":["Eunbyeol Cho","Min Jae Lee","Kyunghoon Hur","Jiyoun Kim","Jinsung Yoon","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2303.08290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08289v1","updated":"2023-03-15T00:35:03Z","published":"2023-03-15T00:35:03Z","title":"Improving Adversarial Robustness with Hypersphere Embedding and\n  Angular-based Regularizations","summary":"  Adversarial training (AT) methods have been found to be effective against\nadversarial attacks on deep neural networks. Many variants of AT have been\nproposed to improve its performance. Pang et al. [1] have recently shown that\nincorporating hypersphere embedding (HE) into the existing AT procedures\nenhances robustness. We observe that the existing AT procedures are not\ndesigned for the HE framework, and thus fail to adequately learn the angular\ndiscriminative information available in the HE framework. In this paper, we\npropose integrating HE into AT with regularization terms that exploit the rich\nangular information available in the HE framework. Specifically, our method,\ntermed angular-AT, adds regularization terms to AT that explicitly enforce\nweight-feature compactness and inter-class separation; all expressed in terms\nof angular features. Experimental results show that angular-AT further improves\nadversarial robustness.\n","authors":["Olukorede Fakorede","Ashutosh Nirala","Modeste Atsague","Jin Tian"],"pdf_url":"https://arxiv.org/pdf/2303.08289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08288v1","updated":"2023-03-15T00:23:49Z","published":"2023-03-15T00:23:49Z","title":"Attention-likelihood relationship in transformers","summary":"  We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.\n","authors":["Valeria Ruscio","Valentino Maiorca","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2303.08288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08286v1","updated":"2023-03-15T00:17:53Z","published":"2023-03-15T00:17:53Z","title":"Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and\n  Air Quality Index","summary":"  This is a study on the potential widespread usage of alternative fuel\nvehicles, linking them with the socio-economic status of the respective\nconsumers as well as the impact on the resulting air quality index. Research in\nthis area aims to leverage machine learning techniques in order to promote\nappropriate policies for the proliferation of alternative fuel vehicles such as\nelectric vehicles with due justice to different population groups. Pearson\ncorrelation coefficient is deployed in the modeling the relationships between\nsocio-economic data, air quality index and data on alternative fuel vehicles.\nLinear regression is used to conduct predictive modeling on air quality index\nas per the adoption of alternative fuel vehicles, based on socio-economic\nfactors. This work exemplifies artificial intelligence for social good.\n","authors":["Anuradha Singh","Jyoti Yadav","Sarahana Shrestha","Aparna S. Varde"],"pdf_url":"https://arxiv.org/pdf/2303.08286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10160v2","updated":"2023-03-15T00:14:26Z","published":"2023-02-20T18:46:12Z","title":"Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift","summary":"  We develop and analyze a principled approach to kernel ridge regression under\ncovariate shift. The goal is to learn a regression function with small mean\nsquared error over a target distribution, based on unlabeled data from there\nand labeled data that may have a different feature distribution. We propose to\nsplit the labeled data into two subsets and conduct kernel ridge regression on\nthem separately to obtain a collection of candidate models and an imputation\nmodel. We use the latter to fill the missing labels and then select the best\ncandidate model accordingly. Our non-asymptotic excess risk bounds show that in\nquite general scenarios, our estimator adapts to the structure of the target\ndistribution as well as the covariate shift. It achieves the minimax optimal\nerror rate up to a logarithmic factor. The use of pseudo-labels in model\nselection does not have major negative impacts.\n","authors":["Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2302.10160v2.pdf","comment":"41 pages, 1 figure"},{"id":"http://arxiv.org/abs/2210.07179v2","updated":"2023-03-15T00:02:06Z","published":"2022-10-13T17:02:23Z","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for\n  Vision-Language Few-Shot Prompting","summary":"  Large pre-trained models have proved to be remarkable zero- and\n(prompt-based) few-shot learners in unimodal vision and language tasks. We\npropose MAPL, a simple and parameter-efficient method that reuses frozen\npre-trained unimodal models and leverages their strong generalization\ncapabilities in multimodal vision-language (VL) settings. MAPL learns a\nlightweight mapping between the representation spaces of unimodal models using\naligned image-text data, and can generalize to unseen VL tasks from just a few\nin-context examples. The small number of trainable parameters makes MAPL\neffective at low-data and in-domain learning. Moreover, MAPL's modularity\nenables easy extension to other pre-trained models. Extensive experiments on\nseveral visual question answering and image captioning benchmarks show that\nMAPL achieves superior or competitive performance compared to similar methods\nwhile training orders of magnitude fewer parameters. MAPL can be trained in\njust a few hours using modest computational resources and public datasets. We\nrelease our code and pre-trained model weights at\nhttps://github.com/mair-lab/mapl.\n","authors":["Oscar Mañas","Pau Rodriguez","Saba Ahmadi","Aida Nematzadeh","Yash Goyal","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2210.07179v2.pdf","comment":"Accepted at EACL 2023 (main track); 26 pages, 21 figures, 6 tables;\n  Pau Rodriguez and Saba Ahmadi had equal contributions"},{"id":"http://arxiv.org/abs/2007.03481v6","updated":"2023-03-15T23:57:38Z","published":"2020-07-07T14:14:12Z","title":"Necessary and Sufficient Conditions for Inverse Reinforcement Learning\n  of Bayesian Stopping Time Problems","summary":"  This paper presents an inverse reinforcement learning~(IRL) framework for\nBayesian stopping time problems. By observing the actions of a Bayesian\ndecision maker, we provide a necessary and sufficient condition to identify if\nthese actions are consistent with optimizing a cost function. In a Bayesian\n(partially observed) setting, the inverse learner can at best identify\noptimality wrt the observed strategies. Our IRL algorithm identifies optimality\nand then constructs set-valued estimates of the cost function.To achieve this\nIRL objective, we use novel ideas from Bayesian revealed preferences stemming\nfrom microeconomics. We illustrate the proposed IRL scheme using two important\nexamples of stopping time problems, namely, sequential hypothesis testing and\nBayesian search. As a real-world example, we illustrate using a YouTube dataset\ncomprising metadata from 190000 videos how the proposed IRL method predicts\nuser engagement in online multimedia platforms with high accuracy. Finally, for\nfinite datasets, we propose an IRL detection algorithm and give finite sample\nbounds on its error probabilities.\n","authors":["Kunal Pattanayak","Vikram Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2007.03481v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08994v1","updated":"2023-03-15T23:53:32Z","published":"2023-03-15T23:53:32Z","title":"Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy,\n  Computational Cost, and Flexibility","summary":"  The simulation of power system dynamics poses a computationally expensive\ntask. Considering the growing uncertainty of generation and demand patterns,\nthousands of scenarios need to be continuously assessed to ensure the safety of\npower systems. Physics-Informed Neural Networks (PINNs) have recently emerged\nas a promising solution for drastically accelerating computations of non-linear\ndynamical systems. This work investigates the applicability of these methods\nfor power system dynamics, focusing on the dynamic response to load\ndisturbances. Comparing the prediction of PINNs to the solution of conventional\nsolvers, we find that PINNs can be 10 to 1000 times faster than conventional\nsolvers. At the same time, we find them to be sufficiently accurate and\nnumerically stable even for large time steps. To facilitate a deeper\nunderstanding, this paper also presents a new regularisation of Neural Network\n(NN) training by introducing a gradient-based term in the loss function. The\nresulting NNs, which we call dtNNs, help us deliver a comprehensive analysis\nabout the strengths and weaknesses of the NN based approaches, how\nincorporating knowledge of the underlying physics affects NN performance, and\nhow this compares with conventional solvers for power system dynamics.\n","authors":["Jochen Stiasny","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2303.08994v1.pdf","comment":"23 pages, 8 figures, submitted to Electric Power Systems Research"},{"id":"http://arxiv.org/abs/2303.08986v1","updated":"2023-03-15T23:19:45Z","published":"2023-03-15T23:19:45Z","title":"Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and\n  Reducing Overfitting","summary":"  In this work, we present some applications of random matrix theory for the\ntraining of deep neural networks. Recently, random matrix theory (RMT) has been\napplied to the overfitting problem in deep learning. Specifically, it has been\nshown that the spectrum of the weight layers of a deep neural network (DNN) can\nbe studied and understood using techniques from RMT. In this work, these RMT\ntechniques will be used to determine which and how many singular values should\nbe removed from the weight layers of a DNN during training, via singular value\ndecomposition (SVD), so as to reduce overfitting and increase accuracy. We show\nthe results on a simple DNN model trained on MNIST. In general, these\ntechniques may be applied to any fully connected layer of a pretrained DNN to\nreduce the number of parameters in the layer while preserving and sometimes\nincreasing the accuracy of the DNN.\n","authors":["Yitzchak Shmalo","Jonathan Jenkins","Oleksii Krupchytskyi"],"pdf_url":"https://arxiv.org/pdf/2303.08986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08984v1","updated":"2023-03-15T23:11:30Z","published":"2023-03-15T23:11:30Z","title":"Forecasting Particle Accelerator Interruptions Using Logistic LASSO\n  Regression","summary":"  Unforeseen particle accelerator interruptions, also known as interlocks, lead\nto abrupt operational changes despite being necessary safety measures. These\nmay result in substantial loss of beam time and perhaps even equipment damage.\nWe propose a simple yet powerful binary classification model aiming to forecast\nsuch interruptions, in the case of the High Intensity Proton Accelerator\ncomplex at the Paul Scherrer Institut. The model is formulated as logistic\nregression penalized by least absolute shrinkage and selection operator, based\non a statistical two sample test to distinguish between unstable and stable\nstates of the accelerator.\n  The primary objective for receiving alarms prior to interlocks is to allow\nfor countermeasures and reduce beam time loss. Hence, a continuous evaluation\nmetric is developed to measure the saved beam time in any period, given the\nassumption that interlocks could be circumvented by reducing the beam current.\nThe best-performing interlock-to-stable classifier can potentially increase the\nbeam time by around 5 min in a day. Possible instrumentation for fast\nadjustment of the beam current is also listed and discussed.\n","authors":["Sichen Li","Jochem Snuverink","Fernando Perez-Cruz","Andreas Adelmann"],"pdf_url":"https://arxiv.org/pdf/2303.08984v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.08983v1","updated":"2023-03-15T23:10:17Z","published":"2023-03-15T23:10:17Z","title":"Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness\n  with Dataset Reinforcement","summary":"  We propose Dataset Reinforcement, a strategy to improve a dataset once such\nthat the accuracy of any model architecture trained on the reinforced dataset\nis improved at no additional training cost for users. We propose a Dataset\nReinforcement strategy based on data augmentation and knowledge distillation.\nOur generic strategy is designed based on extensive analysis across CNN- and\ntransformer-based models and performing large-scale study of distillation with\nstate-of-the-art models with various data augmentations. We create a reinforced\nversion of the ImageNet training dataset, called ImageNet+, as well as\nreinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained\nwith ImageNet+ are more accurate, robust, and calibrated, and transfer well to\ndownstream tasks (e.g., segmentation and detection). As an example, the\naccuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on\nImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the\nImageNet validation set is also reduced by 9.9%. Using this backbone with\nMask-RCNN for object detection on MS-COCO, the mean average precision improves\nby 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.\nFor MobileNetV3 and Swin-Tiny we observe significant improvements on\nImageNet-R/A/C of up to 10% improved robustness. Models pretrained on ImageNet+\nand fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%\nimproved accuracy.\n","authors":["Fartash Faghri","Hadi Pouransari","Sachin Mehta","Mehrdad Farajtabar","Ali Farhadi","Mohammad Rastegari","Oncel Tuzel"],"pdf_url":"https://arxiv.org/pdf/2303.08983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13476v4","updated":"2023-03-15T23:04:33Z","published":"2022-09-27T15:50:31Z","title":"Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with\n  Extremely Limited Labels","summary":"  Recent studies on contrastive learning have achieved remarkable performance\nsolely by leveraging few labels in the context of medical image segmentation.\nExisting methods mainly focus on instance discrimination and invariant mapping.\nHowever, they face three common pitfalls: (1) tailness: medical image data\nusually follows an implicit long-tail class distribution. Blindly leveraging\nall pixels in training hence can lead to the data imbalance issues, and cause\ndeteriorated performance; (2) consistency: it remains unclear whether a\nsegmentation model has learned meaningful and yet consistent anatomical\nfeatures due to the intra-class variations between different anatomical\nfeatures; and (3) diversity: the intra-slice correlations within the entire\ndataset have received significantly less attention. This motivates us to seek a\nprincipled approach for strategically making use of the dataset itself to\ndiscover similar yet distinct samples from different anatomical views. In this\npaper, we introduce a novel semi-supervised 2D medical image segmentation\nframework termed Mine yOur owN Anatomy (MONA), and make three contributions.\nFirst, prior work argues that every pixel equally matters to the model\ntraining; we observe empirically that this alone is unlikely to define\nmeaningful anatomical features, mainly due to lacking the supervision signal.\nWe show two simple solutions towards learning invariances - through the use of\nstronger data augmentations and nearest neighbors. Second, we construct a set\nof objectives that encourage the model to be capable of decomposing medical\nimages into a collection of anatomical features in an unsupervised manner.\nLastly, we both empirically and theoretically, demonstrate the efficacy of our\nMONA on three benchmark datasets with different labeled settings, achieving new\nstate-of-the-art under different labeled semi-supervised settings\n","authors":["Chenyu You","Weicheng Dai","Fenglin Liu","Yifei Min","Haoran Su","Xiaoran Zhang","Xiaoxiao Li","David A. Clifton","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2209.13476v4.pdf","comment":"In this version: Add theoretical analysis"},{"id":"http://arxiv.org/abs/2303.08978v1","updated":"2023-03-15T22:58:23Z","published":"2023-03-15T22:58:23Z","title":"Active Semi-Supervised Learning by Exploring Per-Sample Uncertainty and\n  Consistency","summary":"  Active Learning (AL) and Semi-supervised Learning are two techniques that\nhave been studied to reduce the high cost of deep learning by using a small\namount of labeled data and a large amount of unlabeled data. To improve the\naccuracy of models at a lower cost, we propose a method called Active\nSemi-supervised Learning (ASSL), which combines AL and SSL. To maximize the\nsynergy between AL and SSL, we focused on the differences between ASSL and AL.\nASSL involves more dynamic model updates than AL due to the use of unlabeled\ndata in the training process, resulting in the temporal instability of the\npredicted probabilities of the unlabeled data. This makes it difficult to\ndetermine the true uncertainty of the unlabeled data in ASSL. To address this,\nwe adopted techniques such as exponential moving average (EMA) and upper\nconfidence bound (UCB) used in reinforcement learning. Additionally, we\nanalyzed the effect of label noise on unsupervised learning by using weak and\nstrong augmentation pairs to address datainconsistency. By considering both\nuncertainty and datainconsistency, we acquired data samples that were used in\nthe proposed ASSL method. Our experiments showed that ASSL achieved about 5.3\ntimes higher computational efficiency than SSL while achieving the same\nperformance, and it outperformed the state-of-the-art AL method.\n","authors":["Jaeseung Lim","Jongkeun Na","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2303.08978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.01636v2","updated":"2023-03-15T22:57:03Z","published":"2021-09-03T17:28:04Z","title":"Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding","summary":"  With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n","authors":["Xin Chen","Qi Zhao","Xinyang Liu"],"pdf_url":"https://arxiv.org/pdf/2109.01636v2.pdf","comment":"Want to review again"},{"id":"http://arxiv.org/abs/2106.13638v3","updated":"2023-03-15T22:50:15Z","published":"2021-06-25T13:42:09Z","title":"Transient Stability Analysis with Physics-Informed Neural Networks","summary":"  We explore the possibility to use physics-informed neural networks to\ndrastically accelerate the solution of ordinary differential-algebraic\nequations that govern the power system dynamics. When it comes to transient\nstability assessment, the traditionally applied methods either carry a\nsignificant computational burden, require model simplifications, or use overly\nconservative surrogate models. Conventional neural networks can circumvent\nthese limitations but are faced with high demand of high-quality training\ndatasets, while they ignore the underlying governing equations.\nPhysics-informed neural networks are different: they incorporate the power\nsystem differential algebraic equations directly into the neural network\ntraining and drastically reduce the need for training data. This paper takes a\ndeep dive into the performance of physics-informed neural networks for power\nsystem transient stability assessment. Introducing a new neural network\ntraining procedure to facilitate a thorough comparison, we explore how\nphysics-informed neural networks compare with conventional\ndifferential-algebraic solvers and classical neural networks in terms of\ncomputation time, requirements in data, and prediction accuracy. We illustrate\nthe findings on the Kundur two-area system, and assess the opportunities and\nchallenges of physics-informed neural networks to serve as a transient\nstability analysis tool, highlighting possible pathways to further develop this\nmethod.\n","authors":["Jochen Stiasny","Georgios S. Misyris","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2106.13638v3.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.08970v1","updated":"2023-03-15T22:46:22Z","published":"2023-03-15T22:46:22Z","title":"Gated Compression Layers for Efficient Always-On Models","summary":"  Mobile and embedded machine learning developers frequently have to compromise\nbetween two inferior on-device deployment strategies: sacrifice accuracy and\naggressively shrink their models to run on dedicated low-power cores; or\nsacrifice battery by running larger models on more powerful compute cores such\nas neural processing units or the main application processor. In this paper, we\npropose a novel Gated Compression layer that can be applied to transform\nexisting neural network architectures into Gated Neural Networks. Gated Neural\nNetworks have multiple properties that excel for on-device use cases that help\nsignificantly reduce power, boost accuracy, and take advantage of heterogeneous\ncompute cores. We provide results across five public image and audio datasets\nthat demonstrate the proposed Gated Compression layer effectively stops up to\n96% of negative samples, compresses 97% of positive samples, while maintaining\nor improving model accuracy.\n","authors":["Haiguang Li","Trausti Thormundsson","Ivan Poupyrev","Nicholas Gillian"],"pdf_url":"https://arxiv.org/pdf/2303.08970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.14375v4","updated":"2023-03-15T22:37:45Z","published":"2022-05-28T09:08:50Z","title":"WaveMix: A Resource-efficient Neural Network for Image Analysis","summary":"  We propose WaveMix -- a novel neural architecture for computer vision that is\nresource-efficient yet generalizable and scalable. WaveMix networks achieve\ncomparable or better accuracy than the state-of-the-art convolutional neural\nnetworks, vision transformers, and token mixers for several tasks, establishing\nnew benchmarks for segmentation on Cityscapes; and for classification on\nPlaces-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix\narchitectures require fewer parameters to achieve these benchmarks compared to\nthe previous state-of-the-art. Moreover, when controlled for the number of\nparameters, WaveMix requires lesser GPU RAM, which translates to savings in\ntime, cost, and energy. To achieve these gains we used multi-level\ntwo-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which\nhas the following advantages: (1) It reorganizes spatial information based on\nthree strong image priors -- scale-invariance, shift-invariance, and sparseness\nof edges, (2) in a lossless manner without adding parameters, (3) while also\nreducing the spatial sizes of feature maps, which reduces the memory and time\nrequired for forward and backward passes, and (4) expanding the receptive field\nfaster than convolutions do. The whole architecture is a stack of self-similar\nand resolution-preserving WaveMix blocks, which allows architectural\nflexibility for various tasks and levels of resource availability. Our code and\ntrained models are publicly available.\n","authors":["Pranav Jeevan","Kavitha Viswanathan","Anandu A S","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2205.14375v4.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.08964v1","updated":"2023-03-15T22:23:32Z","published":"2023-03-15T22:23:32Z","title":"CS-TGN: Community Search via Temporal Graph Neural Networks","summary":"  Searching for local communities is an important research challenge that\nallows for personalized community discovery and supports advanced data analysis\nin various complex networks, such as the World Wide Web, social networks, and\nbrain networks. The evolution of these networks over time has motivated several\nrecent studies to identify local communities in temporal networks. Given any\nquery nodes, Community Search aims to find a densely connected subgraph\ncontaining query nodes. However, existing community search approaches in\ntemporal networks have two main limitations: (1) they adopt pre-defined\nsubgraph patterns to model communities, which cannot find communities that do\nnot conform to these patterns in real-world networks, and (2) they only use the\naggregation of disjoint structural information to measure quality, missing the\ndynamic of connections and temporal properties. In this paper, we propose a\nquery-driven Temporal Graph Convolutional Network (CS-TGN) that can capture\nflexible community structures by learning from the ground-truth communities in\na data-driven manner. CS-TGN first combines the local query-dependent structure\nand the global graph embedding in each snapshot of the network and then uses a\nGRU cell with contextual attention to learn the dynamics of interactions and\nupdate node embeddings over time. We demonstrate how this model can be used for\ninteractive community search in an online setting, allowing users to evaluate\nthe found communities and provide feedback. Experiments on real-world temporal\ngraphs with ground-truth communities validate the superior quality of the\nsolutions obtained and the efficiency of our model in both temporal and\ninteractive static settings.\n","authors":["Farnoosh Hashemi","Ali Behrouz","Milad Rezaei Hajidehi"],"pdf_url":"https://arxiv.org/pdf/2303.08964v1.pdf","comment":"This is the author's version of the paper. Published in companion\n  proceedings of the ACM Web Conference 2023 (WWW '23 Companion)"},{"id":"http://arxiv.org/abs/2303.08958v1","updated":"2023-03-15T22:14:28Z","published":"2023-03-15T22:14:28Z","title":"NESS: Learning Node Embeddings from Static SubGraphs","summary":"  We present a framework for learning Node Embeddings from Static Subgraphs\n(NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we\npropose a novel approach for contrastive learning in the same setting. We\ndemonstrate that using static subgraphs during training with a GAE improves\nnode representation for link prediction tasks compared to current autoencoding\nmethods using the entire graph or stochastic subgraphs. NESS consists of two\nsteps: 1) Partitioning the training graph into subgraphs using random edge\nsplit (RES) during data pre-processing, and 2) Aggregating the node\nrepresentations learned from each subgraph to obtain a joint representation of\nthe graph at test time. Our experiments show that NESS improves the performance\nof a wide range of graph encoders and achieves state-of-the-art (SOTA) results\nfor link prediction on multiple benchmark datasets.\n","authors":["Talip Ucar"],"pdf_url":"https://arxiv.org/pdf/2303.08958v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.08955v1","updated":"2023-03-15T21:55:07Z","published":"2023-03-15T21:55:07Z","title":"Large-scale End-of-Life Prediction of Hard Disks in Distributed\n  Datacenters","summary":"  On a daily basis, data centers process huge volumes of data backed by the\nproliferation of inexpensive hard disks. Data stored in these disks serve a\nrange of critical functional needs from financial, and healthcare to aerospace.\nAs such, premature disk failure and consequent loss of data can be\ncatastrophic. To mitigate the risk of failures, cloud storage providers perform\ncondition-based monitoring and replace hard disks before they fail. By\nestimating the remaining useful life of hard disk drives, one can predict the\ntime-to-failure of a particular device and replace it at the right time,\nensuring maximum utilization whilst reducing operational costs. In this work,\nlarge-scale predictive analyses are performed using severely skewed health\nstatistics data by incorporating customized feature engineering and a suite of\nsequence learners. Past work suggests using LSTMs as an excellent approach to\npredicting remaining useful life. To this end, we present an encoder-decoder\nLSTM model where the context gained from understanding health statistics\nsequences aid in predicting an output sequence of the number of days remaining\nbefore a disk potentially fails. The models developed in this work are trained\nand tested across an exhaustive set of all of the 10 years of S.M.A.R.T. health\ndata in circulation from Backblaze and on a wide variety of disk instances. It\ncloses the knowledge gap on what full-scale training achieves on thousands of\ndevices and advances the state-of-the-art by providing tangible metrics for\nevaluation and generalization for practitioners looking to extend their\nworkflow to all years of health data in circulation across disk manufacturers.\nThe encoder-decoder LSTM posted an RMSE of 0.83 on an exhaustive set while\nbeing able to generalize competitively over the other Seagate family hard\ndrives.\n","authors":["Rohan Mohapatra","Austin Coursey","Saptarshi Sengupta"],"pdf_url":"https://arxiv.org/pdf/2303.08955v1.pdf","comment":"10 pages, 10 figures and 6 tables"},{"id":"http://arxiv.org/abs/2109.09500v3","updated":"2023-03-15T21:52:20Z","published":"2021-09-20T12:53:01Z","title":"Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale\n  Confirmatory Item Factor Analysis","summary":"  We investigate novel parameter estimation and goodness-of-fit (GOF)\nassessment methods for large-scale confirmatory item factor analysis (IFA) with\nmany respondents, items, and latent factors. For parameter estimation, we\nextend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to\nthe confirmatory setting by showing how to handle constraints on loadings and\nfactor correlations. For GOF assessment, we explore simulation-based tests and\nindices that extend the classifier two-sample test (C2ST), a method that tests\nwhether a deep neural network can distinguish between observed data and\nsynthetic data sampled from a fitted IFA model. Proposed extensions include a\ntest of approximate fit wherein the user specifies what percentage of observed\nand synthetic data should be distinguishable as well as a relative fit index\n(RFI) that is similar in spirit to the RFIs used in structural equation\nmodeling. Via simulation studies, we show that: (1) the confirmatory extension\nof Urban and Bauer's (2021) algorithm obtains comparable estimates to a\nstate-of-the-art estimation procedure in less time; (2) C2ST-based GOF tests\ncontrol the empirical type I error rate and detect when the latent\ndimensionality is misspecified; and (3) the sampling distribution of the\nC2ST-based RFI depends on the sample size.\n","authors":["Christopher J. Urban","Daniel J. Bauer"],"pdf_url":"https://arxiv.org/pdf/2109.09500v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.08232v3","updated":"2023-03-15T21:50:38Z","published":"2021-05-18T02:17:59Z","title":"Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact\n  Parameterized and the Overparameterized Regimes","summary":"  The matrix sensing problem is an important low-rank optimization problem that\nhas found a wide range of applications, such as matrix completion, phase\nsynchornization/retrieval, robust PCA, and power system state estimation. In\nthis work, we focus on the general matrix sensing problem with linear\nmeasurements that are corrupted by random noise. We investigate the scenario\nwhere the search rank $r$ is equal to the true rank $r^*$ of the unknown ground\ntruth (the exact parametrized case), as well as the scenario where $r$ is\ngreater than $r^*$ (the overparametrized case). We quantify the role of the\nrestricted isometry property (RIP) in shaping the landscape of the non-convex\nfactorized formulation and assisting with the success of local search\nalgorithms. First, we develop a global guarantee on the maximum distance\nbetween an arbitrary local minimizer of the non-convex problem and the ground\ntruth under the assumption that the RIP constant is smaller than\n$1/(1+\\sqrt{r^*/r})$. We then present a local guarantee for problems with an\narbitrary RIP constant, which states that any local minimizer is either\nconsiderably close to the ground truth or far away from it. More importantly,\nwe prove that this noisy, overparametrized problem exhibits the strict saddle\nproperty, which leads to the global convergence of perturbed gradient descent\nalgorithm in polynomial time. The results of this work provide a comprehensive\nunderstanding of the geometric landscape of the matrix sensing problem in the\nnoisy and overparametrized regime.\n","authors":["Ziye Ma","Yingjie Bi","Javad Lavaei","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2105.08232v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08951v1","updated":"2023-03-15T21:46:35Z","published":"2023-03-15T21:46:35Z","title":"The Tiny Time-series Transformer: Low-latency High-throughput\n  Classification of Astronomical Transients using Deep Model Compression","summary":"  A new golden age in astronomy is upon us, dominated by data. Large\nastronomical surveys are broadcasting unprecedented rates of information,\ndemanding machine learning as a critical component in modern scientific\npipelines to handle the deluge of data. The upcoming Legacy Survey of Space and\nTime (LSST) of the Vera C. Rubin Observatory will raise the big-data bar for\ntime-domain astronomy, with an expected 10 million alerts per-night, and\ngenerating many petabytes of data over the lifetime of the survey. Fast and\nefficient classification algorithms that can operate in real-time, yet robustly\nand accurately, are needed for time-critical events where additional resources\ncan be sought for follow-up analyses. In order to handle such data,\nstate-of-the-art deep learning architectures coupled with tools that leverage\nmodern hardware accelerators are essential. We showcase how the use of modern\ndeep compression methods can achieve a $18\\times$ reduction in model size,\nwhilst preserving classification performance. We also show that in addition to\nthe deep compression techniques, careful choice of file formats can improve\ninference latency, and thereby throughput of alerts, on the order of $8\\times$\nfor local processing, and $5\\times$ in a live production setting. To test this\nin a live setting, we deploy this optimised version of the original time-series\ntransformer, t2, into the community alert broking system of FINK on real Zwicky\nTransient Facility (ZTF) alert data, and compare throughput performance with\nother science modules that exist in FINK. The results shown herein emphasise\nthe time-series transformer's suitability for real-time classification at LSST\nscale, and beyond, and introduce deep model compression as a fundamental tool\nfor improving deploy-ability and scalable inference of deep learning models for\ntransient classification.\n","authors":["Tarek Allam Jr.","Julien Peloton","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2303.08951v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2203.03899v3","updated":"2023-03-15T21:35:43Z","published":"2022-03-08T07:44:47Z","title":"Noisy Low-rank Matrix Optimization: Geometry of Local Minima and\n  Convergence Rate","summary":"  This paper is concerned with low-rank matrix optimization, which has found a\nwide range of applications in machine learning. This problem in the special\ncase of matrix sensing has been studied extensively through the notion of\nRestricted Isometry Property (RIP), leading to a wealth of results on the\ngeometric landscape of the problem and the convergence rate of common\nalgorithms. However, the existing results can handle the problem in the case\nwith a general objective function subject to noisy data only when the RIP\nconstant is close to 0. In this paper, we develop a new mathematical framework\nto solve the above-mentioned problem with a far less restrictive RIP constant.\nWe prove that as long as the RIP constant of the noiseless objective is less\nthan $1/3$, any spurious local solution of the noisy optimization problem must\nbe close to the ground truth solution. By working through the strict saddle\nproperty, we also show that an approximate solution can be found in polynomial\ntime. We characterize the geometry of the spurious local minima of the problem\nin a local region around the ground truth in the case when the RIP constant is\ngreater than $1/3$. Compared to the existing results in the literature, this\npaper offers the strongest RIP bound and provides a complete theoretical\nanalysis on the global and local optimization landscapes of general low-rank\noptimization problems under random corruptions from any finite-variance family.\n","authors":["Ziye Ma","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2203.03899v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11775v3","updated":"2023-03-15T21:35:33Z","published":"2022-05-24T04:26:10Z","title":"Constrained Monotonic Neural Networks","summary":"  Deep neural networks are becoming increasingly popular in approximating\narbitrary functions from noisy data. But wider adoption is being hindered by\nthe need to explain such models and to impose additional constraints on them.\nMonotonicity constraint is one of the most requested properties in real-world\nscenarios and is the focus of this paper. One of the oldest ways to construct a\nmonotonic fully connected neural network is to constrain its weights to be\nnon-negative while employing a monotonic activation function. Unfortunately,\nthis construction does not work with popular non-saturated activation functions\nsuch as ReLU, ELU, SELU etc, as it can only approximate convex functions. We\nshow this shortcoming can be fixed by employing the original activation\nfunction for a part of the neurons in the layer, and employing its point\nreflection for the other part. Our experiments show this approach of building\nmonotonic deep neural networks have matching or better accuracy when compared\nto other state-of-the-art methods such as deep lattice networks or monotonic\nnetworks obtained by heuristic regularization. This method is the simplest one\nin the sense of having the least number of parameters, not requiring any\nmodifications to the learning procedure or steps post-learning steps.\n","authors":["Davor Runje","Sharath M. Shankaranarayana"],"pdf_url":"https://arxiv.org/pdf/2205.11775v3.pdf","comment":"too many typos and errors in statements"},{"id":"http://arxiv.org/abs/2303.08944v1","updated":"2023-03-15T21:30:14Z","published":"2023-03-15T21:30:14Z","title":"Certifiable (Multi)Robustness Against Patch Attacks Using ERM","summary":"  Consider patch attacks, where at test-time an adversary manipulates a test\nimage with a patch in order to induce a targeted misclassification. We consider\na recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The\nPatch-Cleanser algorithm requires a prediction model to have a ``two-mask\ncorrectness'' property, meaning that the prediction model should correctly\nclassify any image when any two blank masks replace portions of the image.\nXiang et al. learn a prediction model to be robust to two-mask operations by\naugmenting the training set with pairs of masks at random locations of training\nimages and performing empirical risk minimization (ERM) on the augmented\ndataset.\n  However, in the non-realizable setting when no predictor is perfectly correct\non all two-mask operations on all images, we exhibit an example where ERM\nfails. To overcome this challenge, we propose a different algorithm that\nprovably learns a predictor robust to all two-mask operations using an ERM\noracle, based on prior work by Feige et al. [2015]. We also extend this result\nto a multiple-group setting, where we can learn a predictor that achieves low\nrobust loss on all groups simultaneously.\n","authors":["Saba Ahmadi","Avrim Blum","Omar Montasser","Kevin Stangl"],"pdf_url":"https://arxiv.org/pdf/2303.08944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06438v2","updated":"2023-03-15T21:01:47Z","published":"2023-03-11T16:29:13Z","title":"On Neural Architectures for Deep Learning-based Source Separation of\n  Co-Channel OFDM Signals","summary":"  We study the single-channel source separation problem involving orthogonal\nfrequency-division multiplexing (OFDM) signals, which are ubiquitous in many\nmodern-day digital communication systems. Related efforts have been pursued in\nmonaural source separation, where state-of-the-art neural architectures have\nbeen adopted to train an end-to-end separator for audio signals (as\n1-dimensional time series). In this work, through a prototype problem based on\nthe OFDM source model, we assess -- and question -- the efficacy of using\naudio-oriented neural architectures in separating signals based on features\npertinent to communication waveforms. Perhaps surprisingly, we demonstrate that\nin some configurations, where perfect separation is theoretically attainable,\nthese audio-oriented neural architectures perform poorly in separating\nco-channel OFDM waveforms. Yet, we propose critical domain-informed\nmodifications to the network parameterization, based on insights from OFDM\nstructures, that can confer about 30 dB improvement in performance.\n","authors":["Gary C. F. Lee","Amir Weiss","Alejandro Lancho","Yury Polyanskiy","Gregory W. Wornell"],"pdf_url":"https://arxiv.org/pdf/2303.06438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08932v1","updated":"2023-03-15T20:57:31Z","published":"2023-03-15T20:57:31Z","title":"Enhancing Data Space Semantic Interoperability through Machine Learning:\n  a Visionary Perspective","summary":"  Our vision paper outlines a plan to improve the future of semantic\ninteroperability in data spaces through the application of machine learning.\nThe use of data spaces, where data is exchanged among members in a\nself-regulated environment, is becoming increasingly popular. However, the\ncurrent manual practices of managing metadata and vocabularies in these spaces\nare time-consuming, prone to errors, and may not meet the needs of all\nstakeholders. By leveraging the power of machine learning, we believe that\nsemantic interoperability in data spaces can be significantly improved. This\ninvolves automatically generating and updating metadata, which results in a\nmore flexible vocabulary that can accommodate the diverse terminologies used by\ndifferent sub-communities. Our vision for the future of data spaces addresses\nthe limitations of conventional data exchange and makes data more accessible\nand valuable for all members of the community.\n","authors":["Zeyd Boukhers","Christoph Lange","Oya Beyan"],"pdf_url":"https://arxiv.org/pdf/2303.08932v1.pdf","comment":"Accepted for publication @ The First International Workshop on\n  Semantics in Dataspaces (In conjunction with The Web Conference - WWW 2023)"},{"id":"http://arxiv.org/abs/2303.08928v1","updated":"2023-03-15T20:55:25Z","published":"2023-03-15T20:55:25Z","title":"Applying unsupervised keyphrase methods on concepts extracted from\n  discharge sheets","summary":"  Clinical notes containing valuable patient information are written by\ndifferent health care providers with various scientific levels and writing\nstyles. It might be helpful for clinicians and researchers to understand what\ninformation is essential when dealing with extensive electronic medical\nrecords. Entities recognizing and mapping them to standard terminologies is\ncrucial in reducing ambiguity in processing clinical notes. Although named\nentity recognition and entity linking are critical steps in clinical natural\nlanguage processing, they can also result in the production of repetitive and\nlow-value concepts. In other hand, all parts of a clinical text do not share\nthe same importance or content in predicting the patient's condition. As a\nresult, it is necessary to identify the section in which each content is\nrecorded and also to identify key concepts to extract meaning from clinical\ntexts. In this study, these challenges have been addressed by using clinical\nnatural language processing techniques. In addition, in order to identify key\nconcepts, a set of popular unsupervised key phrase extraction methods has been\nverified and evaluated. Considering that most of the clinical concepts are in\nthe form of multi-word expressions and their accurate identification requires\nthe user to specify n-gram range, we have proposed a shortcut method to\npreserve the structure of the expression based on TF-IDF. In order to evaluate\nthe pre-processing method and select the concepts, we have designed two types\nof downstream tasks (multiple and binary classification) using the capabilities\nof transformer-based models. The obtained results show the superiority of\nproposed method in combination with SciBERT model, also offer an insight into\nthe efficacy of general extracting essential phrase methods for clinical notes.\n","authors":["Hoda Memarzadeh","Nasser Ghadiri","Matthias Samwald","Maryam Lotfi Shahreza"],"pdf_url":"https://arxiv.org/pdf/2303.08928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.15278v3","updated":"2023-03-15T20:53:33Z","published":"2021-06-29T11:51:57Z","title":"Open-Set Representation Learning through Combinatorial Embedding","summary":"  Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on both labeled and unlabeled examples, and\nextending the horizon of recognition to both known and novel classes. To\naddress this challenging task, we propose a combinatorial learning approach,\nwhich naturally clusters the examples in unseen classes using the compositional\nknowledge given by multiple supervised meta-classifiers on heterogeneous label\nspaces. The representations given by the combinatorial embedding are made more\nrobust by unsupervised pairwise relation learning. The proposed algorithm\ndiscovers novel concepts via a joint optimization for enhancing the\ndiscrimitiveness of unseen classes as well as learning the representations of\nknown classes generalizable to novel ones. Our extensive experiments\ndemonstrate remarkable performance gains by the proposed approach on public\ndatasets for image retrieval and image categorization with novel class\ndiscovery.\n","authors":["Geeho Kim","Junoh Kang","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2106.15278v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.08189v2","updated":"2023-03-15T20:45:23Z","published":"2022-12-15T23:21:49Z","title":"Multi-Resolution Online Deterministic Annealing: A Hierarchical and\n  Progressive Learning Architecture","summary":"  Hierarchical learning algorithms that gradually approximate a solution to a\ndata-driven optimization problem are essential to decision-making systems,\nespecially under limitations on time and computational resources. In this\nstudy, we introduce a general-purpose hierarchical learning architecture that\nis based on the progressive partitioning of a possibly multi-resolution data\nspace. The optimal partition is gradually approximated by solving a sequence of\noptimization sub-problems that yield a sequence of partitions with increasing\nnumber of subsets. We show that the solution of each optimization problem can\nbe estimated online using gradient-free stochastic approximation updates. As a\nconsequence, a function approximation problem can be defined within each subset\nof the partition and solved using the theory of two-timescale stochastic\napproximation algorithms. This simulates an annealing process and defines a\nrobust and interpretable heuristic method to gradually increase the complexity\nof the learning architecture in a task-agnostic manner, giving emphasis to\nregions of the data space that are considered more important according to a\npredefined criterion. Finally, by imposing a tree structure in the progression\nof the partitions, we provide a means to incorporate potential multi-resolution\nstructure of the data space into this approach, significantly reducing its\ncomplexity, while introducing hierarchical variable-rate feature extraction\nproperties similar to certain classes of deep learning architectures.\nAsymptotic convergence analysis and experimental results are provided for\nsupervised and unsupervised learning problems.\n","authors":["Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.08189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07201v2","updated":"2023-03-15T20:32:23Z","published":"2022-12-14T12:59:25Z","title":"Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice\n  Reduction","summary":"  The circular coordinates algorithm of de Silva, Morozov, and\nVejdemo-Johansson takes as input a dataset together with a cohomology class\nrepresenting a $1$-dimensional hole in the data; the output is a map from the\ndata into the circle that captures this hole, and that is of minimum energy in\na suitable sense. However, when applied to several cohomology classes, the\noutput circle-valued maps can be \"geometrically correlated\" even if the chosen\ncohomology classes are linearly independent. It is shown in the original work\nthat less correlated maps can be obtained with suitable integer linear\ncombinations of the cohomology classes, with the linear combinations being\nchosen by inspection. In this paper, we identify a formal notion of geometric\ncorrelation between circle-valued maps which, in the Riemannian manifold case,\ncorresponds to the Dirichlet form, a bilinear form derived from the Dirichlet\nenergy. We describe a systematic procedure for constructing low energy\ntorus-valued maps on data, starting from a set of linearly independent\ncohomology classes. We showcase our procedure with computational examples. Our\nmain algorithm is based on the Lenstra--Lenstra--Lov\\'asz algorithm from\ncomputational number theory.\n","authors":["Luis Scoccola","Hitesh Gakhar","Johnathan Bush","Nikolas Schonsheck","Tatum Rask","Ling Zhou","Jose A. Perea"],"pdf_url":"https://arxiv.org/pdf/2212.07201v2.pdf","comment":"24 pages, 12 figures. To appear in proceedings of 39th International\n  Symposium on Computational Geometry"},{"id":"http://arxiv.org/abs/2303.08915v1","updated":"2023-03-15T20:18:27Z","published":"2023-03-15T20:18:27Z","title":"LRDB: LSTM Raw data DNA Base-caller based on long-short term models in\n  an active learning environment","summary":"  The first important step in extracting DNA characters is using the output\ndata of MinION devices in the form of electrical current signals. Various\ncutting-edge base callers use this data to detect the DNA characters based on\nthe input. In this paper, we discuss several shortcomings of prior base callers\nin the case of time-critical applications, privacy-aware design, and the\nproblem of catastrophic forgetting. Next, we propose the LRDB model, a\nlightweight open-source model for private developments with a better\nread-identity (0.35% increase) for the target bacterial samples in the paper.\nWe have limited the extent of training data and benefited from the transfer\nlearning algorithm to make the active usage of the LRDB viable in critical\napplications. Henceforth, less training time for adapting to new DNA samples\n(in our case, Bacterial samples) is needed. Furthermore, LRDB can be modified\nconcerning the user constraints as the results show a negligible accuracy loss\nin case of using fewer parameters. We have also assessed the noise-tolerance\nproperty, which offers about a 1.439% decline in accuracy for a 15dB noise\ninjection, and the performance metrics show that the model executes in a medium\nspeed range compared with current cutting-edge models.\n","authors":["Ahmad Rezaei","Mahdi Taheri","Ali Mahani","Sebastian Magierowski"],"pdf_url":"https://arxiv.org/pdf/2303.08915v1.pdf","comment":"12 figures, 6 table"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.08737v1","updated":"2023-03-15T16:21:50Z","published":"2023-03-15T16:21:50Z","title":"Evaluating gesture-generation in a large-scale open challenge: The GENEA\n  Challenge 2022","summary":"  This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems. The\ndataset was based on 18 hours of full-body motion capture, including fingers,\nof different persons engaging in a dyadic conversation. Ten teams participated\nin the challenge across two tiers: full-body and upper-body gesticulation. For\neach tier, we evaluated both the human-likeness of the gesture motion and its\nappropriateness for the specific speech signal. Our evaluations decouple\nhuman-likeness from gesture appropriateness, which has been a difficult problem\nin the field.\n  The evaluation results are a revolution, and a revelation. Some synthetic\nconditions are rated as significantly more human-like than human motion\ncapture. To the best of our knowledge, this has never been shown before on a\nhigh-fidelity avatar. On the other hand, all synthetic motion is found to be\nvastly less appropriate for the speech than the original motion-capture\nrecordings. We also find that conventional objective metrics do not correlate\nwell with subjective human-likeness ratings in this large evaluation. The one\nexception is the Fr\\'echet gesture distance (FGD), which achieves a Kendall's\ntau rank correlation of around -0.5. Based on the challenge results we\nformulate numerous recommendations for system building and evaluation.\n","authors":["Taras Kucherenko","Pieter Wolfert","Youngwoo Yoon","Carla Viegas","Teodor Nikolov","Mihail Tsakov","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2303.08737v1.pdf","comment":"The first three authors made equal contributions and share joint\n  first authorship. arXiv admin note: substantial text overlap with\n  arXiv:2208.10441"},{"id":"http://arxiv.org/abs/2303.08536v1","updated":"2023-03-15T11:29:36Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v1.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2303.08409v1","updated":"2023-03-15T07:21:28Z","published":"2023-03-15T07:21:28Z","title":"Lana: A Language-Capable Navigator for Instruction Following and\n  Generation","summary":"  Recently, visual-language navigation (VLN) -- entailing robot agents to\nfollow navigation instructions -- has shown great advance. However, existing\nliterature put most emphasis on interpreting instructions into actions, only\ndelivering \"dumb\" wayfinding agents. In this article, we devise LANA, a\nlanguage-capable navigation agent which is able to not only execute\nhuman-written navigation commands, but also provide route descriptions to\nhumans. This is achieved by simultaneously learning instruction following and\ngeneration with only one single model. More specifically, two encoders,\nrespectively for route and language encoding, are built and shared by two\ndecoders, respectively, for action prediction and instruction generation, so as\nto exploit cross-task knowledge and capture task-specific characteristics.\nThroughout pretraining and fine-tuning, both instruction following and\ngeneration are set as optimization objectives. We empirically verify that,\ncompared with recent advanced task-specific solutions, LANA attains better\nperformances on both instruction following and route description, with nearly\nhalf complexity. In addition, endowed with language generation capability, LANA\ncan explain to humans its behaviors and assist human's wayfinding. This work is\nexpected to foster future efforts towards building more trustworthy and\nsocially-intelligent navigation robots.\n","authors":["Xiaohan Wang","Wenguan Wang","Jiayi Shao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08409v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08336v1","updated":"2023-03-15T02:54:27Z","published":"2023-03-15T02:54:27Z","title":"Progressive Frame Patching for FoV-based Point Cloud Video Streaming","summary":"  Immersive multimedia applications, such as Virtual, Augmented and Mixed\nReality, have become more practical with advances in hardware and software for\nacquiring and rendering 3D media as well as 5G/6G wireless networks. Such\napplications require the delivery of volumetric video to users with six degrees\nof freedom (6-DoF) movements. Point Cloud has become a popular volumetric video\nformat due to its flexibility and simplicity. A dense point cloud consumes much\nhigher bandwidth than a 2D/360 degree video frame. User Field of View (FoV) is\nmore dynamic with 6-DoF movement than 3-DoF movement. A user's view quality of\na 3D object is affected by points occlusion and distance, which are constantly\nchanging with user and object movements. To save bandwidth, FoV-adaptive\nstreaming predicts user FoV and only downloads the data falling in the\npredicted FoV, but it is vulnerable to FoV prediction errors, which is\nsignificant when a long buffer is used for smoothed streaming. In this work, we\npropose a multi-round progressive refinement framework for point cloud-based\nvolumetric video streaming. Instead of sequentially downloading frames, we\nsimultaneously downloads/patches multiple frames falling into a sliding\ntime-window, leveraging on the scalability of point-cloud coding. The rate\nallocation among all tiles of active frames are solved analytically using the\nheterogeneous tile utility functions calibrated by the predicted user FoV.\nMulti-frame patching takes advantage of the streaming smoothness resulted from\nlong buffer and the FoV prediction accuracy at short buffer length. We evaluate\nour solution using simulations driven by real point cloud videos, bandwidth\ntraces and 6-DoF FoV traces of real users. The experiments show that our\nsolution is robust against bandwidth/FoV prediction errors, and can deliver\nhigh and smooth quality in the face of bandwidth variations and dynamic user\nmovements.\n","authors":["Tongyu Zong","Yixiang Mao","Chen Li","Yong Liu","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08318v1","updated":"2023-03-15T02:13:34Z","published":"2023-03-15T02:13:34Z","title":"Micro-video Tagging via Jointly Modeling Social Influence and Tag\n  Relation","summary":"  The last decade has witnessed the proliferation of micro-videos on various\nuser-generated content platforms. According to our statistics, around 85.7\\% of\nmicro-videos lack annotation. In this paper, we focus on annotating\nmicro-videos with tags. Existing methods mostly focus on analyzing video\ncontent, neglecting users' social influence and tag relation. Meanwhile,\nexisting tag relation construction methods suffer from either deficient\nperformance or low tag coverage. To jointly model social influence and tag\nrelation, we formulate micro-video tagging as a link prediction problem in a\nconstructed heterogeneous network. Specifically, the tag relation (represented\nby tag ontology) is constructed in a semi-supervised manner. Then, we combine\ntag relation, video-tag annotation, and user-follow relation to build the\nnetwork. Afterward, a better video and tag representation are derived through\nBehavior Spread modeling and visual and linguistic knowledge aggregation.\nFinally, the semantic similarity between each micro-video and all candidate\ntags is calculated in this video-tag network. Extensive experiments on\nindustrial datasets of three verticals verify the superiority of our model\ncompared with several state-of-the-art baselines.\n","authors":["Xiao Wang","Tian Gan","Yinwei Wei","Jianlong Wu","Dai Meng","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2303.08318v1.pdf","comment":"Accepted by Proceedings of the 30th ACM International Conference on\n  Multimedia (2022)"},{"id":"http://arxiv.org/abs/2212.04979v3","updated":"2023-03-15T06:48:23Z","published":"2022-12-09T16:39:09Z","title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive\n  Captioners","summary":"  We explore an efficient approach to establish a foundational video-text\nmodel. We present VideoCoCa that maximally reuses a pretrained image-text\ncontrastive captioner (CoCa) model and adapt it to video-text tasks with\nminimal extra training. While previous works adapt image-text models with\nvarious cross-frame fusion modules, we find that the generative attentional\npooling and contrastive attentional pooling layers in CoCa are instantly\nadaptable to flattened frame embeddings, yielding state-of-the-art results on\nzero-shot video classification and zero-shot text-to-video retrieval.\nFurthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve\nstrong results on video question-answering and video captioning.\n","authors":["Shen Yan","Tao Zhu","Zirui Wang","Yuan Cao","Mi Zhang","Soham Ghosh","Yonghui Wu","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2212.04979v3.pdf","comment":"Tech report. arXiv v3: update text"},{"id":"http://arxiv.org/abs/2303.09272v1","updated":"2023-03-15T06:40:57Z","published":"2023-03-15T06:40:57Z","title":"Copyright Protection and Accountability of Generative AI:Attack,\n  Watermarking and Attribution","summary":"  Generative AI (e.g., Generative Adversarial Networks - GANs) has become\nincreasingly popular in recent years. However, Generative AI introduces\nsignificant concerns regarding the protection of Intellectual Property Rights\n(IPR) (resp. model accountability) pertaining to images (resp. toxic images)\nand models (resp. poisoned models) generated. In this paper, we propose an\nevaluation framework to provide a comprehensive overview of the current state\nof the copyright protection measures for GANs, evaluate their performance\nacross a diverse range of GAN architectures, and identify the factors that\naffect their performance and future research directions. Our findings indicate\nthat the current IPR protection methods for input images, model watermarking,\nand attribution networks are largely satisfactory for a wide range of GANs. We\nhighlight that further attention must be directed towards protecting training\nsets, as the current approaches fail to provide robust IPR protection and\nprovenance tracing on training sets.\n","authors":["Haonan Zhong","Jiamin Chang","Ziyue Yang","Tingmin Wu","Pathum Chamikara Mahawaga Arachchige","Chehara Pathmabandu","Minhui Xue"],"pdf_url":"https://arxiv.org/pdf/2303.09272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09279v1","updated":"2023-03-15T06:20:51Z","published":"2023-03-15T06:20:51Z","title":"Privacy-Preserving Video Conferencing via Thermal-Generative Images","summary":"  Due to the COVID-19 epidemic, video conferencing has evolved as a new\nparadigm of communication and teamwork. However, private and personal\ninformation can be easily leaked through cameras during video conferencing.\nThis includes leakage of a person's appearance as well as the contents in the\nbackground. This paper proposes a novel way of using online low-resolution\nthermal images as conditions to guide the synthesis of RGB images, bringing a\npromising solution for real-time video conferencing when privacy leakage is a\nconcern. SPADE-SR (Spatially-Adaptive De-normalization with Self Resampling), a\nvariant of SPADE, is adopted to incorporate the spatial property of a thermal\nheatmap and the non-thermal property of a normal, privacy-free pre-recorded RGB\nimage provided in a form of latent code. We create a PAIR-LRT-Human (LRT =\nLow-Resolution Thermal) dataset to validate our claims. The result enables a\nconvenient way of video conferencing where users no longer need to groom\nthemselves and tidy up backgrounds for a short meeting. Additionally, it allows\na user to switch to a different appearance and background during a conference.\n","authors":["Sheng-Yang Chiu","Yu-Ting Huang","Chieh-Ting Lin","Yu-Chee Tseng","Jen-Jee Chen","Meng-Hsuan Tu","Bo-Chen Tung","YuJou Nieh"],"pdf_url":"https://arxiv.org/pdf/2303.09279v1.pdf","comment":null}]},"2023-03-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2210.07228v2","updated":"2023-03-16T17:54:53Z","published":"2022-10-13T17:55:51Z","title":"Language Model Decoding as Likelihood-Utility Alignment","summary":"  A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of a decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios, and their findings do not generalize\nacross tasks. We argue that the misalignment between the model's likelihood and\nthe task-specific notion of utility is the key factor to understanding the\neffectiveness of decoding algorithms. To structure the discussion, we introduce\na taxonomy of misalignment mitigation strategies (MMSs), providing a unifying\nview of decoding as a tool for alignment. The MMS taxonomy groups decoding\nalgorithms based on their implicit assumptions about likelihood--utility\nmisalignment, yielding general statements about their applicability across\ntasks. Specifically, by analyzing the correlation between the likelihood and\nthe utility of predictions across a diverse set of tasks, we provide empirical\nevidence supporting the proposed taxonomy and a set of principles to structure\nreasoning when choosing a decoding algorithm. Crucially, our analysis is the\nfirst to relate likelihood-based decoding algorithms with algorithms that rely\non external information, such as value-guided methods and prompting, and covers\nthe most diverse set of tasks to date. Code, data, and models are available at\nhttps://github.com/epfl-dlab/understanding-decoding.\n","authors":["Martin Josifoski","Maxime Peyrard","Frano Rajic","Jiheng Wei","Debjit Paul","Valentin Hartmann","Barun Patra","Vishrav Chaudhary","Emre Kıcıman","Boi Faltings","Robert West"],"pdf_url":"https://arxiv.org/pdf/2210.07228v2.pdf","comment":"Accepted at EACL (Findings) 2023"},{"id":"http://arxiv.org/abs/2303.09522v1","updated":"2023-03-16T17:38:15Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04558v2","updated":"2023-03-16T17:12:03Z","published":"2023-01-11T16:35:33Z","title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language\n  Processing","summary":"  Self-supervised learning in vision-language processing exploits semantic\nalignment between imaging and text modalities. Prior work in biomedical VLP has\nmostly relied on the alignment of single image and report pairs even though\nclinical notes commonly refer to prior images. This does not only introduce\npoor alignment between the modalities but also a missed opportunity to exploit\nrich self-supervision through existing temporal content in the data. In this\nwork, we explicitly account for prior images and reports when available during\nboth training and fine-tuning. Our approach, named BioViL-T, uses a\nCNN-Transformer hybrid multi-image encoder trained jointly with a text model.\nIt is designed to be versatile to arising challenges such as pose variations\nand missing input images across time. The resulting model excels on downstream\ntasks both in single- and multi-image setups, achieving state-of-the-art\nperformance on (I) progression classification, (II) phrase grounding, and (III)\nreport generation, whilst offering consistent improvements on disease\nclassification and sentence-similarity tasks. We release a novel multi-modal\ntemporal benchmark dataset, MS-CXR-T, to quantify the quality of\nvision-language representations in terms of temporal semantics. Our\nexperimental results show the advantages of incorporating prior images and\nreports to make most use of the data.\n","authors":["Shruthi Bannur","Stephanie Hyland","Qianchu Liu","Fernando Pérez-García","Maximilian Ilse","Daniel C. Castro","Benedikt Boecking","Harshita Sharma","Kenza Bouzid","Anja Thieme","Anton Schwaighofer","Maria Wetscherek","Matthew P. Lungren","Aditya Nori","Javier Alvarez-Valle","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2301.04558v2.pdf","comment":"To appear in CVPR 2023"},{"id":"http://arxiv.org/abs/2212.02691v2","updated":"2023-03-16T17:04:39Z","published":"2022-12-06T01:31:37Z","title":"LUNA: Language Understanding with Number Augmentations on Transformers\n  via Number Plugins and Pre-training","summary":"  Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -> 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).\n","authors":["Hongwei Han","Jialiang Xu","Mengyu Zhou","Yijia Shao","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.02691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13860v2","updated":"2023-03-16T16:35:26Z","published":"2022-09-28T06:31:19Z","title":"Natural Language Processing Methods to Identify Oncology Patients at\n  High Risk for Acute Care with Clinical Notes","summary":"  Clinical notes are an essential component of a health record. This paper\nevaluates how natural language processing (NLP) can be used to identify the\nrisk of acute care use (ACU) in oncology patients, once chemotherapy starts.\nRisk prediction using structured health data (SHD) is now standard, but\npredictions using free-text formats are complex. This paper explores the use of\nfree-text notes for the prediction of ACU instead of SHD. Deep Learning models\nwere compared to manually engineered language features. Results show that SHD\nmodels minimally outperform NLP models; an l1-penalised logistic regression\nwith SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same\nmodel with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a\ntransformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows\nhow language models can be used in clinical applications and underlines how\nrisk bias is different for diverse patient groups, even using only free-text\ndata.\n","authors":["Claudio Fanconi","Marieke van Buchem","Tina Hernandez-Boussard"],"pdf_url":"https://arxiv.org/pdf/2209.13860v2.pdf","comment":"11 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.09438v1","updated":"2023-03-16T16:13:36Z","published":"2023-03-16T16:13:36Z","title":"Trustera: A Live Conversation Redaction System","summary":"  Trustera, the first functional system that redacts personally identifiable\ninformation (PII) in real-time spoken conversations to remove agents' need to\nhear sensitive information while preserving the naturalness of live\ncustomer-agent conversations. As opposed to post-call redaction, audio masking\nstarts as soon as the customer begins speaking to a PII entity. This\nsignificantly reduces the risk of PII being intercepted or stored in insecure\ndata storage. Trustera's architecture consists of a pipeline of automatic\nspeech recognition, natural language understanding, and a live audio redactor\nmodule. The system's goal is three-fold: redact entities that are PII, mask the\naudio that goes to the agent, and at the same time capture the entity, so that\nthe captured PII can be used for a payment transaction or caller\nidentification. Trustera is currently being used by thousands of agents to\nsecure customers' sensitive information.\n","authors":["Evandro Gouvêa","Ali Dadgar","Shahab Jalalvand","Rathi Chengalvarayan","Badrinath Jayakumar","Ryan Price","Nicholas Ruiz","Jennifer McGovern","Srinivas Bangalore","Ben Stern"],"pdf_url":"https://arxiv.org/pdf/2303.09438v1.pdf","comment":"5"},{"id":"http://arxiv.org/abs/2303.09435v1","updated":"2023-03-16T16:10:16Z","published":"2023-03-16T16:10:16Z","title":"Jump to Conclusions: Short-Cutting Transformers With Linear\n  Transformations","summary":"  Transformer-based language models (LMs) create hidden representations of\ntheir inputs at every layer, but only use final-layer representations for\nprediction. This obscures the internal decision-making process of the model and\nthe utility of its intermediate representations. One way to elucidate this is\nto cast the hidden representations as final representations, bypassing the\ntransformer computation in-between. In this work, we suggest a simple method\nfor such casting, by using linear transformations. We show that our approach\nproduces more accurate approximations than the prevailing practice of\ninspecting hidden representations from all layers in the space of the final\nlayer. Moreover, in the context of language modeling, our method allows\n\"peeking\" into early layer representations of GPT-2 and BERT, showing that\noften LMs already predict the final output in early layers. We then demonstrate\nthe practicality of our method to recent early exit strategies, showing that\nwhen aiming, for example, at retention of 95% accuracy, our approach saves\nadditional 7.9% layers for GPT-2 and 5.4% layers for BERT, on top of the\nsavings of the original approach. Last, we extend our method to linearly\napproximate sub-modules, finding that attention is most tolerant to this\nchange.\n","authors":["Alexander Yom Din","Taelin Karidi","Leshem Choshen","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2303.09435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07807v3","updated":"2023-03-16T16:04:39Z","published":"2022-06-15T20:37:32Z","title":"How Adults Understand What Young Children Say","summary":"  Children's early speech often bears little resemblance to that of adults, and\nyet parents and other caregivers are able to interpret that speech and react\naccordingly. Here we investigate how these adult inferences as listeners\nreflect sophisticated beliefs about what children are trying to communicate, as\nwell as how children are likely to pronounce words. Using a Bayesian framework\nfor modeling spoken word recognition, we find that computational models can\nreplicate adult interpretations of children's speech only when they include\nstrong, context-specific prior expectations about the messages that children\nwill want to communicate. This points to a critical role of adult cognitive\nprocesses in supporting early communication and reveals how children can\nactively prompt adults to take actions on their behalf even when they have only\na nascent understanding of the adult language. We discuss the wide-ranging\nimplications of the powerful listening capabilities of adults for theories of\nfirst language acquisition.\n","authors":["Stephan C. Meylan","Ruthe Foushee","Nicole H. Wong","Elika Bergelson","Roger P. Levy"],"pdf_url":"https://arxiv.org/pdf/2206.07807v3.pdf","comment":"24 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.09421v1","updated":"2023-03-16T15:54:23Z","published":"2023-03-16T15:54:23Z","title":"Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual\n  approaches for news genre, topic and persuasion technique classification","summary":"  This paper describes our approach for SemEval-2023 Task 3: Detecting the\ncategory, the framing, and the persuasion techniques in online news in a\nmulti-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of\nfully trained and adapter mBERT models which was ranked joint-first for German,\nand had the highest mean rank of multi-language teams. For Subtask 2 (Framing),\nwe achieved first place in 3 languages, and the best average rank across all\nthe languages, by using two separate ensembles: a monolingual\nRoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task\nadaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a\nmonolingual RoBERTa-Base model for English and a multilingual mBERT model for\nthe remaining languages, which achieved top 10 for all languages, including 2nd\nfor English. For each subtask, we compare monolingual and multilingual\napproaches, and consider class imbalance techniques.\n","authors":["Ben Wu","Olesya Razuvayevskaya","Freddy Heppell","João A. Leite","Carolina Scarton","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2303.09421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01944v3","updated":"2023-03-16T15:27:46Z","published":"2022-12-04T22:34:16Z","title":"Automaton-Based Representations of Task Knowledge from Generative\n  Language Models","summary":"  Automaton-based representations of task knowledge play an important role in\ncontrol and planning for sequential decision-making problems. However,\nobtaining the high-level task knowledge required to build such automata is\noften difficult. Meanwhile, large-scale generative language models (GLMs) can\nautomatically generate relevant task knowledge. However, the textual outputs\nfrom GLMs cannot be formally verified or used for sequential decision-making.\nWe propose a novel algorithm named GLM2FSA, which constructs a finite state\nautomaton (FSA) encoding high-level task knowledge from a brief\nnatural-language description of the task goal. GLM2FSA first sends queries to a\nGLM to extract task knowledge in textual form, and then it builds an FSA to\nrepresent this text-based knowledge. The proposed algorithm thus fills the gap\nbetween natural-language task descriptions and automaton-based representations,\nand the constructed FSA can be formally verified against user-defined\nspecifications. We accordingly propose a method to iteratively refine the\nqueries to the GLM based on the outcomes, e.g., counter-examples, from\nverification. We demonstrate GLM2FSA's ability to build and refine\nautomaton-based representations of everyday tasks (e.g., crossing a road or\nmaking a phone call), and also of tasks that require highly-specialized\nknowledge (e.g., executing secure multi-party computation).\n","authors":["Yunhao Yang","Jean-Raphaël Gaglione","Cyrus Neary","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2212.01944v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08614v5","updated":"2023-03-16T15:15:32Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents the first system for\ncomplex questions that can seamlessly operate over a mixture of RDF datasets\nand text corpora, or individual sources, in a unified framework. Our method,\ncalled UNIQORN, builds a context graph on-the-fly, by retrieving\nquestion-relevant evidences from the RDF data and/or a text corpus, using\nfine-tuned BERT models. The resulting graph is typically rich but highly noisy.\nUNIQORN copes with this input by a graph algorithm for Group Steiner Trees,\nthat identifies the best answer candidates in the context graph. Experimental\nresults on several benchmarks of complex questions with multiple entities and\nrelations, show that \\uniqorn significantly outperforms state-of-the-art\nmethods for QA over heterogeneous sources. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v5.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2302.03507v2","updated":"2023-03-16T15:10:57Z","published":"2023-02-05T07:41:17Z","title":"Meta-Learning Siamese Network for Few-Shot Text Classification","summary":"  Few-shot learning has been used to tackle the problem of label scarcity in\ntext classification, of which meta-learning based methods have shown to be\neffective, such as the prototypical networks (PROTO). Despite the success of\nPROTO, there still exist three main problems: (1) ignore the randomness of the\nsampled support sets when computing prototype vectors; (2) disregard the\nimportance of labeled samples; (3) construct meta-tasks in a purely random\nmanner. In this paper, we propose a Meta-Learning Siamese Network, namely,\nMeta-SN, to address these issues. Specifically, instead of computing prototype\nvectors from the sampled support sets, Meta-SN utilizes external knowledge\n(e.g. class names and descriptive texts) for class labels, which is encoded as\nthe low-dimensional embeddings of prototype vectors. In addition, Meta-SN\npresents a novel sampling strategy for constructing meta-tasks, which gives\nhigher sampling probabilities to hard-to-classify samples. Extensive\nexperiments are conducted on six benchmark datasets to show the clear\nsuperiority of Meta-SN over other state-of-the-art models. For reproducibility,\nall the datasets and codes are provided at https://github.com/hccngu/Meta-SN.\n","authors":["Chengcheng Han","Yuhe Wang","Yingnan Fu","Xiang Li","Minghui Qiu","Ming Gao","Aoying Zhou"],"pdf_url":"https://arxiv.org/pdf/2302.03507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09366v1","updated":"2023-03-16T14:51:44Z","published":"2023-03-16T14:51:44Z","title":"The Scope of In-Context Learning for the Extraction of Medical Temporal\n  Constraints","summary":"  Medications often impose temporal constraints on everyday patient activity.\nViolations of such medical temporal constraints (MTCs) lead to a lack of\ntreatment adherence, in addition to poor health outcomes and increased\nhealthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in\nboth patient education materials and clinical texts. Computationally\nrepresenting MTCs in DUGs will advance patient-centric healthcare applications\nby helping to define safe patient activity patterns. We define a novel taxonomy\nof MTCs found in DUGs and develop a novel context-free grammar (CFG) based\nmodel to computationally represent MTCs from unstructured DUGs. Additionally,\nwe release three new datasets with a combined total of N = 836 DUGs labeled\nwith normalized MTCs. We develop an in-context learning (ICL) solution for\nautomatically extracting and normalizing MTCs found in DUGs, achieving an\naverage F1 score of 0.62 across all datasets. Finally, we rigorously\ninvestigate ICL model performance against a baseline model, across datasets and\nMTC types, and through in-depth error analysis.\n","authors":["Parker Seegmiller","Joseph Gatto","Madhusudan Basak","Diane Cook","Hassan Ghasemzadeh","John Stankovic","Sarah Preum"],"pdf_url":"https://arxiv.org/pdf/2303.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09364v1","updated":"2023-03-16T14:47:52Z","published":"2023-03-16T14:47:52Z","title":"Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics","summary":"  Emotion recognition from a given music track has heavily relied on acoustic\nfeatures, social tags, and metadata but is seldom focused on lyrics. There are\nno datasets of Indian language songs that contain both valence and arousal\nmanual ratings of lyrics. We present a new manually annotated dataset of Telugu\nsongs' lyrics collected from Spotify with valence and arousal annotated on a\ndiscrete scale. A fairly high inter-annotator agreement was observed for both\nvalence and arousal. Subsequently, we create two music emotion recognition\nmodels by using two classification techniques to identify valence, arousal and\nrespective emotion quadrant from lyrics. Support vector machine (SVM) with term\nfrequency-inverse document frequency (TF-IDF) features and fine-tuning the\npre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and\nquadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the\nSVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90%,\n80.71% and 58.33% for valence, arousal and quadrant classifications,\nrespectively, on 10-fold cross-validation. In addition, we compare our lyrics\nannotations with Spotify's annotations of valence and energy (same as arousal),\nwhich are based on entire music tracks. The implications of our findings are\ndiscussed. Finally, we make the dataset publicly available with lyrics,\nannotations and Spotify IDs.\n","authors":["R Guru Ravi Shanker","B Manikanta Gupta","BV Koushik","Vinoo Alluri"],"pdf_url":"https://arxiv.org/pdf/2303.09364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09325v1","updated":"2023-03-16T13:58:45Z","published":"2023-03-16T13:58:45Z","title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher\n  Education Programming Courses?","summary":"  We evaluated the capability of generative pre-trained transformers (GPT), to\npass assessments in introductory and intermediate Python programming courses at\nthe postsecondary level. Discussions of potential uses (e.g., exercise\ngeneration, code explanation) and misuses (e.g., cheating) of this emerging\ntechnology in programming education have intensified, but to date there has not\nbeen a rigorous analysis of the models' capabilities in the realistic context\nof a full-fledged programming course with diverse set of assessment\ninstruments. We evaluated GPT on three Python courses that employ assessments\nranging from simple multiple-choice questions (no code involved) to complex\nprogramming projects with code bases distributed into multiple files (599\nexercises overall). Further, we studied if and how successfully GPT models\nleverage feedback provided by an auto-grader. We found that the current models\nare not capable of passing the full spectrum of assessments typically involved\nin a Python programming course (<70% on even entry-level modules). Yet, it is\nclear that a straightforward application of these easily accessible models\ncould enable a learner to obtain a non-trivial portion of the overall available\nscore (>55%) in introductory and intermediate courses alike. While the models\nexhibit remarkable capabilities, including correcting solutions based on\nauto-grader's feedback, some limitations exist (e.g., poor handling of\nexercises requiring complex chains of reasoning steps). These findings can be\nleveraged by instructors wishing to adapt their assessments so that GPT becomes\na valuable assistant for a learner as opposed to an end-to-end solution.\n","authors":["Jaromir Savelka","Arav Agarwal","Christopher Bogart","Yifan Song","Majd Sakr"],"pdf_url":"https://arxiv.org/pdf/2303.09325v1.pdf","comment":"7 pages. arXiv admin note: text overlap with arXiv:2303.08033"},{"id":"http://arxiv.org/abs/2303.09306v1","updated":"2023-03-16T13:31:31Z","published":"2023-03-16T13:31:31Z","title":"Towards Robust Bangla Complex Named Entity Recognition","summary":"  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n","authors":["HAZ Sameen Shahgir","Ramisa Alam","Md. Zarif Ul Alam"],"pdf_url":"https://arxiv.org/pdf/2303.09306v1.pdf","comment":"Winning Solution for the Bangla Complex Named Entity Recognition\n  Challenge"},{"id":"http://arxiv.org/abs/2303.09266v1","updated":"2023-03-16T12:44:16Z","published":"2023-03-16T12:44:16Z","title":"SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for\n  Accelerating BERT Inference","summary":"  Dynamic early exiting has been proven to improve the inference speed of the\npre-trained language model like BERT. However, all samples must go through all\nconsecutive layers before early exiting and more complex samples usually go\nthrough more layers, which still exists redundant computation. In this paper,\nwe propose a novel dynamic early exiting combined with layer skipping for BERT\ninference named SmartBERT, which adds a skipping gate and an exiting operator\ninto each layer of BERT. SmartBERT can adaptively skip some layers and\nadaptively choose whether to exit. Besides, we propose cross-layer contrastive\nlearning and combine it into our training phases to boost the intermediate\nlayers and classifiers which would be beneficial for early exiting. To keep the\nconsistent usage of skipping gates between training and inference phases, we\npropose a hard weight mechanism during training phase. We conduct experiments\non eight classification datasets of the GLUE benchmark. Experimental results\nshow that SmartBERT achieves 2-3x computation reduction with minimal accuracy\ndrops compared with BERT and our method outperforms previous methods in both\nefficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,\nwe prove that the early exiting based on entropy hardly works, and the skipping\nmechanism is essential for reducing computation.\n","authors":["Boren Hu","Yun Zhu","Jiacheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.09266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10276v2","updated":"2023-03-16T10:01:05Z","published":"2023-02-11T07:30:21Z","title":"See Your Heart: Psychological states Interpretation through Visual\n  Creations","summary":"  In psychoanalysis, generating interpretations to one's psychological state\nthrough visual creations is facing significant demands. The two main tasks of\nexisting studies in the field of computer vision, sentiment/emotion\nclassification and affective captioning, can hardly satisfy the requirement of\npsychological interpreting. To meet the demands for psychoanalysis, we\nintroduce a challenging task, \\textbf{V}isual \\textbf{E}motion\n\\textbf{I}nterpretation \\textbf{T}ask (VEIT). VEIT requires AI to generate\nreasonable interpretations of creator's psychological state through visual\ncreations. To support the task, we present a multimodal dataset termed SpyIn\n(\\textbf{S}and\\textbf{p}la\\textbf{y} \\textbf{In}terpretation Dataset), which is\npsychological theory supported and professional annotated. Dataset analysis\nillustrates that SpyIn is not only able to support VEIT, but also more\nchallenging compared with other captioning datasets. Building on SpyIn, we\nconduct experiments of several image captioning method, and propose a\nvisual-semantic combined model which obtains a SOTA result on SpyIn. The\nresults indicate that VEIT is a more challenging task requiring scene graph\ninformation and psychological knowledge. Our work also show a promise for AI to\nanalyze and explain inner world of humanity through visual creations.\n","authors":["Likun Yang","Xiaokun Feng","Xiaotang Chen","Shiyu Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2302.10276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09184v1","updated":"2023-03-16T09:53:57Z","published":"2023-03-16T09:53:57Z","title":"Block-wise Bit-Compression of Transformer-based Models","summary":"  With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.\n","authors":["Gaochen Dong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09184v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2109.03490v3","updated":"2023-03-16T09:01:45Z","published":"2021-09-08T08:27:47Z","title":"Cross-linguistic differences in gender congruency effects: Evidence from\n  meta-analyses","summary":"  It has been proposed that the order in which words are prepared for\nproduction depends on the speaker's language. When producing the translation\nequivalent of the small cat, speakers of German or Dutch select the\ngender-marked determiner at a relatively early stage of production. Speakers of\nFrench or Italian postpone the encoding of a determiner or adjective until the\nphonological form of the noun is available. Hence, even though the words are\nproduced in the same order (e.g., die kleine Katze in German, le petit chat in\nFrench), they are not planned in the same order and might require different\namounts of advanced planning prior to production onset. This distinction\nbetween early and late selection languages was proposed to account for the\nobservation that speakers of Germanic and Slavic languages, but not of Romance\nlanguages, are slower to name pictures in the context of a distractor word of a\ndifferent gender. Meta-analyses are conducted to provide the first direct test\nof this cross-linguistic difference and to test a prediction of the late\nselection hypothesis. They confirm the existence of the gender congruency\neffect in German/Slavic languages and its absence in Romance languages when\ntarget and distractor words are presented simultaneously. They do not allow\nconfirming the hypothesis that in the latter languages, a similar effect\nemerges when the presentation of the distractor is delayed. Overall, these\nanalyses confirm the cross-linguistic difference but show that the evidence\navailable to date is not sufficient to confirm or reject the late selection\nhypothesis as an explanation of this difference. We highlight specific\ndirections for future research.\n","authors":["Audrey Bürki","Emiel van den Hoven","Niels O. Schiller","Nikolay DImitrov"],"pdf_url":"https://arxiv.org/pdf/2109.03490v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10154v2","updated":"2023-03-16T08:57:14Z","published":"2022-12-20T10:46:40Z","title":"Human-Guided Fair Classification for Natural Language Processing","summary":"  Text classifiers have promising applications in high-stake tasks such as\nresume screening and content moderation. These classifiers must be fair and\navoid discriminatory decisions by being invariant to perturbations of sensitive\nattributes such as gender or ethnicity. However, there is a gap between human\nintuition about these perturbations and the formal similarity specifications\ncapturing them. While existing research has started to address this gap,\ncurrent methods are based on hardcoded word replacements, resulting in\nspecifications with limited expressivity or ones that fail to fully align with\nhuman intuition (e.g., in cases of asymmetric counterfactuals). This work\nproposes novel methods for bridging this gap by discovering expressive and\nintuitive individual fairness specifications. We show how to leverage\nunsupervised style transfer and GPT-3's zero-shot capabilities to automatically\ngenerate expressive candidate pairs of semantically similar sentences that\ndiffer along sensitive attributes. We then validate the generated pairs via an\nextensive crowdsourcing study, which confirms that a lot of these pairs align\nwith human intuition about fairness in the context of toxicity classification.\nFinally, we show how limited amounts of human feedback can be leveraged to\nlearn a similarity specification that can be used to train downstream\nfairness-aware models.\n","authors":["Florian E. Dorner","Momchil Peychev","Nikola Konstantinov","Naman Goel","Elliott Ash","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2212.10154v2.pdf","comment":"Published at ICLR 2023 (notable top 25%). 30 pages, 1 figure"},{"id":"http://arxiv.org/abs/2301.09244v2","updated":"2023-03-16T08:09:49Z","published":"2023-01-23T02:20:39Z","title":"Efficient Encoders for Streaming Sequence Tagging","summary":"  A naive application of state-of-the-art bidirectional encoders for streaming\nsequence tagging would require encoding each token from scratch for each new\ntoken in an incremental streaming input (like transcribed speech). The lack of\nre-usability of previous computation leads to a higher number of Floating Point\nOperations (or FLOPs) and higher number of unnecessary label flips. Increased\nFLOPs consequently lead to higher wall-clock time and increased label flipping\nleads to poorer streaming performance. In this work, we present a Hybrid\nEncoder with Adaptive Restart (HEAR) that addresses these issues while\nmaintaining the performance of bidirectional encoders over the offline (or\ncomplete) inputs while improving performance on streaming (or incomplete)\ninputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to\nperform sequence tagging, along with an Adaptive Restart Module (ARM) to\nselectively guide the restart of bidirectional portion of the encoder. Across\nfour sequence tagging tasks, HEAR offers FLOP savings in streaming settings\nupto 71.1% and also outperforms bidirectional encoders for streaming\npredictions by upto +10% streaming exact match.\n","authors":["Ayush Kaushal","Aditya Gupta","Shyam Upadhyay","Manaal Faruqui"],"pdf_url":"https://arxiv.org/pdf/2301.09244v2.pdf","comment":"EACL 2023 Camera-ready"},{"id":"http://arxiv.org/abs/2303.09136v1","updated":"2023-03-16T08:01:22Z","published":"2023-03-16T08:01:22Z","title":"A Short Survey of Viewing Large Language Models in Legal Aspect","summary":"  Large language models (LLMs) have transformed many fields, including natural\nlanguage processing, computer vision, and reinforcement learning. These models\nhave also made a significant impact in the field of law, where they are being\nincreasingly utilized to automate various legal tasks, such as legal judgement\nprediction, legal document analysis, and legal document writing. However, the\nintegration of LLMs into the legal field has also raised several legal\nproblems, including privacy concerns, bias, and explainability. In this survey,\nwe explore the integration of LLMs into the field of law. We discuss the\nvarious applications of LLMs in legal tasks, examine the legal challenges that\narise from their use, and explore the data resources that can be used to\nspecialize LLMs in the legal domain. Finally, we discuss several promising\ndirections and conclude this paper. By doing so, we hope to provide an overview\nof the current state of LLMs in law and highlight the potential benefits and\nchallenges of their integration.\n","authors":["Zhongxiang Sun"],"pdf_url":"https://arxiv.org/pdf/2303.09136v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.09128v1","updated":"2023-03-16T07:45:46Z","published":"2023-03-16T07:45:46Z","title":"Exploring Distributional Shifts in Large Language Models for Code\n  Analysis","summary":"  We systematically study the capacity of two large language models for code -\nCodeT5 and Codex - to generalize to out-of-domain data. In this study, we\nconsider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. This\nmakes recognition of in-domain vs out-of-domain data at the time of deployment\ntrivial. We establish that samples from each new domain present both models\nwith a significant challenge of distribution shift. We study how well different\nestablished methods can adapt models to better generalize to new domains. Our\nexperiments show that while multitask learning alone is a reasonable baseline,\ncombining it with few-shot finetuning on examples retrieved from training data\ncan achieve very strong performance. In fact, according to our experiments,\nthis solution can outperform direct finetuning for very low-data scenarios.\nFinally, we consider variations of this approach to create a more broadly\napplicable method to adapt to multiple domains at once. We find that in the\ncase of code generation, a model adapted to multiple domains simultaneously\nperforms on par with those adapted to each domain individually.\n","authors":["Shushan Arakelyan","Rocktim Jyoti Das","Yi Mao","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2303.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11337v2","updated":"2023-03-16T07:38:02Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via\n  Contrastive Prompt-Tuning","summary":"  Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n  To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09100v1","updated":"2023-03-16T06:09:15Z","published":"2023-03-16T06:09:15Z","title":"Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models","summary":"  For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt learning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize prompt learning with\nthe visual knowledge and view images and the corresponding prompts as patch and\ntoken sets under optimal transport, which pushes the prompt tokens to\nfaithfully capture the label-specific visual concepts, instead of overfitting\nthe training categories. Moreover, the proposed model can also be\nstraightforwardly extended to the conditional case where the\ninstance-conditional prompts are generated to improve the generalizability.\nExtensive experiments on 15 datasets show promising transferability and\ngeneralization performance of our proposed model.\n","authors":["Xinyang Liu","Dongsheng Wang","Miaoge Li","Zhibin Duan","Yishi Xu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09093v1","updated":"2023-03-16T05:36:38Z","published":"2023-03-16T05:36:38Z","title":"GLEN: General-Purpose Event Detection for Thousands of Types","summary":"  The development of event extraction systems has been hindered by the absence\nof wide-coverage, large-scale datasets. To make event extraction systems more\naccessible, we build a general-purpose event detection dataset GLEN, which\ncovers 3,465 different event types, making it over 20x larger in ontology than\nany current dataset. GLEN is created by utilizing the DWD Overlay, which\nprovides a mapping between Wikidata Qnodes and PropBank rolesets. This enables\nus to use the abundant existing annotation for PropBank as distant supervision.\nIn addition, we also propose a new multi-stage event detection model\nspecifically designed to handle the large ontology size and partial labels in\nGLEN. We show that our model exhibits superior performance (~10% F1 gain)\ncompared to both conventional classification baselines and newer\ndefinition-based models. Finally, we perform error analysis and show that label\nnoise is still the largest challenge for improving performance.\n","authors":["Qiusi Zhan","Sha Li","Kathryn Conger","Martha Palmer","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2303.09093v1.pdf","comment":"The first two authors contributed equally. (15 pages, 11 figures)"},{"id":"http://arxiv.org/abs/2303.08302v2","updated":"2023-03-16T05:34:52Z","published":"2023-03-15T01:27:15Z","title":"A Comprehensive Study on Post-Training Quantization for Large Language\n  Models","summary":"  Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study of those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bit precision is similar to 5 bits).\nWe also present recommendations about how to utilize quantization for \\llms\nwith different sizes, and leave suggestions of future opportunities and system\nwork that are not resolved in this work.\n","authors":["Zhewei Yao","Cheng Li","Xiaoxia Wu","Stephen Youn","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.08302v2.pdf","comment":"25 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.09092v1","updated":"2023-03-16T05:32:02Z","published":"2023-03-16T05:32:02Z","title":"Investigating Failures to Generalize for Coreference Resolution Models","summary":"  Coreference resolution models are often evaluated on multiple datasets.\nDatasets vary, however, in how coreference is realized -- i.e., how the\ntheoretical concept of coreference is operationalized in the dataset -- due to\nfactors such as the choice of corpora and annotation guidelines. We investigate\nthe extent to which errors of current coreference resolution models are\nassociated with existing differences in operationalization across datasets\n(OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and\nbreak down model performance into categories corresponding to several types of\ncoreference, including coreferring generic mentions, compound modifiers, and\ncopula predicates, among others. This break down helps us investigate how\nstate-of-the-art models might vary in their ability to generalize across\ndifferent coreference types. In our experiments, for example, models trained on\nOntoNotes perform poorly on generic mentions and copula predicates in PreCo.\nOur findings help calibrate expectations of current coreference resolution\nmodels; and, future work can explicitly account for those types of coreference\nthat are empirically associated with poor generalization when developing\nmodels.\n","authors":["Ian Porada","Alexandra Olteanu","Kaheer Suleman","Adam Trischler","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2303.09092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08774v2","updated":"2023-03-16T04:59:24Z","published":"2023-03-15T17:15:04Z","title":"GPT-4 Technical Report","summary":"  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n","authors":[" OpenAI"],"pdf_url":"https://arxiv.org/pdf/2303.08774v2.pdf","comment":"99 pages"},{"id":"http://arxiv.org/abs/2303.09075v1","updated":"2023-03-16T04:21:19Z","published":"2023-03-16T04:21:19Z","title":"Self-Consistent Learning: Cooperation between Generators and\n  Discriminators","summary":"  Using generated data to improve the performance of downstream discriminative\nmodels has recently gained popularity due to the great development of\npre-trained language models. In most previous studies, generative models and\ndiscriminative models are trained separately and thus could not adapt to any\nchanges in each other. As a result, the generated samples can easily deviate\nfrom the real data distribution, while the improvement of the discriminative\nmodel quickly reaches saturation. Generative adversarial networks (GANs) train\ngenerative models via an adversarial process with discriminative models to\nachieve joint training. However, the training of standard GANs is notoriously\nunstable and often falls short of convergence. In this paper, to address these\nissues, we propose a $\\textit{self-consistent learning}$ framework, in which a\ndiscriminator and a generator are cooperatively trained in a closed-loop form.\nThe discriminator and the generator enhance each other during multiple rounds\nof alternating training until a scoring consensus is reached. This framework\nproves to be easy to train and free from instabilities such as mode collapse\nand non-convergence. Extensive experiments on sentence semantic matching\ndemonstrate the effectiveness of the proposed framework: the discriminator\nachieves 10+ AP of improvement on the zero-shot setting and new\nstate-of-the-art performance on the full-data setting.\n","authors":["Tong Wu","Hao Wang","Zhongshen Zeng","Wei Wang","Hai-Tao Zheng","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09067v1","updated":"2023-03-16T03:56:17Z","published":"2023-03-16T03:56:17Z","title":"Secret-Keeping in Question Answering","summary":"  Existing question-answering research focuses on unanswerable questions in the\ncontext of always providing an answer when a system can\\dots but what about\ncases where a system {\\bf should not} answer a question. This can either be to\nprotect sensitive users or sensitive information. Many models expose sensitive\ninformation under interrogation by an adversarial user. We seek to determine if\nit is possible to teach a question-answering system to keep a specific fact\nsecret. We design and implement a proof-of-concept architecture and through our\nevaluation determine that while possible, there are numerous directions for\nfuture research to reduce system paranoia (false positives), information\nleakage (false negatives) and extend the implementation of the work to more\ncomplex problems with preserving secrecy in the presence of information\naggregation.\n","authors":["Nathaniel W. Rollings","Kent O'Sullivan","Sakshum Kulshrestha"],"pdf_url":"https://arxiv.org/pdf/2303.09067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06537v6","updated":"2023-03-16T03:14:42Z","published":"2021-10-13T07:19:47Z","title":"Well-classified Examples are Underestimated in Classification with Deep\n  Neural Networks","summary":"  The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n","authors":["Guangxiang Zhao","Wenkai Yang","Xuancheng Ren","Lei Li","Yunfang Wu","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2110.06537v6.pdf","comment":"Accepted by AAAI 2022; 18 pages, 11 figures, 13 tables"},{"id":"http://arxiv.org/abs/2303.08335v2","updated":"2023-03-16T03:09:09Z","published":"2023-03-15T02:51:57Z","title":"FactReranker: Fact-guided Reranker for Faithful Radiology Report\n  Summarization","summary":"  Automatic radiology report summarization is a crucial clinical task, whose\nkey challenge is to maintain factual accuracy between produced summaries and\nground truth radiology findings. Existing research adopts reinforcement\nlearning to directly optimize factual consistency metrics such as CheXBert or\nRadGraph score. However, their decoding method using greedy search or beam\nsearch considers no factual consistency when picking the optimal candidate,\nleading to limited factual consistency improvement. To address it, we propose a\nnovel second-stage summarizing approach FactReranker, the first attempt that\nlearns to choose the best summary from all candidates based on their estimated\nfactual consistency score. We propose to extract medical facts of the input\nmedical report, its gold summary, and candidate summaries based on the RadGraph\nschema and design the fact-guided reranker to efficiently incorporate the\nextracted medical facts for selecting the optimal summary. We decompose the\nfact-guided reranker into the factual knowledge graph generation and the\nfactual scorer, which allows the reranker to model the mapping between the\nmedical facts of the input text and its gold summary, thus can select the\noptimal summary even the gold summary can't be observed during inference. We\nalso present a fact-based ranking metric (RadMRR) for measuring the ability of\nthe reranker on selecting factual consistent candidates. Experimental results\non two benchmark datasets demonstrate the superiority of our method in\ngenerating summaries with higher factual consistency scores when compared with\nexisting methods.\n","authors":["Qianqian Xie","Jiayu Zhou","Yifan Peng","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08335v2.pdf","comment":"11 pages, 5 figures, 7 tables, submitted to KDD 2023"},{"id":"http://arxiv.org/abs/2303.02563v3","updated":"2023-03-16T02:35:42Z","published":"2023-03-05T03:18:56Z","title":"FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis","summary":"  This paper presents a novel approach for explainability in financial analysis\nby utilizing the Pearson correlation coefficient to establish a relationship\nbetween aspect-based sentiment analysis and stock prices. The proposed\nmethodology involves constructing an aspect list from financial news articles\nand analyzing sentiment intensity scores for each aspect. These scores are then\ncompared to the stock prices for the relevant companies using the Pearson\ncoefficient to determine any significant correlations. The results indicate\nthat the proposed approach provides a more detailed and accurate understanding\nof the relationship between sentiment analysis and stock prices, which can be\nuseful for investors and financial analysts in making informed decisions.\nAdditionally, this methodology offers a transparent and interpretable way to\nexplain the sentiment analysis results and their impact on stock prices.\nOverall, the findings of this paper demonstrate the importance of\nexplainability in financial analysis and highlight the potential benefits of\nutilizing the Pearson coefficient for analyzing aspect-based sentiment analysis\nand stock prices. The proposed approach offers a valuable tool for\nunderstanding the complex relationships between financial news sentiment and\nstock prices, providing a new perspective on the financial market and aiding in\nmaking informed investment decisions.\n","authors":["Keane Ong","Wihan van der Heever","Ranjan Satapathy","Gianmarco Mengaldo","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2303.02563v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09038v1","updated":"2023-03-16T02:21:39Z","published":"2023-03-16T02:21:39Z","title":"Translating Radiology Reports into Plain Language using ChatGPT and\n  GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential","summary":"  The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.1 in the five-point system with 0.07 places of information missing\nand 0.11 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n","authors":["Qing Lyu","Josh Tan","Mike E. Zapadka","Janardhana Ponnatapuram","Chuang Niu","Ge Wang","Christopher T. Whitlow"],"pdf_url":"https://arxiv.org/pdf/2303.09038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09031v1","updated":"2023-03-16T02:02:18Z","published":"2023-03-16T02:02:18Z","title":"A Picture is Worth a Thousand Words: Language Models Plan from Pixels","summary":"  Planning is an important capability of artificial agents that perform\nlong-horizon tasks in real-world environments. In this work, we explore the use\nof pre-trained language models (PLMs) to reason about plan sequences from text\ninstructions in embodied visual environments. Prior PLM based approaches for\nplanning either assume observations are available in the form of text (e.g.,\nprovided by a captioning model), reason about plans from the instruction alone,\nor incorporate information about the visual environment in limited ways (such\nas a pre-trained affordance function). In contrast, we show that PLMs can\naccurately plan even when observations are directly encoded as input prompts\nfor the PLM. We show that this simple approach outperforms prior approaches in\nexperiments on the ALFWorld and VirtualHome benchmarks.\n","authors":["Anthony Z. Liu","Lajanugen Logeswaran","Sungryull Sohn","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2303.09031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01903v2","updated":"2023-03-16T01:49:29Z","published":"2023-03-03T13:05:15Z","title":"Prompting Large Language Models with Answer Heuristics for\n  Knowledge-based Visual Question Answering","summary":"  Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have sought to use a large language model (i.e.,\nGPT-3) as an implicit knowledge engine to acquire the necessary knowledge for\nanswering. Despite the encouraging results achieved by these methods, we argue\nthat they have not fully activated the capacity of GPT-3 as the provided input\ninformation is insufficient. In this paper, we present Prophet -- a\nconceptually simple framework designed to prompt GPT-3 with answer heuristics\nfor knowledge-based VQA. Specifically, we first train a vanilla VQA model on a\nspecific knowledge-based VQA dataset without external knowledge. After that, we\nextract two types of complementary answer heuristics from the model: answer\ncandidates and answer-aware examples. Finally, the two types of answer\nheuristics are encoded into the prompts to enable GPT-3 to better comprehend\nthe task thus enhancing its capacity. Prophet significantly outperforms all\nexisting state-of-the-art methods on two challenging knowledge-based VQA\ndatasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their\ntesting sets, respectively.\n","authors":["Zhenwei Shao","Zhou Yu","Meng Wang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2303.01903v2.pdf","comment":"Accepted to CVPR 2023, code available at\n  https://github.com/MILVLG/prophet"},{"id":"http://arxiv.org/abs/2303.09014v1","updated":"2023-03-16T01:04:45Z","published":"2023-03-16T01:04:45Z","title":"ART: Automatic multi-step reasoning and tool-use for large language\n  models","summary":"  Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.\n","authors":["Bhargavi Paranjape","Scott Lundberg","Sameer Singh","Hannaneh Hajishirzi","Luke Zettlemoyer","Marco Tulio Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2303.09014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03929v3","updated":"2023-03-16T22:57:25Z","published":"2022-11-08T00:59:05Z","title":"Comparative layer-wise analysis of self-supervised speech models","summary":"  Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive successes on downstream tasks, we still have a\nlimited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.\n","authors":["Ankita Pasad","Bowen Shi","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2211.03929v3.pdf","comment":"Accepted to ICASSP 2023. Code:\n  https://github.com/ankitapasad/layerwise-analysis"},{"id":"http://arxiv.org/abs/2212.08756v4","updated":"2023-03-16T21:31:25Z","published":"2022-12-16T23:37:44Z","title":"Multi-Scales Data Augmentation Approach In Natural Language Inference\n  For Artifacts Mitigation And Pre-Trained Model Optimization","summary":"  Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.\n","authors":["Zhenyuan Lu"],"pdf_url":"https://arxiv.org/pdf/2212.08756v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09639v1","updated":"2023-03-16T20:39:44Z","published":"2023-03-16T20:39:44Z","title":"Neural Architecture Search for Effective Teacher-Student Knowledge\n  Transfer in Language Models","summary":"  Large pre-trained language models have achieved state-of-the-art results on a\nvariety of downstream tasks. Knowledge Distillation (KD) of a smaller student\nmodel addresses their inefficiency, allowing for deployment in\nresource-constraint environments. KD however remains ineffective, as the\nstudent is manually selected from a set of existing options already pre-trained\non large corpora, a sub-optimal choice within the space of all possible student\narchitectures. This paper proposes KD-NAS, the use of Neural Architecture\nSearch (NAS) guided by the Knowledge Distillation process to find the optimal\nstudent model for distillation from a teacher, for a given natural language\ntask. In each episode of the search process, a NAS controller predicts a reward\nbased on a combination of accuracy on the downstream task and latency of\ninference. The top candidate architectures are then distilled from the teacher\non a small proxy set. Finally the architecture(s) with the highest reward is\nselected, and distilled on the full downstream task training set. When\ndistilling on the MNLI task, our KD-NAS model produces a 2 point improvement in\naccuracy on GLUE tasks with equivalent GPU latency with respect to a\nhand-crafted student architecture available in the literature. Using Knowledge\nDistillation, this model also achieves a 1.4x speedup in GPU Latency (3.2x\nspeedup on CPU) with respect to a BERT-Base Teacher, while maintaining 97%\nperformance on GLUE Tasks (without CoLA). We also obtain an architecture with\nequivalent performance as the hand-crafted student model on the GLUE benchmark,\nbut with a 15% speedup in GPU latency (20% speedup in CPU latency) and 0.8\ntimes the number of parameters\n","authors":["Aashka Trivedi","Takuma Udagawa","Michele Merler","Rameswar Panda","Yousef El-Kurdi","Bishwaranjan Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2303.09639v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.09618v1","updated":"2023-03-16T19:47:41Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09601v1","updated":"2023-03-16T19:01:29Z","published":"2023-03-16T19:01:29Z","title":"Psychotherapy AI Companion with Reinforcement Learning Recommendations\n  and Interpretable Policy Dynamics","summary":"  We introduce a Reinforcement Learning Psychotherapy AI Companion that\ngenerates topic recommendations for therapists based on patient responses. The\nsystem uses Deep Reinforcement Learning (DRL) to generate multi-objective\npolicies for four different psychiatric conditions: anxiety, depression,\nschizophrenia, and suicidal cases. We present our experimental results on the\naccuracy of recommended topics using three different scales of working alliance\nratings: task, bond, and goal. We show that the system is able to capture the\nreal data (historical topics discussed by the therapists) relatively well, and\nthat the best performing models vary by disorder and rating scale. To gain\ninterpretable insights into the learned policies, we visualize policy\ntrajectories in a 2D principal component analysis space and transition\nmatrices. These visualizations reveal distinct patterns in the policies trained\nwith different reward signals and trained on different clinical diagnoses. Our\nsystem's success in generating DIsorder-Specific Multi-Objective Policies\n(DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in\nproviding personalized and efficient therapeutic recommendations.\n","authors":["Baihan Lin","Guillermo Cecchi","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2303.09601v1.pdf","comment":"WWW 2023. This work supersede our prior work arxiv:2208.13077 by\n  studying the interpretability of RL-based therapy agents with policy\n  visualizations"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.09556v1","updated":"2023-03-16T17:59:56Z","published":"2023-03-16T17:59:56Z","title":"Efficient Diffusion Training via Min-SNR Weighting Strategy","summary":"  Denoising diffusion models have been a mainstream approach for image\ngeneration, however, training these models often suffers from slow convergence.\nIn this paper, we discovered that the slow convergence is partly due to\nconflicting optimization directions between timesteps. To address this issue,\nwe treat the diffusion training as a multi-task learning problem, and introduce\na simple yet effective approach referred to as Min-SNR-$\\gamma$. This method\nadapts loss weights of timesteps based on clamped signal-to-noise ratios, which\neffectively balances the conflicts among timesteps. Our results demonstrate a\nsignificant improvement in converging speed, 3.4$\\times$ faster than previous\nweighting strategies. It is also more effective, achieving a new record FID\nscore of 2.06 on the ImageNet $256\\times256$ benchmark using smaller\narchitectures than that employed in previous state-of-the-art.\n","authors":["Tiankai Hang","Shuyang Gu","Chen Li","Jianmin Bao","Dong Chen","Han Hu","Xin Geng","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2303.09556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03239v2","updated":"2023-03-16T17:59:51Z","published":"2022-12-06T18:59:50Z","title":"Perspective Fields for Single Image Camera Calibration","summary":"  Geometric camera calibration is often required for applications that\nunderstand the perspective of the image. We propose perspective fields as a\nrepresentation that models the local perspective properties of an image.\nPerspective Fields contain per-pixel information about the camera view,\nparameterized as an up vector and a latitude value. This representation has a\nnumber of advantages as it makes minimal assumptions about the camera model and\nis invariant or equivariant to common image editing operations like cropping,\nwarping, and rotation. It is also more interpretable and aligned with human\nperception. We train a neural network to predict Perspective Fields and the\npredicted Perspective Fields can be converted to calibration parameters easily.\nWe demonstrate the robustness of our approach under various scenarios compared\nwith camera calibration-based methods and show example applications in image\ncompositing.\n","authors":["Linyi Jin","Jianming Zhang","Yannick Hold-Geoffroy","Oliver Wang","Kevin Matzen","Matthew Sticha","David F. Fouhey"],"pdf_url":"https://arxiv.org/pdf/2212.03239v2.pdf","comment":"CVPR 2023 Camera Ready. Project Page\n  https://jinlinyi.github.io/PerspectiveFields/"},{"id":"http://arxiv.org/abs/2303.09555v1","updated":"2023-03-16T17:59:50Z","published":"2023-03-16T17:59:50Z","title":"SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse\n  Environments","summary":"  While significant research progress has been made in robot learning for\ncontrol, unique challenges arise when simultaneously co-optimizing morphology.\nExisting work has typically been tailored for particular environments or\nrepresentations. In order to more fully understand inherent design and\nperformance tradeoffs and accelerate the development of new breeds of soft\nrobots, a comprehensive virtual platform with well-established tasks,\nenvironments, and evaluation metrics is needed. In this work, we introduce\nSoftZoo, a soft robot co-design platform for locomotion in diverse\nenvironments. SoftZoo supports an extensive, naturally-inspired material set,\nincluding the ability to simulate environments such as flat ground, desert,\nwetland, clay, ice, snow, shallow water, and ocean. Further, it provides a\nvariety of tasks relevant for soft robotics, including fast locomotion, agile\nturning, and path following, as well as differentiable design representations\nfor morphology and control. Combined, these elements form a feature-rich\nplatform for analysis and development of soft robot co-design algorithms. We\nbenchmark prevalent representations and co-design algorithms, and shed light on\n1) the interplay between environment, morphology, and behavior; 2) the\nimportance of design space representations; 3) the ambiguity in muscle\nformation and controller synthesis; and 4) the value of differentiable physics.\nWe envision that SoftZoo will serve as a standard platform and template an\napproach toward the development of novel representations and algorithms for\nco-designing soft robots' behavioral and morphological intelligence.\n","authors":["Tsun-Hsuan Wang","Pingchuan Ma","Andrew Everett Spielberg","Zhou Xian","Hao Zhang","Joshua B. Tenenbaum","Daniela Rus","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.09555v1.pdf","comment":"ICLR 2023. Project page:\n  https://sites.google.com/view/softzoo-iclr-2023"},{"id":"http://arxiv.org/abs/2211.13757v2","updated":"2023-03-16T17:59:32Z","published":"2022-11-24T18:59:01Z","title":"Diffusion-SDF: Conditional Generative Modeling of Signed Distance\n  Functions","summary":"  Probabilistic diffusion models have achieved state-of-the-art results for\nimage synthesis, inpainting, and text-to-image tasks. However, they are still\nin the early stages of generating complex 3D shapes. This work proposes\nDiffusion-SDF, a generative model for shape completion, single-view\nreconstruction, and reconstruction of real-scanned point clouds. We use neural\nsigned distance functions (SDFs) as our 3D representation to parameterize the\ngeometry of various signals (e.g., point clouds, 2D images) through neural\nnetworks. Neural SDFs are implicit functions and diffusing them amounts to\nlearning the reversal of their neural network weights, which we solve using a\ncustom modulation module. Extensive experiments show that our method is capable\nof both realistic unconditional generation and conditional generation from\npartial inputs. This work expands the domain of diffusion models from learning\n2D, explicit representations, to 3D, implicit representations.\n","authors":["Gene Chou","Yuval Bahat","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2211.13757v2.pdf","comment":"revised experiments and added link to code and supplement"},{"id":"http://arxiv.org/abs/2303.09554v1","updated":"2023-03-16T17:59:22Z","published":"2023-03-16T17:59:22Z","title":"PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D\n  Supervision","summary":"  Impressive progress in generative models and implicit representations gave\nrise to methods that can generate 3D shapes of high quality. However, being\nable to locally control and edit shapes is another essential property that can\nunlock several content creation applications. Local control can be achieved\nwith part-aware models, but existing methods require 3D supervision and cannot\nproduce textures. In this work, we devise PartNeRF, a novel part-aware\ngenerative model for editable 3D shape synthesis that does not require any\nexplicit 3D supervision. Our model generates objects as a set of locally\ndefined NeRFs, augmented with an affine transformation. This enables several\nediting operations such as applying transformations on parts, mixing parts from\ndifferent objects etc. To ensure distinct, manipulable parts we enforce a hard\nassignment of rays to parts that makes sure that the color of each ray is only\ndetermined by a single NeRF. As a result, altering one part does not affect the\nappearance of the others. Evaluations on various ShapeNet categories\ndemonstrate the ability of our model to generate editable 3D objects of\nimproved fidelity, compared to previous part-based generative approaches that\nrequire 3D supervision or models relying on NeRFs.\n","authors":["Konstantinos Tertikas","Pascalidou Despoina","Boxiao Pan","Jeong Joon Park","Mikaela Angelina Uy","Ioannis Emiris","Yannis Avrithis","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.09554v1.pdf","comment":"To appear in CVPR 2023, Project Page:\n  https://ktertikas.github.io/part_nerf"},{"id":"http://arxiv.org/abs/2303.09553v1","updated":"2023-03-16T17:59:20Z","published":"2023-03-16T17:59:20Z","title":"LERF: Language Embedded Radiance Fields","summary":"  Humans describe the physical world using natural language to refer to\nspecific 3D locations based on a vast range of properties: visual appearance,\nsemantics, abstract associations, or actionable affordances. In this work we\npropose Language Embedded Radiance Fields (LERFs), a method for grounding\nlanguage embeddings from off-the-shelf models like CLIP into NeRF, which enable\nthese types of open-ended language queries in 3D. LERF learns a dense,\nmulti-scale language field inside NeRF by volume rendering CLIP embeddings\nalong training rays, supervising these embeddings across training views to\nprovide multi-view consistency and smooth the underlying language field. After\noptimization, LERF can extract 3D relevancy maps for a broad range of language\nprompts interactively in real-time, which has potential use cases in robotics,\nunderstanding vision-language models, and interacting with 3D scenes. LERF\nenables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings\nwithout relying on region proposals or masks, supporting long-tail\nopen-vocabulary queries hierarchically across the volume. The project website\ncan be found at https://lerf.io .\n","authors":["Justin Kerr","Chung Min Kim","Ken Goldberg","Angjoo Kanazawa","Matthew Tancik"],"pdf_url":"https://arxiv.org/pdf/2303.09553v1.pdf","comment":"Project website can be found at https://lerf.io"},{"id":"http://arxiv.org/abs/2303.09551v1","updated":"2023-03-16T17:59:08Z","published":"2023-03-16T17:59:08Z","title":"SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving","summary":"  3D scene understanding plays a vital role in vision-based autonomous driving.\nWhile most existing methods focus on 3D object detection, they have difficulty\ndescribing real-world objects of arbitrary shapes and infinite classes. Towards\na more comprehensive perception of a 3D scene, in this paper, we propose a\nSurroundOcc method to predict the 3D occupancy with multi-camera images. We\nfirst extract multi-scale features for each image and adopt spatial 2D-3D\nattention to lift them to the 3D volume space. Then we apply 3D convolutions to\nprogressively upsample the volume features and impose supervision on multiple\nlevels. To obtain dense occupancy prediction, we design a pipeline to generate\ndense occupancy ground truth without expansive occupancy annotations.\nSpecifically, we fuse multi-frame LiDAR scans of dynamic objects and static\nscenes separately. Then we adopt Poisson Reconstruction to fill the holes and\nvoxelize the mesh to get dense occupancy labels. Extensive experiments on\nnuScenes and SemanticKITTI datasets demonstrate the superiority of our method.\nCode and dataset are available at https://github.com/weiyithu/SurroundOcc\n","authors":["Yi Wei","Linqing Zhao","Wenzhao Zheng","Zheng Zhu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2303.09551v1.pdf","comment":"Code is available at https://github.com/weiyithu/SurroundOcc"},{"id":"http://arxiv.org/abs/2303.09541v1","updated":"2023-03-16T17:53:44Z","published":"2023-03-16T17:53:44Z","title":"Diffusion-HPC: Generating Synthetic Images with Realistic Humans","summary":"  Recent text-to-image generative models have exhibited remarkable abilities in\ngenerating high-fidelity and photo-realistic images. However, despite the\nvisually impressive results, these models often struggle to preserve plausible\nhuman structure in the generations. Due to this reason, while generative models\nhave shown promising results in aiding downstream image recognition tasks by\ngenerating large volumes of synthetic data, they remain infeasible for\nimproving downstream human pose perception and understanding. In this work, we\npropose Diffusion model with Human Pose Correction (Diffusion HPC), a\ntext-conditioned method that generates photo-realistic images with plausible\nposed humans by injecting prior knowledge about human body structure. We show\nthat Diffusion HPC effectively improves the realism of human generations.\nFurthermore, as the generations are accompanied by 3D meshes that serve as\nground truths, Diffusion HPC's generated image-mesh pairs are well-suited for\ndownstream human mesh recovery task, where a shortage of 3D training data has\nlong been an issue.\n","authors":["Zhenzhen Weng","Laura Bravo-Sánchez","Serena Yeung"],"pdf_url":"https://arxiv.org/pdf/2303.09541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09540v1","updated":"2023-03-16T17:53:24Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09536v1","updated":"2023-03-16T17:52:45Z","published":"2023-03-16T17:52:45Z","title":"Deep Metric Learning for Unsupervised Remote Sensing Change Detection","summary":"  Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from\nMulti-Temporal Remote Sensing Images (MT-RSIs), which aids in various RS\napplications such as land cover, land use, human development analysis, and\ndisaster response. The performance of existing RS-CD methods is attributed to\ntraining on large annotated datasets. Furthermore, most of these models are\nless transferable in the sense that the trained model often performs very\npoorly when there is a domain gap between training and test datasets. This\npaper proposes an unsupervised CD method based on deep metric learning that can\ndeal with both of these issues. Given an MT-RSI, the proposed method generates\ncorresponding change probability map by iteratively optimizing an unsupervised\nCD loss without training it on a large dataset. Our unsupervised CD method\nconsists of two interconnected deep networks, namely Deep-Change Probability\nGenerator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to\npredict change and no change probability maps for a given MT-RSI, while D-FE is\nused to extract deep features of MT-RSI that will be further used in the\nproposed unsupervised CD loss. We use transfer learning capability to\ninitialize the parameters of D-FE. We iteratively optimize the parameters of\nD-CPG and D-FE for a given MT-RSI by minimizing the proposed unsupervised\n``similarity-dissimilarity loss''. This loss is motivated by the principle of\nmetric learning where we simultaneously maximize the distance between change\npair-wise pixels while minimizing the distance between no-change pair-wise\npixels in bi-temporal image domain and their deep feature domain. The\nexperiments conducted on three CD datasets show that our unsupervised CD method\nachieves significant improvements over the state-of-the-art supervised and\nunsupervised CD methods. Code available at https://github.com/wgcban/Metric-CD\n","authors":["Wele Gedara Chaminda Bandara","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.09536v1.pdf","comment":"Code available at https://github.com/wgcban/Metric-CD"},{"id":"http://arxiv.org/abs/2303.09535v1","updated":"2023-03-16T17:51:13Z","published":"2023-03-16T17:51:13Z","title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing","summary":"  The diffusion-based generative models have achieved remarkable success in\ntext-based image generation. However, since it contains enormous randomness in\ngeneration progress, it is still challenging to apply such models for\nreal-world visual content editing, especially in videos. In this paper, we\npropose FateZero, a zero-shot text-based editing method on real-world videos\nwithout per-prompt training or use-specific mask. To edit videos consistently,\nwe propose several techniques based on the pre-trained models. Firstly, in\ncontrast to the straightforward DDIM inversion technique, our approach captures\nintermediate attention maps during inversion, which effectively retain both\nstructural and motion information. These maps are directly fused in the editing\nprocess rather than generated during denoising. To further minimize semantic\nleakage of the source video, we then fuse self-attentions with a blending mask\nobtained by cross-attention features from the source prompt. Furthermore, we\nhave implemented a reform of the self-attention mechanism in denoising UNet by\nintroducing spatial-temporal attention to ensure frame consistency. Yet\nsuccinct, our method is the first one to show the ability of zero-shot\ntext-driven video style and local attribute editing from the trained\ntext-to-image model. We also have a better zero-shot shape-aware editing\nability based on the text-to-video model. Extensive experiments demonstrate our\nsuperior temporal consistency and editing capability than previous works.\n","authors":["Chenyang Qi","Xiaodong Cun","Yong Zhang","Chenyang Lei","Xintao Wang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09535v1.pdf","comment":"Project page: https://fate-zero-edit.github.io; Github repository:\n  https://github.com/ChenyangQiQi/FateZero;"},{"id":"http://arxiv.org/abs/2303.09534v1","updated":"2023-03-16T17:51:02Z","published":"2023-03-16T17:51:02Z","title":"InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views","summary":"  We introduce an on-ground Pedestrian World Model, a computational model that\ncan predict how pedestrians move around an observer in the crowd on the ground\nplane, but from just the egocentric-views of the observer. Our model,\nInCrowdFormer, fully leverages the Transformer architecture by modeling\npedestrian interaction and egocentric to top-down view transformation with\nattention, and autoregressively predicts on-ground positions of a variable\nnumber of people with an encoder-decoder architecture. We encode the\nuncertainties arising from unknown pedestrian heights with latent codes to\npredict the posterior distributions of pedestrian positions. We validate the\neffectiveness of InCrowdFormer on a novel prediction benchmark of real\nmovements. The results show that InCrowdFormer accurately predicts the future\ncoordination of pedestrians. To the best of our knowledge, InCrowdFormer is the\nfirst-of-its-kind pedestrian world model which we believe will benefit a wide\nrange of egocentric-view applications including crowd navigation, tracking, and\nsynthesis.\n","authors":["Mai Nishimura","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2303.09534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09530v1","updated":"2023-03-16T17:46:32Z","published":"2023-03-16T17:46:32Z","title":"Tackling Clutter in Radar Data -- Label Generation and Detection Using\n  PointNet++","summary":"  Radar sensors employed for environment perception, e.g. in autonomous\nvehicles, output a lot of unwanted clutter. These points, for which no\ncorresponding real objects exist, are a major source of errors in following\nprocessing steps like object detection or tracking. We therefore present two\nnovel neural network setups for identifying clutter. The input data, network\narchitectures and training configuration are adjusted specifically for this\ntask. Special attention is paid to the downsampling of point clouds composed of\nmultiple sensor scans. In an extensive evaluation, the new setups display\nsubstantially better performance than existing approaches. Because there is no\nsuitable public data set in which clutter is annotated, we design a method to\nautomatically generate the respective labels. By applying it to existing data\nwith object annotations and releasing its code, we effectively create the first\nfreely available radar clutter data set representing real-world driving\nscenarios. Code and instructions are accessible at\nwww.github.com/kopp-j/clutter-ds.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2303.09530v1.pdf","comment":"To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), London, UK, 2023"},{"id":"http://arxiv.org/abs/2303.09523v1","updated":"2023-03-16T17:39:11Z","published":"2023-03-16T17:39:11Z","title":"Fast 3D Volumetric Image Reconstruction from 2D MRI Slices by Parallel\n  Processing","summary":"  Magnetic Resonance Imaging (MRI) is a technology for non-invasive imaging of\nanatomical features in detail. It can help in functional analysis of organs of\na specimen but it is very costly. In this work, methods for (i) virtual\nthree-dimensional (3D) reconstruction from a single sequence of two-dimensional\n(2D) slices of MR images of a human spine and brain along a single axis, and\n(ii) generation of missing inter-slice data are proposed. Our approach helps in\npreserving the edges, shape, size, as well as the internal tissue structures of\nthe object being captured. The sequence of original 2D slices along a single\naxis is divided into smaller equal sub-parts which are then reconstructed using\nedge preserved kriging interpolation to predict the missing slice information.\nIn order to speed up the process of interpolation, we have used multiprocessing\nby carrying out the initial interpolation on parallel cores. From the 3D matrix\nthus formed, shearlet transform is applied to estimate the edges considering\nthe 2D blocks along the $Z$ axis, and to minimize the blurring effect using a\nproposed mean-median logic. Finally, for visualization, the sub-matrices are\nmerged into a final 3D matrix. Next, the newly formed 3D matrix is split up\ninto voxels and marching cubes method is applied to get the approximate 3D\nimage for viewing. To the best of our knowledge it is a first of its kind\napproach based on kriging interpolation and multiprocessing for 3D\nreconstruction from 2D slices, and approximately 98.89\\% accuracy is achieved\nwith respect to similarity metrics for image comparison. The time required for\nreconstruction has also been reduced by approximately 70\\% with multiprocessing\neven for a large input data set compared to that with single core processing.\n","authors":["Somoballi Ghoshal","Shremoyee Goswami","Amlan Chakrabarti","Susmita Sur-Kolay"],"pdf_url":"https://arxiv.org/pdf/2303.09523v1.pdf","comment":"19 pages, figure 14"},{"id":"http://arxiv.org/abs/2303.09522v1","updated":"2023-03-16T17:38:15Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04968v2","updated":"2023-03-16T17:35:55Z","published":"2022-12-09T16:30:17Z","title":"SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse\n  Volume Rendering","summary":"  We propose an end-to-end inverse rendering pipeline called SupeRVol that\nallows us to recover 3D shape and material parameters from a set of color\nimages in a super-resolution manner. To this end, we represent both the\nbidirectional reflectance distribution function (BRDF) and the signed distance\nfunction (SDF) by multi-layer perceptrons. In order to obtain both the surface\nshape and its reflectance properties, we revert to a differentiable volume\nrenderer with a physically based illumination model that allows us to decouple\nreflectance and lighting. This physical model takes into account the effect of\nthe camera's point spread function thereby enabling a reconstruction of shape\nand material in a super-resolution quality. Experimental validation confirms\nthat SupeRVol achieves state of the art performance in terms of inverse\nrendering quality. It generates reconstructions that are sharper than the\nindividual input images, making this method ideally suited for 3D modeling from\nlow-resolution imagery.\n","authors":["Mohammed Brahimi","Bjoern Haefner","Tarun Yenamandra","Bastian Goldluecke","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2212.04968v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09330v2","updated":"2023-03-16T17:34:15Z","published":"2022-08-19T13:18:35Z","title":"Low-light Enhancement Method Based on Attention Map Net","summary":"  Low-light image enhancement is a crucial preprocessing task for some complex\nvision tasks. Target detection, image segmentation, and image recognition\noutcomes are all directly impacted by the impact of image enhancement. However,\nthe majority of the currently used image enhancement techniques do not produce\nsatisfactory outcomes, and these enhanced networks have relatively weak\nrobustness. We suggest an improved network called BrightenNet that uses U-Net\nas its primary structure and incorporates a number of different attention\nmechanisms as a solution to this issue. In a specific application, we employ\nthe network as the generator and LSGAN as the training framework to achieve\nbetter enhancement results. We demonstrate the validity of the proposed network\nBrightenNet in the experiments that follow in this paper. The results it\nproduced can both preserve image details and conform to human vision standards.\n","authors":["Mengfei Wu","Xucheng Xue","Taiji Lan","Xinwei Xu"],"pdf_url":"https://arxiv.org/pdf/2208.09330v2.pdf","comment":"This paper contains some errors in the analysis presented in the\n  introduction section, such as misunderstanding some of the improved methods\n  in comparison to traditional methods like histogram equalization. These\n  errors have impacted the quality and reliability of my research, and could\n  potentially mislead readers and colleagues"},{"id":"http://arxiv.org/abs/2303.09514v1","updated":"2023-03-16T17:31:40Z","published":"2023-03-16T17:31:40Z","title":"MATIS: Masked-Attention Transformers for Surgical Instrument\n  Segmentation","summary":"  We propose Masked-Attention Transformers for Surgical Instrument Segmentation\n(MATIS), a two-stage, fully transformer-based method that leverages modern\npixel-wise attention mechanisms for instrument segmentation. MATIS exploits the\ninstance-level nature of the task by employing a masked attention module that\ngenerates and classifies a set of fine instrument region proposals. Our method\nincorporates long-term video-level information through video transformers to\nimprove temporal consistency and enhance mask classification. We validate our\napproach in the two standard public benchmarks, Endovis 2017 and Endovis 2018.\nOur experiments demonstrate that MATIS' per-frame baseline outperforms previous\nstate-of-the-art methods and that including our temporal consistency module\nboosts our model's performance further.\n","authors":["Nicolás Ayobi","Alejandra Pérez-Rondón","Santiago Rodríguez","Pablo Arbeláez"],"pdf_url":"https://arxiv.org/pdf/2303.09514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09508v1","updated":"2023-03-16T17:24:41Z","published":"2023-03-16T17:24:41Z","title":"LDMVFI: Video Frame Interpolation with Latent Diffusion Models","summary":"  Existing works on video frame interpolation (VFI) mostly employ deep neural\nnetworks trained to minimize the L1 or L2 distance between their outputs and\nground-truth frames. Despite recent advances, existing VFI methods tend to\nproduce perceptually inferior results, particularly for challenging scenarios\nincluding large motions and dynamic textures. Towards developing\nperceptually-oriented VFI methods, we propose latent diffusion model-based VFI,\nLDMVFI. This approaches the VFI problem from a generative perspective by\nformulating it as a conditional generation problem. As the first effort to\naddress VFI using latent diffusion models, we rigorously benchmark our method\nfollowing the common evaluation protocol adopted in the existing VFI\nliterature. Our quantitative experiments and user study indicate that LDMVFI is\nable to interpolate video content with superior perceptual quality compared to\nthe state of the art, even in the high-resolution regime. Our source code will\nbe made available here.\n","authors":["Duolikun Danier","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2303.09508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11146v2","updated":"2023-03-16T17:17:37Z","published":"2022-12-13T12:42:12Z","title":"The Challenges of HTR Model Training: Feedback from the Project Donner\n  le gout de l'archive a l'ere numerique","summary":"  The arrival of handwriting recognition technologies offers new possibilities\nfor research in heritage studies. However, it is now necessary to reflect on\nthe experiences and the practices developed by research teams. Our use of the\nTranskribus platform since 2018 has led us to search for the most significant\nways to improve the performance of our handwritten text recognition (HTR)\nmodels which are made to transcribe French handwriting dating from the 17th\ncentury. This article therefore reports on the impacts of creating transcribing\nprotocols, using the language model at full scale and determining the best way\nto use base models in order to help increase the performance of HTR models.\nCombining all of these elements can indeed increase the performance of a single\nmodel by more than 20% (reaching a Character Error Rate below 5%). This article\nalso discusses some challenges regarding the collaborative nature of HTR\nplatforms such as Transkribus and the way researchers can share their data\ngenerated in the process of creating or training handwritten text recognition\nmodels.\n","authors":["Couture Beatrice","Verret Farah","Gohier Maxime","Deslandres Dominique"],"pdf_url":"https://arxiv.org/pdf/2212.11146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09495v1","updated":"2023-03-16T17:15:25Z","published":"2023-03-16T17:15:25Z","title":"Among Us: Adversarially Robust Collaborative Perception by Consensus","summary":"  Multiple robots could perceive a scene (e.g., detect objects) collaboratively\nbetter than individuals, although easily suffer from adversarial attacks when\nusing deep learning. This could be addressed by the adversarial defense, but\nits training requires the often-unknown attacking mechanism. Differently, we\npropose ROBOSAC, a novel sampling-based defense strategy generalizable to\nunseen attackers. Our key idea is that collaborative perception should lead to\nconsensus rather than dissensus in results compared to individual perception.\nThis leads to our hypothesize-and-verify framework: perception results with and\nwithout collaboration from a random subset of teammates are compared until\nreaching a consensus. In such a framework, more teammates in the sampled subset\noften entail better perception performance but require longer sampling time to\nreject potential attackers. Thus, we derive how many sampling trials are needed\nto ensure the desired size of an attacker-free subset, or equivalently, the\nmaximum size of such a subset that we can successfully sample within a given\nnumber of trials. We validate our method on the task of collaborative 3D object\ndetection in autonomous driving scenarios.\n","authors":["Yiming Li","Qi Fang","Jiamu Bai","Siheng Chen","Felix Juefei-Xu","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09494v1","updated":"2023-03-16T17:15:08Z","published":"2023-03-16T17:15:08Z","title":"Knowledge Distillation for Adaptive MRI Prostate Segmentation Based on\n  Limit-Trained Multi-Teacher Models","summary":"  With numerous medical tasks, the performance of deep models has recently\nexperienced considerable improvements. These models are often adept learners.\nYet, their intricate architectural design and high computational complexity\nmake deploying them in clinical settings challenging, particularly with devices\nwith limited resources. To deal with this issue, Knowledge Distillation (KD)\nhas been proposed as a compression method and an acceleration technology. KD is\nan efficient learning strategy that can transfer knowledge from a burdensome\nmodel (i.e., teacher model) to a lightweight model (i.e., student model). Hence\nwe can obtain a compact model with low parameters with preserving the teacher's\nperformance. Therefore, we develop a KD-based deep model for prostate MRI\nsegmentation in this work by combining features-based distillation with\nKullback-Leibler divergence, Lovasz, and Dice losses. We further demonstrate\nits effectiveness by applying two compression procedures: 1) distilling\nknowledge to a student model from a single well-trained teacher, and 2) since\nmost of the medical applications have a small dataset, we train multiple\nteachers that each one trained with a small set of images to learn an adaptive\nstudent model as close to the teachers as possible considering the desired\naccuracy and fast inference time. Extensive experiments were conducted on a\npublic multi-site prostate tumor dataset, showing that the proposed adaptation\nKD strategy improves the dice similarity score by 9%, outperforming all tested\nwell-established baseline models.\n","authors":["Eddardaa Ben Loussaief","Hatem Rashwan","Mohammed Ayad","Mohammed Zakaria Hassan","Domenec Puig"],"pdf_url":"https://arxiv.org/pdf/2303.09494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04558v2","updated":"2023-03-16T17:12:03Z","published":"2023-01-11T16:35:33Z","title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language\n  Processing","summary":"  Self-supervised learning in vision-language processing exploits semantic\nalignment between imaging and text modalities. Prior work in biomedical VLP has\nmostly relied on the alignment of single image and report pairs even though\nclinical notes commonly refer to prior images. This does not only introduce\npoor alignment between the modalities but also a missed opportunity to exploit\nrich self-supervision through existing temporal content in the data. In this\nwork, we explicitly account for prior images and reports when available during\nboth training and fine-tuning. Our approach, named BioViL-T, uses a\nCNN-Transformer hybrid multi-image encoder trained jointly with a text model.\nIt is designed to be versatile to arising challenges such as pose variations\nand missing input images across time. The resulting model excels on downstream\ntasks both in single- and multi-image setups, achieving state-of-the-art\nperformance on (I) progression classification, (II) phrase grounding, and (III)\nreport generation, whilst offering consistent improvements on disease\nclassification and sentence-similarity tasks. We release a novel multi-modal\ntemporal benchmark dataset, MS-CXR-T, to quantify the quality of\nvision-language representations in terms of temporal semantics. Our\nexperimental results show the advantages of incorporating prior images and\nreports to make most use of the data.\n","authors":["Shruthi Bannur","Stephanie Hyland","Qianchu Liu","Fernando Pérez-García","Maximilian Ilse","Daniel C. Castro","Benedikt Boecking","Harshita Sharma","Kenza Bouzid","Anja Thieme","Anton Schwaighofer","Maria Wetscherek","Matthew P. Lungren","Aditya Nori","Javier Alvarez-Valle","Ozan Oktay"],"pdf_url":"https://arxiv.org/pdf/2301.04558v2.pdf","comment":"To appear in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09484v1","updated":"2023-03-16T17:00:45Z","published":"2023-03-16T17:00:45Z","title":"A Novel Autoencoders-LSTM Model for Stroke Outcome Prediction using\n  Multimodal MRI Data","summary":"  Patient outcome prediction is critical in management of ischemic stroke. In\nthis paper, a novel machine learning model is proposed for stroke outcome\nprediction using multimodal Magnetic Resonance Imaging (MRI). The proposed\nmodel consists of two serial levels of Autoencoders (AEs), where different AEs\nat level 1 are used for learning unimodal features from different MRI\nmodalities and a AE at level 2 is used to combine the unimodal features into\ncompressed multimodal features. The sequences of multimodal features of a given\npatient are then used by an LSTM network for predicting outcome score. The\nproposed AE2-LSTM model is proved to be an effective approach for better\naddressing the multimodality and volumetric nature of MRI data. Experimental\nresults show that the proposed AE2-LSTM outperforms the existing state-of-the\nart models by achieving highest AUC=0.71 and lowest MAE=0.34.\n","authors":["Nima Hatami","Laura Mechtouff","David Rousseau","Tae-Hee Cho","Omer Eker","Yves Berthezene","Carole Frindel"],"pdf_url":"https://arxiv.org/pdf/2303.09484v1.pdf","comment":"The IEEE International Symposium on Biomedical Imaging (ISBI). arXiv\n  admin note: text overlap with arXiv:2205.05545"},{"id":"http://arxiv.org/abs/2303.09483v1","updated":"2023-03-16T17:00:42Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.15736v3","updated":"2023-03-16T16:51:02Z","published":"2022-11-28T19:33:39Z","title":"Post-training Quantization on Diffusion Models","summary":"  Denoising diffusion (score-based) generative models have recently achieved\nsignificant accomplishments in generating realistic and diverse data. These\napproaches define a forward diffusion process for transforming data into noise\nand a backward denoising process for sampling data from noise. Unfortunately,\nthe generation process of current denoising diffusion models is notoriously\nslow due to the lengthy iterative noise estimations, which rely on cumbersome\nneural networks. It prevents the diffusion models from being widely deployed,\nespecially on edge devices. Previous works accelerate the generation process of\ndiffusion model (DM) via finding shorter yet effective sampling trajectories.\nHowever, they overlook the cost of noise estimation with a heavy network in\nevery iteration. In this work, we accelerate generation from the perspective of\ncompressing the noise estimation network. Due to the difficulty of retraining\nDMs, we exclude mainstream training-aware compression paradigms and introduce\npost-training quantization (PTQ) into DM acceleration. However, the output\ndistributions of noise estimation networks change with time-step, making\nprevious PTQ methods fail in DMs since they are designed for single-time step\nscenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three\naspects: quantized operations, calibration dataset, and calibration metric. We\nsummarize and use several observations derived from all-inclusive\ninvestigations to formulate our method, which especially targets the unique\nmulti-time-step structure of DMs. Experimentally, our method can directly\nquantize full-precision DMs into 8-bit models while maintaining or even\nimproving their performance in a training-free manner. Importantly, our method\ncan serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM.\nThe code is available at https://github.com/42Shawn/PTQ4DM .\n","authors":["Yuzhang Shang","Zhihang Yuan","Bin Xie","Bingzhe Wu","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2211.15736v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09472v1","updated":"2023-03-16T16:47:14Z","published":"2023-03-16T16:47:14Z","title":"DiffIR: Efficient Diffusion Model for Image Restoration","summary":"  Diffusion model (DM) has achieved SOTA performance by modeling the image\nsynthesis process into a sequential application of a denoising network.\nHowever, different from image synthesis generating each pixel from scratch,\nmost pixels of image restoration (IR) are given. Thus, for IR, traditional DMs\nrunning massive iterations on a large model to estimate whole images or feature\nmaps is inefficient. To address this issue, we propose an efficient DM for IR\n(DiffIR), which consists of a compact IR prior extraction network (CPEN),\ndynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR\nhas two training stages: pretraining and training DM. In pretraining, we input\nground-truth images into CPEN$_{S1}$ to capture a compact IR prior\nrepresentation (IPR) to guide DIRformer. In the second stage, we train the DM\nto directly estimate the same IRP as pretrained CPEN$_{S1}$ only using LQ\nimages. We observe that since the IPR is only a compact vector, DiffIR can use\nfewer iterations than traditional DM to obtain accurate estimations and\ngenerate more stable and realistic results. Since the iterations are few, our\nDiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoising\nnetwork, which can further reduce the estimation error influence. We conduct\nextensive experiments on several IR tasks and achieve SOTA performance while\nconsuming less computational costs.\n","authors":["Bin Xia","Yulun Zhang","Shiyin Wang","Yitong Wang","Xinglong Wu","Yapeng Tian","Wenming Yang","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.09472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02408v2","updated":"2023-03-16T16:39:59Z","published":"2022-11-04T12:36:36Z","title":"Rickrolling the Artist: Injecting Backdoors into Text Encoders for\n  Text-to-Image Synthesis","summary":"  While text-to-image synthesis currently enjoys great popularity among\nresearchers and the general public, the security of these models has been\nneglected so far. Many text-guided image generation models rely on pre-trained\ntext encoders from external sources, and their users trust that the retrieved\nmodels will behave as promised. Unfortunately, this might not be the case. We\nintroduce backdoor attacks against text-guided generative models and\ndemonstrate that their text encoders pose a major tampering risk. Our attacks\nonly slightly alter an encoder so that no suspicious model behavior is apparent\nfor image generations with clean prompts. By then inserting a single character\ntrigger into the prompt, e.g., a non-Latin character or emoji, the adversary\ncan trigger the model to either generate images with pre-defined attributes or\nimages following a hidden, potentially malicious description. We empirically\ndemonstrate the high effectiveness of our attacks on Stable Diffusion and\nhighlight that the injection process of a single backdoor takes less than two\nminutes. Besides phrasing our approach solely as an attack, it can also force\nan encoder to forget phrases related to certain concepts, such as nudity or\nviolence, and help to make image generation safer.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2211.02408v2.pdf","comment":"30 pages, 20 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.13185v2","updated":"2023-03-16T16:34:38Z","published":"2022-12-26T15:13:13Z","title":"Generalized Differentiable RANSAC","summary":"  We propose $\\nabla$-RANSAC, a generalized differentiable RANSAC that allows\nlearning the entire randomized robust estimation pipeline. The proposed\napproach enables the use of relaxation techniques for estimating the gradients\nin the sampling distribution, which are then propagated through a\ndifferentiable solver. The trainable quality function marginalizes over the\nscores from all the models estimated within $\\nabla$-RANSAC to guide the\nnetwork learning accurate and useful inlier probabilities or to train feature\ndetection and matching networks. Our method directly maximizes the probability\nof drawing a good hypothesis, allowing us to learn better sampling\ndistribution. We test $\\nabla$-RANSAC on a number of real-world scenarios on\nfundamental and essential matrix estimation, both outdoors and indoors, with\nhandcrafted and learning-based features. It is superior to the state-of-the-art\nin terms of accuracy while running at a similar speed to its less accurate\nalternatives. The code and trained models are available at\nhttps://github.com/weitong8591/differentiable_ransac.\n","authors":["Tong Wei","Yash Patel","Alexander Shekhovtsov","Jiri Matas","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2212.13185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09447v1","updated":"2023-03-16T16:23:13Z","published":"2023-03-16T16:23:13Z","title":"Steering Prototype with Prompt-tuning for Rehearsal-free Continual\n  Learning","summary":"  Prototype, as a representation of class embeddings, has been explored to\nreduce memory footprint or mitigate forgetting for continual learning\nscenarios. However, prototype-based methods still suffer from abrupt\nperformance deterioration due to semantic drift and prototype interference. In\nthis study, we propose Contrastive Prototypical Prompt (CPP) and show that\ntask-specific prompt-tuning, when optimized over a contrastive learning\nobjective, can effectively address both obstacles and significantly improve the\npotency of prototypes. Our experiments demonstrate that CPP excels in four\nchallenging class-incremental learning benchmarks, resulting in 4% to 6%\nabsolute improvements over state-of-the-art methods. Moreover, CPP does not\nrequire a rehearsal buffer and it largely bridges the performance gap between\ncontinual learning and offline joint-learning, showcasing a promising design\nscheme for continual learning systems under a Transformer architecture.\n","authors":["Zhuowei Li","Long Zhao","Zizhao Zhang","Han Zhang","Di Liu","Ting Liu","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2303.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09440v1","updated":"2023-03-16T16:15:42Z","published":"2023-03-16T16:15:42Z","title":"Enhanced detection of the presence and severity of COVID-19 from CT\n  scans using lung segmentation","summary":"  Improving automated analysis of medical imaging will provide clinicians more\noptions in providing care for patients. The 2023 AI-enabled Medical Image\nAnalysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides\nan opportunity to test and refine machine learning methods for detecting the\npresence and severity of COVID-19 in patients from CT scans. This paper\npresents version 2 of Cov3d, a deep learning model submitted in the 2022\ncompetition. The model has been improved through a preprocessing step which\nsegments the lungs in the CT scan and crops the input to this region. It\nresults in a validation macro F1 score for predicting the presence of COVID-19\nin the CT scans at 92.2% which is significantly above the baseline of 74%. It\ngives a macro F1 score for predicting the severity of COVID-19 on the\nvalidation set for task 2 as 67% which is above the baseline of 38%.\n","authors":["Robert Turnbull"],"pdf_url":"https://arxiv.org/pdf/2303.09440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10408v2","updated":"2023-03-16T16:12:56Z","published":"2022-11-18T18:18:53Z","title":"Improved Cross-view Completion Pre-training for Stereo Matching and\n  Optical Flow","summary":"  Despite impressive performance for high-level downstream tasks,\nself-supervised pre-training methods have not yet fully delivered on dense\ngeometric vision tasks such as stereo matching or optical flow. The application\nof selfsupervised concepts, such as instance discrimination or masked image\nmodeling, to geometric tasks is an active area of research. In this work, we\nbuild on the recent crossview completion framework, a variation of masked image\nmodeling that leverages a second view from the same scene which makes it well\nsuited for binocular downstream tasks. The applicability of this concept has so\nfar been limited in at least two ways: (a) by the difficulty of collecting\nrealworld image pairs -- in practice only synthetic data have been used -- and\n(b) by the lack of generalization of vanilla transformers to dense downstream\ntasks for which relative position is more meaningful than absolute position. We\nexplore three avenues of improvement: first, we introduce a method to collect\nsuitable real-world image pairs at large scale. Second, we experiment with\nrelative positional embeddings and show that they enable vision transformers to\nperform substantially better. Third, we scale up vision transformer based\ncross-completion architectures, which is made possible by the use of large\namounts of data. With these improvements, we show for the first time that\nstateof-the-art results on stereo matching and optical flow can be reached\nwithout using any classical task-specific techniques like correlation volume,\niterative estimation, image warping or multi-scale reasoning, thus paving the\nway towards universal vision models.\n","authors":["Philippe Weinzaepfel","Thomas Lucas","Vincent Leroy","Yohann Cabon","Vaibhav Arora","Romain Brégier","Gabriela Csurka","Leonid Antsfeld","Boris Chidlovskii","Jérôme Revaud"],"pdf_url":"https://arxiv.org/pdf/2211.10408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09431v1","updated":"2023-03-16T16:06:03Z","published":"2023-03-16T16:06:03Z","title":"NeRFMeshing: Distilling Neural Radiance Fields into\n  Geometrically-Accurate 3D Meshes","summary":"  With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis\nhas recently made a big leap forward. At the core, NeRF proposes that each 3D\npoint can emit radiance, allowing to conduct view synthesis using\ndifferentiable volumetric rendering. While neural radiance fields can\naccurately represent 3D scenes for computing the image rendering, 3D meshes are\nstill the main scene representation supported by most computer graphics and\nsimulation pipelines, enabling tasks such as real time rendering and\nphysics-based simulations. Obtaining 3D meshes from neural radiance fields\nstill remains an open challenge since NeRFs are optimized for view synthesis,\nnot enforcing an accurate underlying geometry on the radiance field. We thus\npropose a novel compact and flexible architecture that enables easy 3D surface\nreconstruction from any NeRF-driven approach. Upon having trained the radiance\nfield, we distill the volumetric 3D representation into a Signed Surface\nApproximation Network, allowing easy extraction of the 3D mesh and appearance.\nOur final 3D mesh is physically accurate and can be rendered in real time on an\narray of devices.\n","authors":["Marie-Julie Rakotosaona","Fabian Manhardt","Diego Martin Arroyo","Michael Niemeyer","Abhijit Kundu","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2303.09431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09429v1","updated":"2023-03-16T16:02:24Z","published":"2023-03-16T16:02:24Z","title":"Data Roaming and Early Fusion for Composed Image Retrieval","summary":"  We study the task of Composed Image Retrieval (CoIR), where a query is\ncomposed of two modalities, image and text, extending the user's expression\nability. Previous methods typically address this task by a separate encoding of\neach query modality, followed by late fusion of the extracted features. In this\npaper, we propose a new approach, Cross-Attention driven Shift Encoder (CASE),\nemploying early fusion between modalities through a cross-attention module with\nan additional auxiliary task. We show that our method outperforms the existing\nstate-of-the-art, on established benchmarks (FashionIQ and CIRR) by a large\nmargin. However, CoIR datasets are a few orders of magnitude smaller compared\nto other vision and language (V&L) datasets, and some suffer from serious flaws\n(e.g., queries with a redundant modality). We address these shortcomings by\nintroducing Large Scale Composed Image Retrieval (LaSCo), a new CoIR dataset\nx10 times larger than current ones. Pre-training on LaSCo yields a further\nperformance boost. We further suggest a new analysis of CoIR datasets and\nmethods, for detecting modality redundancy or necessity, in queries.\n","authors":["Matan Levy","Rami Ben-Ari","Nir Darshan","Dani Lischinski"],"pdf_url":"https://arxiv.org/pdf/2303.09429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15656v2","updated":"2023-03-16T16:01:22Z","published":"2022-11-28T18:59:02Z","title":"SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation","summary":"  High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code and self-recorded\ndataset will be available at https://github.com/haomo-ai/SuperFusion.\n","authors":["Hao Dong","Xianjing Zhang","Jintao Xu","Rui Ai","Weihao Gu","Huimin Lu","Juho Kannala","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09427v1","updated":"2023-03-16T16:00:18Z","published":"2023-03-16T16:00:18Z","title":"Logical Implications for Visual Question Answering Consistency","summary":"  Despite considerable recent progress in Visual Question Answering (VQA)\nmodels, inconsistent or contradictory answers continue to cast doubt on their\ntrue reasoning capabilities. However, most proposed methods use indirect\nstrategies or strong assumptions on pairs of questions and answers to enforce\nmodel consistency. Instead, we propose a novel strategy intended to improve\nmodel performance by directly reducing logical inconsistencies. To do this, we\nintroduce a new consistency loss term that can be used by a wide range of the\nVQA models and which relies on knowing the logical relation between pairs of\nquestions and answers. While such information is typically not available in VQA\ndatasets, we propose to infer these logical relations using a dedicated\nlanguage model and use these in our proposed consistency loss function. We\nconduct extensive experiments on the VQA Introspect and DME datasets and show\nthat our method brings improvements to state-of-the-art VQA models, while being\nrobust across different architectures and settings.\n","authors":["Sergio Tascon-Morales","Pablo Márquez-Neila","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2303.09427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09417v1","updated":"2023-03-16T15:51:59Z","published":"2023-03-16T15:51:59Z","title":"All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and\n  Redundancy Reduction","summary":"  Nearest neighbour based methods have proved to be one of the most successful\nself-supervised learning (SSL) approaches due to their high generalization\ncapabilities. However, their computational efficiency decreases when more than\none neighbour is used. In this paper, we propose a novel contrastive SSL\napproach, which we call All4One, that reduces the distance between neighbour\nrepresentations using ''centroids'' created through a self-attention mechanism.\nWe use a Centroid Contrasting objective along with single Neighbour Contrasting\nand Feature Contrasting objectives. Centroids help in learning contextual\ninformation from multiple neighbours whereas the neighbour contrast enables\nlearning representations directly from the neighbours and the feature contrast\nallows learning representations unique to the features. This combination\nenables All4One to outperform popular instance discrimination approaches by\nmore than 1% on linear classification evaluation for popular benchmark datasets\nand obtains state-of-the-art (SoTA) results. Finally, we show that All4One is\nrobust towards embedding dimensionalities and augmentations, surpassing NNCLR\nand Barlow Twins by more than 5% on low dimensionality and weak augmentation\nsettings. The source code would be made available soon.\n","authors":["Imanol G. Estepa","Ignacio Sarasúa","Bhalaji Nagarajan","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2303.09417v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09416v1","updated":"2023-03-16T15:49:24Z","published":"2023-03-16T15:49:24Z","title":"Symbolic Perception Risk in Autonomous Driving","summary":"  We develop a novel framework to assess the risk of misperception in a traffic\nsign classification task in the presence of exogenous noise. We consider the\nproblem in an autonomous driving setting, where visual input quality gradually\nimproves due to improved resolution, and less noise since the distance to\ntraffic signs decreases. Using the estimated perception statistics obtained\nusing the standard classification algorithms, we aim to quantify the risk of\nmisperception to mitigate the effects of imperfect visual observation. By\nexploring perception outputs, their expected high-level actions, and potential\ncosts, we show the closed-form representation of the conditional value-at-risk\n(CVaR) of misperception. Several case studies support the effectiveness of our\nproposed methodology.\n","authors":["Guangyi Liu","Disha Kamale","Cristian-Ioan Vasile","Nader Motee"],"pdf_url":"https://arxiv.org/pdf/2303.09416v1.pdf","comment":"Accepted at 2023 American Control Conference"},{"id":"http://arxiv.org/abs/2303.09412v1","updated":"2023-03-16T15:44:31Z","published":"2023-03-16T15:44:31Z","title":"NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing\n  Diverse Intrinsic and Extrinsic Camera Parameters","summary":"  Novel view synthesis using neural radiance fields (NeRF) is the\nstate-of-the-art technique for generating high-quality images from novel\nviewpoints. Existing methods require a priori knowledge about extrinsic and\nintrinsic camera parameters. This limits their applicability to synthetic\nscenes, or real-world scenarios with the necessity of a preprocessing step.\nCurrent research on the joint optimization of camera parameters and NeRF\nfocuses on refining noisy extrinsic camera parameters and often relies on the\npreprocessing of intrinsic camera parameters. Further approaches are limited to\ncover only one single camera intrinsic. To address these limitations, we\npropose a novel end-to-end trainable approach called NeRFtrinsic Four. We\nutilize Gaussian Fourier features to estimate extrinsic camera parameters and\ndynamically predict varying intrinsic camera parameters through the supervision\nof the projection error. Our approach outperforms existing joint optimization\nmethods on LLFF and BLEFF. In addition to these existing datasets, we introduce\na new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic\nFour is a step forward in joint optimization NeRF-based view synthesis and\nenables more realistic and flexible rendering in real-world scenarios with\nvarying camera parameters.\n","authors":["Hannah Schieber","Fabian Deuser","Bernhard Egger","Norbert Oswald","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09410v1","updated":"2023-03-16T15:44:15Z","published":"2023-03-16T15:44:15Z","title":"Narrator: Towards Natural Control of Human-Scene Interaction Generation\n  via Relationship Reasoning","summary":"  Naturally controllable human-scene interaction (HSI) generation has an\nimportant role in various fields, such as VR/AR content creation and\nhuman-centered AI. However, existing methods are unnatural and unintuitive in\ntheir controllability, which heavily limits their application in practice.\nTherefore, we focus on a challenging task of naturally and controllably\ngenerating realistic and diverse HSIs from textual descriptions. From human\ncognition, the ideal generative model should correctly reason about spatial\nrelationships and interactive actions. To that end, we propose Narrator, a\nnovel relationship reasoning-based generative approach using a conditional\nvariation autoencoder for naturally controllable generation given a 3D scene\nand a textual description. Also, we model global and local spatial\nrelationships in a 3D scene and a textual description respectively based on the\nscene graph, and introduce a partlevel action mechanism to represent\ninteractions as atomic body part states. In particular, benefiting from our\nrelationship reasoning, we further propose a simple yet effective multi-human\ngeneration strategy, which is the first exploration for controllable\nmulti-human scene interaction generation. Our extensive experiments and\nperceptual studies show that Narrator can controllably generate diverse\ninteractions and significantly outperform existing works. The code and dataset\nwill be available for research purposes.\n","authors":["Haibiao Xuan","Xiongzheng Li","Jinsong Zhang","Hongwen Zhang","Yebin Liu","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2303.09410v1.pdf","comment":"Project page: http://cic.tju.edu.cn/faculty/likun/projects/Narrator"},{"id":"http://arxiv.org/abs/2211.12872v2","updated":"2023-03-16T15:19:40Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v2.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2303.09383v1","updated":"2023-03-16T15:13:09Z","published":"2023-03-16T15:13:09Z","title":"Predicting Human Attention using Computational Attention","summary":"  Most models of visual attention are aimed at predicting either top-down or\nbottom-up control, as studied using different visual search and free-viewing\ntasks. We propose Human Attention Transformer (HAT), a single model predicting\nboth forms of attention control. HAT is the new state-of-the-art (SOTA) in\npredicting the scanpath of fixations made during target-present and\ntarget-absent search, and matches or exceeds SOTA in the prediction of taskless\nfree-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel\ntransformer-based architecture and a simplified foveated retina that\ncollectively create a spatio-temporal awareness akin to the dynamic visual\nworking memory of humans. Unlike previous methods that rely on a coarse grid of\nfixation cells and experience information loss due to fixation discretization,\nHAT features a dense-prediction architecture and outputs a dense heatmap for\neach fixation, thus avoiding discretizing fixations. HAT sets a new standard in\ncomputational attention, which emphasizes both effectiveness and generality.\nHAT's demonstrated scope and applicability will likely inspire the development\nof new attention models that can better predict human behavior in various\nattention-demanding scenarios.\n","authors":["Zhibo Yang","Sounak Mondal","Seoyoung Ahn","Gregory Zelinsky","Minh Hoai","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.09383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07207v2","updated":"2023-03-16T15:12:07Z","published":"2022-12-14T13:10:27Z","title":"MAELi -- Masked Autoencoder for Large-Scale LiDAR Point Clouds","summary":"  We demonstrate how the often overlooked inherent properties of large-scale\nLiDAR point clouds can be effectively utilized for self-supervised\nrepresentation learning. In pursuit of this goal, we design a highly\ndata-efficient feature pre-training backbone that considerably reduces the need\nfor tedious 3D annotations to train state-of-the-art object detectors. We\npropose Masked AutoEncoder for LiDAR point clouds (MAELi) that intuitively\nleverages the sparsity of LiDAR point clouds in both the encoder and decoder\nduring reconstruction. Our approach results in more expressive and useful\nfeatures, which can be directly applied to downstream perception tasks, such as\n3D object detection for autonomous driving. In a novel reconstruction schema,\nMAELi distinguishes between free and occluded space and employs a new masking\nstrategy that targets the LiDAR's inherent spherical projection. To demonstrate\nthe potential of MAELi, we pre-train one of the most widely-used 3D backbones\nin an end-to-end manner and show the effectiveness of our unsupervised\npre-trained features on various 3D object detection architectures. Our method\nachieves significant performance improvements when only a small fraction of\nlabeled frames is available for fine-tuning object detectors. For instance,\nwith ~800 labeled frames, MAELi features enhance a SECOND model by\n+10.79APH/LEVEL 2 on Waymo Vehicles.\n","authors":["Georg Krispel","David Schinagl","Christian Fruhwirth-Reisinger","Horst Possegger","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2212.07207v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2303.09375v1","updated":"2023-03-16T15:04:10Z","published":"2023-03-16T15:04:10Z","title":"DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human\n  Avatars","summary":"  We present DINAR, an approach for creating realistic rigged fullbody avatars\nfrom single RGB images. Similarly to previous works, our method uses neural\ntextures combined with the SMPL-X body model to achieve photo-realistic quality\nof avatars while keeping them easy to animate and fast to infer. To restore the\ntexture, we use a latent diffusion model and show how such model can be trained\nin the neural texture space. The use of the diffusion model allows us to\nrealistically reconstruct large unseen regions such as the back of a person\ngiven the frontal view. The models in our pipeline are trained using 2D images\nand videos only. In the experiments, our approach achieves state-of-the-art\nrendering quality and good generalization to new poses and viewpoints. In\nparticular, the approach improves state-of-the-art on the SnapshotPeople public\nbenchmark.\n","authors":["David Svitov","Dmitrii Gudkov","Renat Bashirov","Victor Lemptisky"],"pdf_url":"https://arxiv.org/pdf/2303.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09373v1","updated":"2023-03-16T15:01:50Z","published":"2023-03-16T15:01:50Z","title":"3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive\n  Segmentation of Heterogeneous Infant Brain MRI","summary":"  Robust segmentation of infant brain MRI across multiple ages, modalities, and\nsites remains challenging due to the intrinsic heterogeneity caused by\ndifferent MRI scanners, vendors, or acquisition sequences, as well as varying\nstages of neurodevelopment. To address this challenge, previous studies have\nexplored domain adaptation (DA) algorithms from various perspectives, including\nfeature alignment, entropy minimization, contrast synthesis (style transfer),\nand pseudo-labeling. This paper introduces a novel framework called MAPSeg\n(Masked Autoencoding and Pseudo-labelling Segmentation) to address the\nchallenges of cross-age, cross-modality, and cross-site segmentation of\nsubcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as\nwell as masked pseudo-labeling, the model is able to jointly learn from labeled\nsource domain data and unlabeled target domain data. We evaluated our framework\non expert-annotated datasets acquired from different ages and sites. MAPSeg\nconsistently outperformed other methods, including previous state-of-the-art\nsupervised baselines, domain generalization, and domain adaptation frameworks\nin segmenting subcortical regions regardless of age, modality, or acquisition\nsite. The code and pretrained encoder will be publicly available at\nhttps://github.com/XuzheZ/MAPSeg\n","authors":["Xuzhe Zhang","Yuhao Wu","Jia Guo","Jerod M. Rasmussen","Thomas G. O'Connor","Hyagriv N. Simhan","Sonja Entringer","Pathik D. Wadhwa","Claudia Buss","Cristiane S. Duarte","Andrea Jackowski","Hai Li","Jonathan Posner","Andrew F. Laine","Yun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09129v2","updated":"2023-03-16T14:57:16Z","published":"2022-12-18T16:53:13Z","title":"SUCRe: Leveraging Scene Structure for Underwater Color Restoration","summary":"  Underwater images are altered by the physical characteristics of the medium\nthrough which light rays pass before reaching the optical sensor. Scattering\nand wavelength-dependent absorption significantly modify the captured colors\ndepending on the distance of observed elements to the image plane. In this\npaper, we aim to recover an image of the scene as if the water had no effect on\nlight propagation. We introduce SUCRe, a new method that exploits the scene's\n3D structure for underwater color restoration. By following points in multiple\nimages and tracking their intensities at different distances to the sensor, we\nconstrain the optimization of the parameters in an underwater image formation\nmodel and retrieve unattenuated pixel intensities. We conduct extensive\nquantitative and qualitative analyses of our approach in a variety of scenarios\nranging from natural light to deep-sea environments using three underwater\ndatasets acquired from real-world scenarios and one synthetic dataset. We also\ncompare the performance of the proposed approach with that of a wide range of\nexisting state-of-the-art methods. The results demonstrate a consistent benefit\nof exploiting multiple views across a spectrum of objective metrics. Our code\nis publicly available at https://github.com/clementinboittiaux/sucre.\n","authors":["Clémentin Boittiaux","Ricard Marxer","Claire Dune","Aurélien Arnaubec","Maxime Ferrera","Vincent Hugel"],"pdf_url":"https://arxiv.org/pdf/2212.09129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09370v1","updated":"2023-03-16T14:55:31Z","published":"2023-03-16T14:55:31Z","title":"Learning Physical-Spatio-Temporal Features for Video Shadow Removal","summary":"  Shadow removal in a single image has received increasing attention in recent\nyears. However, removing shadows over dynamic scenes remains largely\nunder-explored. In this paper, we propose the first data-driven video shadow\nremoval model, termed PSTNet, by exploiting three essential characteristics of\nvideo shadows, i.e., physical property, spatio relation, and temporal\ncoherence. Specifically, a dedicated physical branch was established to conduct\nlocal illumination estimation, which is more applicable for scenes with complex\nlighting and textures, and then enhance the physical features via a mask-guided\nattention strategy. Then, we develop a progressive aggregation module to\nenhance the spatio and temporal characteristics of features maps, and\neffectively integrate the three kinds of features. Furthermore, to tackle the\nlack of datasets of paired shadow videos, we synthesize a dataset (SVSRD-85)\nwith aid of the popular game GTAV by controlling the switch of the shadow\nrenderer. Experiments against 9 state-of-the-art models, including image shadow\nremovers and image/video restoration methods, show that our method improves the\nbest SOTA in terms of RMSE error for the shadow area by 14.7. In addition, we\ndevelop a lightweight model adaptation strategy to make our synthetic-driven\nmodel effective in real world scenes. The visual comparison on the public\nSBU-TimeLapse dataset verifies the generalization ability of our model in real\nscenes.\n","authors":["Zhihao Chen","Liang Wan","Yefan Xiao","Lei Zhu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09354v1","updated":"2023-03-16T14:32:50Z","published":"2023-03-16T14:32:50Z","title":"The NCI Imaging Data Commons as a platform for reproducible research in\n  computational pathology","summary":"  Objective: Reproducibility is critical for translating machine learning-based\n(ML) solutions in computational pathology (CompPath) into practice. However, an\nincreasing number of studies report difficulties in reproducing ML results. The\nNCI Imaging Data Commons (IDC) is a public repository of >120 cancer image\ncollections, including >38,000 whole-slide images (WSIs), that is designed to\nbe used with cloud-based ML services. Here, we explore the potential of the IDC\nto facilitate reproducibility of CompPath research.\n  Materials and Methods: The IDC realizes the FAIR principles: All images are\nencoded according to the DICOM standard, persistently identified, discoverable\nvia rich metadata, and accessible via open tools. Taking advantage of this, we\nimplemented two experiments in which a representative ML-based method for\nclassifying lung tumor tissue was trained and/or evaluated on different\ndatasets from the IDC. To assess reproducibility, the experiments were run\nmultiple times with independent but identically configured sessions of common\nML services.\n  Results: The AUC values of different runs of the same experiment were\ngenerally consistent and in the same order of magnitude as a similar,\npreviously published study. However, there were occasional small variations in\nAUC values of up to 0.044, indicating a practical limit to reproducibility.\n  Discussion and conclusion: By realizing the FAIR principles, the IDC enables\nother researchers to reuse exactly the same datasets. Cloud-based ML services\nenable others to run CompPath experiments in an identically configured\ncomputing environment without having to own high-performance hardware. The\ncombination of both makes it possible to approach the reproducibility limit.\n","authors":["Daniela P. Schacherer","Markus D. Herrmann","David A. Clunie","Henning Höfener","William Clifford","William J. R. Longabaugh","Steve Pieper","Ron Kikinis","Andrey Fedorov","André Homeyer"],"pdf_url":"https://arxiv.org/pdf/2303.09354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09352v1","updated":"2023-03-16T14:32:22Z","published":"2023-03-16T14:32:22Z","title":"Hubs and Hyperspheres: Reducing Hubness and Improving Transductive\n  Few-shot Learning with Hyperspherical Embeddings","summary":"  Distance-based classification is frequently used in transductive few-shot\nlearning (FSL). However, due to the high-dimensionality of image\nrepresentations, FSL classifiers are prone to suffer from the hubness problem,\nwhere a few points (hubs) occur frequently in multiple nearest neighbour lists\nof other points. Hubness negatively impacts distance-based classification when\nhubs from one class appear often among the nearest neighbors of points from\nanother class, degrading the classifier's performance. To address the hubness\nproblem in FSL, we first prove that hubness can be eliminated by distributing\nrepresentations uniformly on the hypersphere. We then propose two new\napproaches to embed representations on the hypersphere, which we prove optimize\na tradeoff between uniformity and local similarity preservation -- reducing\nhubness while retaining class structure. Our experiments show that the proposed\nmethods reduce hubness, and significantly improves transductive FSL accuracy\nfor a wide range of classifiers.\n","authors":["Daniel J. Trosten","Rwiddhi Chakraborty","Sigurd Løkse","Kristoffer Knutsen Wickstrøm","Robert Jenssen","Michael C. Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2303.09352v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09347v1","updated":"2023-03-16T14:27:45Z","published":"2023-03-16T14:27:45Z","title":"CSSL-MHTR: Continual Self-Supervised Learning for Scalable Multi-script\n  Handwritten Text Recognition","summary":"  Self-supervised learning has recently emerged as a strong alternative in\ndocument analysis. These approaches are now capable of learning high-quality\nimage representations and overcoming the limitations of supervised methods,\nwhich require a large amount of labeled data. However, these methods are unable\nto capture new knowledge in an incremental fashion, where data is presented to\nthe model sequentially, which is closer to the realistic scenario. In this\npaper, we explore the potential of continual self-supervised learning to\nalleviate the catastrophic forgetting problem in handwritten text recognition,\nas an example of sequence recognition. Our method consists in adding\nintermediate layers called adapters for each task, and efficiently distilling\nknowledge from the previous model while learning the current task. Our proposed\nframework is efficient in both computation and memory complexity. To\ndemonstrate its effectiveness, we evaluate our method by transferring the\nlearned model to diverse text recognition downstream tasks, including Latin and\nnon-Latin scripts. As far as we know, this is the first application of\ncontinual self-supervised learning for handwritten text recognition. We attain\nstate-of-the-art performance on English, Italian and Russian scripts, whilst\nadding only a few parameters per task. The code and trained models will be\npublicly available.\n","authors":["Marwa Dhiaf","Mohamed Ali Souibgui","Kai Wang","Yuyang Liu","Yousri Kessentini","Alicia Fornés","Ahmed Cheikh Rouhou"],"pdf_url":"https://arxiv.org/pdf/2303.09347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.06867v2","updated":"2023-03-16T14:25:47Z","published":"2021-02-13T05:59:52Z","title":"CPP-Net: Context-aware Polygon Proposal Network for Nucleus Segmentation","summary":"  Nucleus segmentation is a challenging task due to the crowded distribution\nand blurry boundaries of nuclei. Recent approaches represent nuclei by means of\npolygons to differentiate between touching and overlapping nuclei and have\naccordingly achieved promising performance. Each polygon is represented by a\nset of centroid-to-boundary distances, which are in turn predicted by features\nof the centroid pixel for a single nucleus. However, using the centroid pixel\nalone does not provide sufficient contextual information for robust prediction\nand thus degrades the segmentation accuracy. To handle this problem, we propose\na Context-aware Polygon Proposal Network (CPP-Net) for nucleus segmentation.\nFirst, we sample a point set rather than one single pixel within each cell for\ndistance prediction. This strategy substantially enhances contextual\ninformation and thereby improves the robustness of the prediction. Second, we\npropose a Confidence-based Weighting Module, which adaptively fuses the\npredictions from the sampled point set. Third, we introduce a novel Shape-Aware\nPerceptual (SAP) loss that constrains the shape of the predicted polygons.\nHere, the SAP loss is based on an additional network that is pre-trained by\nmeans of mapping the centroid probability map and the pixel-to-boundary\ndistance maps to a different nucleus representation. Extensive experiments\njustify the effectiveness of each component in the proposed CPP-Net. Finally,\nCPP-Net is found to achieve state-of-the-art performance on three publicly\navailable databases, namely DSB2018, BBBC06, and PanNuke. Code of this paper is\navailable at \\url{https://github.com/csccsccsccsc/cpp-net\n","authors":["Shengcong Chen","Changxing Ding","Minfeng Liu","Jun Cheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2102.06867v2.pdf","comment":"Accepted Version to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2303.09340v1","updated":"2023-03-16T14:21:45Z","published":"2023-03-16T14:21:45Z","title":"Improving Automated Hemorrhage Detection in Sparse-view Computed\n  Tomography via Deep Convolutional Neural Network based Artifact Reduction","summary":"  Intracranial hemorrhage poses a serious health problem requiring rapid and\noften intensive medical treatment. For diagnosis, a Cranial Computed Tomography\n(CCT) scan is usually performed. However, the increased health risk caused by\nradiation is a concern. The most important strategy to reduce this potential\nrisk is to keep the radiation dose as low as possible and consistent with the\ndiagnostic task. Sparse-view CT can be an effective strategy to reduce dose by\nreducing the total number of views acquired, albeit at the expense of image\nquality. In this work, we use a U-Net architecture to reduce artifacts from\nsparse-view CCTs, predicting fully sampled reconstructions from sparse-view\nones. We evaluate the hemorrhage detectability in the predicted CCTs with a\nhemorrhage classification convolutional neural network, trained on fully\nsampled CCTs to detect and classify different sub-types of hemorrhages. Our\nresults suggest that the automated classification and detection accuracy of\nhemorrhages in sparse-view CCTs can be improved substantially by the U-Net.\nThis demonstrates the feasibility of rapid automated hemorrhage detection on\nlow-dose CT data to assist radiologists in routine clinical practice.\n","authors":["Johannes Thalhammer","Manuel Schultheiss","Tina Dorosti","Tobias Lasser","Franz Pfeiffer","Daniela Pfeiffer","Florian Schaff"],"pdf_url":"https://arxiv.org/pdf/2303.09340v1.pdf","comment":"11 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2302.11464v2","updated":"2023-03-16T14:20:14Z","published":"2023-02-22T15:57:03Z","title":"Debiased Mapping for Full-Reference Image Quality Assessment","summary":"  Mapping images to deep feature space for comparisons has been wildly adopted\nin recent learning-based full-reference image quality assessment (FR-IQA)\nmodels. Analogous to the classical classification task, the ideal mapping space\nfor quality regression should possess both inter-class separability and\nintra-class compactness. The inter-class separability that focuses on the\ndiscrimination of images with different quality levels has been highly\nemphasized in existing models. However, the intra-class compactness that\nmaintains small objective quality variance of images with the same or\nindistinguishable quality escapes the research attention, potentially leading\nto the perception-biased measures. In this paper, we reveal that such bias is\nmainly caused by the unsuitable subspace that the features are projected and\ncompared in. To account for this, we develop the Debiased Mapping based quality\nMeasure (DMM), which relies on the orthonormal bases of deep learning features\nformed by singular value decomposition (SVD). The SVD in deep learning feature\ndomain, which overwhelmingly separates the quality variations with singular\nvalues and projection bases, facilitates the quality inference with dedicatedly\ndesigned distance measure. Experiments on different IQA databases demonstrate\nthe mapping method is able to mitigate the perception bias efficiently, and the\nsuperior performance on quality prediction verifies the effectiveness of our\nmethod. The implementation will be publicly available.\n","authors":["Baoliang Chen","Hanwei Zhu","Lingyu Zhu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2302.11464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09339v1","updated":"2023-03-16T14:19:50Z","published":"2023-03-16T14:19:50Z","title":"ShabbyPages: A Reproducible Document Denoising and Binarization Dataset","summary":"  Document denoising and binarization are fundamental problems in the document\nprocessing space, but current datasets are often too small and lack sufficient\ncomplexity to effectively train and benchmark modern data-driven machine\nlearning models. To fill this gap, we introduce ShabbyPages, a new document\nimage dataset designed for training and benchmarking document denoisers and\nbinarizers. ShabbyPages contains over 6,000 clean \"born digital\" images with\nsynthetically-noised counterparts (\"shabby pages\") that were augmented using\nthe Augraphy document augmentation tool to appear as if they have been printed\nand faxed, photocopied, or otherwise altered through physical processes. In\nthis paper, we discuss the creation process of ShabbyPages and demonstrate the\nutility of ShabbyPages by training convolutional denoisers which remove real\nnoise features with a high degree of human-perceptible fidelity, establishing\nbaseline performance for a new ShabbyPages benchmark.\n","authors":["Alexander Groleau","Kok Wei Chee","Stefan Larson","Samay Maini","Jonathan Boarman"],"pdf_url":"https://arxiv.org/pdf/2303.09339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09336v1","updated":"2023-03-16T14:18:48Z","published":"2023-03-16T14:18:48Z","title":"Image Enhancement for Remote Photoplethysmography in a Low-Light\n  Environment","summary":"  With the improvement of sensor technology and significant algorithmic\nadvances, the accuracy of remote heart rate monitoring technology has been\nsignificantly improved. Despite of the significant algorithmic advances, the\nperformance of rPPG algorithm can degrade in the long-term, high-intensity\ncontinuous work occurred in evenings or insufficient light environments. One of\nthe main challenges is that the lost facial details and low contrast cause the\nfailure of detection and tracking. Also, insufficient lighting in video\ncapturing hurts the quality of physiological signal. In this paper, we collect\na large-scale dataset that was designed for remote heart rate estimation\nrecorded with various illumination variations to evaluate the performance of\nthe rPPG algorithm (Green, ICA, and POS). We also propose a low-light\nenhancement solution (technical solution) for remote heart rate estimation\nunder the low-light condition. Using collected dataset, we found 1) face\ndetection algorithm cannot detect faces in video captured in low light\nconditions; 2) A decrease in the amplitude of the pulsatile signal will lead to\nthe noise signal to be in the dominant position; and 3) the chrominance-based\nmethod suffers from the limitation in the assumption about skin-tone will not\nhold, and Green and ICA method receive less influence than POS in dark\nilluminance environment. The proposed solution for rPPG process is effective to\ndetect and improve the signal-to-noise ratio and precision of the pulsatile\nsignal.\n","authors":["Lin Xi","Weihai Chen","Changchen Zhao","Xingming Wu","Jianhua Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09336v1.pdf","comment":"Accepted by FG2020"},{"id":"http://arxiv.org/abs/2303.09334v1","updated":"2023-03-16T14:15:32Z","published":"2023-03-16T14:15:32Z","title":"Depth-Aware Image Compositing Model for Parallax Camera Motion Blur","summary":"  Camera motion introduces spatially varying blur due to the depth changes in\nthe 3D world. This work investigates scene configurations where such blur is\nproduced under parallax camera motion. We present a simple, yet accurate, Image\nCompositing Blur (ICB) model for depth-dependent spatially varying blur. The\n(forward) model produces realistic motion blur from a single image, depth map,\nand camera trajectory. Furthermore, we utilize the ICB model, combined with a\ncoordinate-based MLP, to learn a sharp neural representation from the blurred\ninput. Experimental results are reported for synthetic and real examples. The\nresults verify that the ICB forward model is computationally efficient and\nproduces realistic blur, despite the lack of occlusion information.\nAdditionally, our method for restoring a sharp representation proves to be a\ncompetitive approach for the deblurring task.\n","authors":["German F. Torres","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2303.09334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08320v2","updated":"2023-03-16T14:12:31Z","published":"2023-03-15T02:16:39Z","title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video\n  Generation","summary":"  A diffusion probabilistic model (DPM), which constructs a forward diffusion\nprocess by gradually adding noise to data points and learns the reverse\ndenoising process to generate new samples, has been shown to handle complex\ndata distribution. Despite its recent success in image synthesis, applying DPMs\nto video generation is still challenging due to high-dimensional data spaces.\nPrevious methods usually adopt a standard diffusion process, where frames in\nthe same video clip are destroyed with independent noises, ignoring the content\nredundancy and temporal correlation. This work presents a decomposed diffusion\nprocess via resolving the per-frame noise into a base noise that is shared\namong all frames and a residual noise that varies along the time axis. The\ndenoising pipeline employs two jointly-learned networks to match the noise\ndecomposition accordingly. Experiments on various datasets confirm that our\napproach, termed as VideoFusion, surpasses both GAN-based and diffusion-based\nalternatives in high-quality video generation. We further show that our\ndecomposed formulation can benefit from pre-trained image diffusion models and\nwell-support text-conditioned video creation.\n","authors":["Zhengxiong Luo","Dayou Chen","Yingya Zhang","Yan Huang","Liang Wang","Yujun Shen","Deli Zhao","Jingren Zhou","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.08320v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.09319v1","updated":"2023-03-16T13:50:20Z","published":"2023-03-16T13:50:20Z","title":"Unified Multi-Modal Latent Diffusion for Joint Subject and Text\n  Conditional Image Generation","summary":"  Language-guided image generation has achieved great success nowadays by using\ndiffusion models. However, texts can be less detailed to describe\nhighly-specific subjects such as a particular dog or a certain car, which makes\npure text-to-image generation not accurate enough to satisfy user requirements.\nIn this work, we present a novel Unified Multi-Modal Latent Diffusion\n(UMM-Diffusion) which takes joint texts and images containing specified\nsubjects as input sequences and generates customized images with the subjects.\nTo be more specific, both input texts and images are encoded into one unified\nmulti-modal latent space, in which the input images are learned to be projected\nto pseudo word embedding and can be further combined with text to guide image\ngeneration. Besides, to eliminate the irrelevant parts of the input images such\nas background or illumination, we propose a novel sampling technique of\ndiffusion models used by the image generator which fuses the results guided by\nmulti-modal input and pure text input. By leveraging the large-scale\npre-trained text-to-image generator and the designed image encoder, our method\nis able to generate high-quality images with complex semantics from both\naspects of input texts and images.\n","authors":["Yiyang Ma","Huan Yang","Wenjing Wang","Jianlong Fu","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2303.09319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09310v1","updated":"2023-03-16T13:35:56Z","published":"2023-03-16T13:35:56Z","title":"GLH-Water: A Large-Scale Dataset for Global Surface Water Detection in\n  Large-Size Very-High-Resolution Satellite Imagery","summary":"  Global surface water detection in very-high-resolution (VHR) satellite\nimagery can directly serve major applications such as refined flood mapping and\nwater resource assessment. Although achievements have been made in detecting\nsurface water in small-size satellite images corresponding to local geographic\nscales, datasets and methods suitable for mapping and analyzing global surface\nwater have yet to be explored. To encourage the development of this task and\nfacilitate the implementation of relevant applications, we propose the\nGLH-water dataset that consists of 250 satellite images and manually labeled\nsurface water annotations that are distributed globally and contain water\nbodies exhibiting a wide variety of types (e.g., rivers, lakes, and ponds in\nforests, irrigated fields, bare areas, and urban areas). Each image is of the\nsize 12,800 $\\times$ 12,800 pixels at 0.3 meter spatial resolution. To build a\nbenchmark for GLH-water, we perform extensive experiments employing\nrepresentative surface water detection models, popular semantic segmentation\nmodels, and ultra-high resolution segmentation models. Furthermore, we also\ndesign a strong baseline with the novel pyramid consistency loss (PCL) to\ninitially explore this challenge. Finally, we implement the cross-dataset and\npilot area generalization experiments, and the superior performance illustrates\nthe strong generalization and practical application of GLH-water. The dataset\nis available at https://jack-bo1220.github.io/project/GLH-water.html.\n","authors":["Yansheng Li","Bo Dang","Wanchun Li","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09307v1","updated":"2023-03-16T13:33:24Z","published":"2023-03-16T13:33:24Z","title":"Depth Super-Resolution from Explicit and Implicit High-Frequency\n  Features","summary":"  We propose a novel multi-stage depth super-resolution network, which\nprogressively reconstructs high-resolution depth maps from explicit and\nimplicit high-frequency features. The former are extracted by an efficient\ntransformer processing both local and global contexts, while the latter are\nobtained by projecting color images into the frequency domain. Both are\ncombined together with depth features by means of a fusion strategy within a\nmulti-stage and multi-scale framework. Experiments on the main benchmarks, such\nas NYUv2, Middlebury, DIML and RGBDD, show that our approach outperforms\nexisting methods by a large margin (~20% on NYUv2 and DIML against the\ncontemporary work DADA, with 16x upsampling), establishing a new\nstate-of-the-art in the guided depth super-resolution task.\n","authors":["Xin Qiao","Chenyang Ge","Youmin Zhang","Yanhui Zhou","Fabio Tosi","Matteo Poggi","Stefano Mattoccia"],"pdf_url":"https://arxiv.org/pdf/2303.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12417v3","updated":"2023-03-16T13:29:20Z","published":"2022-11-19T10:09:46Z","title":"ProCC: Progressive Cross-primitive Compatibility for Open-World\n  Compositional Zero-Shot Learning","summary":"  Open-World Compositional Zero-shot Learning (OW-CZSL) aims to recognize novel\ncompositions of state and object primitives in images with no priors on the\ncompositional space, which induces a tremendously large output space containing\nall possible state-object compositions. Existing works either learn the joint\ncompositional state-object embedding or predict simple primitives with separate\nclassifiers. However, the former heavily relies on external word embedding\nmethods, and the latter ignores the interactions of interdependent primitives,\nrespectively. In this paper, we revisit the primitive prediction approach and\npropose a novel method, termed Progressive Cross-primitive Compatibility\n(ProCC), to mimic the human learning process for OW-CZSL tasks. Specifically,\nthe cross-primitive compatibility module explicitly learns to model the\ninteractions of state and object features with the trainable memory units,\nwhich efficiently acquires cross-primitive visual attention to reason\nhigh-feasibility compositions, without the aid of external knowledge. Moreover,\nconsidering the partial-supervision setting (pCZSL) as well as the imbalance\nissue of multiple task prediction, we design a progressive training paradigm to\nenable the primitive classifiers to interact to obtain discriminative\ninformation in an easy-to-hard manner. Extensive experiments on three widely\nused benchmark datasets demonstrate that our method outperforms other\nrepresentative methods on both OW-CZSL and pCZSL settings by large margins.\n","authors":["Fushuo Huo","Wenchao Xu","Song Guo","Jingcai Guo","Haozhao Wang","Ziming Liu"],"pdf_url":"https://arxiv.org/pdf/2211.12417v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00647v2","updated":"2023-03-16T13:17:34Z","published":"2022-10-02T22:45:11Z","title":"IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable\n  Novel View Synthesis","summary":"  Existing inverse rendering combined with neural rendering\nmethods~\\cite{zhang2021physg, zhang2022modeling} can only perform editable\nnovel view synthesis on object-specific scenes, while we present intrinsic\nneural radiance fields, dubbed IntrinsicNeRF, which introduce intrinsic\ndecomposition into the NeRF-based~\\cite{mildenhall2020nerf} neural rendering\nmethod and can extend its application to room-scale scenes. Since intrinsic\ndecomposition is a fundamentally under-constrained inverse problem, we propose\na novel distance-aware point sampling and adaptive reflectance iterative\nclustering optimization method, which enables IntrinsicNeRF with traditional\nintrinsic decomposition constraints to be trained in an unsupervised manner,\nresulting in temporally consistent intrinsic decomposition results. To cope\nwith the problem that different adjacent instances of similar reflectance in a\nscene are incorrectly clustered together, we further propose a hierarchical\nclustering method with coarse-to-fine optimization to obtain a fast\nhierarchical indexing representation. It supports compelling real-time\naugmented applications such as recoloring and illumination variation. Extensive\nexperiments and editing samples on both object-specific/room-scale scenes and\nsynthetic/real-word data demonstrate that we can obtain consistent intrinsic\ndecomposition results and high-fidelity novel view synthesis even for\nchallenging sequences. Project page: https://zju3dv.github.io/intrinsic_nerf.\n","authors":["Weicai Ye","Shuo Chen","Chong Bao","Hujun Bao","Marc Pollefeys","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.00647v2.pdf","comment":"Project webpage: https://zju3dv.github.io/intrinsic_nerf/, code:\n  https://github.com/zju3dv/IntrinsicNeRF"},{"id":"http://arxiv.org/abs/2303.09295v1","updated":"2023-03-16T13:15:03Z","published":"2023-03-16T13:15:03Z","title":"DIRE for Diffusion-Generated Image Detection","summary":"  Diffusion models have shown remarkable success in visual synthesis, but have\nalso raised concerns about potential abuse for malicious purposes. In this\npaper, we seek to build a detector for telling apart real images from\ndiffusion-generated images. We find that existing detectors struggle to detect\nimages generated by diffusion models, even if we include generated images from\na specific diffusion model in their training data. To address this issue, we\npropose a novel image representation called DIffusion Reconstruction Error\n(DIRE), which measures the error between an input image and its reconstruction\ncounterpart by a pre-trained diffusion model. We observe that\ndiffusion-generated images can be approximately reconstructed by a diffusion\nmodel while real images cannot. It provides a hint that DIRE can serve as a\nbridge to distinguish generated and real images. DIRE provides an effective way\nto detect images generated by most diffusion models, and it is general for\ndetecting generated images from unseen diffusion models and robust to various\nperturbations. Furthermore, we establish a comprehensive diffusion-generated\nbenchmark including images generated by eight diffusion models to evaluate the\nperformance of diffusion-generated image detectors. Extensive experiments on\nour collected benchmark demonstrate that DIRE exhibits superiority over\nprevious generated-image detectors. The code and dataset are available at\nhttps://github.com/ZhendongWang6/DIRE.\n","authors":["Zhendong Wang","Jianmin Bao","Wengang Zhou","Weilun Wang","Hezhen Hu","Hong Chen","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2303.09295v1.pdf","comment":"A general diffusion-generated image detector"},{"id":"http://arxiv.org/abs/2303.09293v1","updated":"2023-03-16T13:13:13Z","published":"2023-03-16T13:13:13Z","title":"A transformer-based approach to video frame-level prediction in\n  Affective Behaviour Analysis In-the-wild","summary":"  In recent years, transformer architecture has been a dominating paradigm in\nmany applications, including affective computing. In this report, we propose\nour transformer-based model to handle Emotion Classification Task in the 5th\nAffective Behavior Analysis In-the-wild Competition. By leveraging the\nattentive model and the synthetic dataset, we attain a score of 0.4775 on the\nvalidation set of Aff-Wild2, the dataset provided by the organizer.\n","authors":["Dang-Khanh Nguyen","Ngoc-Huynh Ho","Sudarshan Pant","Hyung-Jeong Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09293v1.pdf","comment":"3 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.09289v1","updated":"2023-03-16T13:10:58Z","published":"2023-03-16T13:10:58Z","title":"Image Classifiers Leak Sensitive Attributes About Their Classes","summary":"  Neural network-based image classifiers are powerful tools for computer vision\ntasks, but they inadvertently reveal sensitive attribute information about\ntheir classes, raising concerns about their privacy. To investigate this\nprivacy leakage, we introduce the first Class Attribute Inference Attack\n(Caia), which leverages recent advances in text-to-image synthesis to infer\nsensitive attributes of individual classes in a black-box setting, while\nremaining competitive with related white-box attacks. Our extensive experiments\nin the face recognition domain show that Caia can accurately infer undisclosed\nsensitive attributes, such as an individual's hair color, gender and racial\nappearance, which are not part of the training labels. Interestingly, we\ndemonstrate that adversarial robust models are even more vulnerable to such\nprivacy leakage than standard models, indicating that a trade-off between\nrobustness and privacy exists.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Felix Friedrich","Manuel Brack","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2303.09289v1.pdf","comment":"40 pages, 32 figures, 4 tables"},{"id":"http://arxiv.org/abs/2210.09996v2","updated":"2023-03-16T13:07:42Z","published":"2022-10-18T17:01:35Z","title":"Perceptual Grouping in Contrastive Vision-Language Models","summary":"  Recent advances in zero-shot image recognition suggest that vision-language\nmodels learn generic visual representations with a high degree of semantic\ninformation that may be arbitrarily probed with natural language phrases.\nUnderstanding an image, however, is not just about understanding what content\nresides within an image, but importantly, where that content resides. In this\nwork we examine how well vision-language models are able to understand where\nobjects reside within an image and group together visually related parts of the\nimagery. We demonstrate how contemporary vision and language representation\nlearning models based on contrastive losses and large web-based data capture\nlimited object localization information. We propose a minimal set of\nmodifications that results in models that uniquely learn both semantic and\nspatial information. We measure this performance in terms of zero-shot image\nrecognition, unsupervised bottom-up and top-down semantic segmentations, as\nwell as robustness analyses. We find that the resulting model achieves\nstate-of-the-art results in terms of unsupervised segmentation, and demonstrate\nthat the learned representations are uniquely robust to spurious correlations\nin datasets designed to probe the causal behavior of vision models.\n","authors":["Kanchana Ranasinghe","Brandon McKinzie","Sachin Ravi","Yinfei Yang","Alexander Toshev","Jonathon Shlens"],"pdf_url":"https://arxiv.org/pdf/2210.09996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09270v1","updated":"2023-03-16T12:53:07Z","published":"2023-03-16T12:53:07Z","title":"SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a\n  Spectral Perspective","summary":"  Contrastive Language-Image Pre-Training (CLIP) has refreshed the state of the\nart for a broad range of vision-language cross-modal tasks. Particularly, it\nhas created an intriguing research line of text-guided image style transfer,\ndispensing with the need for style reference images as in traditional style\ntransfer methods. However, directly using CLIP to guide the transfer of style\nleads to undesirable artifacts (mainly written words and unrelated visual\nentities) spread over the image, partly due to the entanglement of visual and\nwritten concepts inherent in CLIP. Inspired by the use of spectral analysis in\nfiltering linguistic information at different granular levels, we analyse the\npatch embeddings from the last layer of the CLIP vision encoder from the\nperspective of spectral analysis and find that the presence of undesirable\nartifacts is highly correlated to some certain frequency components. We propose\nSpectralCLIP, which implements a spectral filtering layer on top of the CLIP\nvision encoder, to alleviate the artifact issue. Experimental results show that\nSpectralCLIP prevents the generation of artifacts effectively in quantitative\nand qualitative terms, without impairing the stylisation quality. We further\napply SpectralCLIP to text-conditioned image generation and show that it\nprevents written words in the generated images. Code is available at\nhttps://github.com/zipengxuc/SpectralCLIP.\n","authors":["Zipeng Xu","Songlong Xing","Enver Sangineto","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.09270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09269v1","updated":"2023-03-16T12:45:19Z","published":"2023-03-16T12:45:19Z","title":"ELFIS: Expert Learning for Fine-grained Image Recognition Using Subsets","summary":"  Fine-Grained Visual Recognition (FGVR) tackles the problem of distinguishing\nhighly similar categories. One of the main approaches to FGVR, namely subset\nlearning, tries to leverage information from existing class taxonomies to\nimprove the performance of deep neural networks. However, these methods rely on\nthe existence of handcrafted hierarchies that are not necessarily optimal for\nthe models. In this paper, we propose ELFIS, an expert learning framework for\nFGVR that clusters categories of the dataset into meta-categories using both\ndataset-inherent lexical and model-specific information. A set of neural\nnetworks-based experts are trained focusing on the meta-categories and are\nintegrated into a multi-task framework. Extensive experimentation shows\nimprovements in the SoTA FGVR benchmarks of up to +1.3% of accuracy using both\nCNNs and transformer-based networks. Overall, the obtained results evidence\nthat ELFIS can be applied on top of any classification model, enabling the\nobtention of SoTA results. The source code will be made public soon.\n","authors":["Pablo Villacorta","Jesús M. Rodríguez-de-Vera","Marc Bolaños","Ignacio Sarasúa","Bhalaji Nagarajan","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2303.09269v1.pdf","comment":"Pablo Villacorta and Jes\\'us M. Rodr\\'iguez-de-Vera contributed\n  equally to this work. 16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.09268v1","updated":"2023-03-16T12:44:44Z","published":"2023-03-16T12:44:44Z","title":"StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized\n  Tokenizer of a Large-Scale Generative Model","summary":"  Despite the progress made in the style transfer task, most previous work\nfocus on transferring only relatively simple features like color or texture,\nwhile missing more abstract concepts such as overall art expression or\npainter-specific traits. However, these abstract semantics can be captured by\nmodels like DALL-E or CLIP, which have been trained using huge datasets of\nimages and textual documents. In this paper, we propose StylerDALLE, a style\ntransfer method that exploits both of these models and uses natural language to\ndescribe abstract art styles. Specifically, we formulate the language-guided\nstyle transfer task as a non-autoregressive token sequence translation, i.e.,\nfrom input content image to output stylized image, in the discrete latent space\nof a large-scale pretrained vector-quantized tokenizer. To incorporate style\ninformation, we propose a Reinforcement Learning strategy with CLIP-based\nlanguage supervision that ensures stylization and content preservation\nsimultaneously. Experimental results demonstrate the superiority of our method,\nwhich can effectively transfer art styles using language instructions at\ndifferent granularities. Code is available at\nhttps://github.com/zipengxuc/StylerDALLE.\n","authors":["Zipeng Xu","Enver Sangineto","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.09268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07653v2","updated":"2023-03-16T12:22:50Z","published":"2023-03-14T06:45:13Z","title":"NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from\n  Multi-view Images","summary":"  We study the problem of reconstructing 3D feature curves of an object from a\nset of calibrated multi-view images. To do so, we learn a neural implicit field\nrepresenting the density distribution of 3D edges which we refer to as Neural\nEdge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based\nrendering loss where a 2D edge map is rendered at a given view and is compared\nto the ground-truth edge map extracted from the image of that view. The\nrendering-based differentiable optimization of NEF fully exploits 2D edge\ndetection, without needing a supervision of 3D edges, a 3D geometric operator\nor cross-view edge correspondence. Several technical designs are devised to\nensure learning a range-limited and view-independent NEF for robust edge\nextraction. The final parametric 3D curves are extracted from NEF with an\niterative optimization method. On our benchmark with synthetic data, we\ndemonstrate that NEF outperforms existing state-of-the-art methods on all\nmetrics. Project page: https://yunfan1202.github.io/NEF/.\n","authors":["Yunfan Ye","Renjiao Yi","Zhirui Gao","Chenyang Zhu","Zhiping Cai","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07653v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09253v1","updated":"2023-03-16T12:06:59Z","published":"2023-03-16T12:06:59Z","title":"A Survey of Deep Visual Cross-Domain Few-Shot Learning","summary":"  Few-Shot transfer learning has become a major focus of research as it allows\nrecognition of new classes with limited labeled data. While it is assumed that\ntrain and test data have the same data distribution, this is often not the case\nin real-world applications. This leads to decreased model transfer effects when\nthe new class distribution differs significantly from the learned classes.\nResearch into Cross-Domain Few-Shot (CDFS) has emerged to address this issue,\nforming a more challenging and realistic setting. In this survey, we provide a\ndetailed taxonomy of CDFS from the problem setting and corresponding solutions\nview. We summarise the existing CDFS network architectures and discuss the\nsolution ideas for each direction the taxonomy indicates. Furthermore, we\nintroduce various CDFS downstream applications and outline classification,\ndetection, and segmentation benchmarks and corresponding standards for\nevaluation. We also discuss the challenges of CDFS research and explore\npotential directions for future investigation. Through this review, we aim to\nprovide comprehensive guidance on CDFS research, enabling researchers to gain\ninsight into the state-of-the-art while allowing them to build upon existing\nsolutions to develop their own CDFS models.\n","authors":["Wenjian Wang","Lijuan Duan","Yuxi Wang","Junsong Fan","Zhi Gong","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09252v1","updated":"2023-03-16T12:06:02Z","published":"2023-03-16T12:06:02Z","title":"GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation\n  Learning","summary":"  A vision-language foundation model pretrained on very large-scale image-text\npaired data has the potential to provide generalizable knowledge representation\nfor downstream visual recognition and detection tasks, especially on\nsupplementing the undersampled categories in downstream model training. Recent\nstudies utilizing CLIP for object detection have shown that a two-stage\ndetector design typically outperforms a one-stage detector, while requiring\nmore expensive training resources and longer inference time. In this work, we\npropose a one-stage detector GridCLIP that narrows its performance gap to those\nof two-stage detectors, with approximately 43 and 5 times faster than its\ntwo-stage counterpart (ViLD) in the training and test process respectively.\nGridCLIP learns grid-level representations to adapt to the intrinsic principle\nof one-stage detection learning by expanding the conventional CLIP image-text\nholistic mapping to a more fine-grained, grid-text alignment. This differs from\nthe region-text mapping in two-stage detectors that apply CLIP directly by\ntreating regions as images. Specifically, GridCLIP performs Grid-level\nAlignment to adapt the CLIP image-level representations to grid-level\nrepresentations by aligning to CLIP category representations to learn the\nannotated (especially frequent) categories. To learn generalizable visual\nrepresentations of broader categories, especially undersampled ones, we perform\nImage-level Alignment during training to propagate broad pre-learned categories\nin the CLIP image encoder from the image-level to the grid-level\nrepresentations. Experiments show that the learned CLIP-based grid-level\nrepresentations boost the performance of undersampled (infrequent and novel)\ncategories, reaching comparable detection performance on the LVIS benchmark.\n","authors":["Jiayi Lin","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2303.09252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10278v3","updated":"2023-03-16T11:55:47Z","published":"2022-11-18T15:09:56Z","title":"Unsupervised 3D Pose Transfer with Cross Consistency and Dual\n  Reconstruction","summary":"  The goal of 3D pose transfer is to transfer the pose from the source mesh to\nthe target mesh while preserving the identity information (e.g., face, body\nshape) of the target mesh. Deep learning-based methods improved the efficiency\nand performance of 3D pose transfer. However, most of them are trained under\nthe supervision of the ground truth, whose availability is limited in\nreal-world scenarios. In this work, we present X-DualNet, a simple yet\neffective approach that enables unsupervised 3D pose transfer. In X-DualNet, we\nintroduce a generator $G$ which contains correspondence learning and pose\ntransfer modules to achieve 3D pose transfer. We learn the shape correspondence\nby solving an optimal transport problem without any key point annotations and\ngenerate high-quality meshes with our elastic instance normalization (ElaIN) in\nthe pose transfer module. With $G$ as the basic component, we propose a cross\nconsistency learning scheme and a dual reconstruction objective to learn the\npose transfer without supervision. Besides that, we also adopt an\nas-rigid-as-possible deformer in the training process to fine-tune the body\nshape of the generated results. Extensive experiments on human and animal data\ndemonstrate that our framework can successfully achieve comparable performance\nas the state-of-the-art supervised approaches.\n","authors":["Chaoyue Song","Jiacheng Wei","Ruibo Li","Fayao Liu","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2211.10278v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2109.15025"},{"id":"http://arxiv.org/abs/2303.09248v1","updated":"2023-03-16T11:53:29Z","published":"2023-03-16T11:53:29Z","title":"Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception\n  from Monocular Video","summary":"  We present a novel real-time capable learning method that jointly perceives a\n3D scene's geometry structure and semantic labels. Recent approaches to\nreal-time 3D scene reconstruction mostly adopt a volumetric scheme, where a\ntruncated signed distance function (TSDF) is directly regressed. However, these\nvolumetric approaches tend to focus on the global coherence of their\nreconstructions, which leads to a lack of local geometrical detail. To overcome\nthis issue, we propose to leverage the latent geometrical prior knowledge in 2D\nimage features by explicit depth prediction and anchored feature generation, to\nrefine the occupancy learning in TSDF volume. Besides, we find that this\ncross-dimensional feature refinement methodology can also be adopted for the\nsemantic segmentation task. Hence, we proposed an end-to-end cross-dimensional\nrefinement neural network (CDRNet) to extract both 3D mesh and 3D semantic\nlabeling in real time. The experiment results show that the proposed method\nachieves state-of-the-art 3D perception efficiency on multiple datasets, which\nindicates the great potential of our method for industrial applications.\n","authors":["Ziyang Hong","C. Patrick Yue"],"pdf_url":"https://arxiv.org/pdf/2303.09248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14888v2","updated":"2023-03-16T11:48:19Z","published":"2022-08-31T14:28:36Z","title":"Feature Alignment by Uncertainty and Self-Training for Source-Free\n  Unsupervised Domain Adaptation","summary":"  Most unsupervised domain adaptation (UDA) methods assume that labeled source\nimages are available during model adaptation. However, this assumption is often\ninfeasible owing to confidentiality issues or memory constraints on mobile\ndevices. Some recently developed approaches do not require source images during\nadaptation, but they show limited performance on perturbed images. To address\nthese problems, we propose a novel source-free UDA method that uses only a\npre-trained source model and unlabeled target images. Our method captures the\naleatoric uncertainty by incorporating data augmentation and trains the feature\ngenerator with two consistency objectives. The feature generator is encouraged\nto learn consistent visual features away from the decision boundaries of the\nhead classifier. Thus, the adapted model becomes more robust to image\nperturbations. Inspired by self-supervised learning, our method promotes\ninter-space alignment between the prediction space and the feature space while\nincorporating intra-space consistency within the feature space to reduce the\ndomain gap between the source and target domains. We also consider epistemic\nuncertainty to boost the model adaptation performance. Extensive experiments on\npopular UDA benchmark datasets demonstrate that the proposed source-free method\nis comparable or even superior to vanilla UDA methods. Moreover, the adapted\nmodels show more robust results when input images are perturbed.\n","authors":["JoonHo Lee","Gyemin Lee"],"pdf_url":"https://arxiv.org/pdf/2208.14888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09245v1","updated":"2023-03-16T11:45:52Z","published":"2023-03-16T11:45:52Z","title":"Cross-head Supervision for Crowd Counting with Noisy Annotations","summary":"  Noisy annotations such as missing annotations and location shifts often exist\nin crowd counting datasets due to multi-scale head sizes, high occlusion, etc.\nThese noisy annotations severely affect the model training, especially for\ndensity map-based methods. To alleviate the negative impact of noisy\nannotations, we propose a novel crowd counting model with one convolution head\nand one transformer head, in which these two heads can supervise each other in\nnoisy areas, called Cross-Head Supervision. The resultant model, CHS-Net, can\nsynergize different types of inductive biases for better counting. In addition,\nwe develop a progressive cross-head supervision learning strategy to stabilize\nthe training process and provide more reliable supervision. Extensive\nexperimental results on ShanghaiTech and QNRF datasets demonstrate superior\nperformance over state-of-the-art methods. Code is available at\nhttps://github.com/RaccoonDML/CHSNet.\n","authors":["Mingliang Dai","Zhizhong Huang","Jiaqi Gao","Hongming Shan","Junping Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09245v1.pdf","comment":"accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2211.11950v2","updated":"2023-03-16T11:43:07Z","published":"2022-11-22T02:04:09Z","title":"UpCycling: Semi-supervised 3D Object Detection without Sharing Raw-level\n  Unlabeled Scenes","summary":"  Semi-supervised Learning (SSL) has received increasing attention in\nautonomous driving to reduce the enormous burden of 3D annotation. In this\npaper, we propose UpCycling, a novel SSL framework for 3D object detection with\nzero additional raw-level point cloud: learning from unlabeled de-identified\nintermediate features (i.e., smashed data) to preserve privacy. Since these\nintermediate features are naturally produced by the inference pipeline, no\nadditional computation is required on autonomous vehicles. However, generating\neffective consistency loss for unlabeled feature-level scene turns out to be a\ncritical challenge. The latest SSL frameworks for 3D object detection that\nenforce consistency regularization between different augmentations of an\nunlabeled raw-point scene become detrimental when applied to intermediate\nfeatures. To solve the problem, we introduce a novel combination of hybrid\npseudo labels and feature-level Ground Truth sampling (F-GT), which safely\naugments unlabeled multi-type 3D scene features and provides high-quality\nsupervision. We implement UpCycling on two representative 3D object detection\nmodels: SECOND-IoU and PV-RCNN. Experiments on widely-used datasets (Waymo,\nKITTI, and Lyft) verify that UpCycling outperforms other augmentation methods\napplied at the feature level. In addition, while preserving privacy, UpCycling\nperforms better or comparably to the state-of-the-art methods that utilize\nraw-level unlabeled data in both domain adaptation and partial-label scenarios.\n","authors":["Sunwook Hwang","Youngseok Kim","Seongwon Kim","Saewoong Bahk","Hyung-Sin Kim"],"pdf_url":"https://arxiv.org/pdf/2211.11950v2.pdf","comment":"We have updated the results to fix errors in the experimental\n  process, which resulted in some logical changes. We also have added new\n  experiments related to privacy protection. The previous version (v1) has been\n  discarded"},{"id":"http://arxiv.org/abs/2303.09240v1","updated":"2023-03-16T11:35:59Z","published":"2023-03-16T11:35:59Z","title":"Human Reaction Intensity Estimation with Ensemble of Multi-task Networks","summary":"  Facial expression in-the-wild is essential for various interactive computing\ndomains. Especially, \"Emotional Reaction Intensity\" (ERI) is an important topic\nin the facial expression recognition task. In this paper, we propose a\nmulti-emotional task learning-based approach and present preliminary results\nfor the ERI challenge introduced in the 5th affective behavior analysis\nin-the-wild (ABAW) competition. Our method achieved the mean PCC score of\n0.3254.\n","authors":["JiYeon Oh","Daun Kim","Jae-Yeop Jeong","Yeong-Gi Hong","Jin-Woo Jeong"],"pdf_url":"https://arxiv.org/pdf/2303.09240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07347v2","updated":"2023-03-16T11:26:39Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v2.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.09234v1","updated":"2023-03-16T11:18:04Z","published":"2023-03-16T11:18:04Z","title":"NAISR: A 3D Neural Additive Model for Interpretable Shape Representation","summary":"  Deep implicit functions (DIFs) have emerged as a powerful paradigm for many\ncomputer vision tasks such as 3D shape reconstruction, generation,\nregistration, completion, editing, and understanding. However, given a set of\n3D shapes with associated covariates there is at present no shape\nrepresentation method which allows to precisely represent the shapes while\ncapturing the individual dependencies on each covariate. Such a method would be\nof high utility to researchers to discover knowledge hidden in a population of\nshapes. We propose a 3D Neural Additive Model for Interpretable Shape\nRepresentation (NAISR) which describes individual shapes by deforming a shape\natlas in accordance to the effect of disentangled covariates. Our approach\ncaptures shape population trends and allows for patient-specific predictions\nthrough shape transfer. NAISR is the first approach to combine the benefits of\ndeep implicit shape representations with an atlas deforming according to\nspecified covariates. Although our driving problem is the construction of an\nairway atlas, NAISR is a general approach for modeling, representing, and\ninvestigating shape populations. We evaluate NAISR with respect to shape\nreconstruction, shape disentanglement, shape evolution, and shape transfer for\nthe pediatric upper airway. Our experiments demonstrate that NAISR achieves\ncompetitive shape reconstruction performance while retaining interpretability.\n","authors":["Yining Jiao","Carlton Zdanski","Julia Kimbell","Andrew Prince","Cameron Worden","Samuel Kirse","Christopher Rutter","Benjamin Shields","William Dunn","Jisan Mahmud","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2303.09234v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2303.02455v3","updated":"2023-03-16T11:17:20Z","published":"2023-03-04T16:56:29Z","title":"DistilPose: Tokenized Pose Regression with Heatmap Distillation","summary":"  In the field of human pose estimation, regression-based methods have been\ndominated in terms of speed, while heatmap-based methods are far ahead in terms\nof performance. How to take advantage of both schemes remains a challenging\nproblem. In this paper, we propose a novel human pose estimation framework\ntermed DistilPose, which bridges the gaps between heatmap-based and\nregression-based methods. Specifically, DistilPose maximizes the transfer of\nknowledge from the teacher model (heatmap-based) to the student model\n(regression-based) through Token-distilling Encoder (TDE) and Simulated\nHeatmaps. TDE aligns the feature spaces of heatmap-based and regression-based\nmodels by introducing tokenization, while Simulated Heatmaps transfer explicit\nguidance (distribution and confidence) from teacher heatmaps into student\nmodels. Extensive experiments show that the proposed DistilPose can\nsignificantly improve the performance of the regression-based models while\nmaintaining efficiency. Specifically, on the MSCOCO validation dataset,\nDistilPose-S obtains 71.6% mAP with 5.36M parameter, 2.38 GFLOPs and 40.2 FPS,\nwhich saves 12.95x, 7.16x computational cost and is 4.9x faster than its\nteacher model with only 0.9 points performance drop. Furthermore, DistilPose-L\nobtains 74.4% mAP on MSCOCO validation dataset, achieving a new\nstate-of-the-art among predominant regression-based models.\n","authors":["Suhang Ye","Yingyi Zhang","Jie Hu","Liujuan Cao","Shengchuan Zhang","Lei Shen","Jun Wang","Shouhong Ding","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.02455v3.pdf","comment":"accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.09233v1","updated":"2023-03-16T11:16:02Z","published":"2023-03-16T11:16:02Z","title":"SwinVFTR: A Novel Volumetric Feature-learning Transformer for 3D OCT\n  Fluid Segmentation","summary":"  Accurately segmenting fluid in 3D volumetric optical coherence tomography\n(OCT) images is a crucial yet challenging task for detecting eye diseases.\nTraditional autoencoding-based segmentation approaches have limitations in\nextracting fluid regions due to successive resolution loss in the encoding\nphase and the inability to recover lost information in the decoding phase.\nAlthough current transformer-based models for medical image segmentation\naddresses this limitation, they are not designed to be applied out-of-the-box\nfor 3D OCT volumes, which have a wide-ranging channel-axis size based on\ndifferent vendor device and extraction technique. To address these issues, we\npropose SwinVFTR, a new transformer-based architecture designed for precise\nfluid segmentation in 3D volumetric OCT images. We first utilize a channel-wise\nvolumetric sampling for training on OCT volumes with varying depths (B-scans).\nNext, the model uses a novel shifted window transformer block in the encoder to\nachieve better localization and segmentation of fluid regions. Additionally, we\npropose a new volumetric attention block for spatial and depth-wise attention,\nwhich improves upon traditional residual skip connections. Consequently,\nutilizing multi-class dice loss, the proposed architecture outperforms other\nexisting architectures on the three publicly available vendor-specific OCT\ndatasets, namely Spectralis, Cirrus, and Topcon, with mean dice scores of 0.72,\n0.59, and 0.68, respectively. Additionally, SwinVFTR outperforms other\narchitectures in two additional relevant metrics, mean intersection-over-union\n(Mean-IOU) and structural similarity measure (SSIM).\n","authors":["Sharif Amit Kamran","Khondker Fariha Hossain","Alireza Tavakkoli","Stewart Lee Zuckerbrod","Kenton M. Sanders","Salah A. Baker"],"pdf_url":"https://arxiv.org/pdf/2303.09233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09232v1","updated":"2023-03-16T11:15:55Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wand","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12746v2","updated":"2023-03-16T11:12:35Z","published":"2022-09-26T14:55:21Z","title":"LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN\n  Latent Space","summary":"  As the methods evolve, inversion is mainly divided into two steps. The first\nstep is Image Embedding, in which an encoder or optimization process embeds\nimages to get the corresponding latent codes. Afterward, the second step aims\nto refine the inversion and editing results, which we named Result Refinement.\nAlthough the second step significantly improves fidelity, perception and\neditability are almost unchanged, deeply dependent on inverse latent codes\nattained in the first step. Therefore, a crucial problem is gaining the latent\ncodes with better perception and editability while retaining the reconstruction\nfidelity. In this work, we first point out that these two characteristics are\nrelated to the degree of alignment (or disalignment) of the inverse codes with\nthe synthetic distribution. Then, we propose Latent Space Alignment Inversion\nParadigm (LSAP), which consists of evaluation metric and solution for this\nproblem. Specifically, we introduce Normalized Style Space ($\\mathcal{S^N}$\nspace) and $\\mathcal{S^N}$ Cosine Distance (SNCD) to measure disalignment of\ninversion methods. Since our proposed SNCD is differentiable, it can be\noptimized in both encoder-based and optimization-based embedding methods to\nconduct a uniform solution. Extensive experiments in various domains\ndemonstrate that SNCD effectively reflects perception and editability, and our\nalignment paradigm archives the state-of-the-art in both two steps. Code is\navailable on https://github.com/caopulan/GANInverter/tree/main/configs/lsap.\n","authors":["Pu Cao","Lu Yang","Dongxu Liu","Zhiwei Liu","Shan Li","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2209.12746v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2303.09230v1","updated":"2023-03-16T11:09:22Z","published":"2023-03-16T11:09:22Z","title":"Towards a Smaller Student: Capacity Dynamic Distillation for Efficient\n  Image Retrieval","summary":"  Previous Knowledge Distillation based efficient image retrieval methods\nemploys a lightweight network as the student model for fast inference. However,\nthe lightweight student model lacks adequate representation capacity for\neffective knowledge imitation during the most critical early training period,\ncausing final performance degeneration. To tackle this issue, we propose a\nCapacity Dynamic Distillation framework, which constructs a student model with\neditable representation capacity. Specifically, the employed student model is\ninitially a heavy model to fruitfully learn distilled knowledge in the early\ntraining epochs, and the student model is gradually compressed during the\ntraining. To dynamically adjust the model capacity, our dynamic framework\ninserts a learnable convolutional layer within each residual block in the\nstudent model as the channel importance indicator. The indicator is optimized\nsimultaneously by the image retrieval loss and the compression loss, and a\nretrieval-guided gradient resetting mechanism is proposed to release the\ngradient conflict. Extensive experiments show that our method has superior\ninference speed and accuracy, e.g., on the VeRi-776 dataset, given the\nResNet101 as a teacher, our method saves 67.13% model parameters and 65.67%\nFLOPs (around 24.13% and 21.94% higher than state-of-the-arts) without\nsacrificing accuracy (around 2.11% mAP higher than state-of-the-arts).\n","authors":["Yi Xie","Huaidong Zhang","Xuemiao Xu","Jianqing Zhu","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2303.09230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05017v2","updated":"2023-03-16T11:03:09Z","published":"2023-02-10T02:24:56Z","title":"A survey on facial image deblurring","summary":"  When a facial image is blurred, it significantly affects high-level vision\ntasks such as face recognition. The purpose of facial image deblurring is to\nrecover a clear image from a blurry input image, which can improve the\nrecognition accuracy, etc. However, general deblurring methods do not perform\nwell on facial images. Therefore, some face deblurring methods have been\nproposed to improve performance by adding semantic or structural information as\nspecific priors according to the characteristics of the facial images. In this\npaper, we survey and summarize recently published methods for facial image\ndeblurring, most of which are based on deep learning. First, we provide a brief\nintroduction to the modeling of image blurring. Next, we summarize face\ndeblurring methods into two categories: model-based methods and deep\nlearning-based methods. Furthermore, we summarize the datasets, loss functions,\nand performance evaluation metrics commonly used in the neural network training\nprocess. We show the performance of classical methods on these datasets and\nmetrics and provide a brief discussion on the differences between model-based\nand learning-based methods. Finally, we discuss the current challenges and\npossible future research directions.\n","authors":["Bingnan Wang","Fanjiang Xu","Quan Zheng"],"pdf_url":"https://arxiv.org/pdf/2302.05017v2.pdf","comment":"Accepted to computational visual media"},{"id":"http://arxiv.org/abs/2208.06416v2","updated":"2023-03-16T11:02:59Z","published":"2022-08-15T04:30:00Z","title":"Uni6Dv2: Noise Elimination for 6D Pose Estimation","summary":"  Uni6D is the first 6D pose estimation approach to employ a unified backbone\nnetwork to extract features from both RGB and depth images. We discover that\nthe principal reasons of Uni6D performance limitations are Instance-Outside and\nInstance-Inside noise. Uni6D's simple pipeline design inherently introduces\nInstance-Outside noise from background pixels in the receptive field, while\nignoring Instance-Inside noise in the input depth data. In this paper, we\npropose a two-step denoising approach for dealing with the aforementioned noise\nin Uni6D. To reduce noise from non-instance regions, an instance segmentation\nnetwork is utilized in the first step to crop and mask the instance. A\nlightweight depth denoising module is proposed in the second step to calibrate\nthe depth feature before feeding it into the pose regression network. Extensive\nexperiments show that our Uni6Dv2 reliably and robustly eliminates noise,\noutperforming Uni6D without sacrificing too much inference efficiency. It also\nreduces the need for annotated real data that requires costly labeling.\n","authors":["Mingshan Sun","Ye Zheng","Tianpeng Bao","Jianqiu Chen","Guoqiang Jin","Liwei Wu","Rui Zhao","Xiaoke Jiang"],"pdf_url":"https://arxiv.org/pdf/2208.06416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12141v2","updated":"2023-03-16T10:58:20Z","published":"2023-01-28T09:31:20Z","title":"What Decreases Editing Capability? Domain-Specific Hybrid Refinement for\n  Improved GAN Inversion","summary":"  Recently, inversion methods have focused on additional high-rate information\nin the generator (e.g., weights or intermediate features) to refine inversion\nand editing results from embedded latent codes. Although these techniques gain\nreasonable improvement in reconstruction, they decrease editing capability,\nespecially on complex images (e.g., containing occlusions, detailed\nbackgrounds, and artifacts). A vital crux is refining inversion results,\navoiding editing capability degradation. To tackle this problem, we introduce\nDomain-Specific Hybrid Refinement (DHR), which draws on the advantages and\ndisadvantages of two mainstream refinement techniques to maintain editing\nability with fidelity improvement. Specifically, we first propose\nDomain-Specific Segmentation to segment images into two parts: in-domain and\nout-of-domain parts. The refinement process aims to maintain the editability\nfor in-domain areas and improve two domains' fidelity. We refine these two\nparts by weight modulation and feature modulation, which we call Hybrid\nModulation Refinement. Our proposed method is compatible with all latent code\nembedding methods. Extension experiments demonstrate that our approach achieves\nstate-of-the-art in real image inversion and editing. Code is available at\nhttps://github.com/caopulan/GANInverter/tree/main/configs/dhr.\n","authors":["Pu Cao","Lu Yang","Dongxu Liu","Shan Li","Yao Zhang","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2301.12141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00307v2","updated":"2023-03-16T10:57:44Z","published":"2022-02-01T10:10:13Z","title":"Laplacian2Mesh: Laplacian-Based Mesh Understanding","summary":"  Geometric deep learning has sparked a rising interest in computer graphics to\nperform shape understanding tasks, such as shape classification and semantic\nsegmentation. When the input is a polygonal surface, one has to suffer from the\nirregular mesh structure. Motivated by the geometric spectral theory, we\nintroduce Laplacian2Mesh, a novel and flexible convolutional neural network\n(CNN) framework for coping with irregular triangle meshes (vertices may have\nany valence). By mapping the input mesh surface to the multi-dimensional\nLaplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis\ntasks directly using the mature CNNs, without the need to deal with the\nirregular connectivity of the mesh structure. We further define a mesh pooling\noperation such that the receptive field of the network can be expanded while\nretaining the original vertex set as well as the connections between them.\nBesides, we introduce a channel-wise self-attention block to learn the\nindividual importance of feature ingredients. Laplacian2Mesh not only decouples\nthe geometry from the irregular connectivity of the mesh structure but also\nbetter captures the global features that are central to shape classification\nand segmentation. Extensive tests on various datasets demonstrate the\neffectiveness and efficiency of Laplacian2Mesh, particularly in terms of the\ncapability of being vulnerable to noise to fulfill various learning tasks.\n","authors":["Qiujie Dong","Zixiong Wang","Manyi Li","Junjie Gao","Shuangmin Chen","Zhenyu Shu","Shiqing Xin","Changhe Tu","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2202.00307v2.pdf","comment":"Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)"},{"id":"http://arxiv.org/abs/2303.09219v1","updated":"2023-03-16T10:48:59Z","published":"2023-03-16T10:48:59Z","title":"MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with\n  Cycle Consistency","summary":"  3D single object tracking (SOT) is an indispensable part of automated\ndriving. Existing approaches rely heavily on large, densely labeled datasets.\nHowever, annotating point clouds is both costly and time-consuming. Inspired by\nthe great success of cycle tracking in unsupervised 2D SOT, we introduce the\nfirst semi-supervised approach to 3D SOT. Specifically, we introduce two\ncycle-consistency strategies for supervision: 1) Self tracking cycles, which\nleverage labels to help the model converge better in the early stages of\ntraining; 2) forward-backward cycles, which strengthen the tracker's robustness\nto motion variations and the template noise caused by the template update\nstrategy. Furthermore, we propose a data augmentation strategy named SOTMixup\nto improve the tracker's robustness to point cloud diversity. SOTMixup\ngenerates training samples by sampling points in two point clouds with a mixing\nrate and assigns a reasonable loss weight for training according to the mixing\nrate. The resulting MixCycle approach generalizes to appearance matching-based\ntrackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trained\nwith $\\textbf{10%}$ labels outperforms P2B trained with $\\textbf{100%}$ labels,\nand achieves a $\\textbf{28.4%}$ precision improvement when using $\\textbf{1%}$\nlabels. Our code will be publicly released.\n","authors":["Qiao Wu","Jiaqi Yang","Kun Sun","Chu'ai Zhang","Yanning Zhang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.09219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02710v2","updated":"2023-03-16T10:47:13Z","published":"2022-12-06T02:11:34Z","title":"Beyond Object Recognition: A New Benchmark towards Object Concept\n  Learning","summary":"  Understanding objects is a central building block of artificial intelligence,\nespecially for embodied AI. Even though object recognition excels with deep\nlearning, current machines still struggle to learn higher-level knowledge,\ne.g., what attributes an object has, and what can we do with an object. In this\nwork, we propose a challenging Object Concept Learning (OCL) task to push the\nenvelope of object understanding. It requires machines to reason out object\naffordances and simultaneously give the reason: what attributes make an object\npossesses these affordances. To support OCL, we build a densely annotated\nknowledge base including extensive labels for three levels of object concept\n(category, attribute, affordance), and the causal relations of three levels. By\nanalyzing the causal structure of OCL, we present a baseline, Object Concept\nReasoning Network (OCRN). It leverages causal intervention and concept\ninstantiation to infer the three levels following their causal relations. In\nexperiments, OCRN effectively infers the object knowledge while following the\ncausalities well. Our data and code are available at https://mvig-rhos.com/ocl.\n","authors":["Yong-Lu Li","Yue Xu","Xinyu Xu","Xiaohan Mao","Yuan Yao","Siqi Liu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2212.02710v2.pdf","comment":"Preprint. Webpage: https://mvig-rhos.com/ocl"},{"id":"http://arxiv.org/abs/2303.09212v1","updated":"2023-03-16T10:35:32Z","published":"2023-03-16T10:35:32Z","title":"GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense\n  Supervision","summary":"  Airway segmentation, especially bronchioles segmentation, is an important but\nchallenging task because distal bronchus are sparsely distributed and of a fine\nscale. Existing neural networks usually exploit sparse topology to learn the\nconnectivity of bronchioles and inefficient shallow features to capture such\nhigh-frequency information, leading to the breakage or missed detection of\nindividual thin branches. To address these problems, we contribute a new\nbronchial segmentation method based on Group Deep Dense Supervision (GDDS) that\nemphasizes fine-scale bronchioles segmentation in a simple-but-effective\nmanner. First, Deep Dense Supervision (DDS) is proposed by constructing local\ndense topology skillfully and implementing dense topological learning on a\nspecific shallow feature layer. GDDS further empowers the shallow features with\nbetter perception ability to detect bronchioles, even the ones that are not\neasily discernible to the naked eye. Extensive experiments on the BAS benchmark\ndataset have shown that our method promotes the network to have a high\nsensitivity in capturing fine-scale branches and outperforms state-of-the-art\nmethods by a large margin (+12.8 % in BD and +8.8 % in TD) while only\nintroducing a small number of extra parameters.\n","authors":["Mingyue Zhao","Shang Zhao","Quan Quan","Li Fan","Xiaolan Qiu","Shiyuan Liu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09212v1.pdf","comment":"Submitted to MICCAI2023"},{"id":"http://arxiv.org/abs/2303.09200v1","updated":"2023-03-16T10:19:14Z","published":"2023-03-16T10:19:14Z","title":"Reduction of rain-induced errors for wind speed estimation on SAR\n  observations using convolutional neural networks","summary":"  Synthetic Aperture Radar is known to be able to provide high-resolution\nestimates of surface wind speed. These estimates usually rely on a Geophysical\nModel Function (GMF) that has difficulties accounting for non-wind processes\nsuch as rain events. Convolutional neural network, on the other hand, have the\ncapacity to use contextual information and have demonstrated their ability to\ndelimit rainfall areas. By carefully building a large dataset of SAR\nobservations from the Copernicus Sentinel-1 mission, collocated with both GMF\nand atmospheric model wind speeds as well as rainfall estimates, we were able\nto train a wind speed estimator with reduced errors under rain. Collocations\nwith in-situ wind speed measurements from buoys show a root mean square error\nthat is reduced by 27% (resp. 45%) under rainfall estimated at more than 1 mm/h\n(resp. 3 mm/h). These results demonstrate the capacity of deep learning models\nto correct rain-related errors in SAR products.\n","authors":["Aurélien Colin","Pierre Tandeo","Charles Peureux","Romain Husson","Ronan Fablet"],"pdf_url":"https://arxiv.org/pdf/2303.09200v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09199v1","updated":"2023-03-16T10:17:33Z","published":"2023-03-16T10:17:33Z","title":"A Generative Model for Digital Camera Noise Synthesis","summary":"  Noise synthesis is a challenging low-level vision task aiming to generate\nrealistic noise given a clean image along with the camera settings. To this\nend, we propose an effective generative model which utilizes clean features as\nguidance followed by noise injections into the network. Specifically, our\ngenerator follows a UNet-like structure with skip connections but without\ndownsampling and upsampling layers. Firstly, we extract deep features from a\nclean image as the guidance and concatenate a Gaussian noise map to the\ntransition point between the encoder and decoder as the noise source. Secondly,\nwe propose noise synthesis blocks in the decoder in each of which we inject\nGaussian noise to model the noise characteristics. Thirdly, we propose to\nutilize an additional Style Loss and demonstrate that this allows better noise\ncharacteristics supervision in the generator. Through a number of new\nexperiments, we evaluate the temporal variance and the spatial correlation of\nthe generated noise which we hope can provide meaningful insights for future\nworks. Finally, we show that our proposed approach outperforms existing methods\nfor synthesizing camera noise.\n","authors":["Mingyang Song","Yang Zhang","Tunç O. Aydın","Elham Amin Mansour","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2303.09199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07976v2","updated":"2023-03-16T10:16:49Z","published":"2022-03-15T15:05:40Z","title":"On the Pitfalls of Batch Normalization for End-to-End Video Learning: A\n  Study on Surgical Workflow Analysis","summary":"  Batch Normalization's (BN) unique property of depending on other samples in a\nbatch is known to cause problems in several tasks, including sequential\nmodeling. Yet, BN-related issues are hardly studied for long video\nunderstanding, despite the ubiquitous use of BN in CNNs for feature extraction.\nEspecially in surgical workflow analysis, where the lack of pretrained feature\nextractors has lead to complex, multi-stage training pipelines, limited\nawareness of BN issues may have hidden the benefits of training CNNs and\ntemporal models end to end. In this paper, we %present and analyze known as\nwell as novel pitfalls of BN in video learning, including issues specific to\nonline tasks such as a 'cheating' effect in anticipation. We observe that BN's\nproperties create major obstacles for end-to-end learning. However, using\nBN-free backbones, even simple CNN-LSTMs beat state of the art in two surgical\ntasks by utilizing adequate end-to-end training strategies which maximize\ntemporal context. We conclude that awareness of BN's pitfalls is crucial for\neffective end-to-end learning in surgical tasks. By reproducing results on\nnatural-video datasets, we hope our insights will benefit other areas of video\nlearning as well. Code: \\url{https://gitlab.com/nct_tso_public/pitfalls_bn}.\n","authors":["Dominik Rivoir","Isabel Funke","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2203.07976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03770v2","updated":"2023-03-16T10:09:12Z","published":"2023-03-07T10:04:55Z","title":"Guiding Pseudo-labels with Uncertainty Estimation for Source-free\n  Unsupervised Domain Adaptation","summary":"  Standard Unsupervised Domain Adaptation (UDA) methods assume the availability\nof both source and target data during the adaptation. In this work, we\ninvestigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific\ncase of UDA where a model is adapted to a target domain without access to\nsource data. We propose a novel approach for the SF-UDA setting based on a loss\nreweighting strategy that brings robustness against the noise that inevitably\naffects the pseudo-labels. The classification loss is reweighted based on the\nreliability of the pseudo-labels that is measured by estimating their\nuncertainty. Guided by such reweighting strategy, the pseudo-labels are\nprogressively refined by aggregating knowledge from neighbouring samples.\nFurthermore, a self-supervised contrastive framework is leveraged as a target\nspace regulariser to enhance such knowledge aggregation. A novel negative pairs\nexclusion strategy is proposed to identify and exclude negative pairs made of\nsamples sharing the same class, even in presence of some noise in the\npseudo-labels. Our method outperforms previous methods on three major\nbenchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C\nand DomainNet with a performance gain of +1.8% on both benchmarks and on PACS\nwith +12.3% in the single-source setting and +6.6% in multi-target adaptation.\nAdditional analyses demonstrate that the proposed approach is robust to the\nnoise, which results in significantly more accurate pseudo-labels compared to\nstate-of-the-art approaches.\n","authors":["Mattia Litrico","Alessio Del Bue","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2303.03770v2.pdf","comment":"To be published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.09190v1","updated":"2023-03-16T10:01:12Z","published":"2023-03-16T10:01:12Z","title":"A Framework for Real-time Object Detection and Image Restoration","summary":"  Object detection and single image super-resolution are classic problems in\ncomputer vision (CV). The object detection task aims to recognize the objects\nin input images, while the image restoration task aims to reconstruct high\nquality images from given low quality images. In this paper, a two-stage\nframework for object detection and image restoration is proposed. The first\nstage uses YOLO series algorithms to complete the object detection and then\nperforms image cropping. In the second stage, this work improves Swin\nTransformer and uses the new proposed algorithm to connect the Swin Transformer\nlayer to design a new neural network architecture. We name the newly proposed\nnetwork for image restoration SwinOIR. This work compares the model performance\nof different versions of YOLO detection algorithms on MS COCO dataset and\nPascal VOC dataset, demonstrating the suitability of different YOLO network\nmodels for the first stage of the framework in different scenarios. For image\nsuper-resolution task, it compares the model performance of using different\nmethods of connecting Swin Transformer layers and design different sizes of\nSwinOIR for use in different life scenarios. Our implementation code is\nreleased at https://github.com/Rubbbbbbbbby/SwinOIR.\n","authors":["Rui-Yang Ju","Chih-Chia Chen","Jen-Shiun Chiang","Yu-Shian Lin","Wei-Han Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10276v2","updated":"2023-03-16T10:01:05Z","published":"2023-02-11T07:30:21Z","title":"See Your Heart: Psychological states Interpretation through Visual\n  Creations","summary":"  In psychoanalysis, generating interpretations to one's psychological state\nthrough visual creations is facing significant demands. The two main tasks of\nexisting studies in the field of computer vision, sentiment/emotion\nclassification and affective captioning, can hardly satisfy the requirement of\npsychological interpreting. To meet the demands for psychoanalysis, we\nintroduce a challenging task, \\textbf{V}isual \\textbf{E}motion\n\\textbf{I}nterpretation \\textbf{T}ask (VEIT). VEIT requires AI to generate\nreasonable interpretations of creator's psychological state through visual\ncreations. To support the task, we present a multimodal dataset termed SpyIn\n(\\textbf{S}and\\textbf{p}la\\textbf{y} \\textbf{In}terpretation Dataset), which is\npsychological theory supported and professional annotated. Dataset analysis\nillustrates that SpyIn is not only able to support VEIT, but also more\nchallenging compared with other captioning datasets. Building on SpyIn, we\nconduct experiments of several image captioning method, and propose a\nvisual-semantic combined model which obtains a SOTA result on SpyIn. The\nresults indicate that VEIT is a more challenging task requiring scene graph\ninformation and psychological knowledge. Our work also show a promise for AI to\nanalyze and explain inner world of humanity through visual creations.\n","authors":["Likun Yang","Xiaokun Feng","Xiaotang Chen","Shiyu Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2302.10276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02469v2","updated":"2023-03-16T09:59:52Z","published":"2022-12-05T18:24:06Z","title":"One-shot Implicit Animatable Avatars with Model-based Priors","summary":"  Existing neural rendering methods for creating human avatars typically either\nrequire dense input signals such as video or multi-view images, or leverage a\nlearned prior from large-scale specific 3D human datasets such that\nreconstruction can be performed with sparse-view inputs. Most of these methods\nfail to achieve realistic reconstruction when only a single image is available.\nTo enable the data-efficient creation of realistic animatable 3D humans, we\npropose ELICIT, a novel method for learning human-specific neural radiance\nfields from a single image. Inspired by the fact that humans can effortlessly\nestimate the body geometry and imagine full-body clothing from a single image,\nwe leverage two priors in ELICIT: 3D geometry prior and visual semantic prior.\nSpecifically, ELICIT utilizes the 3D body shape geometry prior from a skinned\nvertex-based template model (i.e., SMPL) and implements the visual clothing\nsemantic prior with the CLIP-based pre-trained models. Both priors are used to\njointly guide the optimization for creating plausible content in the invisible\nareas. Taking advantage of the CLIP models, ELICIT can use text descriptions to\ngenerate text-conditioned unseen regions. In order to further improve visual\ndetails, we propose a segmentation-based sampling strategy that locally refines\ndifferent parts of the avatar. Comprehensive evaluations on multiple popular\nbenchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT\nhas outperformed strong baseline methods of avatar creation when only a single\nimage is available. The code is public for research purposes at\nhttps://elicit3d.github.io/\n","authors":["Yangyi Huang","Hongwei Yi","Weiyang Liu","Haofan Wang","Boxi Wu","Wenxiao Wang","Binbin Lin","Debing Zhang","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2212.02469v2.pdf","comment":"Project website: https://elicit3d.github.io"},{"id":"http://arxiv.org/abs/2303.09187v1","updated":"2023-03-16T09:55:43Z","published":"2023-03-16T09:55:43Z","title":"PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with\n  Progressive Video Transformers","summary":"  Existing methods of multi-person video 3D human Pose and Shape Estimation\n(PSE) typically adopt a two-stage strategy, which first detects human instances\nin each frame and then performs single-person PSE with temporal model. However,\nthe global spatio-temporal context among spatial instances can not be captured.\nIn this paper, we propose a new end-to-end multi-person 3D Pose and Shape\nestimation framework with progressive Video Transformer, termed PSVT. In PSVT,\na spatio-temporal encoder (STE) captures the global feature dependencies among\nspatial objects. Then, spatio-temporal pose decoder (STPD) and shape decoder\n(STSD) capture the global dependencies between pose queries and feature tokens,\nshape queries and feature tokens, respectively. To handle the variances of\nobjects as time proceeds, a novel scheme of progressive decoding is used to\nupdate pose and shape queries at each frame. Besides, we propose a novel\npose-guided attention (PGA) for shape decoder to better predict shape\nparameters. The two components strengthen the decoder of PSVT to improve\nperformance. Extensive experiments on the four datasets show that PSVT achieves\nstage-of-the-art results.\n","authors":["Zhongwei Qiu","Yang Qiansheng","Jian Wang","Haocheng Feng","Junyu Han","Errui Ding","Chang Xu","Dongmei Fu","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09187v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.09181v1","updated":"2023-03-16T09:51:41Z","published":"2023-03-16T09:51:41Z","title":"Global Knowledge Calibration for Fast Open-Vocabulary Segmentation","summary":"  Recent advancements in pre-trained vision-language models, such as CLIP, have\nenabled the segmentation of arbitrary concepts solely from textual inputs, a\nprocess commonly referred to as open-vocabulary semantic segmentation (OVS).\nHowever, existing OVS techniques confront a fundamental challenge: the trained\nclassifier tends to overfit on the base classes observed during training,\nresulting in suboptimal generalization performance to unseen classes. To\nmitigate this issue, recent studies have proposed the use of an additional\nfrozen pre-trained CLIP for classification. Nonetheless, this approach incurs\nheavy computational overheads as the CLIP vision encoder must be repeatedly\nforward-passed for each mask, rendering it impractical for real-world\napplications. To address this challenge, our objective is to develop a fast OVS\nmodel that can perform comparably or better without the extra computational\nburden of the CLIP image encoder during inference. To this end, we propose a\ncore idea of preserving the generalizable representation when fine-tuning on\nknown classes. Specifically, we introduce a text diversification strategy that\ngenerates a set of synonyms for each training category, which prevents the\nlearned representation from collapsing onto specific known category names.\nAdditionally, we employ a text-guided knowledge distillation method to preserve\nthe generalizable knowledge of CLIP. Extensive experiments demonstrate that our\nproposed model achieves robust generalization performance across various\ndatasets. Furthermore, we perform a preliminary exploration of open-vocabulary\nvideo segmentation and present a benchmark that can facilitate future\nopen-vocabulary research in the video domain.\n","authors":["Kunyang Han","Yong Liu","Jun Hao Liew","Henghui Ding","Yunchao Wei","Jiajun Liu","Yitong Wang","Yansong Tang","Yujiu Yang","Jiashi Feng","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.09181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04129v2","updated":"2023-03-16T09:47:53Z","published":"2022-12-08T08:04:06Z","title":"Deep Incubation: Training Large Models by Divide-and-Conquering","summary":"  Recent years have witnessed a remarkable success of large deep learning\nmodels. However, training these models is challenging due to high computational\ncosts, painfully slow convergence, and overfitting issues. In this paper, we\npresent Deep Incubation, a novel approach that enables the efficient and\neffective training of large models by dividing them into smaller sub-modules\nthat can be trained separately and assembled seamlessly. A key challenge for\nimplementing this idea is to ensure the compatibility of the independently\ntrained sub-modules. To address this issue, we first introduce a global, shared\nmeta model, which is leveraged to implicitly link all the modules together, and\ncan be designed as an extremely small network with negligible computational\noverhead. Then we propose a module incubation algorithm, which trains each\nsub-module to replace the corresponding component of the meta model and\naccomplish a given learning task. Despite the simplicity, our approach\neffectively encourages each sub-module to be aware of its role in the target\nlarge model, such that the finally-learned sub-modules can collaborate with\neach other smoothly after being assembled. Empirically, our method outperforms\nend-to-end (E2E) training in terms of both final accuracy and training\nefficiency. For example, on top of ViT-Huge, it improves the accuracy by 2.7%\non ImageNet or achieves similar performance with 4x less training time.\nNotably, the gains are significant for downstream tasks as well (e.g., object\ndetection and image segmentation on COCO and ADE20K). Code is available at\nhttps://github.com/LeapLabTHU/Deep-Incubation.\n","authors":["Zanlin Ni","Yulin Wang","Jiangwei Yu","Haojun Jiang","Yue Cao","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2212.04129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09174v1","updated":"2023-03-16T09:37:38Z","published":"2023-03-16T09:37:38Z","title":"Grab What You Need: Rethinking Complex Table Structure Recognition with\n  Flexible Components Deliberation","summary":"  Recently, Table Structure Recognition (TSR) task, aiming at identifying table\nstructure into machine readable formats, has received increasing interest in\nthe community. While impressive success, most single table component-based\nmethods can not perform well on unregularized table cases distracted by not\nonly complicated inner structure but also exterior capture distortion. In this\npaper, we raise it as Complex TSR problem, where the performance degeneration\nof existing methods is attributable to their inefficient component usage and\nredundant post-processing. To mitigate it, we shift our perspective from table\ncomponent extraction towards the efficient multiple components leverage, which\nawaits further exploration in the field. Specifically, we propose a seminal\nmethod, termed GrabTab, equipped with newly proposed Component Deliberator.\nThanks to its progressive deliberation mechanism, our GrabTab can flexibly\naccommodate to most complex tables with reasonable components selected but\nwithout complicated post-processing involved. Quantitative experimental results\non public benchmarks demonstrate that our method significantly outperforms the\nstate-of-the-arts, especially under more challenging scenes.\n","authors":["Hao Liu","Xin Li","Mingming Gong","Bing Liu","Yunfei Wu","Deqiang Jiang","Yinsong Liu","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2303.09174v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.08597v2","updated":"2023-03-16T09:32:42Z","published":"2023-03-15T13:07:21Z","title":"Aerial-Ground Person Re-ID","summary":"  Person re-ID matches persons across multiple non-overlapping cameras. Despite\nthe increasing deployment of airborne platforms in surveillance, current\nexisting person re-ID benchmarks' focus is on ground-ground matching and very\nlimited efforts on aerial-aerial matching. We propose a new benchmark dataset -\nAG-ReID, which performs person re-ID matching in a new setting: across aerial\nand ground cameras. Our dataset contains 21,983 images of 388 identities and 15\nsoft attributes for each identity. The data was collected by a UAV flying at\naltitudes between 15 to 45 meters and a ground-based CCTV camera on a\nuniversity campus. Our dataset presents a novel elevated-viewpoint challenge\nfor person re-ID due to the significant difference in person appearance across\nthese cameras. We propose an explainable algorithm to guide the person re-ID\nmodel's training with soft attributes to address this challenge. Experiments\ndemonstrate the efficacy of our method on the aerial-ground person re-ID task.\nThe dataset will be published and the baseline codes will be open-sourced to\nfacilitate research in this area.\n","authors":["Huy Nguyen","Kien Nguyen","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2303.08597v2.pdf","comment":"6 pages, 5 Figures, accepted by ICME"},{"id":"http://arxiv.org/abs/2303.09171v1","updated":"2023-03-16T09:29:05Z","published":"2023-03-16T09:29:05Z","title":"Fine-Grained and High-Faithfulness Explanations for Convolutional Neural\n  Networks","summary":"  Recently, explaining CNNs has become a research hotspot. CAM (Class\nActivation Map)-based methods and LRP (Layer-wise Relevance Propagation) method\nare two common explanation methods. However, due to the small spatial\nresolution of the last convolutional layer, the CAM-based methods can often\nonly generate coarse-grained visual explanations that provide a coarse location\nof the target object. LRP and its variants, on the other hand, can generate\nfine-grained explanations. But the faithfulness of the explanations is too low.\nIn this paper, we propose FG-CAM (fine-grained CAM), which extends the\nCAM-based methods to generate fine-grained visual explanations with high\nfaithfulness. FG-CAM uses the relationship between two adjacent layers of\nfeature maps with resolution difference to gradually increase the explanation\nresolution, while finding the contributing pixels and filtering out the pixels\nthat do not contribute at each step. Our method not only solves the shortcoming\nof CAM-based methods without changing their characteristics, but also generates\nfine-grained explanations that have higher faithfulness than LRP and its\nvariants. We also present FG-CAM with denoising, which is a variant of FG-CAM\nand is able to generate less noisy explanations with almost no change in\nexplanation faithfulness. Experimental results show that the performance of\nFG-CAM is almost unaffected by the explanation resolution. FG-CAM outperforms\nexisting CAM-based methods significantly in the both shallow and intermediate\nconvolutional layers, and outperforms LRP and its variations significantly in\nthe input layer.\n","authors":["Changqing Qiu","Fusheng Jin","Yining Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09171v1.pdf","comment":"This paper proposes FG-CAM, a novel visual explanation method, which\n  can generate fine-grained visual explanations with high faithfulness in\n  shallow and intermediate convolutional layers as well as in the input layer"},{"id":"http://arxiv.org/abs/2303.09170v1","updated":"2023-03-16T09:27:40Z","published":"2023-03-16T09:27:40Z","title":"NLUT: Neuarl-based 3D Lookup Tables for Video Photorealistic Style\n  Transfer","summary":"  Video photorealistic style transfer is desired to generate videos with a\nsimilar photorealistic style to the style image while maintaining temporal\nconsistency. However, existing methods obtain stylized video sequences by\nperforming frame-by-frame photorealistic style transfer, which is inefficient\nand does not ensure the temporal consistency of the stylized video. To address\nthis issue, we use neural network-based 3D Lookup Tables (LUTs) for the\nphotorealistic transfer of videos, achieving a balance between efficiency and\neffectiveness. We first train a neural network for generating photorealistic\nstylized 3D LUTs on a large-scale dataset; then, when performing photorealistic\nstyle transfer for a specific video, we select a keyframe and style image in\nthe video as the data source and fine-turn the neural network; finally, we\nquery the 3D LUTs generated by the fine-tuned neural network for the colors in\nthe video, resulting in a super-fast photorealistic style transfer, even\nprocessing 8K video takes less than 2 millisecond per frame. The experimental\nresults show that our method not only realizes the photorealistic style\ntransfer of arbitrary style images but also outperforms the existing methods in\nterms of visual quality and consistency. Project\npage:https://semchan.github.io/NLUT_Project.\n","authors":["Yaosen Chen","Han Yang","Yuexin Yang","Yuegen Liu","Wei Wang","Xuming Wen","Chaoping Xie"],"pdf_url":"https://arxiv.org/pdf/2303.09170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04319v2","updated":"2023-03-16T09:18:49Z","published":"2022-12-08T15:18:28Z","title":"On the Robustness of Normalizing Flows for Inverse Problems in Imaging","summary":"  Conditional normalizing flows can generate diverse image samples for solving\ninverse problems. Most normalizing flows for inverse problems in imaging employ\nthe conditional affine coupling layer that can generate diverse images quickly.\nHowever, unintended severe artifacts are occasionally observed in the output of\nthem. In this work, we address this critical issue by investigating the origins\nof these artifacts and proposing the conditions to avoid them. First of all, we\nempirically and theoretically reveal that these problems are caused by\n\"exploding inverse\" in the conditional affine coupling layer for certain\nout-of-distribution (OOD) conditional inputs. Then, we further validated that\nthe probability of causing erroneous artifacts in pixels is highly correlated\nwith a Mahalanobis distance-based OOD score for inverse problems in imaging.\nLastly, based on our investigations, we propose a remark to avoid exploding\ninverse and then based on it, we suggest a simple remedy that substitutes the\naffine coupling layers with the modified rational quadratic spline coupling\nlayers in normalizing flows, to encourage the robustness of generated image\nsamples. Our experimental results demonstrated that our suggested methods\neffectively suppressed critical artifacts occurring in normalizing flows for\nsuper-resolution space generation and low-light image enhancement.\n","authors":["Seongmin Hong","Inbum Park","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2212.04319v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2303.09167v1","updated":"2023-03-16T09:14:47Z","published":"2023-03-16T09:14:47Z","title":"Emotional Reaction Intensity Estimation Based on Multimodal Data","summary":"  This paper introduces our method for the Emotional Reaction Intensity (ERI)\nEstimation Challenge, in CVPR 2023: 5th Workshop and Competition on Affective\nBehavior Analysis in-the-wild (ABAW). Based on the multimodal data provided by\nthe originazers, we extract acoustic and visual features with different\npretrained models. The multimodal features are mixed together by Transformer\nEncoders with cross-modal attention mechnism. In this paper, 1. better features\nare extracted with the SOTA pretrained models. 2. Compared with the baseline,\nwe improve the Pearson's Correlations Coefficient a lot. 3. We process the data\nwith some special skills to enhance performance ability of our model.\n","authors":["Shangfei Wang","Jiaqiang Wu","Feiyi Zheng","Xin Li","Xuewei Li","Suwen Wang","Yi Wu","Yanan Chang","Xiangyu Miao"],"pdf_url":"https://arxiv.org/pdf/2303.09167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08887v2","updated":"2023-03-16T09:07:29Z","published":"2022-11-16T12:48:52Z","title":"Stare at What You See: Masked Image Modeling without Reconstruction","summary":"  Masked Autoencoders (MAE) have been prevailing paradigms for large-scale\nvision representation pre-training. By reconstructing masked image patches from\na small portion of visible image regions, MAE forces the model to infer\nsemantic correlation within an image. Recently, some approaches apply\nsemantic-rich teacher models to extract image features as the reconstruction\ntarget, leading to better performance. However, unlike the low-level features\nsuch as pixel values, we argue the features extracted by powerful teacher\nmodels already encode rich semantic correlation across regions in an intact\nimage.This raises one question: is reconstruction necessary in Masked Image\nModeling (MIM) with a teacher model? In this paper, we propose an efficient MIM\nparadigm named MaskAlign. MaskAlign simply learns the consistency of visible\npatch features extracted by the student model and intact image features\nextracted by the teacher model. To further advance the performance and tackle\nthe problem of input inconsistency between the student and teacher model, we\npropose a Dynamic Alignment (DA) module to apply learnable alignment. Our\nexperimental results demonstrate that masked modeling does not lose\neffectiveness even without reconstruction on masked regions. Combined with\nDynamic Alignment, MaskAlign can achieve state-of-the-art performance with much\nhigher efficiency. Code and models will be available at\nhttps://github.com/OpenPerceptionX/maskalign.\n","authors":["Hongwei Xue","Peng Gao","Hongyang Li","Yu Qiao","Hao Sun","Houqiang Li","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.08887v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09165v1","updated":"2023-03-16T09:03:52Z","published":"2023-03-16T09:03:52Z","title":"A New Benchmark: On the Utility of Synthetic Data with Blender for Bare\n  Supervised Learning and Downstream Domain Adaptation","summary":"  Deep learning in computer vision has achieved great success with the price of\nlarge-scale labeled training data. However, exhaustive data annotation is\nimpracticable for each task of all domains of interest, due to high labor costs\nand unguaranteed labeling accuracy. Besides, the uncontrollable data collection\nprocess produces non-IID training and test data, where undesired duplication\nmay exist. All these nuisances may hinder the verification of typical theories\nand exposure to new findings. To circumvent them, an alternative is to generate\nsynthetic data via 3D rendering with domain randomization. We in this work push\nforward along this line by doing profound and extensive research on bare\nsupervised learning and downstream domain adaptation. Specifically, under the\nwell-controlled, IID data setting enabled by 3D rendering, we systematically\nverify the typical, important learning insights, e.g., shortcut learning, and\ndiscover the new laws of various data regimes and network architectures in\ngeneralization. We further investigate the effect of image formation factors on\ngeneralization, e.g., object scale, material texture, illumination, camera\nviewpoint, and background in a 3D scene. Moreover, we use the\nsimulation-to-reality adaptation as a downstream task for comparing the\ntransferability between synthetic and real data when used for pre-training,\nwhich demonstrates that synthetic data pre-training is also promising to\nimprove real test results. Lastly, to promote future research, we develop a new\nlarge-scale synthetic-to-real benchmark for image classification, termed S2RDA,\nwhich provides more significant challenges for transfer from simulation to\nreality. The code and datasets are available at\nhttps://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.\n","authors":["Hui Tang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.09165v1.pdf","comment":"24 pages, 14 figures, 5 tables, accepted by the IEEE/CVF Conference\n  on Computer Vision and Pattern Recognition (CVPR), 2023. The proposed new\n  synthetic-to-real benchmark S2RDA is available at\n  https://pan.baidu.com/s/17C5lRDf7cpGR1kAVS2jS-Q?pwd=61tt"},{"id":"http://arxiv.org/abs/2303.07937v3","updated":"2023-03-16T09:03:42Z","published":"2023-03-14T14:24:31Z","title":"Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D\n  Generation","summary":"  Text-to-3D generation has shown rapid progress in recent days with the advent\nof score distillation, a methodology of using pretrained text-to-2D diffusion\nmodels to optimize neural radiance field (NeRF) in the zero-shot setting.\nHowever, the lack of 3D awareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible 3D scene. To address\nthis issue, we propose 3DFuse, a novel framework that incorporates 3D awareness\ninto pretrained 2D diffusion models, enhancing the robustness and 3D\nconsistency of score distillation-based methods. We realize this by first\nconstructing a coarse 3D structure of a given text prompt and then utilizing\nprojected, view-specific depth map as a condition for the diffusion model.\nAdditionally, we introduce a training strategy that enables the 2D diffusion\nmodel learns to handle the errors and sparsity within the coarse 3D structure\nfor robust generation, as well as a method for ensuring semantic consistency\nthroughout all viewpoints of the scene. Our framework surpasses the limitations\nof prior arts, and has significant implications for 3D consistent generation of\n2D diffusion models.\n","authors":["Junyoung Seo","Wooseok Jang","Min-Seop Kwak","Jaehoon Ko","Hyeonsu Kim","Junho Kim","Jin-Hwa Kim","Jiyoung Lee","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07937v3.pdf","comment":"Project page https://ku-cvlab.github.io/3DFuse/"},{"id":"http://arxiv.org/abs/2303.09164v1","updated":"2023-03-16T09:03:17Z","published":"2023-03-16T09:03:17Z","title":"Multimodal Feature Extraction and Fusion for Emotional Reaction\n  Intensity Estimation and Expression Classification in Videos with\n  Transformers","summary":"  In this paper, we present our solutions to the two sub-challenges of\nAffective Behavior Analysis in the wild (ABAW) 2023: the Emotional Reaction\nIntensity (ERI) Estimation Challenge and Expression (Expr) Classification\nChallenge. ABAW 2023 focuses on the problem of affective behavior analysis in\nthe wild, with the goal of creating machines and robots that have the ability\nto understand human feelings, emotions and behaviors, which can effectively\ncontribute to the advent of a more intelligent future. In our work, we use\ndifferent models and tools for the Hume-Reaction dataset to extract features of\nvarious aspects, such as audio features, video features, etc. By analyzing,\ncombining, and studying these multimodal features, we effectively improve the\naccuracy of the model for multimodal sentiment prediction. For the Emotional\nReaction Intensity (ERI) Estimation Challenge, our method shows excellent\nresults with a Pearson coefficient on the validation dataset, exceeding the\nbaseline method by 84 percent.\n","authors":["Jia Li","Yin Chen","Xuesong Zhang","Jiantao Nie","Yangchen Yu","Ziqiang Li","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2303.09164v1.pdf","comment":"Solutions of HFUT-CVers Team at the 5th ABAW Competition (CVPR 2023\n  workshop)"},{"id":"http://arxiv.org/abs/2303.07130v2","updated":"2023-03-16T08:59:24Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) based scoring method is used to identify the extent of\nlung involvement observed on a CT scan. This paper presents a domain\nknowledge-based pipeline for extracting regions of infection in COVID-19\npatients using a combination of image-processing algorithms and a pre-trained\nUNET model. The severity of the infection is then classified into different\ncategories using an ensemble of three machine-learning models: Extreme Gradient\nBoosting, Extremely Randomized Trees, and Support Vector Machine. The proposed\nsystem was evaluated on a validation dataset in the AI-Enabled Medical Image\nAnalysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and\nachieved a macro F1 score of 64\\%. These results demonstrate the potential of\ncombining domain knowledge with machine learning techniques for accurate\nCOVID-19 diagnosis using CT scans. The implementation of the proposed system\nfor severity analysis is available at\n\\textit{https://github.com/aanandt/Enhancing-COVID-19-Severity-Analysis-through-Ensemble-Methods.git\n}\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09162v1","updated":"2023-03-16T08:57:33Z","published":"2023-03-16T08:57:33Z","title":"EmotiEffNet Facial Features in Uni-task Emotion Recognition in Video at\n  ABAW-5 competition","summary":"  In this article, the results of our team for the fifth Affective Behavior\nAnalysis in-the-wild (ABAW) competition are presented. The usage of the\npre-trained convolutional networks from the EmotiEffNet family for frame-level\nfeature extraction is studied. In particular, we propose an ensemble of a\nmulti-layered perceptron and the LightAutoML-based classifier. The\npost-processing by smoothing the results for sequential frames is implemented.\nExperimental results for the large-scale Aff-Wild2 database demonstrate that\nour model achieves a much greater macro-averaged F1-score for facial expression\nrecognition and action unit detection and concordance correlation coefficients\nfor valence/arousal estimation when compared to baseline.\n","authors":["Andrey V. Savchenko"],"pdf_url":"https://arxiv.org/pdf/2303.09162v1.pdf","comment":"7 pages; 5 figures; 3 tables"},{"id":"http://arxiv.org/abs/2303.09158v1","updated":"2023-03-16T08:47:36Z","published":"2023-03-16T08:47:36Z","title":"Facial Affect Recognition based on Transformer Encoder and Audiovisual\n  Fusion for the ABAW5 Challenge","summary":"  In this paper, we present our solutions for the 5th Workshop and Competition\non Affective Behavior Analysis in-the-wild (ABAW), which includes four\nsub-challenges of Valence-Arousal (VA) Estimation, Expression (Expr)\nClassification, Action Unit (AU) Detection and Emotional Reaction Intensity\n(ERI) Estimation. The 5th ABAW competition focuses on facial affect recognition\nutilizing different modalities and datasets. In our work, we extract powerful\naudio and visual features using a large number of sota models. These features\nare fused by Transformer Encoder and TEMMA. Besides, to avoid the possible\nimpact of large dimensional differences between various features, we design an\nAffine Module to align different features to the same dimension. Extensive\nexperiments demonstrate that the superiority of the proposed method. For the VA\nEstimation sub-challenge, our method obtains the mean Concordance Correlation\nCoefficient (CCC) of 0.6066. For the Expression Classification sub-challenge,\nthe average F1 Score is 0.4055. For the AU Detection sub-challenge, the average\nF1 Score is 0.5296. For the Emotional Reaction Intensity Estimation\nsub-challenge, the average pearson's correlations coefficient on the validation\nset is 0.3968. All of the results of four sub-challenges outperform the\nbaseline with a large margin.\n","authors":["Ziyang Zhang","Liuwei An","Zishun Cui","Ao xu","Tengteng Dong"],"pdf_url":"https://arxiv.org/pdf/2303.09158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03684v2","updated":"2023-03-16T08:41:44Z","published":"2023-03-07T06:54:48Z","title":"MOSO: Decomposing MOtion, Scene and Object for Video Prediction","summary":"  Motion, scene and object are three primary visual components of a video. In\nparticular, objects represent the foreground, scenes represent the background,\nand motion traces their dynamics. Based on this insight, we propose a two-stage\nMOtion, Scene and Object decomposition framework (MOSO) for video prediction,\nconsisting of MOSO-VQVAE and MOSO-Transformer. In the first stage, MOSO-VQVAE\ndecomposes a previous video clip into the motion, scene and object components,\nand represents them as distinct groups of discrete tokens. Then, in the second\nstage, MOSO-Transformer predicts the object and scene tokens of the subsequent\nvideo clip based on the previous tokens and adds dynamic motion at the token\nlevel to the generated object and scene tokens. Our framework can be easily\nextended to unconditional video generation and video frame interpolation tasks.\nExperimental results demonstrate that our method achieves new state-of-the-art\nperformance on five challenging benchmarks for video prediction and\nunconditional video generation: BAIR, RoboNet, KTH, KITTI and UCF101. In\naddition, MOSO can produce realistic videos by combining objects and scenes\nfrom different videos.\n","authors":["Mingzhen Sun","Weining Wang","Xinxin Zhu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2303.03684v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07863v2","updated":"2023-03-16T08:34:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v2.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2303.09153v1","updated":"2023-03-16T08:34:03Z","published":"2023-03-16T08:34:03Z","title":"Reliable Image Dehazing by NeRF","summary":"  We present an image dehazing algorithm with high quality, wide application,\nand no data training or prior needed. We analyze the defects of the original\ndehazing model, and propose a new and reliable dehazing reconstruction and\ndehazing model based on the combination of optical scattering model and\ncomputer graphics lighting rendering model. Based on the new haze model and the\nimages obtained by the cameras, we can reconstruct the three-dimensional space,\naccurately calculate the objects and haze in the space, and use the\ntransparency relationship of haze to perform accurate haze removal. To obtain a\n3D simulation dataset we used the Unreal 5 computer graphics rendering engine.\nIn order to obtain real shot data in different scenes, we used fog generators,\narray cameras, mobile phones, underwater cameras and drones to obtain haze\ndata. We use formula derivation, simulation data set and real shot data set\nresult experimental results to prove the feasibility of the new method.\nCompared with various other methods, we are far ahead in terms of calculation\nindicators (4 dB higher quality average scene), color remains more natural, and\nthe algorithm is more robust in different scenarios and best in the subjective\nperception.\n","authors":["Zheyan Jin","Shiqi Chen","Huajun Feng","Zhihai Xu","Qi Li","Yueting Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09153v1.pdf","comment":"12pages, 8figures"},{"id":"http://arxiv.org/abs/2303.09152v1","updated":"2023-03-16T08:34:02Z","published":"2023-03-16T08:34:02Z","title":"Learning a Room with the Occ-SDF Hybrid: Signed Distance Function\n  Mingled with Occupancy Aids Scene Representation","summary":"  Implicit neural rendering, which uses signed distance function (SDF)\nrepresentation with geometric priors (such as depth or surface normal), has led\nto impressive progress in the surface reconstruction of large-scale scenes.\nHowever, applying this method to reconstruct a room-level scene from images may\nmiss structures in low-intensity areas or small and thin objects. We conducted\nexperiments on three datasets to identify limitations of the original color\nrendering loss and priors-embedded SDF scene representation.\n  We found that the color rendering loss results in optimization bias against\nlow-intensity areas, causing gradient vanishing and leaving these areas\nunoptimized. To address this issue, we propose a feature-based color rendering\nloss that utilizes non-zero feature values to bring back optimization signals.\nAdditionally, the SDF representation can be influenced by objects along a ray\npath, disrupting the monotonic change of SDF values when a single object is\npresent. To counteract this, we explore using the occupancy representation,\nwhich encodes each point separately and is unaffected by objects along a\nquerying ray. Our experimental results demonstrate that the joint forces of the\nfeature-based rendering loss and Occ-SDF hybrid representation scheme can\nprovide high-quality reconstruction results, especially in challenging\nroom-level scenarios. The code would be released.\n","authors":["Xiaoyang Lyu","Peng Dai","Zizhang Li","Dongyu Yan","Yi Lin","Yifan Peng","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2303.09152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.09223v3","updated":"2023-03-16T08:28:00Z","published":"2021-09-19T21:00:44Z","title":"Initial Test of \"BabyRobot\" Behaviour on a Teleoperated Toy\n  Substitution: Improving the Motor Skills of Toddlers","summary":"  This article introduces \"Baby Robot\", a robot aiming to improve motor skills\nof babies and toddlers. Authors developed a car-like toy that moves\nautonomously using reinforcement learning and computer vision techniques. The\nrobot behaviour is to escape from a target baby that has been previously\nrecognized, or at least detected, while avoiding obstacles, so that the\nsecurity of the baby is not compromised. A myriad of commercial toys with a\nsimilar mobility improvement purpose are into the market; however, there is no\none that bets for an intelligent autonomous movement, as they perform simple\nyet repetitive trajectories in the best of the cases. Two crawling toys -- one\nin representation of \"Baby Robot\" -- were tested in a real environment with\nrespect to regular toys in order to check how they improved the toddlers\nmobility. These real-life experiments were conducted with our proposed robot in\na kindergarten, where a group of children interacted with the toys. Significant\nimprovement in the motion skills of participants were detected.\n","authors":["Eric Canas","Alba M. G. Garcia","Anais Garrell","Cecilio Angulo"],"pdf_url":"https://arxiv.org/pdf/2109.09223v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15586v2","updated":"2023-03-16T08:25:00Z","published":"2022-10-27T16:22:50Z","title":"Joint Multi-Person Body Detection and Orientation Estimation via One\n  Unified Embedding","summary":"  Human body orientation estimation (HBOE) is widely applied into various\napplications, including robotics, surveillance, pedestrian analysis and\nautonomous driving. Although many approaches have been addressing the HBOE\nproblem from specific under-controlled scenes to challenging in-the-wild\nenvironments, they assume human instances are already detected and take a well\ncropped sub-image as the input. This setting is less efficient and prone to\nerrors in real application, such as crowds of people. In the paper, we propose\na single-stage end-to-end trainable framework for tackling the HBOE problem\nwith multi-persons. By integrating the prediction of bounding boxes and\ndirection angles in one embedding, our method can jointly estimate the location\nand orientation of all bodies in one image directly. Our key idea is to\nintegrate the HBOE task into the multi-scale anchor channel predictions of\npersons for concurrently benefiting from engaged intermediate features.\nTherefore, our approach can naturally adapt to difficult instances involving\nlow resolution and occlusion as in object detection. We validated the\nefficiency and effectiveness of our method in the recently presented benchmark\nMEBOW with extensive experiments. Besides, we completed ambiguous instances\nignored by the MEBOW dataset, and provided corresponding weak body-orientation\nlabels to keep the integrity and consistency of it for supporting studies\ntoward multi-persons. Our work is available at\nhttps://github.com/hnuzhy/JointBDOE.\n","authors":["Huayi Zhou","Fei Jiang","Jiaxin Si","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2210.15586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09145v1","updated":"2023-03-16T08:21:10Z","published":"2023-03-16T08:21:10Z","title":"Facial Affective Behavior Analysis Method for 5th ABAW Competition","summary":"  Facial affective behavior analysis is important for human-computer\ninteraction. 5th ABAW competition includes three challenges from Aff-Wild2\ndatabase. Three common facial affective analysis tasks are involved, i.e.\nvalence-arousal estimation, expression classification, action unit recognition.\nFor the three challenges, we construct three different models to solve the\ncorresponding problems to improve the results, such as data unbalance and data\nnoise. For the experiments of three challenges, we train the models on the\nprovided training data and validate the models on the validation data.\n","authors":["Shangfei Wang","Yanan Chang","Yi Wu","Xiangyu Miao","Jiaqiang Wu","Zhouan Zhu","Jiahe Wang","Yufei Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.09145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07896v2","updated":"2023-03-16T08:09:58Z","published":"2023-03-14T13:31:05Z","title":"Exploring Weakly Supervised Semantic Segmentation Ensembles for Medical\n  Imaging Systems","summary":"  Reliable classification and detection of certain medical conditions, in\nimages, with state-of-the-art semantic segmentation networks, require vast\namounts of pixel-wise annotation. However, the public availability of such\ndatasets is minimal. Therefore, semantic segmentation with image-level labels\npresents a promising alternative to this problem. Nevertheless, very few works\nhave focused on evaluating this technique and its applicability to the medical\nsector. Due to their complexity and the small number of training examples in\nmedical datasets, classifier-based weakly supervised networks like class\nactivation maps (CAMs) struggle to extract useful information from them.\nHowever, most state-of-the-art approaches rely on them to achieve their\nimprovements. Therefore, we propose a framework that can still utilize the\nlow-quality CAM predictions of complicated datasets to improve the accuracy of\nour results. Our framework achieves that by first utilizing lower threshold\nCAMs to cover the target object with high certainty; second, by combining\nmultiple low-threshold CAMs that even out their errors while highlighting the\ntarget object. We performed exhaustive experiments on the popular multi-modal\nBRATS and prostate DECATHLON segmentation challenge datasets. Using the\nproposed framework, we have demonstrated an improved dice score of up to 8% on\nBRATS and 6% on DECATHLON datasets compared to the previous state-of-the-art.\n","authors":["Erik Ostrowski","Bharath Srinivas Prabakaran","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01332v2","updated":"2023-03-16T08:09:39Z","published":"2023-03-02T15:10:08Z","title":"Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion\n  Segmentation","summary":"  Precise ischemic lesion segmentation plays an essential role in improving\ndiagnosis and treatment planning for ischemic stroke, one of the prevalent\ndiseases with the highest mortality rate. While numerous deep neural network\napproaches have recently been proposed to tackle this problem, these methods\nrequire large amounts of annotated regions during training, which can be\nimpractical in the medical domain where annotated data is scarce. As a remedy,\nwe present a prototypical few-shot segmentation approach for ischemic lesion\nsegmentation using only one annotated sample during training. The proposed\napproach leverages a novel self-supervised training mechanism that is tailored\nto the task of ischemic stroke lesion segmentation by exploiting color-coded\nparametric maps generated from Computed Tomography Perfusion scans. We\nillustrate the benefits of our proposed training mechanism, leading to\nconsiderable improvements in performance in the few-shot setting. Given a\nsingle annotated patient, an average Dice score of 0.58 is achieved for the\nsegmentation of ischemic lesions.\n","authors":["Luca Tomasetti","Stine Hansen","Mahdieh Khanmohammadi","Kjersti Engan","Liv Jorunn Høllesli","Kathinka Dæhli Kurz","Michael Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2303.01332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04821v2","updated":"2023-03-16T08:07:49Z","published":"2022-12-08T18:55:31Z","title":"PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers\n  using Synthetic Scene Data","summary":"  Action recognition models have achieved impressive results by incorporating\nscene-level annotations, such as objects, their relations, 3D structure, and\nmore. However, obtaining annotations of scene structure for videos requires a\nsignificant amount of effort to gather and annotate, making these methods\nexpensive to train. In contrast, synthetic datasets generated by graphics\nengines provide powerful alternatives for generating scene-level annotations\nacross multiple tasks. In this work, we propose an approach to leverage\nsynthetic scene data for improving video understanding. We present a multi-task\nprompt learning approach for video transformers, where a shared video\ntransformer backbone is enhanced by a small set of specialized parameters for\neach task. Specifically, we add a set of ``task prompts'', each corresponding\nto a different task, and let each prompt predict task-related annotations. This\ndesign allows the model to capture information shared among synthetic scene\ntasks as well as information shared between synthetic scene tasks and a real\nvideo downstream task throughout the entire network. We refer to this approach\nas ``Promptonomy'', since the prompts model task-related structure. We propose\nthe PromptonomyViT model (PViT), a video transformer that incorporates various\ntypes of scene-level information from synthetic data using the ``Promptonomy''\napproach. PViT shows strong performance improvements on multiple video\nunderstanding tasks and datasets.\n","authors":["Roei Herzig","Ofir Abramovich","Elad Ben-Avraham","Assaf Arbelle","Leonid Karlinsky","Ariel Shamir","Trevor Darrell","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2212.04821v2.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.06869v2","updated":"2023-03-16T08:05:09Z","published":"2023-03-13T05:37:40Z","title":"Adaptive Data-Free Quantization","summary":"  Data-free quantization (DFQ) recovers the performance of quantized network\n(Q) without accessing the original data, but generates the fake sample via a\ngenerator (G) by learning from full-precision network (P), which, however, is\ntotally independent of Q, overlooking the adaptability of the knowledge from\ngenerated samples, i.e., informative or not to the learning process of Q,\nresulting into the overflow of generalization error. Building on this, several\ncritical questions -- how to measure the sample adaptability to Q under varied\nbit-width scenarios? how to generate the samples with large adaptability to\nimprove Q's generalization? whether the largest adaptability is the best? To\nanswer the above questions, in this paper, we propose an Adaptive Data-Free\nQuantization (AdaDFQ) method, which revisits DFQ from a zero-sum game\nperspective upon the sample adaptability between two players -- a generator and\na quantized network. Following this viewpoint, we further define the\ndisagreement and agreement samples to form two boundaries, where the margin is\noptimized to address the over-and-under fitting issues, so as to generate the\nsamples with adaptive adaptability to Q. Our AdaDFQ reveals: 1) the largest\nadaptability is NOT the best for sample generation to benefit Q's\ngeneralization; 2) the knowledge of the generated sample should not be\ninformative to Q only, but also related to the category and distribution\ninformation of the training data for P. The theoretical and empirical analysis\nvalidate the advantages of AdaDFQ over the state-of-the-arts. Our code is\navailable at https: github.com/hfutqian/AdaDFQ.\n","authors":["Biao Qian","Yang Wang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06869v2.pdf","comment":"9 pages, 6 figures, Camera ready version for CVPR 2023. arXiv admin\n  note: substantial text overlap with arXiv:2302.09572"},{"id":"http://arxiv.org/abs/2107.03024v3","updated":"2023-03-16T07:43:08Z","published":"2021-07-07T05:39:58Z","title":"Rethinking Sampling Strategies for Unsupervised Person Re-identification","summary":"  Unsupervised person re-identification (re-ID) remains a challenging task.\nWhile extensive research has focused on the framework design and loss function,\nthis paper shows that sampling strategy plays an equally important role. We\nanalyze the reasons for the performance differences between various sampling\nstrategies under the same framework and loss function. We suggest that\ndeteriorated over-fitting is an important factor causing poor performance, and\nenhancing statistical stability can rectify this problem. Inspired by that, a\nsimple yet effective approach is proposed, termed group sampling, which gathers\nsamples from the same class into groups. The model is thereby trained using\nnormalized group samples, which helps alleviate the negative impact of\nindividual samples. Group sampling updates the pipeline of pseudo-label\ngeneration by guaranteeing that samples are more efficiently classified into\nthe correct classes. It regulates the representation learning process,\nenhancing statistical stability for feature representation in a progressive\nfashion. Extensive experiments on Market-1501, DukeMTMC-reID and MSMT17 show\nthat group sampling achieves performance comparable to state-of-the-art methods\nand outperforms the current techniques under purely camera-agnostic settings.\nCode has been available at https://github.com/ucas-vg/GroupSampling.\n","authors":["Xumeng Han","Xuehui Yu","Guorong Li","Jian Zhao","Gang Pan","Qixiang Ye","Jianbin Jiao","Zhenjun Han"],"pdf_url":"https://arxiv.org/pdf/2107.03024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09124v1","updated":"2023-03-16T07:39:07Z","published":"2023-03-16T07:39:07Z","title":"Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes","summary":"  Neuroimaging measures of the brain's white matter connections can enable the\nprediction of non-imaging phenotypes, such as demographic and cognitive\nmeasures. Existing works have investigated traditional microstructure and\nconnectivity measures from diffusion MRI tractography, without considering the\nshape of the connections reconstructed by tractography. In this paper, we\ninvestigate the potential of fiber tract shape features for predicting\nnon-imaging phenotypes, both individually and in combination with traditional\nfeatures. We focus on three basic shape features: length, diameter, and\nelongation. Two different prediction methods are used, including a traditional\nregression method and a deep-learning-based prediction method. Experiments use\nan efficient two-stage fusion strategy for prediction using microstructure,\nconnectivity, and shape measures. To reduce predictive bias due to brain size,\nnormalized shape features are also investigated. Experimental results on the\nHuman Connectome Project (HCP) young adult dataset (n=1065) demonstrate that\nindividual shape features are predictive of non-imaging phenotypes. When\ncombined with microstructure and connectivity features, shape features\nsignificantly improve performance for predicting the cognitive score TPVT (NIH\nToolbox picture vocabulary test). Overall, this study demonstrates that the\nshape of fiber tracts contains useful information for the description and study\nof the living human brain using machine learning.\n","authors":["Wan Liu","Yuqian Chen","Chuyang Ye","Nikos Makris","Yogesh Rathi","Weidong Cai","Fan Zhang","Lauren J. O' Donnell"],"pdf_url":"https://arxiv.org/pdf/2303.09124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11337v2","updated":"2023-03-16T07:38:02Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via\n  Contrastive Prompt-Tuning","summary":"  Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n  To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09119v1","updated":"2023-03-16T07:32:31Z","published":"2023-03-16T07:32:31Z","title":"Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation","summary":"  Animating virtual avatars to make co-speech gestures facilitates various\napplications in human-machine interaction. The existing methods mainly rely on\ngenerative adversarial networks (GANs), which typically suffer from notorious\nmode collapse and unstable training, thus making it difficult to learn accurate\naudio-gesture joint distributions. In this work, we propose a novel\ndiffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to\neffectively capture the cross-modal audio-to-gesture associations and preserve\ntemporal coherence for high-fidelity audio-driven co-speech gesture generation.\nSpecifically, we first establish the diffusion-conditional generation process\non clips of skeleton sequences and audio to enable the whole framework. Then, a\nnovel Diffusion Audio-Gesture Transformer is devised to better attend to the\ninformation from multiple modalities and model the long-term temporal\ndependency. Moreover, to eliminate temporal inconsistency, we propose an\neffective Diffusion Gesture Stabilizer with an annealed noise sampling\nstrategy. Benefiting from the architectural advantages of diffusion models, we\nfurther incorporate implicit classifier-free guidance to trade off between\ndiversity and gesture quality. Extensive experiments demonstrate that\nDiffGesture achieves state-of-theart performance, which renders coherent\ngestures with better mode coverage and stronger audio correlations. Code is\navailable at https://github.com/Advocate99/DiffGesture.\n","authors":["Lingting Zhu","Xian Liu","Xuanyu Liu","Rui Qian","Ziwei Liu","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2303.09119v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023. 10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.14197v2","updated":"2023-03-16T07:25:06Z","published":"2022-12-29T07:03:29Z","title":"Self-Supervised Pre-training for 3D Point Clouds via View-Specific\n  Point-to-Image Translation","summary":"  The past few years have witnessed the prevalence of self-supervised\nrepresentation learning within the language and 2D vision communities. However,\nsuch advancements have not been fully migrated to the 3D point cloud learning\ncommunity. Different from existing pre-training paradigms for 3D point clouds\nfalling into the scope of generative modeling or contrastive learning, this\npaper proposes a translative pre-training framework, namely PointVST, driven by\na novel self-supervised pretext task of cross-modal translation from 3D point\nclouds to their corresponding diverse forms of 2D rendered images. More\nspecifically, we start by deducing view-conditioned point-wise embeddings via\nthe insertion of a viewpoint indicator and then adaptively aggregate a\nview-specific global codeword fed into the subsequent 2D convolutional\ntranslation heads for image generation. Extensive experiments on various\ndownstream tasks of 3D shape analysis and scene understanding demonstrate that\nPointVST shows consistent and prominent performance superiority over current\nstate-of-the-art methods. Our code will be made publicly available.\n","authors":["Qijian Zhang","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2212.14197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09117v1","updated":"2023-03-16T07:23:55Z","published":"2023-03-16T07:23:55Z","title":"Visual-Linguistic Causal Intervention for Radiology Report Generation","summary":"  Automatic radiology report generation is essential for computer-aided\ndiagnosis and medication guidance. Importantly, automatic radiology report\ngeneration (RRG) can relieve the heavy burden of radiologists by generating\nmedical reports automatically from visual-linguistic data relations. However,\ndue to the spurious correlations within image-text data induced by visual and\nlinguistic biases, it is challenging to generate accurate reports that reliably\ndescribe abnormalities. Besides, the cross-modal confounder is usually\nunobservable and difficult to be eliminated explicitly. In this paper, we\nmitigate the cross-modal data bias for RRG from a new perspective, i.e.,\nvisual-linguistic causal intervention, and propose a novel Visual-Linguistic\nCausal Intervention (VLCI) framework for RRG, which consists of a visual\ndeconfounding module (VDM) and a linguistic deconfounding module (LDM), to\nimplicitly deconfound the visual-linguistic confounder by causal front-door\nintervention. Specifically, the VDM explores and disentangles the visual\nconfounder from the patch-based local and global features without object\ndetection due to the absence of universal clinic semantic extraction.\nSimultaneously, the LDM eliminates the linguistic confounder caused by salient\nvisual features and high-frequency context without constructing specific\ndictionaries. Extensive experiments on IU-Xray and MIMIC-CXR datasets show that\nour VLCI outperforms the state-of-the-art RRG methods significantly. Source\ncode and models are available at https://github.com/WissingChen/VLCI.\n","authors":["Weixing Chen","Yang Liu","Ce Wang","Guanbin Li","Jiarui Zhu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.09117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01177v3","updated":"2023-03-16T07:04:35Z","published":"2021-12-02T12:48:37Z","title":"MutualFormer: Multi-Modality Representation Learning via Cross-Diffusion\n  Attention","summary":"  Aggregating multi-modality data to obtain reliable data representation\nattracts more and more attention. Recent studies demonstrate that Transformer\nmodels usually work well for multi-modality tasks. Existing Transformers\ngenerally either adopt the Cross-Attention (CA) mechanism or simple\nconcatenation to achieve the information interaction among different modalities\nwhich generally ignore the issue of modality gap. In this work, we re-think\nTransformer and extend it to MutualFormer for multi-modality data\nrepresentation. Rather than CA in Transformer, MutualFormer employs our new\ndesign of Cross-Diffusion Attention (CDA) to conduct the information\ncommunication among different modalities. Comparing with CA, the main\nadvantages of the proposed CDA are three aspects. First, the crossaffinities in\nCDA are defined based on the individual modality affinities in the metric space\nwhich thus can naturally avoid the issue of modality/domain gap in feature\nbased CA definition. Second, CDA provides a general scheme which can either be\nused for multimodality representation or serve as the post-optimization for\nexisting CA models. Third, CDA is implemented efficiently. We successfully\napply the MutualFormer on different multi-modality learning tasks (i.e.,\nRGB-Depth SOD, RGB-NIR object ReID). Extensive experiments demonstrate the\neffectiveness of the proposed MutualFormer.\n","authors":["Xixi Wang","Xiao Wang","Bo Jiang","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2112.01177v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09115v1","updated":"2023-03-16T07:02:03Z","published":"2023-03-16T07:02:03Z","title":"Learning for Amalgamation: A Multi-Source Transfer Learning Framework\n  For Sentiment Classification","summary":"  Transfer learning plays an essential role in Deep Learning, which can\nremarkably improve the performance of the target domain, whose training data is\nnot sufficient. Our work explores beyond the common practice of transfer\nlearning with a single pre-trained model. We focus on the task of Vietnamese\nsentiment classification and propose LIFA, a framework to learn a unified\nembedding from several pre-trained models. We further propose two more LIFA\nvariants that encourage the pre-trained models to either cooperate or compete\nwith one another. Studying these variants sheds light on the success of LIFA by\nshowing that sharing knowledge among the models is more beneficial for transfer\nlearning. Moreover, we construct the AISIA-VN-Review-F dataset, the first\nlarge-scale Vietnamese sentiment classification database. We conduct extensive\nexperiments on the AISIA-VN-Review-F and existing benchmarks to demonstrate the\nefficacy of LIFA compared to other techniques. To contribute to the Vietnamese\nNLP research, we publish our source code and datasets to the research community\nupon acceptance.\n","authors":["Cuong V. Nguyen","Khiem H. Le","Anh M. Tran","Quang H. Pham","Binh T. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.09115v1.pdf","comment":"Information Sciences"},{"id":"http://arxiv.org/abs/2303.09114v1","updated":"2023-03-16T07:00:36Z","published":"2023-03-16T07:00:36Z","title":"AU-aware graph convolutional network for Macro- and Micro-expression\n  spotting","summary":"  Automatic Micro-Expression (ME) spotting in long videos is a crucial step in\nME analysis but also a challenging task due to the short duration and low\nintensity of MEs. When solving this problem, previous works generally lack in\nconsidering the structures of human faces and the correspondence between\nexpressions and relevant facial muscles. To address this issue for better\nperformance of ME spotting, this paper seeks to extract finer spatial features\nby modeling the relationships between facial Regions of Interest (ROIs).\nSpecifically, we propose a graph convolutional-based network, called\nAction-Unit-aWare Graph Convolutional Network (AUW-GCN). Furthermore, to inject\nprior information and to cope with the problem of small datasets, AU-related\nstatistics are encoded into the network. Comprehensive experiments show that\nour results outperform baseline methods consistently and achieve new SOTA\nperformance in two benchmark datasets,CAS(ME)^2 and SAMM-LV. Our code is\navailable at https://github.com/xjtupanda/AUW-GCN.\n","authors":["Shukang Yin","Shiwei Wu","Tong Xu","Shifeng Liu","Sirui Zhao","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09114v1.pdf","comment":"Accepted by ICME-2023"},{"id":"http://arxiv.org/abs/2303.09105v1","updated":"2023-03-16T06:37:16Z","published":"2023-03-16T06:37:16Z","title":"Rethinking Model Ensemble in Transfer-based Adversarial Attacks","summary":"  Deep learning models are vulnerable to adversarial examples. Transfer-based\nadversarial attacks attract tremendous attention as they can identify the\nweaknesses of deep learning models in a black-box manner. An effective strategy\nto improve the transferability of adversarial examples is attacking an ensemble\nof models. However, previous works simply average the outputs of different\nmodels, lacking an in-depth analysis on how and why model ensemble can strongly\nimprove the transferability. In this work, we rethink the ensemble in\nadversarial attacks and define the common weakness of model ensemble with the\nproperties of the flatness of loss landscape and the closeness to the local\noptimum of each model. We empirically and theoretically show that these two\nproperties are strongly correlated with the transferability and propose a\nCommon Weakness Attack (CWA) to generate more transferable adversarial examples\nby promoting these two properties. Experimental results on both image\nclassification and object detection tasks validate the effectiveness of our\napproach to improve the adversarial transferability, especially when attacking\nadversarially trained models.\n","authors":["Huanran Chen","Yichi Zhang","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.09105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09103v1","updated":"2023-03-16T06:23:43Z","published":"2023-03-16T06:23:43Z","title":"Machine learning based biomedical image processing for echocardiographic\n  images","summary":"  The popularity of Artificial intelligence and machine learning have prompted\nresearchers to use it in the recent researches. The proposed method uses\nK-Nearest Neighbor (KNN) algorithm for segmentation of medical images,\nextracting of image features for analysis by classifying the data based on the\nneural networks. Classification of the images in medical imaging is very\nimportant, KNN is one suitable algorithm which is simple, conceptual and\ncomputational, which provides very good accuracy in results. KNN algorithm is a\nunique user-friendly approach with wide range of applications in machine\nlearning algorithms which are majorly used for the various image processing\napplications including classification, segmentation and regression issues of\nthe image processing. The proposed system uses gray level co-occurrence matrix\nfeatures. The trained neural network has been tested successfully on a group of\nechocardiographic images, errors were compared using regression plot. The\nresults of the algorithm are tested using various quantitative as well as\nqualitative metrics and proven to exhibit better performance in terms of both\nquantitative and qualitative metrics in terms of current state-of-the-art\nmethods in the related area. To compare the performance of trained neural\nnetwork the regression analysis performed showed a good correlation.\n","authors":["Ayesha Heena","Nagashettappa Biradar","Najmuddin M. Maroof","Surbhi Bhatia","Rashmi Agarwal","Kanta Prasad"],"pdf_url":"https://arxiv.org/pdf/2303.09103v1.pdf","comment":"10 figures 4 tables"},{"id":"http://arxiv.org/abs/2303.09101v1","updated":"2023-03-16T06:14:18Z","published":"2023-03-16T06:14:18Z","title":"Contrastive Semi-supervised Learning for Underwater Image Restoration\n  via Reliable Bank","summary":"  Despite the remarkable achievement of recent underwater image restoration\ntechniques, the lack of labeled data has become a major hurdle for further\nprogress. In this work, we propose a mean-teacher based\n\\textbf{Semi}-supervised \\textbf{U}nderwater \\textbf{I}mage\n\\textbf{R}estoration (\\textbf{Semi-UIR}) framework to incorporate the unlabeled\ndata into network training. However, the naive mean-teacher method suffers from\ntwo main problems: (1) The consistency loss used in training might become\nineffective when the teacher's prediction is wrong. (2) Using L1 distance may\ncause the network to overfit wrong labels, resulting in confirmation bias. To\naddress the above problems, we first introduce a reliable bank to store the\n``best-ever\" outputs as pseudo ground truth. To assess the quality of outputs,\nwe conduct an empirical analysis based on the monotonicity property to select\nthe most trustworthy NR-IQA method. Besides, in view of the confirmation bias\nproblem, we incorporate contrastive regularization to prevent the overfitting\non wrong labels. Experimental results on both full-reference and non-reference\nunderwater benchmarks demonstrate that our algorithm has obvious improvement\nover SOTA methods quantitatively and qualitatively. Code has been released at\n\\href{https://github.com/Huang-ShiRui/Semi-UIR}{https://github.com/Huang-ShiRui/Semi-UIR}.\n","authors":["Shirui Huang","Keyan Wang","Huan Liu","Jun Chen","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2303.09101v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09100v1","updated":"2023-03-16T06:09:15Z","published":"2023-03-16T06:09:15Z","title":"Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models","summary":"  For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt learning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize prompt learning with\nthe visual knowledge and view images and the corresponding prompts as patch and\ntoken sets under optimal transport, which pushes the prompt tokens to\nfaithfully capture the label-specific visual concepts, instead of overfitting\nthe training categories. Moreover, the proposed model can also be\nstraightforwardly extended to the conditional case where the\ninstance-conditional prompts are generated to improve the generalizability.\nExtensive experiments on 15 datasets show promising transferability and\ngeneralization performance of our proposed model.\n","authors":["Xinyang Liu","Dongsheng Wang","Miaoge Li","Zhibin Duan","Yishi Xu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06331v2","updated":"2023-03-16T06:08:57Z","published":"2022-12-13T02:22:04Z","title":"DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization","summary":"  LiDAR mapping is important yet challenging in self-driving and mobile\nrobotics. To tackle such a global point cloud registration problem, DeepMapping\nconverts the complex map estimation into a self-supervised training of simple\ndeep networks. Despite its broad convergence range on small datasets,\nDeepMapping still cannot produce satisfactory results on large-scale datasets\nwith thousands of frames. This is due to the lack of loop closures and exact\ncross-frame point correspondences, and the slow convergence of its global\nlocalization network. We propose DeepMapping2 by adding two novel techniques to\naddress these issues: (1) organization of training batch based on map topology\nfrom loop closing, and (2) self-supervised local-to-global point consistency\nloss leveraging pairwise registration. Our experiments and ablation studies on\npublic datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our\nmethod.\n","authors":["Chao Chen","Xinhao Liu","Yiming Li","Li Ding","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2212.06331v2.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.04587v2","updated":"2023-03-16T06:03:08Z","published":"2023-03-08T13:59:41Z","title":"A Prompt Log Analysis of Text-to-Image Generation Systems","summary":"  Recent developments in large language models (LLM) and generative AI have\nunleashed the astonishing capabilities of text-to-image generation systems to\nsynthesize high-quality images that are faithful to a given reference text,\nknown as a \"prompt\". These systems have immediately received lots of attention\nfrom researchers, creators, and common users. Despite the plenty of efforts to\nimprove the generative models, there is limited work on understanding the\ninformation needs of the users of these systems at scale. We conduct the first\ncomprehensive analysis of large-scale prompt logs collected from multiple\ntext-to-image generation systems. Our work is analogous to analyzing the query\nlogs of Web search engines, a line of work that has made critical contributions\nto the glory of the Web search industry and research. Compared with Web search\nqueries, text-to-image prompts are significantly longer, often organized into\nspecial structures that consist of the subject, form, and intent of the\ngeneration tasks and present unique categories of information needs. Users make\nmore edits within creation sessions, which present remarkable exploratory\npatterns. There is also a considerable gap between the user-input prompts and\nthe captions of the images included in the open training data of the generative\nmodels. Our findings provide concrete implications on how to improve\ntext-to-image generation systems for creation purposes.\n","authors":["Yutong Xie","Zhaoying Pan","Jinge Ma","Luo Jie","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2303.04587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09097v1","updated":"2023-03-16T06:01:21Z","published":"2023-03-16T06:01:21Z","title":"IRIS: Interpretable Rubric-Informed Segmentation for Action Quality\n  Assessment","summary":"  AI-driven Action Quality Assessment (AQA) of sports videos can mimic Olympic\njudges to help score performances as a second opinion or for training. However,\nthese AI methods are uninterpretable and do not justify their scores, which is\nimportant for algorithmic accountability. Indeed, to account for their\ndecisions, instead of scoring subjectively, sports judges use a consistent set\nof criteria - rubric - on multiple actions in each performance sequence.\nTherefore, we propose IRIS to perform Interpretable Rubric-Informed\nSegmentation on action sequences for AQA. We investigated IRIS for scoring\nvideos of figure skating performance. IRIS predicts (1) action segments, (2)\ntechnical element score differences of each segment relative to base scores,\n(3) multiple program component scores, and (4) the summed final score. In a\nmodeling study, we found that IRIS performs better than non-interpretable,\nstate-of-the-art models. In a formative user study, practicing figure skaters\nagreed with the rubric-informed explanations, found them useful, and trusted AI\njudgments more. This work highlights the importance of using judgment rubrics\nto account for AI decisions.\n","authors":["Hitoshi Matsuyama","Nobuo Kawaguchi","Brian Y. Lim"],"pdf_url":"https://arxiv.org/pdf/2303.09097v1.pdf","comment":"28th International Conference on Intelligent User Interfaces (IUI\n  2023)"},{"id":"http://arxiv.org/abs/2202.03382v2","updated":"2023-03-16T05:58:10Z","published":"2022-02-07T17:59:04Z","title":"Corrupted Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Corrupted Image Modeling (CIM) for self-supervised visual\npre-training. CIM uses an auxiliary generator with a small trainable BEiT to\ncorrupt the input image instead of using artificial [MASK] tokens, where some\npatches are randomly selected and replaced with plausible alternatives sampled\nfrom the BEiT output distribution. Given this corrupted image, an enhancer\nnetwork learns to either recover all the original image pixels, or predict\nwhether each visual token is replaced by a generator sample or not. The\ngenerator and the enhancer are simultaneously trained and synergistically\nupdated. After pre-training, the enhancer can be used as a high-capacity visual\nencoder for downstream tasks. CIM is a general and flexible visual pre-training\nframework that is suitable for various network architectures. For the first\ntime, CIM demonstrates that both ViT and CNN can learn rich visual\nrepresentations using a unified, non-Siamese framework. Experimental results\nshow that our approach achieves compelling results in vision benchmarks, such\nas ImageNet classification and ADE20K semantic segmentation.\n","authors":["Yuxin Fang","Li Dong","Hangbo Bao","Xinggang Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2202.03382v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09095v1","updated":"2023-03-16T05:54:15Z","published":"2023-03-16T05:54:15Z","title":"SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in\n  Urban Environments","summary":"  We present SLOPER4D, a novel scene-aware dataset collected in large urban\nenvironments to facilitate the research of global human pose estimation (GHPE)\nwith human-scene interaction in the wild. Employing a head-mounted device\nintegrated with a LiDAR and camera, we record 12 human subjects' activities\nover 10 diverse urban scenes from an egocentric view. Frame-wise annotations\nfor 2D key points, 3D pose parameters, and global translations are provided,\ntogether with reconstructed scene point clouds. To obtain accurate 3D ground\ntruth in such large dynamic scenes, we propose a joint optimization method to\nfit local SMPL meshes to the scene and fine-tune the camera calibration during\ndynamic motions frame by frame, resulting in plausible and scene-natural 3D\nhuman poses. Eventually, SLOPER4D consists of 15 sequences of human motions,\neach of which has a trajectory length of more than 200 meters (up to 1,300\nmeters) and covers an area of more than 2,000 $m^2$ (up to 13,000 $m^2$),\nincluding more than 100K LiDAR frames, 300k video frames, and 500K IMU-based\nmotion frames. With SLOPER4D, we provide a detailed and thorough analysis of\ntwo critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in\nurban environments, and benchmark a new task, GHPE. The in-depth analysis\ndemonstrates SLOPER4D poses significant challenges to existing methods and\nproduces great research opportunities. The dataset and code are released at\n\\url{http://www.lidarhumanmotion.net/sloper4d/}\n","authors":["Yudi Dai","Yitai Lin","Xiping Lin","Chenglu Wen","Lan Xu","Hongwei Yi","Siqi Shen","Yuexin Ma","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09095v1.pdf","comment":"8 pages,7 figures, CVPR2023"},{"id":"http://arxiv.org/abs/2303.09088v1","updated":"2023-03-16T05:24:13Z","published":"2023-03-16T05:24:13Z","title":"MetaRegNet: Metamorphic Image Registration Using Flow-Driven Residual\n  Networks","summary":"  Deep learning based methods provide efficient solutions to medical image\nregistration, including the challenging problem of diffeomorphic image\nregistration. However, most methods register normal image pairs, facing\ndifficulty handling those with missing correspondences, e.g., in the presence\nof pathology like tumors. We desire an efficient solution to jointly account\nfor spatial deformations and appearance changes in the pathological regions\nwhere the correspondences are missing, i.e., finding a solution to metamorphic\nimage registration. Some approaches are proposed to tackle this problem, but\nthey cannot properly handle large pathological regions and deformations around\npathologies. In this paper, we propose a deep metamorphic image registration\nnetwork (MetaRegNet), which adopts time-varying flows to drive spatial\ndiffeomorphic deformations and generate intensity variations. We evaluate\nMetaRegNet on two datasets, i.e., BraTS 2021 with brain tumors and 3D-IRCADb-01\nwith liver tumors, showing promising results in registering a healthy and tumor\nimage pair. The source code is available online.\n","authors":["Ankita Joshi","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2303.09088v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2107.11990v2","updated":"2023-03-16T05:23:18Z","published":"2021-07-26T06:54:53Z","title":"Augmentation Pathways Network for Visual Recognition","summary":"  Data augmentation is practically helpful for visual recognition, especially\nat the time of data scarcity. However, such success is only limited to quite a\nfew light augmentations (e.g., random crop, flip). Heavy augmentations are\neither unstable or show adverse effects during training, owing to the big gap\nbetween the original and augmented images. This paper introduces a novel\nnetwork design, noted as Augmentation Pathways (AP), to systematically\nstabilize training on a much wider range of augmentation policies. Notably, AP\ntames various heavy data augmentations and stably boosts performance without a\ncareful selection among augmentation policies. Unlike traditional single\npathway, augmented images are processed in different neural paths. The main\npathway handles the light augmentations, while other pathways focus on the\nheavier augmentations. By interacting with multiple paths in a dependent\nmanner, the backbone network robustly learns from shared visual patterns among\naugmentations, and suppresses the side effect of heavy augmentations at the\nsame time. Furthermore, we extend AP to high-order versions for high-order\nscenarios, demonstrating its robustness and flexibility in practical usage.\nExperimental results on ImageNet demonstrate the compatibility and\neffectiveness on a much wider range of augmentations, while consuming fewer\nparameters and lower computational costs at inference time.\n","authors":["Yalong Bai","Mohan Zhou","Wei Zhang","Bowen Zhou","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2107.11990v2.pdf","comment":"accepted by TPAMI 2023"},{"id":"http://arxiv.org/abs/2303.08817v2","updated":"2023-03-16T05:05:46Z","published":"2023-03-15T17:59:55Z","title":"DeepMIM: Deep Supervision for Masked Image Modeling","summary":"  Deep supervision, which involves extra supervisions to the intermediate\nfeatures of a neural network, was widely used in image classification in the\nearly deep learning era since it significantly reduces the training difficulty\nand eases the optimization like avoiding gradient vanish over the vanilla\ntraining. Nevertheless, with the emergence of normalization techniques and\nresidual connection, deep supervision in image classification was gradually\nphased out. In this paper, we revisit deep supervision for masked image\nmodeling (MIM) that pre-trains a Vision Transformer (ViT) via a\nmask-and-predict scheme. Experimentally, we find that deep supervision drives\nthe shallower layers to learn more meaningful representations, accelerates\nmodel convergence, and expands attention diversities. Our approach, called\nDeepMIM, significantly boosts the representation capability of each layer. In\naddition, DeepMIM is compatible with many MIM models across a range of\nreconstruction targets. For instance, using ViT-B, DeepMIM on MAE achieves 84.2\ntop-1 accuracy on ImageNet, outperforming MAE by +0.6. By combining DeepMIM\nwith a stronger tokenizer CLIP, our model achieves state-of-the-art performance\non various downstream tasks, including image classification (85.6 top-1\naccuracy on ImageNet-1K, outperforming MAE-CLIP by +0.8), object detection\n(52.8 APbox on COCO) and semantic segmentation (53.1 mIoU on ADE20K). Code and\nmodels are available at https://github.com/OliverRensu/DeepMIM.\n","authors":["Sucheng Ren","Fangyun Wei","Samuel Albanie","Zheng Zhang","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2303.08817v2.pdf","comment":"Code and models are available at\n  https://github.com/OliverRensu/DeepMIM"},{"id":"http://arxiv.org/abs/2303.09083v1","updated":"2023-03-16T05:04:10Z","published":"2023-03-16T05:04:10Z","title":"Focus on Your Target: A Dual Teacher-Student Framework for\n  Domain-adaptive Semantic Segmentation","summary":"  We study unsupervised domain adaptation (UDA) for semantic segmentation.\nCurrently, a popular UDA framework lies in self-training which endows the model\nwith two-fold abilities: (i) learning reliable semantics from the labeled\nimages in the source domain, and (ii) adapting to the target domain via\ngenerating pseudo labels on the unlabeled images. We find that, by\ndecreasing/increasing the proportion of training samples from the target\ndomain, the 'learning ability' is strengthened/weakened while the 'adapting\nability' goes in the opposite direction, implying a conflict between these two\nabilities, especially for a single model. To alleviate the issue, we propose a\nnovel dual teacher-student (DTS) framework and equip it with a bidirectional\nlearning strategy. By increasing the proportion of target-domain data, the\nsecond teacher-student model learns to 'Focus on Your Target' while the first\nmodel is not affected. DTS is easily plugged into existing self-training\napproaches. In a standard UDA scenario (training on synthetic, labeled data and\nreal, unlabeled data), DTS shows consistent gains over the baselines and sets\nnew state-of-the-art results of 76.5\\% and 75.1\\% mIoUs on\nGTAv$\\rightarrow$Cityscapes and SYNTHIA$\\rightarrow$Cityscapes, respectively.\n","authors":["Xinyue Huo","Lingxi Xie","Wengang Zhou","Houqiang Li","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09083v1.pdf","comment":"12 pages, 7 figures, 10 tables"},{"id":"http://arxiv.org/abs/2302.11208v2","updated":"2023-03-16T04:54:05Z","published":"2023-02-22T08:48:08Z","title":"KS-DETR: Knowledge Sharing in Attention Learning for Detection\n  Transformer","summary":"  Scaled dot-product attention applies a softmax function on the scaled\ndot-product of queries and keys to calculate weights and then multiplies the\nweights and values. In this work, we study how to improve the learning of\nscaled dot-product attention to improve the accuracy of DETR. Our method is\nbased on the following observations: using ground truth foreground-background\nmask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables\nlearning much better weights/values; with better weights/values, better\nvalues/weights can be learned. We propose a triple-attention module in which\nthe first attention is a plain scaled dot-product attention, the second/third\nattention generates high-quality weights/values (with the assistance of GT\nFg-Bg Mask) and shares the values/weights with the first attention to improve\nthe quality of values/weights. The second and third attentions are removed\nduring inference. We call our method knowledge-sharing DETR (KS-DETR), which is\nan extension of knowledge distillation (KD) in the way that the improved\nweights and values of the teachers (the second and third attentions) are\ndirectly shared, instead of mimicked, by the student (the first attention) to\nenable more efficient knowledge transfer from the teachers to the student.\nExperiments on various DETR-like methods show consistent improvements over the\nbaseline methods on the MS COCO benchmark. Code is available at\nhttps://github.com/edocanonymous/KS-DETR.\n","authors":["Kaikai Zhao","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2302.11208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04748v2","updated":"2023-03-16T04:52:06Z","published":"2023-03-08T17:30:58Z","title":"CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D\n  Dense CLIP","summary":"  Training a 3D scene understanding model requires complicated human\nannotations, which are laborious to collect and result in a model only encoding\nclose-set object semantics. In contrast, vision-language pre-training models\n(e.g., CLIP) have shown remarkable open-world reasoning properties. To this\nend, we propose directly transferring CLIP's feature space to 3D scene\nunderstanding model without any form of supervision. We first modify CLIP's\ninput and forwarding process so that it can be adapted to extract dense pixel\nfeatures for 3D scene contents. We then project multi-view image features to\nthe point cloud and train a 3D scene understanding model with feature\ndistillation. Without any annotations or additional training, our model\nachieves promising annotation-free semantic segmentation results on\nopen-vocabulary semantics and long-tailed concepts. Besides, serving as a\ncross-modal pre-training framework, our method can be used to improve data\nefficiency during fine-tuning. Our model outperforms previous SOTA methods in\nvarious zero-shot and data-efficient learning benchmarks. Most importantly, our\nmodel successfully inherits CLIP's rich-structured knowledge, allowing 3D scene\nunderstanding models to recognize not only object concepts but also open-world\nsemantics.\n","authors":["Junbo Zhang","Runpei Dong","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2303.04748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02110v3","updated":"2023-03-16T23:55:25Z","published":"2023-03-03T17:51:08Z","title":"Need for Objective Task-based Evaluation of Deep Learning-Based\n  Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT","summary":"  Artificial intelligence-based methods have generated substantial interest in\nnuclear medicine. An area of significant interest has been using deep-learning\n(DL)-based approaches for denoising images acquired with lower doses, shorter\nacquisition times, or both. Objective evaluation of these approaches is\nessential for clinical application. DL-based approaches for denoising\nnuclear-medicine images have typically been evaluated using fidelity-based\nfigures of merit (FoMs) such as RMSE and SSIM. However, these images are\nacquired for clinical tasks and thus should be evaluated based on their\nperformance in these tasks. Our objectives were to (1) investigate whether\nevaluation with these FoMs is consistent with objective clinical-task-based\nevaluation; (2) provide a theoretical analysis for determining the impact of\ndenoising on signal-detection tasks; (3) demonstrate the utility of virtual\nclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a\nDL-based method for denoising myocardial perfusion SPECT (MPS) images was\nconducted. The impact of DL-based denoising was evaluated using fidelity-based\nFoMs and AUC, which quantified performance on detecting perfusion defects in\nMPS images as obtained using a model observer with anthropomorphic channels.\nBased on fidelity-based FoMs, denoising using the considered DL-based method\nled to significantly superior performance. However, based on ROC analysis,\ndenoising did not improve, and in fact, often degraded detection-task\nperformance. The results motivate the need for objective task-based evaluation\nof DL-based denoising approaches. Further, this study shows how VCTs provide a\nmechanism to conduct such evaluations using VCTs. Finally, our theoretical\ntreatment reveals insights into the reasons for the limited performance of the\ndenoising approach.\n","authors":["Zitong Yu","Md Ashequr Rahman","Richard Laforest","Thomas H. Schindler","Robert J. Gropler","Richard L. Wahl","Barry A. Siegel","Abhinav K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.02110v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07321v2","updated":"2023-03-16T23:01:54Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09681v1","updated":"2023-03-16T22:56:12Z","published":"2023-03-16T22:56:12Z","title":"Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer","summary":"  Event camera, as an emerging biologically-inspired vision sensor for\ncapturing motion dynamics, presents new potential for 3D human pose tracking,\nor video-based 3D human pose estimation. However, existing works in pose\ntracking either require the presence of additional gray-scale images to\nestablish a solid starting pose, or ignore the temporal dependencies all\ntogether by collapsing segments of event streams to form static image frames.\nMeanwhile, although the effectiveness of Artificial Neural Networks (ANNs,\na.k.a. dense deep learning) has been showcased in many event-based tasks, the\nuse of ANNs tends to neglect the fact that compared to the dense frame-based\nimage sequences, the occurrence of events from an event camera is\nspatiotemporally much sparser. Motivated by the above mentioned issues, we\npresent in this paper a dedicated end-to-end \\textit{sparse deep learning}\napproach for event-based pose tracking: 1) to our knowledge this is the first\ntime that 3D human pose tracking is obtained from events only, thus eliminating\nthe need of accessing to any frame-based images as part of input; 2) our\napproach is based entirely upon the framework of Spiking Neural Networks\n(SNNs), which consists of Spike-Element-Wise (SEW) ResNet and our proposed\nspiking spatiotemporal transformer; 3) a large-scale synthetic dataset is\nconstructed that features a broad and diverse set of annotated 3D human\nmotions, as well as longer hours of event stream data, named SynEventHPD.\nEmpirical experiments demonstrate the superiority of our approach in both\nperformance and efficiency measures. For example, with comparable performance\nto the state-of-the-art ANNs counterparts, our approach achieves a computation\nreduction of 20\\% in FLOPS. Our implementation is made available at\nhttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be released\nupon paper acceptance.\n","authors":["Shihao Zou","Yuxuan Mu","Xinxin Zuo","Sen Wang","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.09681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09679v1","updated":"2023-03-16T22:47:48Z","published":"2023-03-16T22:47:48Z","title":"Segmentation of Retinal Blood Vessels Using Deep Learning","summary":"  The morphology of retinal blood vessels can indicate various diseases in the\nhuman body, and researchers have been working on automatic scanning and\nsegmentation of retinal images to aid diagnosis. This project compares the\nperformance of four neural network architectures in segmenting retinal images,\nusing a combined dataset from different databases, namely the UNet, DR-VNet,\nUNet-ResNet and UNet-VGG.\n","authors":["Ifeyinwa Linda Anene","Yongmin Li"],"pdf_url":"https://arxiv.org/pdf/2303.09679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09677v1","updated":"2023-03-16T22:45:43Z","published":"2023-03-16T22:45:43Z","title":"Instance-Conditioned GAN Data Augmentation for Representation Learning","summary":"  Data augmentation has become a crucial component to train state-of-the-art\nvisual representation models. However, handcrafting combinations of\ntransformations that lead to improved performances is a laborious task, which\ncan result in visually unrealistic samples. To overcome these limitations,\nrecent works have explored the use of generative models as learnable data\naugmentation tools, showing promising results in narrow application domains,\ne.g., few-shot learning and low-data medical imaging. In this paper, we\nintroduce a data augmentation module, called DA_IC-GAN, which leverages\ninstance-conditioned GAN generations and can be used off-the-shelf in\nconjunction with most state-of-the-art training recipes. We showcase the\nbenefits of DA_IC-GAN by plugging it out-of-the-box into the supervised\ntraining of ResNets and DeiT models on the ImageNet dataset, and achieving\naccuracy boosts up to between 1%p and 2%p with the highest capacity models.\nMoreover, the learnt representations are shown to be more robust than the\nbaselines when transferred to a handful of out-of-distribution datasets, and\nexhibit increased invariance to variations of instance and viewpoints. We\nadditionally couple DA_IC-GAN with a self-supervised training recipe and show\nthat we can also achieve an improvement of 1%p in accuracy in some settings.\nWith this work, we strengthen the evidence on the potential of learnable data\naugmentations to improve visual representation learning, paving the road\ntowards non-handcrafted augmentations in model training.\n","authors":["Pietro Astolfi","Arantxa Casanova","Jakob Verbeek","Pascal Vincent","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2303.09677v1.pdf","comment":"TMLR reviews at\n  https://openreview.net/forum?id=1n7q9mxG3T&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)"},{"id":"http://arxiv.org/abs/2303.09674v1","updated":"2023-03-16T22:37:09Z","published":"2023-03-16T22:37:09Z","title":"DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot\n  Object Detection","summary":"  Generalized few-shot object detection aims to achieve precise detection on\nboth base classes with abundant annotations and novel classes with limited\ntraining data. Existing approaches enhance few-shot generalization with the\nsacrifice of base-class performance, or maintain high precision in base-class\ndetection with limited improvement in novel-class adaptation. In this paper, we\npoint out the reason is insufficient Discriminative feature learning for all of\nthe classes. As such, we propose a new training framework, DiGeo, to learn\nGeometry-aware features of inter-class separation and intra-class compactness.\nTo guide the separation of feature clusters, we derive an offline simplex\nequiangular tight frame (ETF) classifier whose weights serve as class centers\nand are maximally and equally separated. To tighten the cluster for each class,\nwe include adaptive class-specific margins into the classification loss and\nencourage the features close to the class centers. Experimental studies on two\nfew-shot benchmark datasets (VOC, COCO) and one long-tail dataset (LVIS)\ndemonstrate that, with a single model, our method can effectively improve\ngeneralization on novel classes without hurting the detection of base classes.\n","authors":["Jiawei Ma","Yulei Niu","Jincheng Xu","Shiyuan Huang","Guangxing Han","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2303.09674v1.pdf","comment":"CVPR 2023 Camera Ready (Supp Attached). Code Link:\n  https://github.com/Phoenix-V/DiGeo"},{"id":"http://arxiv.org/abs/2303.09668v1","updated":"2023-03-16T22:08:29Z","published":"2023-03-16T22:08:29Z","title":"Rt-Track: Robust Tricks for Multi-Pedestrian Tracking","summary":"  Object tracking is divided into single-object tracking (SOT) and multi-object\ntracking (MOT). MOT aims to maintain the identities of multiple objects across\na series of continuous video sequences. In recent years, MOT has made rapid\nprogress. However, modeling the motion and appearance models of objects in\ncomplex scenes still faces various challenging issues. In this paper, we design\na novel direction consistency method for smooth trajectory prediction (STP-DC)\nto increase the modeling of motion information and overcome the lack of\nrobustness in previous methods in complex scenes. Existing methods use\npedestrian re-identification (Re-ID) to model appearance, however, they extract\nmore background information which lacks discriminability in occlusion and\ncrowded scenes. We propose a hyper-grain feature embedding network (HG-FEN) to\nenhance the modeling of appearance models, thus generating robust appearance\ndescriptors. We also proposed other robustness techniques, including CF-ECM for\nstoring robust appearance information and SK-AS for improving association\naccuracy. To achieve state-of-the-art performance in MOT, we propose a robust\ntracker named Rt-track, incorporating various tricks and techniques. It\nachieves 79.5 MOTA, 76.0 IDF1 and 62.1 HOTA on the test set of MOT17.Rt-track\nalso achieves 77.9 MOTA, 78.4 IDF1 and 63.3 HOTA on MOT20, surpassing all\npublished methods.\n","authors":["Yukuan Zhang","Yunhua Jia","Housheng Xie","Mengzhen Li","Limin Zhao","Yang Yang","Shan Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.09668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07345v2","updated":"2023-03-16T22:00:16Z","published":"2023-03-13T17:59:55Z","title":"Erasing Concepts from Diffusion Models","summary":"  Motivated by recent advancements in text-to-image diffusion, we study erasure\nof specific concepts from the model's weights. While Stable Diffusion has shown\npromise in producing explicit or realistic artwork, it has raised concerns\nregarding its potential for misuse. We propose a fine-tuning method that can\nerase a visual concept from a pre-trained diffusion model, given only the name\nof the style and using negative guidance as a teacher. We benchmark our method\nagainst previous approaches that remove sexually explicit content and\ndemonstrate its effectiveness, performing on par with Safe Latent Diffusion and\ncensored training. To evaluate artistic style removal, we conduct experiments\nerasing five modern artists from the network and conduct a user study to assess\nthe human perception of the removed styles. Unlike previous methods, our\napproach can remove concepts from a diffusion model permanently rather than\nmodifying the output at the inference time, so it cannot be circumvented even\nif a user has access to model weights. Our code, data, and results are\navailable at https://erasing.baulab.info/\n","authors":["Rohit Gandikota","Joanna Materzynska","Jaden Fiotto-Kaufman","David Bau"],"pdf_url":"https://arxiv.org/pdf/2303.07345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09665v1","updated":"2023-03-16T21:47:49Z","published":"2023-03-16T21:47:49Z","title":"LOCATE: Localize and Transfer Object Parts for Weakly Supervised\n  Affordance Grounding","summary":"  Humans excel at acquiring knowledge through observation. For example, we can\nlearn to use new tools by watching demonstrations. This skill is fundamental\nfor intelligent systems to interact with the world. A key step to acquire this\nskill is to identify what part of the object affords each action, which is\ncalled affordance grounding. In this paper, we address this problem and propose\na framework called LOCATE that can identify matching object parts across\nimages, to transfer knowledge from images where an object is being used\n(exocentric images used for learning), to images where the object is inactive\n(egocentric ones used to test). To this end, we first find interaction areas\nand extract their feature embeddings. Then we learn to aggregate the embeddings\ninto compact prototypes (human, object part, and background), and select the\none representing the object part. Finally, we use the selected prototype to\nguide affordance grounding. We do this in a weakly supervised manner, learning\nonly from image-level affordance and object labels. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art methods by a large\nmargin on both seen and unseen objects.\n","authors":["Gen Li","Varun Jampani","Deqing Sun","Laura Sevilla-Lara"],"pdf_url":"https://arxiv.org/pdf/2303.09665v1.pdf","comment":"CVPR 2023, Project page: https://reagan1311.github.io/locate/, Video:\n  https://www.youtube.com/watch?v=RLHansdFxII"},{"id":"http://arxiv.org/abs/2303.09663v1","updated":"2023-03-16T21:47:40Z","published":"2023-03-16T21:47:40Z","title":"Efficient Computation Sharing for Multi-Task Visual Scene Understanding","summary":"  Solving multiple visual tasks using individual models can be\nresource-intensive, while multi-task learning can conserve resources by sharing\nknowledge across different tasks. Despite the benefits of multi-task learning,\nsuch techniques can struggle with balancing the loss for each task, leading to\npotential performance degradation. We present a novel computation- and\nparameter-sharing framework that balances efficiency and accuracy to perform\nmultiple visual tasks utilizing individually-trained single-task transformers.\nOur method is motivated by transfer learning schemes to reduce computational\nand parameter storage costs while maintaining the desired performance. Our\napproach involves splitting the tasks into a base task and the other sub-tasks,\nand sharing a significant portion of activations and parameters/weights between\nthe base and sub-tasks to decrease inter-task redundancies and enhance\nknowledge sharing. The evaluation conducted on NYUD-v2 and PASCAL-context\ndatasets shows that our method is superior to the state-of-the-art\ntransformer-based multi-task learning techniques with higher accuracy and\nreduced computational resources. Moreover, our method is extended to video\nstream inputs, further reducing computational costs by efficiently sharing\ninformation across the temporal domain as well as the task domain. Our codes\nand models will be publicly available.\n","authors":["Sara Shoouri","Mingyu Yang","Zichen Fan","Hun-Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2303.09663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09660v1","updated":"2023-03-16T21:37:29Z","published":"2023-03-16T21:37:29Z","title":"Explainable GeoAI: Can saliency maps help interpret artificial\n  intelligence's learning process? An empirical study on natural feature\n  detection","summary":"  Improving the interpretability of geospatial artificial intelligence (GeoAI)\nmodels has become critically important to open the \"black box\" of complex AI\nmodels, such as deep learning. This paper compares popular saliency map\ngeneration techniques and their strengths and weaknesses in interpreting GeoAI\nand deep learning models' reasoning behaviors, particularly when applied to\ngeospatial analysis and image processing tasks. We surveyed two broad classes\nof model explanation methods: perturbation-based and gradient-based methods.\nThe former identifies important image areas, which help machines make\npredictions by modifying a localized area of the input image. The latter\nevaluates the contribution of every single pixel of the input image to the\nmodel's prediction results through gradient backpropagation. In this study,\nthree algorithms-the occlusion method, the integrated gradients method, and the\nclass activation map method-are examined for a natural feature detection task\nusing deep learning. The algorithms' strengths and weaknesses are discussed,\nand the consistency between model-learned and human-understandable concepts for\nobject recognition is also compared. The experiments used two GeoAI-ready\ndatasets to demonstrate the generalizability of the research findings.\n","authors":["Chia-Yu Hsu","Wenwen Li"],"pdf_url":"https://arxiv.org/pdf/2303.09660v1.pdf","comment":"24 Pages, 7 Figures, Accepted to International Journal of\n  Geographical Information Science, 2023"},{"id":"http://arxiv.org/abs/2303.09650v1","updated":"2023-03-16T21:06:13Z","published":"2023-03-16T21:06:13Z","title":"Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution","summary":"  The field of image super-resolution (SR) has witnessed extensive neural\nnetwork designs from CNN to transformer architectures. However, prevailing SR\nmodels suffer from prohibitive memory footprint and intensive computations,\nwhich limits further deployment on computational-constrained platforms. In this\nwork, we investigate the potential of network pruning for super-resolution to\ntake advantage of off-the-shelf network designs and reduce the underlying\ncomputational overhead. Two main challenges remain in applying pruning methods\nfor SR. First, the widely-used filter pruning technique reflects limited\ngranularity and restricted adaptability to diverse network structures. Second,\nexisting pruning methods generally operate upon a pre-trained network for the\nsparse structure determination, failing to get rid of dense model training in\nthe traditional SR paradigm. To address these challenges, we adopt unstructured\npruning with sparse models directly trained from scratch. Specifically, we\npropose a novel Iterative Soft Shrinkage-Percentage (ISS-P) method by\noptimizing the sparse structure of a randomly initialized network at each\niteration and tweaking unimportant weights with a small amount proportional to\nthe magnitude scale on-the-fly. We observe that the proposed ISS-P could\ndynamically learn sparse structures adapting to the optimization process and\npreserve the sparse model's trainability by yielding a more regularized\ngradient throughput. Experiments on benchmark datasets demonstrate the\neffectiveness of the proposed ISS-P compared with state-of-the-art methods over\ndiverse network architectures.\n","authors":["Jiamian Wang","Huan Wang","Yulun Zhang","Yun Fu","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2303.09650v1.pdf","comment":"12 pages, 16 figures, 8 tables"},{"id":"http://arxiv.org/abs/2112.11454v2","updated":"2023-03-16T20:59:14Z","published":"2021-12-21T18:59:34Z","title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping","summary":"  Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied, but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this,\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n","authors":["Omid Taheri","Vasileios Choutas","Michael J. Black","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2112.11454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09648v1","updated":"2023-03-16T20:58:48Z","published":"2023-03-16T20:58:48Z","title":"Shifted-Windows Transformers for the Detection of Cerebral Aneurysms in\n  Microsurgery","summary":"  Purpose: Microsurgical Aneurysm Clipping Surgery (MACS) carries a high risk\nfor intraoperative aneurysm rupture. Automated recognition of instances when\nthe aneurysm is exposed in the surgical video would be a valuable reference\npoint for neuronavigation, indicating phase transitioning and more importantly\ndesignating moments of high risk for rupture. This article introduces the MACS\ndataset containing 16 surgical videos with frame-level expert annotations and\nproposes a learning methodology for surgical scene understanding identifying\nvideo frames with the aneurysm present in the operating microscope's\nfield-of-view. Methods: Despite the dataset imbalance (80% no presence, 20%\npresence) and developed without explicit annotations, we demonstrate the\napplicability of Transformer-based deep learning architectures (MACSSwin-T,\nvidMACSSwin-T) to detect the aneurysm and classify MACS frames accordingly. We\nevaluate the proposed models in multiple-fold cross-validation experiments with\nindependent sets and in an unseen set of 15 images against 10 human experts\n(neurosurgeons). Results: Average (across folds) accuracy of 80.8% (range\n78.5%-82.4%) and 87.1% (range 85.1%-91.3%) is obtained for the image- and\nvideo-level approach respectively, demonstrating that the models effectively\nlearn the classification task. Qualitative evaluation of the models' class\nactivation maps show these to be localized on the aneurysm's actual location.\nDepending on the decision threshold, MACSWin-T achieves 66.7% to 86.7% accuracy\nin the unseen images, compared to 82% of human raters, with moderate to strong\ncorrelation.\n","authors":["Jinfan Zhou","William Muirhead","Simon C. Williams","Danail Stoyanov","Hani J. Marcus","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2303.09648v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.09527v1","updated":"2023-03-16T17:44:39Z","published":"2023-03-16T17:44:39Z","title":"Fairness-aware Differentially Private Collaborative Filtering","summary":"  Recently, there has been an increasing adoption of differential privacy\nguided algorithms for privacy-preserving machine learning tasks. However, the\nuse of such algorithms comes with trade-offs in terms of algorithmic fairness,\nwhich has been widely acknowledged. Specifically, we have empirically observed\nthat the classical collaborative filtering method, trained by differentially\nprivate stochastic gradient descent (DP-SGD), results in a disparate impact on\nuser groups with respect to different user engagement levels. This, in turn,\ncauses the original unfair model to become even more biased against inactive\nusers. To address the above issues, we propose \\textbf{DP-Fair}, a two-stage\nframework for collaborative filtering based algorithms. Specifically, it\ncombines differential privacy mechanisms with fairness constraints to protect\nuser privacy while ensuring fair recommendations. The experimental results,\nbased on Amazon datasets, and user history logs collected from Etsy, one of the\nlargest e-commerce platforms, demonstrate that our proposed method exhibits\nsuperior performance in terms of both overall accuracy and user group fairness\non both shallow and deep recommendation models compared to vanilla DP-SGD.\n","authors":["Zhenhuan Yang","Yingqiang Ge","Congzhe Su","Dingxian Wang","Xiaoting Zhao","Yiming Ying"],"pdf_url":"https://arxiv.org/pdf/2303.09527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09498v1","updated":"2023-03-16T17:19:13Z","published":"2023-03-16T17:19:13Z","title":"Measuring the Impact of Explanation Bias: A Study of Natural Language\n  Justifications for Recommender Systems","summary":"  Despite the potential impact of explanations on decision making, there is a\nlack of research on quantifying their effect on users' choices. This paper\npresents an experimental protocol for measuring the degree to which positively\nor negatively biased explanations can lead to users choosing suboptimal\nrecommendations. Key elements of this protocol include a preference elicitation\nstage to allow for personalizing recommendations, manual identification and\nextraction of item aspects from reviews, and a controlled method for\nintroducing bias through the combination of both positive and negative aspects.\nWe study explanations in two different textual formats: as a list of item\naspects and as fluent natural language text. Through a user study with 129\nparticipants, we demonstrate that explanations can significantly affect users'\nselections and that these findings generalize across explanation formats.\n","authors":["Krisztian Balog","Filip Radlinski","Andrey Petrov"],"pdf_url":"https://arxiv.org/pdf/2303.09498v1.pdf","comment":"Extended Abstracts of the 2023 CHI Conference on Human Factors in\n  Computing Systems (CHI EA '23), 2023"},{"id":"http://arxiv.org/abs/2108.08614v5","updated":"2023-03-16T15:15:32Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents the first system for\ncomplex questions that can seamlessly operate over a mixture of RDF datasets\nand text corpora, or individual sources, in a unified framework. Our method,\ncalled UNIQORN, builds a context graph on-the-fly, by retrieving\nquestion-relevant evidences from the RDF data and/or a text corpus, using\nfine-tuned BERT models. The resulting graph is typically rich but highly noisy.\nUNIQORN copes with this input by a graph algorithm for Group Steiner Trees,\nthat identifies the best answer candidates in the context graph. Experimental\nresults on several benchmarks of complex questions with multiple entities and\nrelations, show that \\uniqorn significantly outperforms state-of-the-art\nmethods for QA over heterogeneous sources. The graph-based methodology provides\nuser-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v5.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2303.09384v1","updated":"2023-03-16T15:13:58Z","published":"2023-03-16T15:13:58Z","title":"LLMSecEval: A Dataset of Natural Language Prompts for Security\n  Evaluations","summary":"  Large Language Models (LLMs) like Codex are powerful tools for performing\ncode completion and code generation tasks as they are trained on billions of\nlines of code from publicly available sources. Moreover, these models are\ncapable of generating code snippets from Natural Language (NL) descriptions by\nlearning languages and programming practices from public GitHub repositories.\nAlthough LLMs promise an effortless NL-driven deployment of software\napplications, the security of the code they generate has not been extensively\ninvestigated nor documented. In this work, we present LLMSecEval, a dataset\ncontaining 150 NL prompts that can be leveraged for assessing the security\nperformance of such models. Such prompts are NL descriptions of code snippets\nprone to various security vulnerabilities listed in MITRE's Top 25 Common\nWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a\nsecure implementation example to facilitate comparative evaluations against\ncode produced by LLMs. As a practical application, we show how LLMSecEval can\nbe used for evaluating the security of snippets automatically generated from NL\ndescriptions.\n","authors":["Catherine Tony","Markus Mutas","Nicolás E. Díaz Ferreyra","Riccardo Scandariato"],"pdf_url":"https://arxiv.org/pdf/2303.09384v1.pdf","comment":"Accepted at MSR '23 Data and Tool Showcase Track"},{"id":"http://arxiv.org/abs/2303.08157v2","updated":"2023-03-16T10:56:37Z","published":"2023-03-14T18:14:40Z","title":"Graph Neural Network Surrogates of Fair Graph Filtering","summary":"  Graph filters that transform prior node values to posterior scores via edge\npropagation often support graph mining tasks affecting humans, such as\nrecommendation and ranking. Thus, it is important to make them fair in terms of\nsatisfying statistical parity constraints between groups of nodes (e.g.,\ndistribute score mass between genders proportionally to their representation).\nTo achieve this while minimally perturbing the original posteriors, we\nintroduce a filter-aware universal approximation framework for posterior\nobjectives. This defines appropriate graph neural networks trained at runtime\nto be similar to filters but also locally optimize a large class of objectives,\nincluding fairness-aware ones. Experiments on a collection of 8 filters and 5\ngraphs show that our approach performs equally well or better than alternatives\nin meeting parity constraints while preserving the AUC of score-based community\nmember recommendation and creating minimal utility loss in prior diffusion.\n","authors":["Emmanouil Krasanakis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2303.08157v2.pdf","comment":"40 pages, 5 figures, 5 papers"},{"id":"http://arxiv.org/abs/2303.04587v2","updated":"2023-03-16T06:03:08Z","published":"2023-03-08T13:59:41Z","title":"A Prompt Log Analysis of Text-to-Image Generation Systems","summary":"  Recent developments in large language models (LLM) and generative AI have\nunleashed the astonishing capabilities of text-to-image generation systems to\nsynthesize high-quality images that are faithful to a given reference text,\nknown as a \"prompt\". These systems have immediately received lots of attention\nfrom researchers, creators, and common users. Despite the plenty of efforts to\nimprove the generative models, there is limited work on understanding the\ninformation needs of the users of these systems at scale. We conduct the first\ncomprehensive analysis of large-scale prompt logs collected from multiple\ntext-to-image generation systems. Our work is analogous to analyzing the query\nlogs of Web search engines, a line of work that has made critical contributions\nto the glory of the Web search industry and research. Compared with Web search\nqueries, text-to-image prompts are significantly longer, often organized into\nspecial structures that consist of the subject, form, and intent of the\ngeneration tasks and present unique categories of information needs. Users make\nmore edits within creation sessions, which present remarkable exploratory\npatterns. There is also a considerable gap between the user-input prompts and\nthe captions of the images included in the open training data of the generative\nmodels. Our findings provide concrete implications on how to improve\ntext-to-image generation systems for creation purposes.\n","authors":["Yutong Xie","Zhaoying Pan","Jinge Ma","Luo Jie","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2303.04587v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.09555v1","updated":"2023-03-16T17:59:50Z","published":"2023-03-16T17:59:50Z","title":"SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse\n  Environments","summary":"  While significant research progress has been made in robot learning for\ncontrol, unique challenges arise when simultaneously co-optimizing morphology.\nExisting work has typically been tailored for particular environments or\nrepresentations. In order to more fully understand inherent design and\nperformance tradeoffs and accelerate the development of new breeds of soft\nrobots, a comprehensive virtual platform with well-established tasks,\nenvironments, and evaluation metrics is needed. In this work, we introduce\nSoftZoo, a soft robot co-design platform for locomotion in diverse\nenvironments. SoftZoo supports an extensive, naturally-inspired material set,\nincluding the ability to simulate environments such as flat ground, desert,\nwetland, clay, ice, snow, shallow water, and ocean. Further, it provides a\nvariety of tasks relevant for soft robotics, including fast locomotion, agile\nturning, and path following, as well as differentiable design representations\nfor morphology and control. Combined, these elements form a feature-rich\nplatform for analysis and development of soft robot co-design algorithms. We\nbenchmark prevalent representations and co-design algorithms, and shed light on\n1) the interplay between environment, morphology, and behavior; 2) the\nimportance of design space representations; 3) the ambiguity in muscle\nformation and controller synthesis; and 4) the value of differentiable physics.\nWe envision that SoftZoo will serve as a standard platform and template an\napproach toward the development of novel representations and algorithms for\nco-designing soft robots' behavioral and morphological intelligence.\n","authors":["Tsun-Hsuan Wang","Pingchuan Ma","Andrew Everett Spielberg","Zhou Xian","Hao Zhang","Joshua B. Tenenbaum","Daniela Rus","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.09555v1.pdf","comment":"ICLR 2023. Project page:\n  https://sites.google.com/view/softzoo-iclr-2023"},{"id":"http://arxiv.org/abs/2303.09552v1","updated":"2023-03-16T17:59:13Z","published":"2023-03-16T17:59:13Z","title":"Dataflow graphs as complete causal graphs","summary":"  Component-based development is one of the core principles behind modern\nsoftware engineering practices. Understanding of causal relationships between\ncomponents of a software system can yield significant benefits to developers.\nYet modern software design approaches make it difficult to track and discover\nsuch relationships at system scale, which leads to growing intellectual debt.\nIn this paper we consider an alternative approach to software design,\nflow-based programming (FBP), and draw the attention of the community to the\nconnection between dataflow graphs produced by FBP and structural causal\nmodels. With expository examples we show how this connection can be leveraged\nto improve day-to-day tasks in software projects, including fault localisation,\nbusiness analysis and experimentation.\n","authors":["Andrei Paleyes","Siyuan Guo","Bernhard Schölkopf","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2303.09552v1.pdf","comment":"Accepted to 2nd International Conference on AI Engineering - Software\n  Engineering for AI (CAIN 23)"},{"id":"http://arxiv.org/abs/2303.09545v1","updated":"2023-03-16T17:56:02Z","published":"2023-03-16T17:56:02Z","title":"WebSHAP: Towards Explaining Any Machine Learning Models Anywhere","summary":"  As machine learning (ML) is increasingly integrated into our everyday Web\nexperience, there is a call for transparent and explainable web-based ML.\nHowever, existing explainability techniques often require dedicated backend\nservers, which limit their usefulness as the Web community moves toward\nin-browser ML for lower latency and greater privacy. To address the pressing\nneed for a client-side explainability solution, we present WebSHAP, the first\nin-browser tool that adapts the state-of-the-art model-agnostic explainability\ntechnique SHAP to the Web environment. Our open-source tool is developed with\nmodern Web technologies such as WebGL that leverage client-side hardware\ncapabilities and make it easy to integrate into existing Web ML applications.\nWe demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval\ndecisions to loan applicants. Reflecting on our work, we discuss the\nopportunities and challenges for future research on transparent Web ML. WebSHAP\nis available at https://github.com/poloclub/webshap.\n","authors":["Zijie J. Wang","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2303.09545v1.pdf","comment":"5 pages, 4 figures. Accepted at the ACM Web Conference 2023 (WWW\n  2023). For a live demo, visit https://poloclub.github.io/webshap/. Code is\n  open-source at https://github.com/poloclub/webshap"},{"id":"http://arxiv.org/abs/2210.07228v2","updated":"2023-03-16T17:54:53Z","published":"2022-10-13T17:55:51Z","title":"Language Model Decoding as Likelihood-Utility Alignment","summary":"  A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of a decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios, and their findings do not generalize\nacross tasks. We argue that the misalignment between the model's likelihood and\nthe task-specific notion of utility is the key factor to understanding the\neffectiveness of decoding algorithms. To structure the discussion, we introduce\na taxonomy of misalignment mitigation strategies (MMSs), providing a unifying\nview of decoding as a tool for alignment. The MMS taxonomy groups decoding\nalgorithms based on their implicit assumptions about likelihood--utility\nmisalignment, yielding general statements about their applicability across\ntasks. Specifically, by analyzing the correlation between the likelihood and\nthe utility of predictions across a diverse set of tasks, we provide empirical\nevidence supporting the proposed taxonomy and a set of principles to structure\nreasoning when choosing a decoding algorithm. Crucially, our analysis is the\nfirst to relate likelihood-based decoding algorithms with algorithms that rely\non external information, such as value-guided methods and prompting, and covers\nthe most diverse set of tasks to date. Code, data, and models are available at\nhttps://github.com/epfl-dlab/understanding-decoding.\n","authors":["Martin Josifoski","Maxime Peyrard","Frano Rajic","Jiheng Wei","Debjit Paul","Valentin Hartmann","Barun Patra","Vishrav Chaudhary","Emre Kıcıman","Boi Faltings","Robert West"],"pdf_url":"https://arxiv.org/pdf/2210.07228v2.pdf","comment":"Accepted at EACL (Findings) 2023"},{"id":"http://arxiv.org/abs/2303.09540v1","updated":"2023-03-16T17:53:24Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09536v1","updated":"2023-03-16T17:52:45Z","published":"2023-03-16T17:52:45Z","title":"Deep Metric Learning for Unsupervised Remote Sensing Change Detection","summary":"  Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from\nMulti-Temporal Remote Sensing Images (MT-RSIs), which aids in various RS\napplications such as land cover, land use, human development analysis, and\ndisaster response. The performance of existing RS-CD methods is attributed to\ntraining on large annotated datasets. Furthermore, most of these models are\nless transferable in the sense that the trained model often performs very\npoorly when there is a domain gap between training and test datasets. This\npaper proposes an unsupervised CD method based on deep metric learning that can\ndeal with both of these issues. Given an MT-RSI, the proposed method generates\ncorresponding change probability map by iteratively optimizing an unsupervised\nCD loss without training it on a large dataset. Our unsupervised CD method\nconsists of two interconnected deep networks, namely Deep-Change Probability\nGenerator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to\npredict change and no change probability maps for a given MT-RSI, while D-FE is\nused to extract deep features of MT-RSI that will be further used in the\nproposed unsupervised CD loss. We use transfer learning capability to\ninitialize the parameters of D-FE. We iteratively optimize the parameters of\nD-CPG and D-FE for a given MT-RSI by minimizing the proposed unsupervised\n``similarity-dissimilarity loss''. This loss is motivated by the principle of\nmetric learning where we simultaneously maximize the distance between change\npair-wise pixels while minimizing the distance between no-change pair-wise\npixels in bi-temporal image domain and their deep feature domain. The\nexperiments conducted on three CD datasets show that our unsupervised CD method\nachieves significant improvements over the state-of-the-art supervised and\nunsupervised CD methods. Code available at https://github.com/wgcban/Metric-CD\n","authors":["Wele Gedara Chaminda Bandara","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.09536v1.pdf","comment":"Code available at https://github.com/wgcban/Metric-CD"},{"id":"http://arxiv.org/abs/2303.09532v1","updated":"2023-03-16T17:48:39Z","published":"2023-03-16T17:48:39Z","title":"Variational Principles for Mirror Descent and Mirror Langevin Dynamics","summary":"  Mirror descent, introduced by Nemirovski and Yudin in the 1970s, is a\nprimal-dual convex optimization method that can be tailored to the geometry of\nthe optimization problem at hand through the choice of a strongly convex\npotential function. It arises as a basic primitive in a variety of\napplications, including large-scale optimization, machine learning, and\ncontrol. This paper proposes a variational formulation of mirror descent and of\nits stochastic variant, mirror Langevin dynamics. The main idea, inspired by\nthe classic work of Brezis and Ekeland on variational principles for gradient\nflows, is to show that mirror descent emerges as a closed-loop solution for a\ncertain optimal control problem, and the Bellman value function is given by the\nBregman divergence between the initial condition and the global minimizer of\nthe objective function.\n","authors":["Belinda Tzen","Anant Raj","Maxim Raginsky","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2303.09532v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2303.09531v1","updated":"2023-03-16T17:47:55Z","published":"2023-03-16T17:47:55Z","title":"GLASU: A Communication-Efficient Algorithm for Federated Learning with\n  Vertically Distributed Graph Data","summary":"  Vertical federated learning (VFL) is a distributed learning paradigm, where\ncomputing clients collectively train a model based on the partial features of\nthe same set of samples they possess. Current research on VFL focuses on the\ncase when samples are independent, but it rarely addresses an emerging scenario\nwhen samples are interrelated through a graph. For graph-structured data, graph\nneural networks (GNNs) are competitive machine learning models, but a naive\nimplementation in the VFL setting causes a significant communication overhead.\nMoreover, the analysis of the training is faced with a challenge caused by the\nbiased stochastic gradients. In this paper, we propose a model splitting method\nthat splits a backbone GNN across the clients and the server and a\ncommunication-efficient algorithm, GLASU, to train such a model. GLASU adopts\nlazy aggregation and stale updates to skip aggregation when evaluating the\nmodel and skip feature exchanges during training, greatly reducing\ncommunication. We offer a theoretical analysis and conduct extensive numerical\nexperiments on real-world datasets, showing that the proposed algorithm\neffectively trains a GNN model, whose performance matches that of the backbone\nGNN when trained in a centralized manner.\n","authors":["Xinwei Zhang","Mingyi Hong","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09530v1","updated":"2023-03-16T17:46:32Z","published":"2023-03-16T17:46:32Z","title":"Tackling Clutter in Radar Data -- Label Generation and Detection Using\n  PointNet++","summary":"  Radar sensors employed for environment perception, e.g. in autonomous\nvehicles, output a lot of unwanted clutter. These points, for which no\ncorresponding real objects exist, are a major source of errors in following\nprocessing steps like object detection or tracking. We therefore present two\nnovel neural network setups for identifying clutter. The input data, network\narchitectures and training configuration are adjusted specifically for this\ntask. Special attention is paid to the downsampling of point clouds composed of\nmultiple sensor scans. In an extensive evaluation, the new setups display\nsubstantially better performance than existing approaches. Because there is no\nsuitable public data set in which clutter is annotated, we design a method to\nautomatically generate the respective labels. By applying it to existing data\nwith object annotations and releasing its code, we effectively create the first\nfreely available radar clutter data set representing real-world driving\nscenarios. Code and instructions are accessible at\nwww.github.com/kopp-j/clutter-ds.\n","authors":["Johannes Kopp","Dominik Kellner","Aldi Piroli","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2303.09530v1.pdf","comment":"To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), London, UK, 2023"},{"id":"http://arxiv.org/abs/2303.09528v1","updated":"2023-03-16T17:45:38Z","published":"2023-03-16T17:45:38Z","title":"Reinforcement Learning for Omega-Regular Specifications on\n  Continuous-Time MDP","summary":"  Continuous-time Markov decision processes (CTMDPs) are canonical models to\nexpress sequential decision-making under dense-time and stochastic\nenvironments. When the stochastic evolution of the environment is only\navailable via sampling, model-free reinforcement learning (RL) is the\nalgorithm-of-choice to compute optimal decision sequence. RL, on the other\nhand, requires the learning objective to be encoded as scalar reward signals.\nSince doing such translations manually is both tedious and error-prone, a\nnumber of techniques have been proposed to translate high-level objectives\n(expressed in logic or automata formalism) to scalar rewards for discrete-time\nMarkov decision processes (MDPs). Unfortunately, no automatic translation\nexists for CTMDPs.\n  We consider CTMDP environments against the learning objectives expressed as\nomega-regular languages. Omega-regular languages generalize regular languages\nto infinite-horizon specifications and can express properties given in popular\nlinear-time logic LTL. To accommodate the dense-time nature of CTMDPs, we\nconsider two different semantics of omega-regular objectives: 1) satisfaction\nsemantics where the goal of the learner is to maximize the probability of\nspending positive time in the good states, and 2) expectation semantics where\nthe goal of the learner is to optimize the long-run expected average time spent\nin the ``good states\" of the automaton. We present an approach enabling correct\ntranslation to scalar reward signals that can be readily used by off-the-shelf\nRL algorithms for CTMDPs. We demonstrate the effectiveness of the proposed\nalgorithms by evaluating it on some popular CTMDP benchmarks with omega-regular\nobjectives.\n","authors":["Amin Falah","Shibashis Guha","Ashutosh Trivedi"],"pdf_url":"https://arxiv.org/pdf/2303.09528v1.pdf","comment":"Full version of paper accepted to ICAPS 2023"},{"id":"http://arxiv.org/abs/2303.09527v1","updated":"2023-03-16T17:44:39Z","published":"2023-03-16T17:44:39Z","title":"Fairness-aware Differentially Private Collaborative Filtering","summary":"  Recently, there has been an increasing adoption of differential privacy\nguided algorithms for privacy-preserving machine learning tasks. However, the\nuse of such algorithms comes with trade-offs in terms of algorithmic fairness,\nwhich has been widely acknowledged. Specifically, we have empirically observed\nthat the classical collaborative filtering method, trained by differentially\nprivate stochastic gradient descent (DP-SGD), results in a disparate impact on\nuser groups with respect to different user engagement levels. This, in turn,\ncauses the original unfair model to become even more biased against inactive\nusers. To address the above issues, we propose \\textbf{DP-Fair}, a two-stage\nframework for collaborative filtering based algorithms. Specifically, it\ncombines differential privacy mechanisms with fairness constraints to protect\nuser privacy while ensuring fair recommendations. The experimental results,\nbased on Amazon datasets, and user history logs collected from Etsy, one of the\nlargest e-commerce platforms, demonstrate that our proposed method exhibits\nsuperior performance in terms of both overall accuracy and user group fairness\non both shallow and deep recommendation models compared to vanilla DP-SGD.\n","authors":["Zhenhuan Yang","Yingqiang Ge","Congzhe Su","Dingxian Wang","Xiaoting Zhao","Yiming Ying"],"pdf_url":"https://arxiv.org/pdf/2303.09527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09522v1","updated":"2023-03-16T17:38:15Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09519v1","updated":"2023-03-16T17:37:22Z","published":"2023-03-16T17:37:22Z","title":"PyVBMC: Efficient Bayesian inference in Python","summary":"  PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo\n(VBMC) algorithm for posterior and model inference for black-box computational\nmodels (Acerbi, 2018, 2020). VBMC is an approximate inference method designed\nfor efficient parameter estimation and model assessment when model evaluations\nare mildly-to-very expensive (e.g., a second or more) and/or noisy.\nSpecifically, VBMC computes:\n  - a flexible (non-Gaussian) approximate posterior distribution of the model\nparameters, from which statistics and posterior samples can be easily\nextracted;\n  - an approximation of the model evidence or marginal likelihood, a metric\nused for Bayesian model selection.\n  PyVBMC can be applied to any computational or statistical model with up to\nroughly 10-15 continuous parameters, with the only requirement that the user\ncan provide a Python function that computes the target log likelihood of the\nmodel, or an approximation thereof (e.g., an estimate of the likelihood\nobtained via simulation or Monte Carlo methods). PyVBMC is particularly\neffective when the model takes more than about a second per evaluation, with\ndramatic speed-ups of 1-2 orders of magnitude when compared to traditional\napproximate inference methods.\n  Extensive benchmarks on both artificial test problems and a large number of\nreal models from the computational sciences, particularly computational and\ncognitive neuroscience, show that VBMC generally - and often vastly -\noutperforms alternative methods for sample-efficient Bayesian inference, and is\napplicable to both exact and simulator-based models (Acerbi, 2018, 2019, 2020).\nPyVBMC brings this state-of-the-art inference algorithm to Python, along with\nan easy-to-use Pythonic interface for running the algorithm and manipulating\nand visualizing its results.\n","authors":["Bobby Huggins","Chengkun Li","Marlon Tobaben","Mikko J. Aarnos","Luigi Acerbi"],"pdf_url":"https://arxiv.org/pdf/2303.09519v1.pdf","comment":"6 pages, 1 figure. Submitted to The Journal of Open Source Software.\n  Documentation is available at https://acerbilab.github.io/pyvbmc and source\n  code is available at https://github.com/acerbilab/pyvbmc"},{"id":"http://arxiv.org/abs/2212.11146v2","updated":"2023-03-16T17:17:37Z","published":"2022-12-13T12:42:12Z","title":"The Challenges of HTR Model Training: Feedback from the Project Donner\n  le gout de l'archive a l'ere numerique","summary":"  The arrival of handwriting recognition technologies offers new possibilities\nfor research in heritage studies. However, it is now necessary to reflect on\nthe experiences and the practices developed by research teams. Our use of the\nTranskribus platform since 2018 has led us to search for the most significant\nways to improve the performance of our handwritten text recognition (HTR)\nmodels which are made to transcribe French handwriting dating from the 17th\ncentury. This article therefore reports on the impacts of creating transcribing\nprotocols, using the language model at full scale and determining the best way\nto use base models in order to help increase the performance of HTR models.\nCombining all of these elements can indeed increase the performance of a single\nmodel by more than 20% (reaching a Character Error Rate below 5%). This article\nalso discusses some challenges regarding the collaborative nature of HTR\nplatforms such as Transkribus and the way researchers can share their data\ngenerated in the process of creating or training handwritten text recognition\nmodels.\n","authors":["Couture Beatrice","Verret Farah","Gohier Maxime","Deslandres Dominique"],"pdf_url":"https://arxiv.org/pdf/2212.11146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.05818v5","updated":"2023-03-16T17:16:53Z","published":"2022-01-15T10:04:05Z","title":"Measuring Non-Probabilistic Uncertainty: A cognitive, logical and\n  computational assessment of known and unknown unknowns","summary":"  There are two reasons why uncertainty may not be adequately described by\nProbability Theory. The first one is due to unique or nearly-unique events,\nthat either never realized or occurred too seldom for frequencies to be\nreliably measured. The second one arises when one fears that something may\nhappen, that one is not even able to figure out, e.g., if one asks: \"Climate\nchange, financial crises, pandemic, war, what next?\"\n  In both cases, simple one-to-one cognitive maps between available\nalternatives and possible consequences eventually melt down. However, such\ndestructions reflect into the changing narratives of business executives,\nemployees and other stakeholders in specific, identifiable and differential\nways. In particular, texts such as consultants' reports or letters to\nshareholders can be analysed in order to detect the impact of both sorts of\nuncertainty onto the causal relations that normally guide decision-making.\n  We propose structural measures of cognitive maps as a means to measure\nnon-probabilistic uncertainty, eventually suggesting that automated text\nanalysis can greatly augment the possibilities offered by these techniques.\nProspective applications may concern actors ranging from statistical institutes\nto businesses as well as the general public.\n","authors":["Florian Ellsaesser","Guido Fioretti","Gail E. James"],"pdf_url":"https://arxiv.org/pdf/2201.05818v5.pdf","comment":"29 pages, 18 figures"},{"id":"http://arxiv.org/abs/2303.09497v1","updated":"2023-03-16T17:16:21Z","published":"2023-03-16T17:16:21Z","title":"Gate Recurrent Unit Network based on Hilbert-Schmidt Independence\n  Criterion for State-of-Health Estimation","summary":"  State-of-health (SOH) estimation is a key step in ensuring the safe and\nreliable operation of batteries. Due to issues such as varying data\ndistribution and sequence length in different cycles, most existing methods\nrequire health feature extraction technique, which can be time-consuming and\nlabor-intensive. GRU can well solve this problem due to the simple structure\nand superior performance, receiving widespread attentions. However, redundant\ninformation still exists within the network and impacts the accuracy of SOH\nestimation. To address this issue, a new GRU network based on Hilbert-Schmidt\nIndependence Criterion (GRU-HSIC) is proposed. First, a zero masking network is\nused to transform all battery data measured with varying lengths every cycle\ninto sequences of the same length, while still retaining information about the\noriginal data size in each cycle. Second, the Hilbert-Schmidt Independence\nCriterion (HSIC) bottleneck, which evolved from Information Bottleneck (IB)\ntheory, is extended to GRU to compress the information from hidden layers. To\nevaluate the proposed method, we conducted experiments on datasets from the\nCenter for Advanced Life Cycle Engineering (CALCE) of the University of\nMaryland and NASA Ames Prognostics Center of Excellence. Experimental results\ndemonstrate that our model achieves higher accuracy than other recurrent\nmodels.\n","authors":["Ziyue Huang","Lujuan Dang","Yuqing Xie","Wentao Ma","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09491v1","updated":"2023-03-16T17:10:39Z","published":"2023-03-16T17:10:39Z","title":"Challenges and Opportunities in Quantum Machine Learning","summary":"  At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.\n","authors":["M. Cerezo","Guillaume Verdon","Hsin-Yuan Huang","Lukasz Cincio","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2303.09491v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.09489v1","updated":"2023-03-16T17:08:21Z","published":"2023-03-16T17:08:21Z","title":"Effectively Modeling Time Series with Simple Discrete State Spaces","summary":"  Time series modeling is a well-established problem, which often requires that\nmethods (1) expressively represent complicated dependencies, (2) forecast long\nhorizons, and (3) efficiently train over long sequences. State-space models\n(SSMs) are classical models for time series, and prior works combine SSMs with\ndeep learning layers for efficient sequence modeling. However, we find\nfundamental limitations with these prior approaches, proving their SSM\nrepresentations cannot express autoregressive time series processes. We thus\nintroduce SpaceTime, a new state-space time series architecture that improves\nall three criteria. For expressivity, we propose a new SSM parameterization\nbased on the companion matrix -- a canonical representation for discrete-time\nprocesses -- which enables SpaceTime's SSM layers to learn desirable\nautoregressive processes. For long horizon forecasting, we introduce a\n\"closed-loop\" variation of the companion SSM, which enables SpaceTime to\npredict many future time-steps by generating its own layer-wise inputs. For\nefficient training and inference, we introduce an algorithm that reduces the\nmemory and compute of a forward pass with the companion matrix. With sequence\nlength $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$\nna\\\"ively to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to\nstate-of-the-art results on extensive and diverse benchmarks, with best or\nsecond-best AUROC on 6 / 7 ECG and speech time series classification, and best\nMSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1)\nfits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more\naccurately on longer horizons than prior state-of-the-art, and (3) speeds up\ntraining on real-world ETTh1 data by 73% and 80% relative wall-clock time over\nTransformers and LSTMs.\n","authors":["Michael Zhang","Khaled K. Saab","Michael Poli","Tri Dao","Karan Goel","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2303.09489v1.pdf","comment":"45 pages, 8 figures, 20 tables, ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09484v1","updated":"2023-03-16T17:00:45Z","published":"2023-03-16T17:00:45Z","title":"A Novel Autoencoders-LSTM Model for Stroke Outcome Prediction using\n  Multimodal MRI Data","summary":"  Patient outcome prediction is critical in management of ischemic stroke. In\nthis paper, a novel machine learning model is proposed for stroke outcome\nprediction using multimodal Magnetic Resonance Imaging (MRI). The proposed\nmodel consists of two serial levels of Autoencoders (AEs), where different AEs\nat level 1 are used for learning unimodal features from different MRI\nmodalities and a AE at level 2 is used to combine the unimodal features into\ncompressed multimodal features. The sequences of multimodal features of a given\npatient are then used by an LSTM network for predicting outcome score. The\nproposed AE2-LSTM model is proved to be an effective approach for better\naddressing the multimodality and volumetric nature of MRI data. Experimental\nresults show that the proposed AE2-LSTM outperforms the existing state-of-the\nart models by achieving highest AUC=0.71 and lowest MAE=0.34.\n","authors":["Nima Hatami","Laura Mechtouff","David Rousseau","Tae-Hee Cho","Omer Eker","Yves Berthezene","Carole Frindel"],"pdf_url":"https://arxiv.org/pdf/2303.09484v1.pdf","comment":"The IEEE International Symposium on Biomedical Imaging (ISBI). arXiv\n  admin note: text overlap with arXiv:2205.05545"},{"id":"http://arxiv.org/abs/2303.09483v1","updated":"2023-03-16T17:00:42Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09478v1","updated":"2023-03-16T16:55:26Z","published":"2023-03-16T16:55:26Z","title":"Arbitrary Order Meta-Learning with Simple Population-Based Evolution","summary":"  Meta-learning, the notion of learning to learn, enables learning systems to\nquickly and flexibly solve new tasks. This usually involves defining a set of\nouter-loop meta-parameters that are then used to update a set of inner-loop\nparameters. Most meta-learning approaches use complicated and computationally\nexpensive bi-level optimisation schemes to update these meta-parameters.\nIdeally, systems should perform multiple orders of meta-learning, i.e. to learn\nto learn to learn and so on, to accelerate their own learning. Unfortunately,\nstandard meta-learning techniques are often inappropriate for these\nhigher-order meta-parameters because the meta-optimisation procedure becomes\ntoo complicated or unstable. Inspired by the higher-order meta-learning we\nobserve in real-world evolution, we show that using simple population-based\nevolution implicitly optimises for arbitrarily-high order meta-parameters.\nFirst, we theoretically prove and empirically show that population-based\nevolution implicitly optimises meta-parameters of arbitrarily-high order in a\nsimple setting. We then introduce a minimal self-referential parameterisation,\nwhich in principle enables arbitrary-order meta-learning. Finally, we show that\nhigher-order meta-learning improves performance on time series forecasting\ntasks.\n","authors":["Chris Lu","Sebastian Towers","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2303.09478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1910.09083v8","updated":"2023-03-16T16:54:32Z","published":"2019-10-20T23:47:33Z","title":"Spectral CUSUM for Online Network Structure Change Detection","summary":"  Detecting abrupt changes in the community structure of a network from noisy\nobservations is a fundamental problem in statistics and machine learning. This\npaper presents an online change detection algorithm called Spectral-CUSUM to\ndetect unknown network structure changes through a generalized likelihood ratio\nstatistic. We characterize the average run length (ARL) and the expected\ndetection delay (EDD) of the Spectral-CUSUM procedure and prove its asymptotic\noptimality. Finally, we demonstrate the good performance of the Spectral-CUSUM\nprocedure and compare it with several baseline methods using simulations and\nreal data examples on seismic event detection using sensor network data.\n","authors":["Minghe Zhang","Liyan Xie","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/1910.09083v8.pdf","comment":"Accepted by IEEE Transactions on Information Theory"},{"id":"http://arxiv.org/abs/2303.09474v1","updated":"2023-03-16T16:50:46Z","published":"2023-03-16T16:50:46Z","title":"Gradient flow on extensive-rank positive semi-definite matrix denoising","summary":"  In this work, we present a new approach to analyze the gradient flow for a\npositive semi-definite matrix denoising problem in an extensive-rank and\nhigh-dimensional regime. We use recent linear pencil techniques of random\nmatrix theory to derive fixed point equations which track the complete time\nevolution of the matrix-mean-square-error of the problem. The predictions of\nthe resulting fixed point equations are validated by numerical experiments. In\nthis short note we briefly illustrate a few predictions of our formalism by way\nof examples, and in particular we uncover continuous phase transitions in the\nextensive-rank and high-dimensional regime, which connect to the classical\nphase transitions of the low-rank problem in the appropriate limit. The\nformalism has much wider applicability than shown in this communication.\n","authors":["Antoine Bodin","Nicolas Macris"],"pdf_url":"https://arxiv.org/pdf/2303.09474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09470v1","updated":"2023-03-16T16:43:24Z","published":"2023-03-16T16:43:24Z","title":"Combining Distance to Class Centroids and Outlier Discounting for\n  Improved Learning with Noisy Labels","summary":"  In this paper, we propose a new approach for addressing the challenge of\ntraining machine learning models in the presence of noisy labels. By combining\na clever usage of distance to class centroids in the items' latent space with a\ndiscounting strategy to reduce the importance of samples far away from all the\nclass centroids (i.e., outliers), our method effectively addresses the issue of\nnoisy labels. Our approach is based on the idea that samples farther away from\ntheir respective class centroid in the early stages of training are more likely\nto be noisy. We demonstrate the effectiveness of our method through extensive\nexperiments on several popular benchmark datasets. Our results show that our\napproach outperforms the state-of-the-art in this area, achieving significant\nimprovements in classification accuracy when the dataset contains noisy labels.\n","authors":["Farooq Ahmad Wani","Maria Sofia Bucarelli","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2303.09470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02408v2","updated":"2023-03-16T16:39:59Z","published":"2022-11-04T12:36:36Z","title":"Rickrolling the Artist: Injecting Backdoors into Text Encoders for\n  Text-to-Image Synthesis","summary":"  While text-to-image synthesis currently enjoys great popularity among\nresearchers and the general public, the security of these models has been\nneglected so far. Many text-guided image generation models rely on pre-trained\ntext encoders from external sources, and their users trust that the retrieved\nmodels will behave as promised. Unfortunately, this might not be the case. We\nintroduce backdoor attacks against text-guided generative models and\ndemonstrate that their text encoders pose a major tampering risk. Our attacks\nonly slightly alter an encoder so that no suspicious model behavior is apparent\nfor image generations with clean prompts. By then inserting a single character\ntrigger into the prompt, e.g., a non-Latin character or emoji, the adversary\ncan trigger the model to either generate images with pre-defined attributes or\nimages following a hidden, potentially malicious description. We empirically\ndemonstrate the high effectiveness of our attacks on Stable Diffusion and\nhighlight that the injection process of a single backdoor takes less than two\nminutes. Besides phrasing our approach solely as an attack, it can also force\nan encoder to forget phrases related to certain concepts, such as nudity or\nviolence, and help to make image generation safer.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2211.02408v2.pdf","comment":"30 pages, 20 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.09468v1","updated":"2023-03-16T16:39:00Z","published":"2023-03-16T16:39:00Z","title":"On the Existence of a Complexity in Fixed Budget Bandit Identification","summary":"  In fixed budget bandit identification, an algorithm sequentially observes\nsamples from several distributions up to a given final time. It then answers a\nquery about the set of distributions. A good algorithm will have a small\nprobability of error. While that probability decreases exponentially with the\nfinal time, the best attainable rate is not known precisely for most\nidentification tasks. We show that if a fixed budget task admits a complexity,\ndefined as a lower bound on the probability of error which is attained by a\nsingle algorithm on all bandit problems, then that complexity is determined by\nthe best non-adaptive sampling procedure for that problem. We show that there\nis no such complexity for several fixed budget identification tasks including\nBernoulli best arm identification with two arms: there is no single algorithm\nthat attains everywhere the best possible rate.\n","authors":["Rémy Degenne"],"pdf_url":"https://arxiv.org/pdf/2303.09468v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2209.13860v2","updated":"2023-03-16T16:35:26Z","published":"2022-09-28T06:31:19Z","title":"Natural Language Processing Methods to Identify Oncology Patients at\n  High Risk for Acute Care with Clinical Notes","summary":"  Clinical notes are an essential component of a health record. This paper\nevaluates how natural language processing (NLP) can be used to identify the\nrisk of acute care use (ACU) in oncology patients, once chemotherapy starts.\nRisk prediction using structured health data (SHD) is now standard, but\npredictions using free-text formats are complex. This paper explores the use of\nfree-text notes for the prediction of ACU instead of SHD. Deep Learning models\nwere compared to manually engineered language features. Results show that SHD\nmodels minimally outperform NLP models; an l1-penalised logistic regression\nwith SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same\nmodel with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a\ntransformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows\nhow language models can be used in clinical applications and underlines how\nrisk bias is different for diverse patient groups, even using only free-text\ndata.\n","authors":["Claudio Fanconi","Marieke van Buchem","Tina Hernandez-Boussard"],"pdf_url":"https://arxiv.org/pdf/2209.13860v2.pdf","comment":"11 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.09447v1","updated":"2023-03-16T16:23:13Z","published":"2023-03-16T16:23:13Z","title":"Steering Prototype with Prompt-tuning for Rehearsal-free Continual\n  Learning","summary":"  Prototype, as a representation of class embeddings, has been explored to\nreduce memory footprint or mitigate forgetting for continual learning\nscenarios. However, prototype-based methods still suffer from abrupt\nperformance deterioration due to semantic drift and prototype interference. In\nthis study, we propose Contrastive Prototypical Prompt (CPP) and show that\ntask-specific prompt-tuning, when optimized over a contrastive learning\nobjective, can effectively address both obstacles and significantly improve the\npotency of prototypes. Our experiments demonstrate that CPP excels in four\nchallenging class-incremental learning benchmarks, resulting in 4% to 6%\nabsolute improvements over state-of-the-art methods. Moreover, CPP does not\nrequire a rehearsal buffer and it largely bridges the performance gap between\ncontinual learning and offline joint-learning, showcasing a promising design\nscheme for continual learning systems under a Transformer architecture.\n","authors":["Zhuowei Li","Long Zhao","Zizhao Zhang","Han Zhang","Di Liu","Ting Liu","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2303.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.14835v4","updated":"2023-03-16T16:22:13Z","published":"2021-05-31T09:49:14Z","title":"Towards Lower Bounds on the Depth of ReLU Neural Networks","summary":"  We contribute to a better understanding of the class of functions that can be\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning any function. In particular, we investigate whether the class of\nexactly representable functions strictly increases by adding more layers (with\nno restrictions on size). As a by-product of our investigations, we settle an\nold conjecture about piecewise linear functions by Wang and Sun (2005) in the\naffirmative. We also present upper bounds on the sizes of neural networks\nrequired to represent functions with logarithmic depth.\n","authors":["Christoph Hertrich","Amitabh Basu","Marco Di Summa","Martin Skutella"],"pdf_url":"https://arxiv.org/pdf/2105.14835v4.pdf","comment":"Authors' accepted manuscript for SIAM Journal on Discrete\n  Mathematics. A preliminary conference version appeared at NeurIPS 2021"},{"id":"http://arxiv.org/abs/2303.09440v1","updated":"2023-03-16T16:15:42Z","published":"2023-03-16T16:15:42Z","title":"Enhanced detection of the presence and severity of COVID-19 from CT\n  scans using lung segmentation","summary":"  Improving automated analysis of medical imaging will provide clinicians more\noptions in providing care for patients. The 2023 AI-enabled Medical Image\nAnalysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides\nan opportunity to test and refine machine learning methods for detecting the\npresence and severity of COVID-19 in patients from CT scans. This paper\npresents version 2 of Cov3d, a deep learning model submitted in the 2022\ncompetition. The model has been improved through a preprocessing step which\nsegments the lungs in the CT scan and crops the input to this region. It\nresults in a validation macro F1 score for predicting the presence of COVID-19\nin the CT scans at 92.2% which is significantly above the baseline of 74%. It\ngives a macro F1 score for predicting the severity of COVID-19 on the\nvalidation set for task 2 as 67% which is above the baseline of 38%.\n","authors":["Robert Turnbull"],"pdf_url":"https://arxiv.org/pdf/2303.09440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00085v2","updated":"2023-03-16T16:12:15Z","published":"2023-02-28T21:04:05Z","title":"AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for\n  Robotic Rehabilitation","summary":"  In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed\n(AAN) controller that utilizes reinforcement learning to supply adaptive\nassistance during a robot assisted handwriting rehabilitation task. Unlike\nprevious AAN controllers, our method does not rely on patient specific\ncontroller parameters or physical models. We propose the use of a virtual\npatient model to generalize AR3n across multiple subjects. The system modulates\nrobotic assistance in realtime based on a subject's tracking error, while\nminimizing the amount of robotic assistance. The controller is experimentally\nvalidated through a set of simulations and human subject experiments. Finally,\na comparative study with a traditional rule-based controller is conducted to\nanalyze differences in assistance mechanisms of the two controllers.\n","authors":["Shrey Pareek","Harris NIsar","Thenkurussi Kesavadas"],"pdf_url":"https://arxiv.org/pdf/2303.00085v2.pdf","comment":"8 pages, 9 figures, conditionally accepted to IEEE RA-M"},{"id":"http://arxiv.org/abs/2303.02475v2","updated":"2023-03-16T15:47:32Z","published":"2023-03-04T18:24:45Z","title":"Synthetic ECG Signal Generation using Probabilistic Diffusion Models","summary":"  Deep learning image processing models have had remarkable success in recent\nyears in generating high quality images. Particularly, the Improved Denoising\nDiffusion Probabilistic Models (DDPM) have shown superiority in image quality\nto the state-of-the-art generative models, which motivated us to investigate\nits capability in generation of the synthetic electrocardiogram (ECG) signals.\nIn this work, synthetic ECG signals are generated by the Improved DDPM and by\nthe Wasserstein GAN with Gradient Penalty (WGAN-GP) models and then compared.\nTo this end, we devise a pipeline to utilize DDPM in its original $2D$ form.\nFirst, the $1D$ ECG time series data are embedded into the $2D$ space, for\nwhich we employed the Gramian Angular Summation/Difference Fields (GASF/GADF)\nas well as Markov Transition Fields (MTF) to generate three $2D$ matrices from\neach ECG time series that, which when put together, form a $3$-channel $2D$\ndatum. Then $2D$ DDPM is used to generate $2D$ $3$-channel synthetic ECG\nimages. The $1$D ECG signals are created by de-embedding the $2D$ generated\nimage files back into the $1D$ space. This work focuses on unconditional models\nand the generation of only \\emph{Normal} ECG signals, where the Normal class\nfrom the MIT BIH Arrhythmia dataset is used as the training phase. The\n\\emph{quality}, \\emph{distribution}, and the \\emph{authenticity} of the\ngenerated ECG signals by each model are compared. Our results show that, in the\nproposed pipeline, the WGAN-GP model is superior to DDPM by far in all the\nconsidered metrics consistently.\n","authors":["Edmond Adib","Amanda Fernandez","Fatemeh Afghah","John Jeff Prevost"],"pdf_url":"https://arxiv.org/pdf/2303.02475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02397v2","updated":"2023-03-16T15:44:19Z","published":"2022-11-04T12:06:14Z","title":"Analysing Diffusion-based Generative Approaches versus Discriminative\n  Approaches for Speech Restoration","summary":"  Diffusion-based generative models have had a high impact on the computer\nvision and speech processing communities these past years. Besides data\ngeneration tasks, they have also been employed for data restoration tasks like\nspeech enhancement and dereverberation. While discriminative models have\ntraditionally been argued to be more powerful e.g. for speech enhancement,\ngenerative diffusion approaches have recently been shown to narrow this\nperformance gap considerably. In this paper, we systematically compare the\nperformance of generative diffusion models and discriminative approaches on\ndifferent speech restoration tasks. For this, we extend our prior contributions\non diffusion-based speech enhancement in the complex time-frequency domain to\nthe task of bandwith extension. We then compare it to a discriminatively\ntrained neural network with the same network architecture on three restoration\ntasks, namely speech denoising, dereverberation and bandwidth extension. We\nobserve that the generative approach performs globally better than its\ndiscriminative counterpart on all tasks, with the strongest benefit for\nnon-additive distortion models, like in dereverberation and bandwidth\nextension. Code and audio examples can be found online at\nhttps://uhh.de/inf-sp-sgmsemultitask\n","authors":["Jean-Marie Lemercier","Julius Richter","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2211.02397v2.pdf","comment":"\\c{opyright} 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.09390v1","updated":"2023-03-16T15:24:29Z","published":"2023-03-16T15:24:29Z","title":"On the Interplay Between Misspecification and Sub-optimality Gap in\n  Linear Contextual Bandits","summary":"  We study linear contextual bandits in the misspecified setting, where the\nexpected reward function can be approximated by a linear function class up to a\nbounded misspecification level $\\zeta>0$. We propose an algorithm based on a\nnovel data selection scheme, which only selects the contextual vectors with\nlarge uncertainty for online regression. We show that, when the\nmisspecification level $\\zeta$ is dominated by $\\tilde O (\\Delta / \\sqrt{d})$\nwith $\\Delta$ being the minimal sub-optimality gap and $d$ being the dimension\nof the contextual vectors, our algorithm enjoys the same gap-dependent regret\nbound $\\tilde O (d^2/\\Delta)$ as in the well-specified setting up to\nlogarithmic factors. In addition, we show that an existing algorithm SupLinUCB\n(Chu et al., 2011) can also achieve a gap-dependent constant regret bound\nwithout the knowledge of sub-optimality gap $\\Delta$. Together with a lower\nbound adapted from Lattimore et al. (2020), our result suggests an interplay\nbetween misspecification level and the sub-optimality gap: (1) the linear\ncontextual bandit model is efficiently learnable when $\\zeta \\leq \\tilde\nO(\\Delta / \\sqrt{d})$; and (2) it is not efficiently learnable when $\\zeta \\geq\n\\tilde \\Omega({\\Delta} / {\\sqrt{d}})$. Experiments on both synthetic and\nreal-world datasets corroborate our theoretical results.\n","authors":["Weitong Zhang","Jiafan He","Zhiyuan Fan","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2303.09390v1.pdf","comment":"28 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2110.15771v4","updated":"2023-03-16T15:20:31Z","published":"2021-10-29T13:39:14Z","title":"Collaborative Pure Exploration in Kernel Bandit","summary":"  In this paper, we formulate a Collaborative Pure Exploration in Kernel Bandit\nproblem (CoPE-KB), which provides a novel model for multi-agent multi-task\ndecision making under limited communication and general reward functions, and\nis applicable to many online learning tasks, e.g., recommendation systems and\nnetwork scheduling. We consider two settings of CoPE-KB, i.e., Fixed-Confidence\n(FC) and Fixed-Budget (FB), and design two optimal algorithms CoopKernelFC (for\nFC) and CoopKernelFB (for FB). Our algorithms are equipped with innovative and\nefficient kernelized estimators to simultaneously achieve computation and\ncommunication efficiency. Matching upper and lower bounds under both the\nstatistical and communication metrics are established to demonstrate the\noptimality of our algorithms. The theoretical bounds successfully quantify the\ninfluences of task similarities on learning acceleration and only depend on the\neffective dimension of the kernelized feature space. Our analytical techniques,\nincluding data dimension decomposition, linear structured instance\ntransformation and (communication) round-speedup induction, are novel and\napplicable to other bandit problems. Empirical evaluations are provided to\nvalidate our theoretical results and demonstrate the performance superiority of\nour algorithms.\n","authors":["Yihan Du","Wei Chen","Yuko Kuroki","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2110.15771v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12872v2","updated":"2023-03-16T15:19:40Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v2.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2303.09384v1","updated":"2023-03-16T15:13:58Z","published":"2023-03-16T15:13:58Z","title":"LLMSecEval: A Dataset of Natural Language Prompts for Security\n  Evaluations","summary":"  Large Language Models (LLMs) like Codex are powerful tools for performing\ncode completion and code generation tasks as they are trained on billions of\nlines of code from publicly available sources. Moreover, these models are\ncapable of generating code snippets from Natural Language (NL) descriptions by\nlearning languages and programming practices from public GitHub repositories.\nAlthough LLMs promise an effortless NL-driven deployment of software\napplications, the security of the code they generate has not been extensively\ninvestigated nor documented. In this work, we present LLMSecEval, a dataset\ncontaining 150 NL prompts that can be leveraged for assessing the security\nperformance of such models. Such prompts are NL descriptions of code snippets\nprone to various security vulnerabilities listed in MITRE's Top 25 Common\nWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a\nsecure implementation example to facilitate comparative evaluations against\ncode produced by LLMs. As a practical application, we show how LLMSecEval can\nbe used for evaluating the security of snippets automatically generated from NL\ndescriptions.\n","authors":["Catherine Tony","Markus Mutas","Nicolás E. Díaz Ferreyra","Riccardo Scandariato"],"pdf_url":"https://arxiv.org/pdf/2303.09384v1.pdf","comment":"Accepted at MSR '23 Data and Tool Showcase Track"},{"id":"http://arxiv.org/abs/2303.09381v1","updated":"2023-03-16T15:11:17Z","published":"2023-03-16T15:11:17Z","title":"Multi-modal Differentiable Unsupervised Feature Selection","summary":"  Multi-modal high throughput biological data presents a great scientific\nopportunity and a significant computational challenge. In multi-modal\nmeasurements, every sample is observed simultaneously by two or more sets of\nsensors. In such settings, many observed variables in both modalities are often\nnuisance and do not carry information about the phenomenon of interest. Here,\nwe propose a multi-modal unsupervised feature selection framework: identifying\ninformative variables based on coupled high-dimensional measurements. Our\nmethod is designed to identify features associated with two types of latent\nlow-dimensional structures: (i) shared structures that govern the observations\nin both modalities and (ii) differential structures that appear in only one\nmodality. To that end, we propose two Laplacian-based scoring operators. We\nincorporate the scores with differentiable gates that mask nuisance features\nand enhance the accuracy of the structure captured by the graph Laplacian. The\nperformance of the new scheme is illustrated using synthetic and real datasets,\nincluding an extended biological application to single-cell multi-omics.\n","authors":["Junchen Yang","Ofir Lindenbaum","Yuval Kluger","Ariel Jaffe"],"pdf_url":"https://arxiv.org/pdf/2303.09381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09373v1","updated":"2023-03-16T15:01:50Z","published":"2023-03-16T15:01:50Z","title":"3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive\n  Segmentation of Heterogeneous Infant Brain MRI","summary":"  Robust segmentation of infant brain MRI across multiple ages, modalities, and\nsites remains challenging due to the intrinsic heterogeneity caused by\ndifferent MRI scanners, vendors, or acquisition sequences, as well as varying\nstages of neurodevelopment. To address this challenge, previous studies have\nexplored domain adaptation (DA) algorithms from various perspectives, including\nfeature alignment, entropy minimization, contrast synthesis (style transfer),\nand pseudo-labeling. This paper introduces a novel framework called MAPSeg\n(Masked Autoencoding and Pseudo-labelling Segmentation) to address the\nchallenges of cross-age, cross-modality, and cross-site segmentation of\nsubcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as\nwell as masked pseudo-labeling, the model is able to jointly learn from labeled\nsource domain data and unlabeled target domain data. We evaluated our framework\non expert-annotated datasets acquired from different ages and sites. MAPSeg\nconsistently outperformed other methods, including previous state-of-the-art\nsupervised baselines, domain generalization, and domain adaptation frameworks\nin segmenting subcortical regions regardless of age, modality, or acquisition\nsite. The code and pretrained encoder will be publicly available at\nhttps://github.com/XuzheZ/MAPSeg\n","authors":["Xuzhe Zhang","Yuhao Wu","Jia Guo","Jerod M. Rasmussen","Thomas G. O'Connor","Hyagriv N. Simhan","Sonja Entringer","Pathik D. Wadhwa","Claudia Buss","Cristiane S. Duarte","Andrea Jackowski","Hai Li","Jonathan Posner","Andrew F. Laine","Yun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09367v1","updated":"2023-03-16T14:52:53Z","published":"2023-03-16T14:52:53Z","title":"Goal-conditioned Offline Reinforcement Learning through State Space\n  Partitioning","summary":"  Offline reinforcement learning (RL) aims to infer sequential decision\npolicies using only offline datasets. This is a particularly difficult setup,\nespecially when learning to achieve multiple different goals or outcomes under\na given scenario with only sparse rewards. For offline learning of\ngoal-conditioned policies via supervised learning, previous work has shown that\nan advantage weighted log-likelihood loss guarantees monotonic policy\nimprovement. In this work we argue that, despite its benefits, this approach is\nstill insufficient to fully address the distribution shift and multi-modality\nproblems. The latter is particularly severe in long-horizon tasks where finding\na unique and optimal policy that goes from a state to the desired goal is\nchallenging as there may be multiple and potentially conflicting solutions. To\ntackle these challenges, we propose a complementary advantage-based weighting\nscheme that introduces an additional source of inductive bias: given a\nvalue-based partitioning of the state space, the contribution of actions\nexpected to lead to target regions that are easier to reach, compared to the\nfinal goal, is further increased. Empirically, we demonstrate that the proposed\napproach, Dual-Advantage Weighted Offline Goal-conditioned RL (DAWOG),\noutperforms several competing offline algorithms in commonly used benchmarks.\nAnalytically, we offer a guarantee that the learnt policy is never worse than\nthe underlying behaviour policy.\n","authors":["Mianchu Wang","Yue Jin","Giovanni Montana"],"pdf_url":"https://arxiv.org/pdf/2303.09367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09366v1","updated":"2023-03-16T14:51:44Z","published":"2023-03-16T14:51:44Z","title":"The Scope of In-Context Learning for the Extraction of Medical Temporal\n  Constraints","summary":"  Medications often impose temporal constraints on everyday patient activity.\nViolations of such medical temporal constraints (MTCs) lead to a lack of\ntreatment adherence, in addition to poor health outcomes and increased\nhealthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in\nboth patient education materials and clinical texts. Computationally\nrepresenting MTCs in DUGs will advance patient-centric healthcare applications\nby helping to define safe patient activity patterns. We define a novel taxonomy\nof MTCs found in DUGs and develop a novel context-free grammar (CFG) based\nmodel to computationally represent MTCs from unstructured DUGs. Additionally,\nwe release three new datasets with a combined total of N = 836 DUGs labeled\nwith normalized MTCs. We develop an in-context learning (ICL) solution for\nautomatically extracting and normalizing MTCs found in DUGs, achieving an\naverage F1 score of 0.62 across all datasets. Finally, we rigorously\ninvestigate ICL model performance against a baseline model, across datasets and\nMTC types, and through in-depth error analysis.\n","authors":["Parker Seegmiller","Joseph Gatto","Madhusudan Basak","Diane Cook","Hassan Ghasemzadeh","John Stankovic","Sarah Preum"],"pdf_url":"https://arxiv.org/pdf/2303.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09354v1","updated":"2023-03-16T14:32:50Z","published":"2023-03-16T14:32:50Z","title":"The NCI Imaging Data Commons as a platform for reproducible research in\n  computational pathology","summary":"  Objective: Reproducibility is critical for translating machine learning-based\n(ML) solutions in computational pathology (CompPath) into practice. However, an\nincreasing number of studies report difficulties in reproducing ML results. The\nNCI Imaging Data Commons (IDC) is a public repository of >120 cancer image\ncollections, including >38,000 whole-slide images (WSIs), that is designed to\nbe used with cloud-based ML services. Here, we explore the potential of the IDC\nto facilitate reproducibility of CompPath research.\n  Materials and Methods: The IDC realizes the FAIR principles: All images are\nencoded according to the DICOM standard, persistently identified, discoverable\nvia rich metadata, and accessible via open tools. Taking advantage of this, we\nimplemented two experiments in which a representative ML-based method for\nclassifying lung tumor tissue was trained and/or evaluated on different\ndatasets from the IDC. To assess reproducibility, the experiments were run\nmultiple times with independent but identically configured sessions of common\nML services.\n  Results: The AUC values of different runs of the same experiment were\ngenerally consistent and in the same order of magnitude as a similar,\npreviously published study. However, there were occasional small variations in\nAUC values of up to 0.044, indicating a practical limit to reproducibility.\n  Discussion and conclusion: By realizing the FAIR principles, the IDC enables\nother researchers to reuse exactly the same datasets. Cloud-based ML services\nenable others to run CompPath experiments in an identically configured\ncomputing environment without having to own high-performance hardware. The\ncombination of both makes it possible to approach the reproducibility limit.\n","authors":["Daniela P. Schacherer","Markus D. Herrmann","David A. Clunie","Henning Höfener","William Clifford","William J. R. Longabaugh","Steve Pieper","Ron Kikinis","Andrey Fedorov","André Homeyer"],"pdf_url":"https://arxiv.org/pdf/2303.09354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09350v1","updated":"2023-03-16T14:31:50Z","published":"2023-03-16T14:31:50Z","title":"Unsupervised domain adaptation by learning using privileged information","summary":"  Successful unsupervised domain adaptation (UDA) is guaranteed only under\nstrong assumptions such as covariate shift and overlap between input domains.\nThe latter is often violated in high-dimensional applications such as image\nclassification which, despite this challenge, continues to serve as inspiration\nand benchmark for algorithm development. In this work, we show that access to\nside information about examples from the source and target domains can help\nrelax these assumptions and increase sample efficiency in learning, at the cost\nof collecting a richer variable set. We call this domain adaptation by learning\nusing privileged information (DALUPI). Tailored for this task, we propose a\nsimple two-stage learning algorithm inspired by our analysis and a practical\nend-to-end algorithm for multi-label image classification. In a suite of\nexperiments, including an application to medical image analysis, we demonstrate\nthat incorporating privileged information in learning can reduce errors in\ndomain transfer compared to classical learning.\n","authors":["Adam Breitholtz","Anton Matsson","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.09350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09340v1","updated":"2023-03-16T14:21:45Z","published":"2023-03-16T14:21:45Z","title":"Improving Automated Hemorrhage Detection in Sparse-view Computed\n  Tomography via Deep Convolutional Neural Network based Artifact Reduction","summary":"  Intracranial hemorrhage poses a serious health problem requiring rapid and\noften intensive medical treatment. For diagnosis, a Cranial Computed Tomography\n(CCT) scan is usually performed. However, the increased health risk caused by\nradiation is a concern. The most important strategy to reduce this potential\nrisk is to keep the radiation dose as low as possible and consistent with the\ndiagnostic task. Sparse-view CT can be an effective strategy to reduce dose by\nreducing the total number of views acquired, albeit at the expense of image\nquality. In this work, we use a U-Net architecture to reduce artifacts from\nsparse-view CCTs, predicting fully sampled reconstructions from sparse-view\nones. We evaluate the hemorrhage detectability in the predicted CCTs with a\nhemorrhage classification convolutional neural network, trained on fully\nsampled CCTs to detect and classify different sub-types of hemorrhages. Our\nresults suggest that the automated classification and detection accuracy of\nhemorrhages in sparse-view CCTs can be improved substantially by the U-Net.\nThis demonstrates the feasibility of rapid automated hemorrhage detection on\nlow-dose CT data to assist radiologists in routine clinical practice.\n","authors":["Johannes Thalhammer","Manuel Schultheiss","Tina Dorosti","Tobias Lasser","Franz Pfeiffer","Daniela Pfeiffer","Florian Schaff"],"pdf_url":"https://arxiv.org/pdf/2303.09340v1.pdf","comment":"11 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.09335v1","updated":"2023-03-16T14:16:19Z","published":"2023-03-16T14:16:19Z","title":"ExoplANNET: A deep learning algorithm to detect and identify planetary\n  signals in radial velocity data","summary":"  The detection of exoplanets with the radial velocity method consists in\ndetecting variations of the stellar velocity caused by an unseen sub-stellar\ncompanion. Instrumental errors, irregular time sampling, and different noise\nsources originating in the intrinsic variability of the star can hinder the\ninterpretation of the data, and even lead to spurious detections. In recent\ntimes, work began to emerge in the field of extrasolar planets that use Machine\nLearning algorithms, some with results that exceed those obtained with the\ntraditional techniques in the field. We seek to explore the scope of the neural\nnetworks in the radial velocity method, in particular for exoplanet detection\nin the presence of correlated noise of stellar origin. In this work, a neural\nnetwork is proposed to replace the computation of the significance of the\nsignal detected with the radial velocity method and to classify it as of\nplanetary origin or not. The algorithm is trained using synthetic data of\nsystems with and without planetary companions. We injected realistic correlated\nnoise in the simulations, based on previous studies of the behaviour of stellar\nactivity. The performance of the network is compared to the traditional method\nbased on null hypothesis significance testing. The network achieves 28 % fewer\nfalse positives. The improvement is observed mainly in the detection of\nsmall-amplitude signals associated with low-mass planets. In addition, its\nexecution time is five orders of magnitude faster than the traditional method.\nThe superior performance exhibited by the algorithm has only been tested on\nsimulated radial velocity data so far. Although in principle it should be\nstraightforward to adapt it for use in real time series, its performance has to\nbe tested thoroughly. Future work should permit evaluating its potential for\nadoption as a valuable tool for exoplanet detection.\n","authors":["L. A. Nieto","R. F. Díaz"],"pdf_url":"https://arxiv.org/pdf/2303.09335v1.pdf","comment":"Paper under peer-review, comments are welcome"},{"id":"http://arxiv.org/abs/2207.06950v4","updated":"2023-03-16T14:13:05Z","published":"2022-07-14T14:23:14Z","title":"Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA\n  Models","summary":"  Low-order functional ANOVA (fANOVA) models have been rediscovered in the\nmachine learning (ML) community under the guise of inherently interpretable\nmachine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and\nGAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting\nfunctional main effects and second-order interactions. We propose a new\nalgorithm, called GAMI-Tree, that is similar to EBM, but has a number of\nfeatures that lead to better performance. It uses model-based trees as base\nlearners and incorporates a new interaction filtering method that is better at\ncapturing the underlying interactions. In addition, our iterative training\nmethod converges to a model with better predictive performance, and the\nembedded purification ensures that interactions are hierarchically orthogonal\nto main effects. The algorithm does not need extensive tuning, and our\nimplementation is fast and efficient. We use simulated and real datasets to\ncompare the performance and interpretability of GAMI-Tree with EBM and\nGAMI-Net.\n","authors":["Linwei Hu","Jie Chen","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2207.06950v4.pdf","comment":"25 pages plus appendix"},{"id":"http://arxiv.org/abs/2303.09331v1","updated":"2023-03-16T14:03:56Z","published":"2023-03-16T14:03:56Z","title":"Model Based Explanations of Concept Drift","summary":"  The notion of concept drift refers to the phenomenon that the distribution\ngenerating the observed data changes over time. If drift is present, machine\nlearning models can become inaccurate and need adjustment. While there do exist\nmethods to detect concept drift or to adjust models in the presence of observed\ndrift, the question of explaining drift, i.e., describing the potentially\ncomplex and high dimensional change of distribution in a human-understandable\nfashion, has hardly been considered so far. This problem is of importance since\nit enables an inspection of the most prominent characteristics of how and where\ndrift manifests itself. Hence, it enables human understanding of the change and\nit increases acceptance of life-long learning models. In this paper, we present\na novel technology characterizing concept drift in terms of the characteristic\nchange of spatial features based on various explanation techniques. To do so,\nwe propose a methodology to reduce the explanation of concept drift to an\nexplanation of models that are trained in a suitable way extracting relevant\ninformation regarding the drift. This way a large variety of explanation\nschemes is available. Thus, a suitable method can be selected for the problem\nof drift explanation at hand. We outline the potential of this approach and\ndemonstrate its usefulness in several examples.\n","authors":["Fabian Hinder","Valerie Vaquet","Johannes Brinkrolf","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2303.09331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04994v3","updated":"2023-03-16T13:24:29Z","published":"2022-10-10T19:59:40Z","title":"Sampling-based inference for large linear models, with application to\n  linearised Laplace","summary":"  Large-scale linear models are ubiquitous throughout machine learning, with\ncontemporary application as surrogate models for neural network uncertainty\nquantification; that is, the linearised Laplace method. Alas, the computational\ncost associated with Bayesian linear models constrains this method's\napplication to small networks, small output spaces and small datasets. We\naddress this limitation by introducing a scalable sample-based Bayesian\ninference method for conjugate Gaussian multi-output linear models, together\nwith a matching method for hyperparameter (regularisation) selection.\nFurthermore, we use a classic feature normalisation method (the g-prior) to\nresolve a previously highlighted pathology of the linearised Laplace method.\nTogether, these contributions allow us to perform linearised neural network\ninference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k\ndatapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M\ndatapoints) and with a U-Net on a high-resolution tomographic reconstruction\ntask (2M parameters, 251k output~dimensions).\n","authors":["Javier Antorán","Shreyas Padhy","Riccardo Barbano","Eric Nalisnick","David Janz","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2210.04994v3.pdf","comment":"Published at ICLR 2023. This latest Arxiv version is extended with a\n  demonstration of the proposed methods on the Imagenet dataset"},{"id":"http://arxiv.org/abs/2303.09297v1","updated":"2023-03-16T13:16:50Z","published":"2023-03-16T13:16:50Z","title":"Explaining Groups of Instances Counterfactually for XAI: A Use Case,\n  Algorithm and User Study for Group-Counterfactuals","summary":"  Counterfactual explanations are an increasingly popular form of post hoc\nexplanation due to their (i) applicability across problem domains, (ii)\nproposed legal compliance (e.g., with GDPR), and (iii) reliance on the\ncontrastive nature of human explanation. Although counterfactual explanations\nare normally used to explain individual predictive-instances, we explore a\nnovel use case in which groups of similar instances are explained in a\ncollective fashion using ``group counterfactuals'' (e.g., to highlight a\nrepeating pattern of illness in a group of patients). These group\ncounterfactuals meet a human preference for coherent, broad explanations\ncovering multiple events/instances. A novel, group-counterfactual algorithm is\nproposed to generate high-coverage explanations that are faithful to the\nto-be-explained model. This explanation strategy is also evaluated in a large,\ncontrolled user study (N=207), using objective (i.e., accuracy) and subjective\n(i.e., confidence, explanation satisfaction, and trust) psychological measures.\nThe results show that group counterfactuals elicit modest but definite\nimprovements in people's understanding of an AI system. The implications of\nthese findings for counterfactual methods and for XAI are discussed.\n","authors":["Greta Warren","Mark T. Keane","Christophe Gueret","Eoin Delaney"],"pdf_url":"https://arxiv.org/pdf/2303.09297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09289v1","updated":"2023-03-16T13:10:58Z","published":"2023-03-16T13:10:58Z","title":"Image Classifiers Leak Sensitive Attributes About Their Classes","summary":"  Neural network-based image classifiers are powerful tools for computer vision\ntasks, but they inadvertently reveal sensitive attribute information about\ntheir classes, raising concerns about their privacy. To investigate this\nprivacy leakage, we introduce the first Class Attribute Inference Attack\n(Caia), which leverages recent advances in text-to-image synthesis to infer\nsensitive attributes of individual classes in a black-box setting, while\nremaining competitive with related white-box attacks. Our extensive experiments\nin the face recognition domain show that Caia can accurately infer undisclosed\nsensitive attributes, such as an individual's hair color, gender and racial\nappearance, which are not part of the training labels. Interestingly, we\ndemonstrate that adversarial robust models are even more vulnerable to such\nprivacy leakage than standard models, indicating that a trade-off between\nrobustness and privacy exists.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Felix Friedrich","Manuel Brack","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2303.09289v1.pdf","comment":"40 pages, 32 figures, 4 tables"},{"id":"http://arxiv.org/abs/2210.09996v2","updated":"2023-03-16T13:07:42Z","published":"2022-10-18T17:01:35Z","title":"Perceptual Grouping in Contrastive Vision-Language Models","summary":"  Recent advances in zero-shot image recognition suggest that vision-language\nmodels learn generic visual representations with a high degree of semantic\ninformation that may be arbitrarily probed with natural language phrases.\nUnderstanding an image, however, is not just about understanding what content\nresides within an image, but importantly, where that content resides. In this\nwork we examine how well vision-language models are able to understand where\nobjects reside within an image and group together visually related parts of the\nimagery. We demonstrate how contemporary vision and language representation\nlearning models based on contrastive losses and large web-based data capture\nlimited object localization information. We propose a minimal set of\nmodifications that results in models that uniquely learn both semantic and\nspatial information. We measure this performance in terms of zero-shot image\nrecognition, unsupervised bottom-up and top-down semantic segmentations, as\nwell as robustness analyses. We find that the resulting model achieves\nstate-of-the-art results in terms of unsupervised segmentation, and demonstrate\nthat the learned representations are uniquely robust to spurious correlations\nin datasets designed to probe the causal behavior of vision models.\n","authors":["Kanchana Ranasinghe","Brandon McKinzie","Sachin Ravi","Yinfei Yang","Alexander Toshev","Jonathon Shlens"],"pdf_url":"https://arxiv.org/pdf/2210.09996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09181v2","updated":"2023-03-16T12:57:21Z","published":"2021-12-16T20:10:56Z","title":"Approximation of functions with one-bit neural networks","summary":"  The celebrated universal approximation theorems for neural networks roughly\nstate that any reasonable function can be arbitrarily well-approximated by a\nnetwork whose parameters are appropriately chosen real numbers. This paper\nexamines the approximation capabilities of one-bit neural networks -- those\nwhose nonzero parameters are $\\pm a$ for some fixed $a\\not=0$. One of our main\ntheorems shows that for any $f\\in C^s([0,1]^d)$ with $\\|f\\|_\\infty<1$ and error\n$\\varepsilon$, there is a $f_{NN}$ such that\n$|f(\\boldsymbol{x})-f_{NN}(\\boldsymbol{x})|\\leq \\varepsilon$ for all\n$\\boldsymbol{x}$ away from the boundary of $[0,1]^d$, and $f_{NN}$ is either\nimplementable by a $\\{\\pm 1\\}$ quadratic network with $O(\\varepsilon^{-2d/s})$\nparameters or a $\\{\\pm \\frac 1 2 \\}$ ReLU network with\n$O(\\varepsilon^{-2d/s}\\log (1/\\varepsilon))$ parameters, as $\\varepsilon\\to0$.\nWe establish new approximation results for iterated multivariate Bernstein\noperators, error estimates for noise-shaping quantization on the Bernstein\nbasis, and novel implementation of the Bernstein polynomials by one-bit\nquadratic and ReLU neural networks.\n","authors":["C. Sinan Güntürk","Weilin Li"],"pdf_url":"https://arxiv.org/pdf/2112.09181v2.pdf","comment":"45 pages, 7 figures, significant changes and additions"},{"id":"http://arxiv.org/abs/2303.09273v1","updated":"2023-03-16T12:56:13Z","published":"2023-03-16T12:56:13Z","title":"Adaptive Modeling of Uncertainties for Traffic Forecasting","summary":"  Deep neural networks (DNNs) have emerged as a dominant approach for\ndeveloping traffic forecasting models. These models are typically trained to\nminimize error on averaged test cases and produce a single-point prediction,\nsuch as a scalar value for traffic speed or travel time. However, single-point\npredictions fail to account for prediction uncertainty that is critical for\nmany transportation management scenarios, such as determining the best- or\nworst-case arrival time. We present QuanTraffic, a generic framework to enhance\nthe capability of an arbitrary DNN model for uncertainty modeling. QuanTraffic\nrequires little human involvement and does not change the base DNN architecture\nduring deployment. Instead, it automatically learns a standard quantile\nfunction during the DNN model training to produce a prediction interval for the\nsingle-point prediction. The prediction interval defines a range where the true\nvalue of the traffic prediction is likely to fall. Furthermore, QuanTraffic\ndevelops an adaptive scheme that dynamically adjusts the prediction interval\nbased on the location and prediction window of the test input. We evaluated\nQuanTraffic by applying it to five representative DNN models for traffic\nforecasting across seven public datasets. We then compared QuanTraffic against\nfive uncertainty quantification methods. Compared to the baseline uncertainty\nmodeling techniques, QuanTraffic with base DNN architectures delivers\nconsistently better and more robust performance than the existing ones on the\nreported datasets.\n","authors":["Ying Wu","Yongchao Ye","Adnan Zeb","James J. Q. Yu","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09273v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.09271v1","updated":"2023-03-16T12:53:45Z","published":"2023-03-16T12:53:45Z","title":"Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles","summary":"  The ability to explain why a machine learning model arrives at a particular\nprediction is crucial when used as decision support by human operators of\ncritical systems. The provided explanations must be provably correct, and\npreferably without redundant information, called minimal explanations. In this\npaper, we aim at finding explanations for predictions made by tree ensembles\nthat are not only minimal, but also minimum with respect to a cost function.\n  To this end, we first present a highly efficient oracle that can determine\nthe correctness of explanations, surpassing the runtime performance of current\nstate-of-the-art alternatives by several orders of magnitude when computing\nminimal explanations.\n  Secondly, we adapt an algorithm called MARCO from related works (calling it\nm-MARCO) for the purpose of computing a single minimum explanation per\nprediction, and demonstrate an overall speedup factor of two compared to the\nMARCO algorithm which enumerates all minimal explanations.\n  Finally, we study the obtained explanations from a range of use cases,\nleading to further insights of their characteristics. In particular, we observe\nthat in several cases, there are more than 100,000 minimal explanations to\nchoose from for a single prediction. In these cases, we see that only a small\nportion of the minimal explanations are also minimum, and that the minimum\nexplanations are significantly less verbose, hence motivating the aim of this\nwork.\n","authors":["John Törnblom","Emil Karlsson","Simin Nadjm-Tehrani"],"pdf_url":"https://arxiv.org/pdf/2303.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09266v1","updated":"2023-03-16T12:44:16Z","published":"2023-03-16T12:44:16Z","title":"SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for\n  Accelerating BERT Inference","summary":"  Dynamic early exiting has been proven to improve the inference speed of the\npre-trained language model like BERT. However, all samples must go through all\nconsecutive layers before early exiting and more complex samples usually go\nthrough more layers, which still exists redundant computation. In this paper,\nwe propose a novel dynamic early exiting combined with layer skipping for BERT\ninference named SmartBERT, which adds a skipping gate and an exiting operator\ninto each layer of BERT. SmartBERT can adaptively skip some layers and\nadaptively choose whether to exit. Besides, we propose cross-layer contrastive\nlearning and combine it into our training phases to boost the intermediate\nlayers and classifiers which would be beneficial for early exiting. To keep the\nconsistent usage of skipping gates between training and inference phases, we\npropose a hard weight mechanism during training phase. We conduct experiments\non eight classification datasets of the GLUE benchmark. Experimental results\nshow that SmartBERT achieves 2-3x computation reduction with minimal accuracy\ndrops compared with BERT and our method outperforms previous methods in both\nefficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,\nwe prove that the early exiting based on entropy hardly works, and the skipping\nmechanism is essential for reducing computation.\n","authors":["Boren Hu","Yun Zhu","Jiacheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.09266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14888v2","updated":"2023-03-16T11:48:19Z","published":"2022-08-31T14:28:36Z","title":"Feature Alignment by Uncertainty and Self-Training for Source-Free\n  Unsupervised Domain Adaptation","summary":"  Most unsupervised domain adaptation (UDA) methods assume that labeled source\nimages are available during model adaptation. However, this assumption is often\ninfeasible owing to confidentiality issues or memory constraints on mobile\ndevices. Some recently developed approaches do not require source images during\nadaptation, but they show limited performance on perturbed images. To address\nthese problems, we propose a novel source-free UDA method that uses only a\npre-trained source model and unlabeled target images. Our method captures the\naleatoric uncertainty by incorporating data augmentation and trains the feature\ngenerator with two consistency objectives. The feature generator is encouraged\nto learn consistent visual features away from the decision boundaries of the\nhead classifier. Thus, the adapted model becomes more robust to image\nperturbations. Inspired by self-supervised learning, our method promotes\ninter-space alignment between the prediction space and the feature space while\nincorporating intra-space consistency within the feature space to reduce the\ndomain gap between the source and target domains. We also consider epistemic\nuncertainty to boost the model adaptation performance. Extensive experiments on\npopular UDA benchmark datasets demonstrate that the proposed source-free method\nis comparable or even superior to vanilla UDA methods. Moreover, the adapted\nmodels show more robust results when input images are perturbed.\n","authors":["JoonHo Lee","Gyemin Lee"],"pdf_url":"https://arxiv.org/pdf/2208.14888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17457v2","updated":"2023-03-16T11:32:24Z","published":"2022-10-31T16:30:48Z","title":"Agglomeration of Polygonal Grids using Graph Neural Networks with\n  applications to Multigrid solvers","summary":"  Agglomeration-based strategies are important both within adaptive refinement\nalgorithms and to construct scalable multilevel algebraic solvers. In order to\nautomatically perform agglomeration of polygonal grids, we propose the use of\nMachine Learning (ML) strategies, that can naturally exploit geometrical\ninformation about the mesh in order to preserve the grid quality, enhancing\nperformance of numerical methods and reducing the overall computational cost.\nIn particular, we employ the k-means clustering algorithm and Graph Neural\nNetworks (GNNs) to partition the connectivity graph of a computational mesh.\nMoreover, GNNs have high online inference speed and the advantage to process\nnaturally and simultaneously both the graph structure of mesh and the\ngeometrical information, such as the areas of the elements or their barycentric\ncoordinates. These techniques are compared with METIS, a standard algorithm for\ngraph partitioning, which is meant to process only the graph information of the\nmesh. We demonstrate that performance in terms of quality metrics is enhanced\nfor ML strategies. Such models also show a good degree of generalization when\napplied to more complex geometries, such as brain MRI scans, and the capability\nof preserving the quality of the grid. The effectiveness of these strategies is\ndemonstrated also when applied to MultiGrid (MG) solvers in a Polygonal\nDiscontinuous Galerkin (PolyDG) framework. In the considered experiments, GNNs\nshow overall the best performance in terms of inference speed, accuracy and\nflexibility of the approach.\n","authors":["P. F. Antonietti","N. Farenga","E. Manuzzi","G. Martinelli","L. Saverio"],"pdf_url":"https://arxiv.org/pdf/2210.17457v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09232v1","updated":"2023-03-16T11:15:55Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wand","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01470v3","updated":"2023-03-16T11:14:39Z","published":"2023-01-04T07:16:46Z","title":"Model Parameter Identification via a Hyperparameter Optimization Scheme\n  for Autonomous Racing Systems","summary":"  In this letter, we propose a model parameter identification method via a\nhyperparameter optimization scheme (MIHO). Our method adopts an efficient\nexplore-exploit strategy to identify the parameters of dynamic models in a\ndata-driven optimization manner. We utilize MIHO for model parameter\nidentification of the AV-21, a full-scaled autonomous race vehicle. We then\nincorporate the optimized parameters for the design of model-based planning and\ncontrol systems of our platform. In experiments, MIHO exhibits more than 13\ntimes faster convergence than traditional parameter identification methods.\nFurthermore, the parametric models learned via MIHO demonstrate good fitness to\nthe given datasets and show generalization ability in unseen dynamic scenarios.\nWe further conduct extensive field tests to validate our model-based system,\ndemonstrating stable obstacle avoidance and high-speed driving up to 217 km/h\nat the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source\ncode for MIHO and videos of the tests are available at\nhttps://github.com/hynkis/MIHO.\n","authors":["Hyunki Seong","Chanyoung Chung","David Hyunchul Shim"],"pdf_url":"https://arxiv.org/pdf/2301.01470v3.pdf","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2209.12746v2","updated":"2023-03-16T11:12:35Z","published":"2022-09-26T14:55:21Z","title":"LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN\n  Latent Space","summary":"  As the methods evolve, inversion is mainly divided into two steps. The first\nstep is Image Embedding, in which an encoder or optimization process embeds\nimages to get the corresponding latent codes. Afterward, the second step aims\nto refine the inversion and editing results, which we named Result Refinement.\nAlthough the second step significantly improves fidelity, perception and\neditability are almost unchanged, deeply dependent on inverse latent codes\nattained in the first step. Therefore, a crucial problem is gaining the latent\ncodes with better perception and editability while retaining the reconstruction\nfidelity. In this work, we first point out that these two characteristics are\nrelated to the degree of alignment (or disalignment) of the inverse codes with\nthe synthetic distribution. Then, we propose Latent Space Alignment Inversion\nParadigm (LSAP), which consists of evaluation metric and solution for this\nproblem. Specifically, we introduce Normalized Style Space ($\\mathcal{S^N}$\nspace) and $\\mathcal{S^N}$ Cosine Distance (SNCD) to measure disalignment of\ninversion methods. Since our proposed SNCD is differentiable, it can be\noptimized in both encoder-based and optimization-based embedding methods to\nconduct a uniform solution. Extensive experiments in various domains\ndemonstrate that SNCD effectively reflects perception and editability, and our\nalignment paradigm archives the state-of-the-art in both two steps. Code is\navailable on https://github.com/caopulan/GANInverter/tree/main/configs/lsap.\n","authors":["Pu Cao","Lu Yang","Dongxu Liu","Zhiwei Liu","Shan Li","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2209.12746v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2202.00307v2","updated":"2023-03-16T10:57:44Z","published":"2022-02-01T10:10:13Z","title":"Laplacian2Mesh: Laplacian-Based Mesh Understanding","summary":"  Geometric deep learning has sparked a rising interest in computer graphics to\nperform shape understanding tasks, such as shape classification and semantic\nsegmentation. When the input is a polygonal surface, one has to suffer from the\nirregular mesh structure. Motivated by the geometric spectral theory, we\nintroduce Laplacian2Mesh, a novel and flexible convolutional neural network\n(CNN) framework for coping with irregular triangle meshes (vertices may have\nany valence). By mapping the input mesh surface to the multi-dimensional\nLaplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis\ntasks directly using the mature CNNs, without the need to deal with the\nirregular connectivity of the mesh structure. We further define a mesh pooling\noperation such that the receptive field of the network can be expanded while\nretaining the original vertex set as well as the connections between them.\nBesides, we introduce a channel-wise self-attention block to learn the\nindividual importance of feature ingredients. Laplacian2Mesh not only decouples\nthe geometry from the irregular connectivity of the mesh structure but also\nbetter captures the global features that are central to shape classification\nand segmentation. Extensive tests on various datasets demonstrate the\neffectiveness and efficiency of Laplacian2Mesh, particularly in terms of the\ncapability of being vulnerable to noise to fulfill various learning tasks.\n","authors":["Qiujie Dong","Zixiong Wang","Manyi Li","Junjie Gao","Shuangmin Chen","Zhenyu Shu","Shiqing Xin","Changhe Tu","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2202.00307v2.pdf","comment":"Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)"},{"id":"http://arxiv.org/abs/2303.08157v2","updated":"2023-03-16T10:56:37Z","published":"2023-03-14T18:14:40Z","title":"Graph Neural Network Surrogates of Fair Graph Filtering","summary":"  Graph filters that transform prior node values to posterior scores via edge\npropagation often support graph mining tasks affecting humans, such as\nrecommendation and ranking. Thus, it is important to make them fair in terms of\nsatisfying statistical parity constraints between groups of nodes (e.g.,\ndistribute score mass between genders proportionally to their representation).\nTo achieve this while minimally perturbing the original posteriors, we\nintroduce a filter-aware universal approximation framework for posterior\nobjectives. This defines appropriate graph neural networks trained at runtime\nto be similar to filters but also locally optimize a large class of objectives,\nincluding fairness-aware ones. Experiments on a collection of 8 filters and 5\ngraphs show that our approach performs equally well or better than alternatives\nin meeting parity constraints while preserving the AUC of score-based community\nmember recommendation and creating minimal utility loss in prior diffusion.\n","authors":["Emmanouil Krasanakis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2303.08157v2.pdf","comment":"40 pages, 5 figures, 5 papers"},{"id":"http://arxiv.org/abs/2212.02710v2","updated":"2023-03-16T10:47:13Z","published":"2022-12-06T02:11:34Z","title":"Beyond Object Recognition: A New Benchmark towards Object Concept\n  Learning","summary":"  Understanding objects is a central building block of artificial intelligence,\nespecially for embodied AI. Even though object recognition excels with deep\nlearning, current machines still struggle to learn higher-level knowledge,\ne.g., what attributes an object has, and what can we do with an object. In this\nwork, we propose a challenging Object Concept Learning (OCL) task to push the\nenvelope of object understanding. It requires machines to reason out object\naffordances and simultaneously give the reason: what attributes make an object\npossesses these affordances. To support OCL, we build a densely annotated\nknowledge base including extensive labels for three levels of object concept\n(category, attribute, affordance), and the causal relations of three levels. By\nanalyzing the causal structure of OCL, we present a baseline, Object Concept\nReasoning Network (OCRN). It leverages causal intervention and concept\ninstantiation to infer the three levels following their causal relations. In\nexperiments, OCRN effectively infers the object knowledge while following the\ncausalities well. Our data and code are available at https://mvig-rhos.com/ocl.\n","authors":["Yong-Lu Li","Yue Xu","Xinyu Xu","Xiaohan Mao","Yuan Yao","Siqi Liu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2212.02710v2.pdf","comment":"Preprint. Webpage: https://mvig-rhos.com/ocl"},{"id":"http://arxiv.org/abs/2303.09216v1","updated":"2023-03-16T10:45:24Z","published":"2023-03-16T10:45:24Z","title":"Controlled Descent Training","summary":"  In this work, a novel and model-based artificial neural network (ANN)\ntraining method is developed supported by optimal control theory. The method\naugments training labels in order to robustly guarantee training loss\nconvergence and improve training convergence rate. Dynamic label augmentation\nis proposed within the framework of gradient descent training where the\nconvergence of training loss is controlled. First, we capture the training\nbehavior with the help of empirical Neural Tangent Kernels (NTK) and borrow\ntools from systems and control theory to analyze both the local and global\ntraining dynamics (e.g. stability, reachability). Second, we propose to\ndynamically alter the gradient descent training mechanism via fictitious labels\nas control inputs and an optimal state feedback policy. In this way, we enforce\nlocally $\\mathcal{H}_2$ optimal and convergent training behavior. The novel\nalgorithm, \\textit{Controlled Descent Training} (CDT), guarantees local\nconvergence. CDT unleashes new potentials in the analysis, interpretation, and\ndesign of ANN architectures. The applicability of the method is demonstrated on\nstandard regression and classification problems.\n","authors":["Viktor Andersson","Balázs Varga","Vincent Szolnoky","Andreas Syrén","Rebecka Jörnsten","Balázs Kulcsár"],"pdf_url":"https://arxiv.org/pdf/2303.09216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01938v2","updated":"2023-03-16T10:45:07Z","published":"2022-12-04T22:18:02Z","title":"Hierarchical Policy Blending As Optimal Transport","summary":"  We present hierarchical policy blending as optimal transport (HiPBOT). This\nhierarchical framework adapts the weights of low-level reactive expert\npolicies, adding a look-ahead planning layer on the parameter space of a\nproduct of expert policies and agents. Our high-level planner realizes a policy\nblending via unbalanced optimal transport, consolidating the scaling of\nunderlying Riemannian motion policies, effectively adjusting their Riemannian\nmatrix, and deciding over the priorities between experts and agents,\nguaranteeing safety and task success. Our experimental results in a range of\napplication scenarios from low-dimensional navigation to high-dimensional\nwhole-body control showcase the efficacy and efficiency of HiPBOT, which\noutperforms state-of-the-art baselines that either perform probabilistic\ninference or define a tree structure of experts, paving the way for new\napplications of optimal transport to robot control. More material at\nhttps://sites.google.com/view/hipobot\n","authors":["An T. Le","Kay Hansel","Jan Peters","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2212.01938v2.pdf","comment":"15 pages, 5 figures, accepted to the 5th Annual Learning for Dynamics\n  & Control Conference (L4DC)"},{"id":"http://arxiv.org/abs/2303.03770v2","updated":"2023-03-16T10:09:12Z","published":"2023-03-07T10:04:55Z","title":"Guiding Pseudo-labels with Uncertainty Estimation for Source-free\n  Unsupervised Domain Adaptation","summary":"  Standard Unsupervised Domain Adaptation (UDA) methods assume the availability\nof both source and target data during the adaptation. In this work, we\ninvestigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific\ncase of UDA where a model is adapted to a target domain without access to\nsource data. We propose a novel approach for the SF-UDA setting based on a loss\nreweighting strategy that brings robustness against the noise that inevitably\naffects the pseudo-labels. The classification loss is reweighted based on the\nreliability of the pseudo-labels that is measured by estimating their\nuncertainty. Guided by such reweighting strategy, the pseudo-labels are\nprogressively refined by aggregating knowledge from neighbouring samples.\nFurthermore, a self-supervised contrastive framework is leveraged as a target\nspace regulariser to enhance such knowledge aggregation. A novel negative pairs\nexclusion strategy is proposed to identify and exclude negative pairs made of\nsamples sharing the same class, even in presence of some noise in the\npseudo-labels. Our method outperforms previous methods on three major\nbenchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C\nand DomainNet with a performance gain of +1.8% on both benchmarks and on PACS\nwith +12.3% in the single-source setting and +6.6% in multi-target adaptation.\nAdditional analyses demonstrate that the proposed approach is robust to the\nnoise, which results in significantly more accurate pseudo-labels compared to\nstate-of-the-art approaches.\n","authors":["Mattia Litrico","Alessio Del Bue","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2303.03770v2.pdf","comment":"To be published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.09184v1","updated":"2023-03-16T09:53:57Z","published":"2023-03-16T09:53:57Z","title":"Block-wise Bit-Compression of Transformer-based Models","summary":"  With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.\n","authors":["Gaochen Dong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09184v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2212.04129v2","updated":"2023-03-16T09:47:53Z","published":"2022-12-08T08:04:06Z","title":"Deep Incubation: Training Large Models by Divide-and-Conquering","summary":"  Recent years have witnessed a remarkable success of large deep learning\nmodels. However, training these models is challenging due to high computational\ncosts, painfully slow convergence, and overfitting issues. In this paper, we\npresent Deep Incubation, a novel approach that enables the efficient and\neffective training of large models by dividing them into smaller sub-modules\nthat can be trained separately and assembled seamlessly. A key challenge for\nimplementing this idea is to ensure the compatibility of the independently\ntrained sub-modules. To address this issue, we first introduce a global, shared\nmeta model, which is leveraged to implicitly link all the modules together, and\ncan be designed as an extremely small network with negligible computational\noverhead. Then we propose a module incubation algorithm, which trains each\nsub-module to replace the corresponding component of the meta model and\naccomplish a given learning task. Despite the simplicity, our approach\neffectively encourages each sub-module to be aware of its role in the target\nlarge model, such that the finally-learned sub-modules can collaborate with\neach other smoothly after being assembled. Empirically, our method outperforms\nend-to-end (E2E) training in terms of both final accuracy and training\nefficiency. For example, on top of ViT-Huge, it improves the accuracy by 2.7%\non ImageNet or achieves similar performance with 4x less training time.\nNotably, the gains are significant for downstream tasks as well (e.g., object\ndetection and image segmentation on COCO and ADE20K). Code is available at\nhttps://github.com/LeapLabTHU/Deep-Incubation.\n","authors":["Zanlin Ni","Yulin Wang","Jiangwei Yu","Haojun Jiang","Yue Cao","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2212.04129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09172v1","updated":"2023-03-16T09:37:10Z","published":"2023-03-16T09:37:10Z","title":"Learning Logic Specifications for Soft Policy Guidance in POMCP","summary":"  Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for\nPartially Observable Markov Decision Processes (POMDPs). It allows scaling to\nlarge state spaces by computing an approximation of the optimal policy locally\nand online, using a Monte Carlo Tree Search based strategy. However, POMCP\nsuffers from sparse reward function, namely, rewards achieved only when the\nfinal goal is reached, particularly in environments with large state spaces and\nlong horizons. Recently, logic specifications have been integrated into POMCP\nto guide exploration and to satisfy safety requirements. However, such\npolicy-related rules require manual definition by domain experts, especially in\nreal-world scenarios. In this paper, we use inductive logic programming to\nlearn logic specifications from traces of POMCP executions, i.e., sets of\nbelief-action pairs generated by the planner. Specifically, we learn rules\nexpressed in the paradigm of answer set programming. We then integrate them\ninside POMCP to provide soft policy bias toward promising actions. In the\ncontext of two benchmark scenarios, rocksample and battery, we show that the\nintegration of learned rules from small task instances can improve performance\nwith fewer Monte Carlo simulations and in larger task instances. We make our\nmodified version of POMCP publicly available at\nhttps://github.com/GiuMaz/pomcp_clingo.git.\n","authors":["Giulio Mazzi","Daniele Meli","Alberto Castellini","Alessandro Farinelli"],"pdf_url":"https://arxiv.org/pdf/2303.09172v1.pdf","comment":"To appear in the Proceedings of 22nd International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS) 2023"},{"id":"http://arxiv.org/abs/2303.09166v1","updated":"2023-03-16T09:14:26Z","published":"2023-03-16T09:14:26Z","title":"Identifiability Results for Multimodal Contrastive Learning","summary":"  Contrastive learning is a cornerstone underlying recent progress in\nmulti-view and multimodal learning, e.g., in representation learning with\nimage/caption pairs. While its effectiveness is not yet fully understood, a\nline of recent work reveals that contrastive learning can invert the data\ngenerating process and recover ground truth latent factors shared between\nviews. In this work, we present new identifiability results for multimodal\ncontrastive learning, showing that it is possible to recover shared factors in\na more general setup than the multi-view setting studied previously.\nSpecifically, we distinguish between the multi-view setting with one generative\nmechanism (e.g., multiple cameras of the same type) and the multimodal setting\nthat is characterized by distinct mechanisms (e.g., cameras and microphones).\nOur work generalizes previous identifiability results by redefining the\ngenerative process in terms of distinct mechanisms with modality-specific\nlatent variables. We prove that contrastive learning can block-identify latent\nfactors shared between modalities, even when there are nontrivial dependencies\nbetween factors. We empirically verify our identifiability results with\nnumerical simulations and corroborate our findings on a complex multimodal\ndataset of image/text pairs. Zooming out, our work provides a theoretical basis\nfor multimodal representation learning and explains in which settings\nmultimodal contrastive learning can be effective in practice.\n","authors":["Imant Daunhawer","Alice Bizeul","Emanuele Palumbo","Alexander Marx","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2303.09166v1.pdf","comment":"ICLR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.07130v2","updated":"2023-03-16T08:59:24Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) based scoring method is used to identify the extent of\nlung involvement observed on a CT scan. This paper presents a domain\nknowledge-based pipeline for extracting regions of infection in COVID-19\npatients using a combination of image-processing algorithms and a pre-trained\nUNET model. The severity of the infection is then classified into different\ncategories using an ensemble of three machine-learning models: Extreme Gradient\nBoosting, Extremely Randomized Trees, and Support Vector Machine. The proposed\nsystem was evaluated on a validation dataset in the AI-Enabled Medical Image\nAnalysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and\nachieved a macro F1 score of 64\\%. These results demonstrate the potential of\ncombining domain knowledge with machine learning techniques for accurate\nCOVID-19 diagnosis using CT scans. The implementation of the proposed system\nfor severity analysis is available at\n\\textit{https://github.com/aanandt/Enhancing-COVID-19-Severity-Analysis-through-Ensemble-Methods.git\n}\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10154v2","updated":"2023-03-16T08:57:14Z","published":"2022-12-20T10:46:40Z","title":"Human-Guided Fair Classification for Natural Language Processing","summary":"  Text classifiers have promising applications in high-stake tasks such as\nresume screening and content moderation. These classifiers must be fair and\navoid discriminatory decisions by being invariant to perturbations of sensitive\nattributes such as gender or ethnicity. However, there is a gap between human\nintuition about these perturbations and the formal similarity specifications\ncapturing them. While existing research has started to address this gap,\ncurrent methods are based on hardcoded word replacements, resulting in\nspecifications with limited expressivity or ones that fail to fully align with\nhuman intuition (e.g., in cases of asymmetric counterfactuals). This work\nproposes novel methods for bridging this gap by discovering expressive and\nintuitive individual fairness specifications. We show how to leverage\nunsupervised style transfer and GPT-3's zero-shot capabilities to automatically\ngenerate expressive candidate pairs of semantically similar sentences that\ndiffer along sensitive attributes. We then validate the generated pairs via an\nextensive crowdsourcing study, which confirms that a lot of these pairs align\nwith human intuition about fairness in the context of toxicity classification.\nFinally, we show how limited amounts of human feedback can be leveraged to\nlearn a similarity specification that can be used to train downstream\nfairness-aware models.\n","authors":["Florian E. Dorner","Momchil Peychev","Nikola Konstantinov","Naman Goel","Elliott Ash","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2212.10154v2.pdf","comment":"Published at ICLR 2023 (notable top 25%). 30 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.09154v1","updated":"2023-03-16T08:34:56Z","published":"2023-03-16T08:34:56Z","title":"Bayesian Generalization Error in Linear Neural Networks with Concept\n  Bottleneck Structure and Multitask Formulation","summary":"  Concept bottleneck model (CBM) is a ubiquitous method that can interpret\nneural networks using concepts. In CBM, concepts are inserted between the\noutput layer and the last intermediate layer as observable values. This helps\nin understanding the reason behind the outputs generated by the neural\nnetworks: the weights corresponding to the concepts from the last hidden layer\nto the output layer. However, it has not yet been possible to understand the\nbehavior of the generalization error in CBM since a neural network is a\nsingular statistical model in general. When the model is singular, a one to one\nmap from the parameters to probability distributions cannot be created. This\nnon-identifiability makes it difficult to analyze the generalization\nperformance. In this study, we mathematically clarify the Bayesian\ngeneralization error and free energy of CBM when its architecture is\nthree-layered linear neural networks. We also consider a multitask problem\nwhere the neural network outputs not only the original output but also the\nconcepts. The results show that CBM drastically changes the behavior of the\nparameter region and the Bayesian generalization error in three-layered linear\nneural networks as compared with the standard version, whereas the multitask\nformulation does not.\n","authors":["Naoki Hayashi","Yoshihide Sawada"],"pdf_url":"https://arxiv.org/pdf/2303.09154v1.pdf","comment":"31 pages, 14 figures, to be submitted to Neurocomputing"},{"id":"http://arxiv.org/abs/2211.11761v2","updated":"2023-03-16T08:12:44Z","published":"2022-11-21T11:29:48Z","title":"From Node Interaction to Hop Interaction: New Effective and Scalable\n  Graph Learning Paradigm","summary":"  Existing Graph Neural Networks (GNNs) follow the message-passing mechanism\nthat conducts information interaction among nodes iteratively. While\nconsiderable progress has been made, such node interaction paradigms still have\nthe following limitation. First, the scalability limitation precludes the broad\napplication of GNNs in large-scale industrial settings since the node\ninteraction among rapidly expanding neighbors incurs high computation and\nmemory costs. Second, the over-smoothing problem restricts the discrimination\nability of nodes, i.e., node representations of different classes will converge\nto indistinguishable after repeated node interactions. In this work, we propose\na novel hop interaction paradigm to address these limitations simultaneously.\nThe core idea is to convert the interaction target among nodes to pre-processed\nmulti-hop features inside each node. We design a simple yet effective HopGNN\nframework that can easily utilize existing GNNs to achieve hop interaction.\nFurthermore, we propose a multi-task learning strategy with a self-supervised\nlearning objective to enhance HopGNN. We conduct extensive experiments on 12\nbenchmark datasets in a wide range of domains, scales, and smoothness of\ngraphs. Experimental results show that our methods achieve superior performance\nwhile maintaining high scalability and efficiency. The code is at\nhttps://github.com/JC-202/HopGNN.\n","authors":["Jie Chen","Zilong Li","Yin Zhu","Junping Zhang","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2211.11761v2.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.01332v2","updated":"2023-03-16T08:09:39Z","published":"2023-03-02T15:10:08Z","title":"Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion\n  Segmentation","summary":"  Precise ischemic lesion segmentation plays an essential role in improving\ndiagnosis and treatment planning for ischemic stroke, one of the prevalent\ndiseases with the highest mortality rate. While numerous deep neural network\napproaches have recently been proposed to tackle this problem, these methods\nrequire large amounts of annotated regions during training, which can be\nimpractical in the medical domain where annotated data is scarce. As a remedy,\nwe present a prototypical few-shot segmentation approach for ischemic lesion\nsegmentation using only one annotated sample during training. The proposed\napproach leverages a novel self-supervised training mechanism that is tailored\nto the task of ischemic stroke lesion segmentation by exploiting color-coded\nparametric maps generated from Computed Tomography Perfusion scans. We\nillustrate the benefits of our proposed training mechanism, leading to\nconsiderable improvements in performance in the few-shot setting. Given a\nsingle annotated patient, an average Dice score of 0.58 is achieved for the\nsegmentation of ischemic lesions.\n","authors":["Luca Tomasetti","Stine Hansen","Mahdieh Khanmohammadi","Kjersti Engan","Liv Jorunn Høllesli","Kathinka Dæhli Kurz","Michael Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2303.01332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09133v1","updated":"2023-03-16T07:56:44Z","published":"2023-03-16T07:56:44Z","title":"Predicting nonlinear reshaping of periodic signals in optical fibre with\n  a neural network","summary":"  We deploy a supervised machine-learning model based on a neural network to\npredict the temporal and spectral reshaping of a simple sinusoidal modulation\ninto a pulse train having a comb structure in the frequency domain, which\noccurs upon nonlinear propagation in an optical fibre. Both normal and\nanomalous second-order dispersion regimes of the fibre are studied, and the\nspeed of the neural network is leveraged to probe the space of input parameters\nfor the generation of custom combs or the occurrence of significant temporal or\nspectral focusing.\n","authors":["Sonia Boscolo","J. M. Dudley","Christophe Finot"],"pdf_url":"https://arxiv.org/pdf/2303.09133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09128v1","updated":"2023-03-16T07:45:46Z","published":"2023-03-16T07:45:46Z","title":"Exploring Distributional Shifts in Large Language Models for Code\n  Analysis","summary":"  We systematically study the capacity of two large language models for code -\nCodeT5 and Codex - to generalize to out-of-domain data. In this study, we\nconsider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. This\nmakes recognition of in-domain vs out-of-domain data at the time of deployment\ntrivial. We establish that samples from each new domain present both models\nwith a significant challenge of distribution shift. We study how well different\nestablished methods can adapt models to better generalize to new domains. Our\nexperiments show that while multitask learning alone is a reasonable baseline,\ncombining it with few-shot finetuning on examples retrieved from training data\ncan achieve very strong performance. In fact, according to our experiments,\nthis solution can outperform direct finetuning for very low-data scenarios.\nFinally, we consider variations of this approach to create a more broadly\napplicable method to adapt to multiple domains at once. We find that in the\ncase of code generation, a model adapted to multiple domains simultaneously\nperforms on par with those adapted to each domain individually.\n","authors":["Shushan Arakelyan","Rocktim Jyoti Das","Yi Mao","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2303.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09126v1","updated":"2023-03-16T07:41:53Z","published":"2023-03-16T07:41:53Z","title":"Evaluation of distance-based approaches for forensic comparison:\n  Application to hand odor evidence","summary":"  The issue of distinguishing between the same-source and different-source\nhypotheses based on various types of traces is a generic problem in forensic\nscience. This problem is often tackled with Bayesian approaches, which are able\nto provide a likelihood ratio that quantifies the relative strengths of\nevidence supporting each of the two competing hypotheses. Here, we focus on\ndistance-based approaches, whose robustness and specifically whose capacity to\ndeal with high-dimensional evidence are very different, and need to be\nevaluated and optimized. A unified framework for direct methods based on\nestimating the likelihoods of the distance between traces under each of the two\ncompeting hypotheses, and indirect methods using logistic regression to\ndiscriminate between same-source and different-source distance distributions,\nis presented. Whilst direct methods are more flexible, indirect methods are\nmore robust and quite natural in machine learning. Moreover, indirect methods\nalso enable the use of a vectorial distance, thus preventing the severe\ninformation loss suffered by scalar distance approaches.Direct and indirect\nmethods are compared in terms of sensitivity, specificity and robustness, with\nand without dimensionality reduction, with and without feature selection, on\nthe example of hand odor profiles, a novel and challenging type of evidence in\nthe field of forensics. Empirical evaluations on a large panel of 534 subjects\nand their 1690 odor traces show the significant superiority of the indirect\nmethods, especially without dimensionality reduction, be it with or without\nfeature selection.\n","authors":["Isabelle Rivals","Cédric Sautier","Guillaume Cognon","Vincent Cuzuel"],"pdf_url":"https://arxiv.org/pdf/2303.09126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11705v2","updated":"2023-03-16T07:40:17Z","published":"2023-01-27T13:32:17Z","title":"FedPH: Privacy-enhanced Heterogeneous Federated Learning","summary":"  Federated Learning is a distributed machine-learning environment that allows\nclients to learn collaboratively without sharing private data. This is\naccomplished by exchanging parameters. However, the differences in data\ndistributions and computing resources among clients make related studies\ndifficult. To address these heterogeneous problems, we propose a novel\nFederated Learning method. Our method utilizes a pre-trained model as the\nbackbone of the local model, with fully connected layers comprising the head.\nThe backbone extracts features for the head, and the embedding vector of\nclasses is shared between clients to improve the head and enhance the\nperformance of the local model. By sharing the embedding vector of classes\ninstead of gradient-based parameters, clients can better adapt to private data,\nand communication between the server and clients is more effective. To protect\nprivacy, we propose a privacy-preserving hybrid method that adds noise to the\nembedding vector of classes. This method has a minimal effect on the\nperformance of the local model when differential privacy is met. We conduct a\ncomprehensive evaluation of our approach on a self-built vehicle dataset,\ncomparing it with other Federated Learning methods under non-independent\nidentically distributed(Non-IID).\n","authors":["Kuang Hangdong","Mi Bo"],"pdf_url":"https://arxiv.org/pdf/2301.11705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09112v1","updated":"2023-03-16T06:57:51Z","published":"2023-03-16T06:57:51Z","title":"SigVIC: Spatial Importance Guided Variable-Rate Image Compression","summary":"  Variable-rate mechanism has improved the flexibility and efficiency of\nlearning-based image compression that trains multiple models for different\nrate-distortion tradeoffs. One of the most common approaches for variable-rate\nis to channel-wisely or spatial-uniformly scale the internal features. However,\nthe diversity of spatial importance is instructive for bit allocation of image\ncompression. In this paper, we introduce a Spatial Importance Guided\nVariable-rate Image Compression (SigVIC), in which a spatial gating unit (SGU)\nis designed for adaptively learning a spatial importance mask. Then, a spatial\nscaling network (SSN) takes the spatial importance mask to guide the feature\nscaling and bit allocation for variable-rate. Moreover, to improve the quality\nof decoded image, Top-K shallow features are selected to refine the decoded\nfeatures through a shallow feature fusion module (SFFM). Experiments show that\nour method outperforms other learning-based methods (whether variable-rate or\nnot) and traditional codecs, with storage saving and high flexibility.\n","authors":["Jiaming Liang","Meiqin Liu","Chao Yao","Chunyu Lin","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.09112v1.pdf","comment":"Accepted by IEEE ICASSP2023 (Camera Ready)"},{"id":"http://arxiv.org/abs/2211.01452v2","updated":"2023-03-16T06:51:31Z","published":"2022-11-02T19:43:22Z","title":"MPCFormer: fast, performant and private Transformer inference with MPC","summary":"  Enabling private inference is crucial for many cloud inference services that\nare based on Transformer models. However, existing private inference solutions\ncan increase the inference latency by more than 60x or significantly compromise\nthe inference quality. In this paper, we design the framework MPCFORMER as a\npractical solution, using Secure Multi-Party Computation (MPC) and Knowledge\nDistillation (KD). Through extensive evaluations, we show that MPCFORMER\nsignificantly speeds up Transformer inference in MPC settings while achieving\nsimilar ML performance to the input model. On the IMDb dataset, it achieves\nsimilar performance to BERTBASE, while being 5.3x faster. On the GLUE\nbenchmark, it achieves 97% performance of BERTBASE with a 2.2x speedup.\nMPCFORMER remains effective with different trained Transformer weights such as\nROBERTABASE and larger models including BERTLarge. Code is available at\nhttps://github.com/MccRee177/MPCFormer.\n","authors":["Dacheng Li","Rulin Shao","Hongyi Wang","Han Guo","Eric P. Xing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.01452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06885v3","updated":"2023-03-16T06:46:14Z","published":"2023-02-14T08:14:30Z","title":"Improving Interpretability of Deep Sequential Knowledge Tracing Models\n  with Question-centric Cognitive Representations","summary":"  Knowledge tracing (KT) is a crucial technique to predict students' future\nperformance by observing their historical learning processes. Due to the\npowerful representation ability of deep neural networks, remarkable progress\nhas been made by using deep learning techniques to solve the KT problem. The\nmajority of existing approaches rely on the \\emph{homogeneous question}\nassumption that questions have equivalent contributions if they share the same\nset of knowledge components. Unfortunately, this assumption is inaccurate in\nreal-world educational scenarios. Furthermore, it is very challenging to\ninterpret the prediction results from the existing deep learning based KT\nmodels. Therefore, in this paper, we present QIKT, a question-centric\ninterpretable KT model to address the above challenges. The proposed QIKT\napproach explicitly models students' knowledge state variations at a\nfine-grained level with question-sensitive cognitive representations that are\njointly learned from a question-centric knowledge acquisition module and a\nquestion-centric problem solving module. Meanwhile, the QIKT utilizes an item\nresponse theory based prediction layer to generate interpretable prediction\nresults. The proposed QIKT model is evaluated on three public real-world\neducational datasets. The results demonstrate that our approach is superior on\nthe KT prediction task, and it outperforms a wide range of deep learning based\nKT models in terms of prediction accuracy with better model interpretability.\nTo encourage reproducible results, we have provided all the datasets and code\nat \\url{https://pykt.org/}.\n","authors":["Jiahao Chen","Zitao Liu","Shuyan Huang","Qiongqiong Liu","Weiqi Luo"],"pdf_url":"https://arxiv.org/pdf/2302.06885v3.pdf","comment":"Accepted at AAAI'23: The 37th AAAI Conference on Artificial\n  Intelligence, 2023"},{"id":"http://arxiv.org/abs/2303.09103v1","updated":"2023-03-16T06:23:43Z","published":"2023-03-16T06:23:43Z","title":"Machine learning based biomedical image processing for echocardiographic\n  images","summary":"  The popularity of Artificial intelligence and machine learning have prompted\nresearchers to use it in the recent researches. The proposed method uses\nK-Nearest Neighbor (KNN) algorithm for segmentation of medical images,\nextracting of image features for analysis by classifying the data based on the\nneural networks. Classification of the images in medical imaging is very\nimportant, KNN is one suitable algorithm which is simple, conceptual and\ncomputational, which provides very good accuracy in results. KNN algorithm is a\nunique user-friendly approach with wide range of applications in machine\nlearning algorithms which are majorly used for the various image processing\napplications including classification, segmentation and regression issues of\nthe image processing. The proposed system uses gray level co-occurrence matrix\nfeatures. The trained neural network has been tested successfully on a group of\nechocardiographic images, errors were compared using regression plot. The\nresults of the algorithm are tested using various quantitative as well as\nqualitative metrics and proven to exhibit better performance in terms of both\nquantitative and qualitative metrics in terms of current state-of-the-art\nmethods in the related area. To compare the performance of trained neural\nnetwork the regression analysis performed showed a good correlation.\n","authors":["Ayesha Heena","Nagashettappa Biradar","Najmuddin M. Maroof","Surbhi Bhatia","Rashmi Agarwal","Kanta Prasad"],"pdf_url":"https://arxiv.org/pdf/2303.09103v1.pdf","comment":"10 figures 4 tables"},{"id":"http://arxiv.org/abs/2210.01162v5","updated":"2023-03-16T06:15:51Z","published":"2022-10-03T18:32:20Z","title":"Learning Minimally-Violating Continuous Control for Infeasible Linear\n  Temporal Logic Specifications","summary":"  This paper explores continuous-time control synthesis for target-driven\nnavigation to satisfy complex high-level tasks expressed as linear temporal\nlogic (LTL). We propose a model-free framework using deep reinforcement\nlearning (DRL) where the underlying dynamic system is unknown (an opaque box).\nUnlike prior work, this paper considers scenarios where the given LTL\nspecification might be infeasible and therefore cannot be accomplished\nglobally. Instead of modifying the given LTL formula, we provide a general\nDRL-based approach to satisfy it with minimal violation. To do this, we\ntransform a previously multi-objective DRL problem, which requires simultaneous\nautomata satisfaction and minimum violation cost, into a single objective. By\nguiding the DRL agent with a sampling-based path planning algorithm for the\npotentially infeasible LTL task, the proposed approach mitigates the myopic\ntendencies of DRL, which are often an issue when learning general LTL tasks\nthat can have long or infinite horizons. This is achieved by decomposing an\ninfeasible LTL formula into several reach-avoid sub-tasks with shorter\nhorizons, which can be trained in a modular DRL architecture. Furthermore, we\novercome the challenge of the exploration process for DRL in complex and\ncluttered environments by using path planners to design rewards that are dense\nin the configuration space. The benefits of the presented approach are\ndemonstrated through testing on various complex nonlinear systems and compared\nwith state-of-the-art baselines. The Video demonstration can be found\nhere:https://youtu.be/jBhx6Nv224E.\n","authors":["Mingyu Cai","Makai Mann","Zachary Serlin","Kevin Leahy","Cristian-Ioan Vasile"],"pdf_url":"https://arxiv.org/pdf/2210.01162v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09100v1","updated":"2023-03-16T06:09:15Z","published":"2023-03-16T06:09:15Z","title":"Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models","summary":"  For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt learning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize prompt learning with\nthe visual knowledge and view images and the corresponding prompts as patch and\ntoken sets under optimal transport, which pushes the prompt tokens to\nfaithfully capture the label-specific visual concepts, instead of overfitting\nthe training categories. Moreover, the proposed model can also be\nstraightforwardly extended to the conditional case where the\ninstance-conditional prompts are generated to improve the generalizability.\nExtensive experiments on 15 datasets show promising transferability and\ngeneralization performance of our proposed model.\n","authors":["Xinyang Liu","Dongsheng Wang","Miaoge Li","Zhibin Duan","Yishi Xu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03382v2","updated":"2023-03-16T05:58:10Z","published":"2022-02-07T17:59:04Z","title":"Corrupted Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Corrupted Image Modeling (CIM) for self-supervised visual\npre-training. CIM uses an auxiliary generator with a small trainable BEiT to\ncorrupt the input image instead of using artificial [MASK] tokens, where some\npatches are randomly selected and replaced with plausible alternatives sampled\nfrom the BEiT output distribution. Given this corrupted image, an enhancer\nnetwork learns to either recover all the original image pixels, or predict\nwhether each visual token is replaced by a generator sample or not. The\ngenerator and the enhancer are simultaneously trained and synergistically\nupdated. After pre-training, the enhancer can be used as a high-capacity visual\nencoder for downstream tasks. CIM is a general and flexible visual pre-training\nframework that is suitable for various network architectures. For the first\ntime, CIM demonstrates that both ViT and CNN can learn rich visual\nrepresentations using a unified, non-Siamese framework. Experimental results\nshow that our approach achieves compelling results in vision benchmarks, such\nas ImageNet classification and ADE20K semantic segmentation.\n","authors":["Yuxin Fang","Li Dong","Hangbo Bao","Xinggang Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2202.03382v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06832v2","updated":"2023-03-16T05:44:50Z","published":"2023-03-13T03:28:36Z","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","summary":"  ODIN is an innovative approach that addresses the problem of dataset\nconstraints by integrating generative AI models. Traditional zero-shot learning\nmethods are constrained by the training dataset. To fundamentally overcome this\nlimitation, ODIN attempts to mitigate the dataset constraints by generating\non-demand datasets based on user requirements. ODIN consists of three main\nmodules: a prompt generator, a text-to-image generator, and an image\npost-processor. To generate high-quality prompts and images, we adopted a large\nlanguage model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,\nStable Diffusion), respectively. We evaluated ODIN on various datasets in terms\nof model accuracy and data diversity to demonstrate its potential, and\nconducted post-experiments for further investigation. Overall, ODIN is a\nfeasible approach that enables Al to learn unseen knowledge beyond the training\ndataset.\n","authors":["SP Choi","Jihun Lee","Hyeongseok Ahn","Sanghee Jung","Bumsoo Kang"],"pdf_url":"https://arxiv.org/pdf/2303.06832v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.08302v2","updated":"2023-03-16T05:34:52Z","published":"2023-03-15T01:27:15Z","title":"A Comprehensive Study on Post-Training Quantization for Large Language\n  Models","summary":"  Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study of those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bit precision is similar to 5 bits).\nWe also present recommendations about how to utilize quantization for \\llms\nwith different sizes, and leave suggestions of future opportunities and system\nwork that are not resolved in this work.\n","authors":["Zhewei Yao","Cheng Li","Xiaoxia Wu","Stephen Youn","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.08302v2.pdf","comment":"25 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.08455v2","updated":"2023-03-16T05:13:33Z","published":"2023-03-15T08:59:03Z","title":"On the uncertainty analysis of the data-enabled physics-informed neural\n  network for solving neutron diffusion eigenvalue problem","summary":"  In practical engineering experiments, the data obtained through detectors are\ninevitably noisy. For the already proposed data-enabled physics-informed neural\nnetwork (DEPINN) \\citep{DEPINN}, we investigate the performance of DEPINN in\ncalculating the neutron diffusion eigenvalue problem from several perspectives\nwhen the prior data contain different scales of noise. Further, in order to\nreduce the effect of noise and improve the utilization of the noisy prior data,\nwe propose innovative interval loss functions and give some rigorous\nmathematical proofs. The robustness of DEPINN is examined on two typical\nbenchmark problems through a large number of numerical results, and the\neffectiveness of the proposed interval loss function is demonstrated by\ncomparison. This paper confirms the feasibility of the improved DEPINN for\npractical engineering applications in nuclear reactor physics.\n","authors":["Yu Yang","Helin Gong","Qihong Yang","Yangtao Deng","Qiaolin He","Shiquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08455v2.pdf","comment":"The experiments in Figures 6 and 10 of the article have errors and\n  need to be rerun"},{"id":"http://arxiv.org/abs/2303.09085v1","updated":"2023-03-16T05:06:06Z","published":"2023-03-16T05:06:06Z","title":"Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back\n  Pain and Sciatica Patients based on Multimodalities and Multimodal Learning","summary":"  Low back pain (LBP) and sciatica may require surgical therapy when they are\nsymptomatic of severe pain. However, there is no effective measures to evaluate\nthe surgical outcomes in advance. This work combined elements of Eastern\nmedicine and machine learning, and developed a preoperative assessment tool to\npredict the prognosis of lumbar spinal surgery in LBP and sciatica patients.\nStandard operative assessments, traditional Chinese medicine body constitution\nassessments, planned surgical approach, and vowel pronunciation recordings were\ncollected and stored in different modalities. Our work provides insights into\nleveraging modality combinations, multimodals, and fusion strategies. The\ninterpretability of models and correlations between modalities were also\ninspected. Based on the recruited 105 patients, we found that combining\nstandard operative assessments, body constitution assessments, and planned\nsurgical approach achieved the best performance in 0.81 accuracy. Our approach\nis effective and can be widely applied in general practice due to simplicity\nand effective.\n","authors":["Li-Chin Chen","Jung-Nien Lai","Hung-En Lin","Hsien-Te Chen","Kuo-Hsuan Hung","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2303.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11208v2","updated":"2023-03-16T04:54:05Z","published":"2023-02-22T08:48:08Z","title":"KS-DETR: Knowledge Sharing in Attention Learning for Detection\n  Transformer","summary":"  Scaled dot-product attention applies a softmax function on the scaled\ndot-product of queries and keys to calculate weights and then multiplies the\nweights and values. In this work, we study how to improve the learning of\nscaled dot-product attention to improve the accuracy of DETR. Our method is\nbased on the following observations: using ground truth foreground-background\nmask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables\nlearning much better weights/values; with better weights/values, better\nvalues/weights can be learned. We propose a triple-attention module in which\nthe first attention is a plain scaled dot-product attention, the second/third\nattention generates high-quality weights/values (with the assistance of GT\nFg-Bg Mask) and shares the values/weights with the first attention to improve\nthe quality of values/weights. The second and third attentions are removed\nduring inference. We call our method knowledge-sharing DETR (KS-DETR), which is\nan extension of knowledge distillation (KD) in the way that the improved\nweights and values of the teachers (the second and third attentions) are\ndirectly shared, instead of mimicked, by the student (the first attention) to\nenable more efficient knowledge transfer from the teachers to the student.\nExperiments on various DETR-like methods show consistent improvements over the\nbaseline methods on the MS COCO benchmark. Code is available at\nhttps://github.com/edocanonymous/KS-DETR.\n","authors":["Kaikai Zhao","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2302.11208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09079v1","updated":"2023-03-16T04:45:06Z","published":"2023-03-16T04:45:06Z","title":"SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is a commonly used approach to learning and\nencoding data representations. By using a pre-trained SSL image encoder and\ntraining a downstream classifier on top of it, impressive performance can be\nachieved on various tasks with very little labeled data. The increasing usage\nof SSL has led to an uptick in security research related to SSL encoders and\nthe development of various Trojan attacks. The danger posed by Trojan attacks\ninserted in SSL encoders lies in their ability to operate covertly and spread\nwidely among various users and devices. The presence of backdoor behavior in\nTrojaned encoders can inadvertently be inherited by downstream classifiers,\nmaking it even more difficult to detect and mitigate the threat. Although\ncurrent Trojan detection methods in supervised learning can potentially\nsafeguard SSL downstream classifiers, identifying and addressing triggers in\nthe SSL encoder before its widespread dissemination is a challenging task. This\nis because downstream tasks are not always known, dataset labels are not\navailable, and even the original training dataset is not accessible during the\nSSL encoder Trojan detection. This paper presents an innovative technique\ncalled SSL-Cleanse that is designed to detect and mitigate backdoor attacks in\nSSL encoders. We evaluated SSL-Cleanse on various datasets using 300 models,\nachieving an average detection success rate of 83.7% on ImageNet-100. After\nmitigating backdoors, on average, backdoored encoders achieve 0.24% attack\nsuccess rate without great accuracy loss, proving the effectiveness of\nSSL-Cleanse.\n","authors":["Mengxin Zheng","Jiaqi Xue","Xun Chen","Lei Jiang","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2303.09079v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2301.11494v3","updated":"2023-03-16T04:27:13Z","published":"2023-01-27T02:10:05Z","title":"Learning Vortex Dynamics for Fluid Inference and Prediction","summary":"  We propose a novel differentiable vortex particle (DVP) method to infer and\npredict fluid dynamics from a single video. Lying at its core is a\nparticle-based latent space to encapsulate the hidden, Lagrangian vortical\nevolution underpinning the observable, Eulerian flow phenomena. Our\ndifferentiable vortex particles are coupled with a learnable,\nvortex-to-velocity dynamics mapping to effectively capture the complex flow\nfeatures in a physically-constrained, low-dimensional space. This\nrepresentation facilitates the learning of a fluid simulator tailored to the\ninput video that can deliver robust, long-term future predictions. The value of\nour method is twofold: first, our learned simulator enables the inference of\nhidden physics quantities (e.g., velocity field) purely from visual\nobservation; secondly, it also supports future prediction, constructing the\ninput video's sequel along with its future dynamics evolution. We compare our\nmethod with a range of existing methods on both synthetic and real-world\nvideos, demonstrating improved reconstruction quality, visual plausibility, and\nphysical integrity.\n","authors":["Yitong Deng","Hong-Xing Yu","Jiajun Wu","Bo Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.11494v3.pdf","comment":"ICLR 2023, project webpage:\n  https://yitongdeng.github.io/vortex_learning_webpage/"},{"id":"http://arxiv.org/abs/2303.06833v2","updated":"2023-03-16T04:19:01Z","published":"2023-03-13T03:29:58Z","title":"Transformer-based Planning for Symbolic Regression","summary":"  Symbolic regression (SR) is a challenging task in machine learning that\ninvolves finding a mathematical expression for a function based on its values.\nRecent advancements in SR have demonstrated the efficacy of pretrained\ntransformer-based models for generating equations as sequences, which benefit\nfrom large-scale pretraining on synthetic datasets and offer considerable\nadvantages over GP-based methods in terms of inference time. However, these\nmodels focus on supervised pretraining goals borrowed from text generation and\nignore equation-specific objectives like accuracy and complexity. To address\nthis, we propose TPSR, a Transformer-based Planning strategy for Symbolic\nRegression that incorporates Monte Carlo Tree Search into the transformer\ndecoding process. TPSR, as opposed to conventional decoding strategies, allows\nfor the integration of non-differentiable feedback, such as fitting accuracy\nand complexity, as external sources of knowledge into the equation generation\nprocess. Extensive experiments on various datasets show that our approach\noutperforms state-of-the-art methods, enhancing the model's fitting-complexity\ntrade-off, extrapolation abilities, and robustness to noise. We also\ndemonstrate that the utilization of various caching mechanisms can further\nenhance the efficiency of TPSR.\n","authors":["Parshin Shojaee","Kazem Meidani","Amir Barati Farimani","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2303.06833v2.pdf","comment":"Parshin Shojaee and Kazem Meidani contributed equally to this work"},{"id":"http://arxiv.org/abs/2303.09068v1","updated":"2023-03-16T04:02:17Z","published":"2023-03-16T04:02:17Z","title":"VFP: Converting Tabular Data for IIoT into Images Considering\n  Correlations of Attributes for Convolutional Neural Networks","summary":"  For tabular data generated from IIoT devices, traditional machine learning\n(ML) techniques based on the decision tree algorithm have been employed.\nHowever, these methods have limitations in processing tabular data where real\nnumber attributes dominate. To address this issue, DeepInsight, REFINED, and\nIGTD were proposed to convert tabular data into images for utilizing\nconvolutional neural networks (CNNs). They gather similar features in some\nspecific spots of an image to make the converted image look like an actual\nimage. Gathering similar features contrasts with traditional ML techniques for\ntabular data, which drops some highly correlated attributes to avoid\noverfitting. Also, previous converting methods fixed the image size, and there\nare wasted or insufficient pixels according to the number of attributes of\ntabular data. Therefore, this paper proposes a new converting method, Vortex\nFeature Positioning (VFP). VFP considers the correlation of features and places\nsimilar features far away from each. Features are positioned in the vortex\nshape from the center of an image, and the number of attributes determines the\nimage size. VFP shows better test performance than traditional ML techniques\nfor tabular data and previous converting methods in five datasets: Iris, Wine,\nDry Bean, Epileptic Seizure, and SECOM, which have differences in the number of\nattributes.\n","authors":["Jong-Ik Park","Cheol-Ho Hong"],"pdf_url":"https://arxiv.org/pdf/2303.09068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09066v1","updated":"2023-03-16T03:48:29Z","published":"2023-03-16T03:48:29Z","title":"High-Dimensional Penalized Bernstein Support Vector Machines","summary":"  The support vector machines (SVM) is a powerful classifier used for binary\nclassification to improve the prediction accuracy. However, the\nnon-differentiability of the SVM hinge loss function can lead to computational\ndifficulties in high dimensional settings. To overcome this problem, we rely on\nBernstein polynomial and propose a new smoothed version of the SVM hinge loss\ncalled the Bernstein support vector machine (BernSVM), which is suitable for\nthe high dimension $p >> n$ regime. As the BernSVM objective loss function is\nof the class $C^2$, we propose two efficient algorithms for computing the\nsolution of the penalized BernSVM. The first algorithm is based on coordinate\ndescent with maximization-majorization (MM) principle and the second one is\nIRLS-type algorithm (iterative re-weighted least squares). Under standard\nassumptions, we derive a cone condition and a restricted strong convexity to\nestablish an upper bound for the weighted Lasso BernSVM estimator. Using a\nlocal linear approximation, we extend the latter result to penalized BernSVM\nwith non convex penalties SCAD and MCP. Our bound holds with high probability\nand achieves a rate of order $\\sqrt{s\\log(p)/n}$, where $s$ is the number of\nactive features. Simulation studies are considered to illustrate the prediction\naccuracy of BernSVM to its competitors and also to compare the performance of\nthe two algorithms in terms of computational timing and error estimation. The\nuse of the proposed method is illustrated through analysis of three large-scale\nreal data examples.\n","authors":["Rachid Kharoubi","Abdallah Mkhadri","Karim Oualkacha"],"pdf_url":"https://arxiv.org/pdf/2303.09066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09065v1","updated":"2023-03-16T03:45:46Z","published":"2023-03-16T03:45:46Z","title":"Maximum Margin Learning of t-SPNs for Cell Classification with Filtering","summary":"  An algorithm based on a deep probabilistic architecture referred to as a\ntree-structured sum-product network (t-SPN) is considered for cell\nclassification. The t-SPN is constructed such that the unnormalized probability\nis represented as conditional probabilities of a subset of most similar cell\nclasses. The constructed t-SPN architecture is learned by maximizing the\nmargin, which is the difference in the conditional probability between the true\nand the most competitive false label. To enhance the generalization ability of\nthe architecture, L2-regularization (REG) is considered along with the maximum\nmargin (MM) criterion in the learning process. To highlight cell features, this\npaper investigates the effectiveness of two generic high-pass filters: ideal\nhigh-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both\nHEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on\nthe max-margin criterion with regularization produced the highest accuracy rate\ncompared to other state-of-the-art algorithms that include convolutional neural\nnetwork (CNN) based algorithms. The ideal high-pass filter was more effective\non the HEp-2 dataset, which is based on immunofluorescence staining, while the\nLOG was more effective on the Feulgen dataset, which is based on Feulgen\nstaining.\n","authors":["Haeyong Kang","Chang D. Yoo","Yongcheon Na"],"pdf_url":"https://arxiv.org/pdf/2303.09065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09063v1","updated":"2023-03-16T03:43:10Z","published":"2023-03-16T03:43:10Z","title":"Plant Disease Detection using Region-Based Convolutional Neural Network","summary":"  Agriculture plays an important role in the food and economy of Bangladesh.\nThe rapid growth of population over the years also has increased the demand for\nfood production. One of the major reasons behind low crop production is\nnumerous bacteria, virus and fungal plant diseases. Early detection of plant\ndiseases and proper usage of pesticides and fertilizers are vital for\npreventing the diseases and boost the yield. Most of the farmers use\ngeneralized pesticides and fertilizers in the entire fields without\nspecifically knowing the condition of the plants. Thus the production cost\noftentimes increases, and, not only that, sometimes this becomes detrimental to\nthe yield. Deep Learning models are found to be very effective to automatically\ndetect plant diseases from images of plants, thereby reducing the need for\nhuman specialists. This paper aims at building a lightweight deep learning\nmodel for predicting leaf disease in tomato plants. By modifying the\nregion-based convolutional neural network, we design an efficient and effective\nmodel that demonstrates satisfactory empirical performance on a benchmark\ndataset. Our proposed model can easily be deployed in a larger system where\ndrones take images of leaves and these images will be fed into our model to\nknow the health condition.\n","authors":["Hasin Rehana","Muhammad Ibrahim","Md. Haider Ali"],"pdf_url":"https://arxiv.org/pdf/2303.09063v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2303.09062v1","updated":"2023-03-16T03:38:08Z","published":"2023-03-16T03:38:08Z","title":"Knowledge Transfer for Pseudo-code Generation from Low Resource\n  Programming Language","summary":"  Generation of pseudo-code descriptions of legacy source code for software\nmaintenance is a manually intensive task. Recent encoder-decoder language\nmodels have shown promise for automating pseudo-code generation for high\nresource programming languages such as C++, but are heavily reliant on the\navailability of a large code-pseudocode corpus. Soliciting such pseudocode\nannotations for codes written in legacy programming languages (PL) is a time\nconsuming and costly affair requiring a thorough understanding of the source\nPL. In this paper, we focus on transferring the knowledge acquired by the\ncode-to-pseudocode neural model trained on a high resource PL (C++) using\nparallel code-pseudocode data. We aim to transfer this knowledge to a legacy PL\n(C) with no PL-pseudocode parallel data for training. To achieve this, we\nutilize an Iterative Back Translation (IBT) approach with a novel test-cases\nbased filtration strategy, to adapt the trained C++-to-pseudocode model to\nC-to-pseudocode model. We observe an improvement of 23.27% in the success rate\nof the generated C codes through back translation, over the successive IBT\niteration, illustrating the efficacy of our approach.\n","authors":["Ankita Sontakke","Kanika Kalra","Manasi Patwardhan","Lovekesh Vig","Raveendra Kumar Medicherla","Ravindra Naik","Shrishti Pradhan"],"pdf_url":"https://arxiv.org/pdf/2303.09062v1.pdf","comment":"11 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2210.02181v2","updated":"2023-03-16T03:28:55Z","published":"2022-10-04T15:46:53Z","title":"DISCOVER: Deep identification of symbolically concise open-form PDEs via\n  enhanced reinforcement-learning","summary":"  The working mechanisms of complex natural systems tend to abide by concise\nand profound partial differential equations (PDEs). Methods that directly mine\nequations from data are called PDE discovery, which reveals consistent physical\nlaws and facilitates our adaptive interaction with the natural world. In this\npaper, an enhanced deep reinforcement-learning framework is proposed to uncover\nsymbolically concise open-form PDEs with little prior knowledge. Particularly,\nbased on a symbol library of basic operators and operands, a structure-aware\nrecurrent neural network agent is designed and seamlessly combined with the\nsparse regression method to generate concise and open-form PDE expressions. All\nof the generated PDEs are evaluated by a meticulously designed reward function\nby balancing fitness to data and parsimony, and updated by the model-based\nreinforcement learning in an efficient way. Customized constraints and\nregulations are formulated to guarantee the rationality of PDEs in terms of\nphysics and mathematics. The experiments demonstrate that our framework is\ncapable of mining open-form governing equations of several dynamic systems,\neven with compound equation terms, fractional structure, and high-order\nderivatives, with excellent efficiency. Without the need for prior knowledge,\nthis method shows great potential for knowledge discovery in more complicated\ncircumstances with exceptional efficiency and scalability.\n","authors":["Mengge Du","Yuntian Chen","Dongxiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.02181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14594v4","updated":"2023-03-16T03:21:22Z","published":"2022-11-26T15:35:36Z","title":"Direct-Effect Risk Minimization for Domain Generalization","summary":"  We study the problem of out-of-distribution (o.o.d.) generalization where\nspurious correlations of attributes vary across training and test domains. This\nis known as the problem of correlation shift and has posed concerns on the\nreliability of machine learning. In this work, we introduce the concepts of\ndirect and indirect effects from causal inference to the domain generalization\nproblem. We argue that models that learn direct effects minimize the worst-case\nrisk across correlation-shifted domains. To eliminate the indirect effects, our\nalgorithm consists of two stages: in the first stage, we learn an\nindirect-effect representation by minimizing the prediction error of domain\nlabels using the representation and the class labels; in the second stage, we\nremove the indirect effects learned in the first stage by matching each data\nwith another data of similar indirect-effect representation but of different\nclass labels in the training and validation phase. Our approach is shown to be\ncompatible with existing methods and improve the generalization performance of\nthem on correlation-shifted datasets. Experiments on 5 correlation-shifted\ndatasets and the DomainBed benchmark verify the effectiveness of our approach.\n","authors":["Yuhui Li","Zejia Wu","Chao Zhang","Hongyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.14594v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06537v6","updated":"2023-03-16T03:14:42Z","published":"2021-10-13T07:19:47Z","title":"Well-classified Examples are Underestimated in Classification with Deep\n  Neural Networks","summary":"  The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n","authors":["Guangxiang Zhao","Wenkai Yang","Xuancheng Ren","Lei Li","Yunfang Wu","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2110.06537v6.pdf","comment":"Accepted by AAAI 2022; 18 pages, 11 figures, 13 tables"},{"id":"http://arxiv.org/abs/2112.10953v2","updated":"2023-03-16T03:13:43Z","published":"2021-12-21T03:03:03Z","title":"An adaptation of InfoMap to absorbing random walks using\n  absorption-scaled graphs","summary":"  InfoMap is a popular approach for detecting densely connected \"communities\"\nof nodes in networks. To detect such communities, InfoMap uses random walks and\nideas from information theory. Motivated by the dynamics of disease spread on\nnetworks, whose nodes may have heterogeneous disease-removal rates, we adapt\nInfoMap to absorbing random walks. To do this, we use absorption-scaled graphs,\nin which the edge weights are scaled according to absorption rates, along with\nMarkov time sweeping. One of our adaptations of InfoMap converges to the\nstandard version of InfoMap in the limit in which the node-absorption rates\napproach $0$. The community structure that we obtain using our adaptations of\nInfoMap can differ markedly from the community structure that one detects using\nmethods that do not take node-absorption rates into account. Additionally, we\ndemonstrate that the community structure that is induced by local dynamics can\nhave important implications for susceptible-infected-recovered (SIR) dynamics\non ring-lattice networks. For example, we find situations in which the outbreak\nduration is maximized when a moderate number of nodes have large\nnode-absorption rates.\n","authors":["Esteban Vargas Bernal","Mason A. Porter","Joseph H. Tien"],"pdf_url":"https://arxiv.org/pdf/2112.10953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1905.10003v4","updated":"2023-03-16T03:10:40Z","published":"2019-05-24T02:29:49Z","title":"Sequential Gaussian Processes for Online Learning of Nonstationary\n  Functions","summary":"  Many machine learning problems can be framed in the context of estimating\nfunctions, and often these are time-dependent functions that are estimated in\nreal-time as observations arrive. Gaussian processes (GPs) are an attractive\nchoice for modeling real-valued nonlinear functions due to their flexibility\nand uncertainty quantification. However, the typical GP regression model\nsuffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$\nwith respect to the number of observations; 2) Updating a GP model sequentially\nis not trivial; and 3) Covariance kernels typically enforce stationarity\nconstraints on the function, while GPs with non-stationary covariance kernels\nare often intractable to use in practice. To overcome these issues, we propose\na sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture\nnon-stationary behavior while allowing for online, distributed inference. Our\napproach empirically improves performance over state-of-the-art methods for\nonline GP estimation in the presence of non-stationarity in time-series data.\nTo demonstrate the utility of our proposed online Gaussian process\nmixture-of-experts approach in applied settings, we show that we can\nsucessfully implement an optimization algorithm using online Gaussian process\nbandits.\n","authors":["Michael Minyi Zhang","Bianca Dumitrascu","Sinead A. Williamson","Barbara E. Engelhardt"],"pdf_url":"https://arxiv.org/pdf/1905.10003v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05735v3","updated":"2023-03-16T02:59:04Z","published":"2023-03-10T06:44:49Z","title":"Hardware Acceleration of Neural Graphics","summary":"  Rendering and inverse-rendering algorithms that drive conventional computer\ngraphics have recently been superseded by neural representations (NR). NRs have\nrecently been used to learn the geometric and the material properties of the\nscenes and use the information to synthesize photorealistic imagery, thereby\npromising a replacement for traditional rendering algorithms with scalable\nquality and predictable performance. In this work we ask the question: Does\nneural graphics (NG) need hardware support? We studied representative NG\napplications showing that, if we want to render 4k res. at 60FPS there is a gap\nof 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,\nthere is an even larger gap of 2-4 OOM between the desired performance and the\nrequired system power. We identify that the input encoding and the MLP kernels\nare the performance bottlenecks, consuming 72%,60% and 59% of application time\nfor multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,\nrespectively. We propose a NG processing cluster, a scalable and flexible\nhardware architecture that directly accelerates the input encoding and MLP\nkernels through dedicated engines and supports a wide range of NG applications.\nWe also accelerate the rest of the kernels by fusing them together in Vulkan,\nwhich leads to 9.94X kernel-level performance improvement compared to un-fused\nimplementation of the pre-processing and the post-processing kernels. Our\nresults show that, NGPC gives up to 58X end-to-end application-level\nperformance improvement, for multi res. hashgrid encoding on average across the\nfour NG applications, the performance benefits are 12X,20X,33X and 39X for the\nscaling factor of 8,16,32 and 64, respectively. Our results show that with\nmulti res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS\nfor NeRF and 8k res. at 120FPS for all our other NG applications.\n","authors":["Muhammad Husnain Mubarik","Ramakrishna Kanungo","Tobias Zirr","Rakesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.05735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02477v4","updated":"2023-03-16T02:48:57Z","published":"2023-02-05T20:29:53Z","title":"Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for\n  Parkinson Disease Treatment","summary":"  Deep brain stimulation (DBS) has shown great promise toward treating motor\nsymptoms caused by Parkinson's disease (PD), by delivering electrical pulses to\nthe Basal Ganglia (BG) region of the brain. However, DBS devices approved by\nthe U.S. Food and Drug Administration (FDA) can only deliver continuous DBS\n(cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces\nbattery lifetime of the device, cannot adapt treatment dynamically for\nactivity, and may cause significant side-effects (e.g., gait impairment). In\nthis work, we introduce an offline reinforcement learning (RL) framework,\nallowing the use of past clinical data to train an RL policy to adjust the\nstimulation amplitude in real time, with the goal of reducing energy use while\nmaintaining the same level of treatment (i.e., control) efficacy as cDBS.\nMoreover, clinical protocols require the safety and performance of such RL\ncontrollers to be demonstrated ahead of deployments in patients. Thus, we also\nintroduce an offline policy evaluation (OPE) method to estimate the performance\nof RL policies using historical data, before deploying them on patients. We\nevaluated our framework on four PD patients equipped with the RC+S DBS system,\nemploying the RL controllers during monthly clinical visits, with the overall\ncontrol efficacy evaluated by severity of symptoms (i.e., bradykinesia and\ntremor), changes in PD biomakers (i.e., local field potentials), and patient\nratings. The results from clinical experiments show that our RL-based\ncontroller maintains the same level of control efficacy as cDBS, but with\nsignificantly reduced stimulation energy. Further, the OPE method is shown\neffective in accurately estimating and ranking the expected returns of RL\ncontrollers.\n","authors":["Qitong Gao","Stephen L. Schimdt","Afsana Chowdhury","Guangyu Feng","Jennifer J. Peters","Katherine Genty","Warren M. Grill","Dennis A. Turner","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2302.02477v4.pdf","comment":"Accepted to International Conference on Cyber Physical Systems\n  (ICCPS) 2023"},{"id":"http://arxiv.org/abs/2303.09051v1","updated":"2023-03-16T02:47:59Z","published":"2023-03-16T02:47:59Z","title":"Robust Evaluation of Diffusion-Based Adversarial Purification","summary":"  We question the current evaluation practice on diffusion-based purification\nmethods. Diffusion-based purification methods aim to remove adversarial effects\nfrom an input data point at test time. The approach gains increasing attention\nas an alternative to adversarial training due to the disentangling between\ntraining and testing. Well-known white-box attacks are often employed to\nmeasure the robustness of the purification. However, it is unknown whether\nthese attacks are the most effective for the diffusion-based purification since\nthe attacks are often tailored for adversarial training. We analyze the current\npractices and provide a new guideline for measuring the robustness of\npurification methods against adversarial attacks. Based on our analysis, we\nfurther propose a new purification strategy showing competitive results against\nthe state-of-the-art adversarial training approaches.\n","authors":["Minjong Lee","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.09051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09048v1","updated":"2023-03-16T02:36:02Z","published":"2023-03-16T02:36:02Z","title":"Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP\n  Platforms","summary":"  In this paper, we present a method for fine-tuning models trained on the Deep\nNoise Suppression (DNS) 2020 Challenge to improve their performance on Voice\nover Internet Protocol (VoIP) applications. Our approach involves adapting the\nDNS 2020 models to the specific acoustic characteristics of VoIP\ncommunications, which includes distortion and artifacts caused by compression,\ntransmission, and platform-specific processing. To this end, we propose a\nmulti-task learning framework for VoIP-DNS that jointly optimizes noise\nsuppression and VoIP-specific acoustics for speech enhancement. We evaluate our\napproach on a diverse VoIP scenarios and show that it outperforms both industry\nperformance and state-of-the-art methods for speech enhancement on VoIP\napplications. Our results demonstrate the potential of models trained on\nDNS-2020 to be improved and tailored to different VoIP platforms using\nVoIP-DNS, whose findings have important applications in areas such as speech\nrecognition, voice assistants, and telecommunication.\n","authors":["Joseph Konan","Ojas Bhargave","Shikhar Agnihotri","Hojeong Lee","Ankit Shah","Shuo Han","Yunyang Zeng","Amanda Shu","Haohui Liu","Xuankai Chang","Hamza Khalid","Minseon Gwak","Kawon Lee","Minjeong Kim","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2303.09048v1.pdf","comment":"Under review at European Association for Signal Processing. 5 pages"},{"id":"http://arxiv.org/abs/2303.09045v1","updated":"2023-03-16T02:31:02Z","published":"2023-03-16T02:31:02Z","title":"Web and Mobile Platforms for Managing Elections based on IoT And Machine\n  Learning Algorithms","summary":"  The global pandemic situation has severely affected all countries. As a\nresult, almost all countries had to adjust to online technologies to continue\ntheir processes. In addition, Sri Lanka is yearly spending ten billion on\nelections. We have examined a proper way of minimizing the cost of hosting\nthese events online. To solve the existing problems and increase the time\npotency and cost reduction we have used IoT and ML-based technologies.\nIoT-based data will identify, register, and be used to secure from fraud, while\nML algorithms manipulate the election data and produce winning predictions,\nweather-based voters attendance, and election violence. All the data will be\nsaved in cloud computing and a standard database to store and access the data.\nThis study mainly focuses on four aspects of an E-voting system. The most\nfrequent problems across the world in E-voting are the security, accuracy, and\nreliability of the systems. E-government systems must be secured against\nvarious cyber-attacks and ensure that only authorized users can access\nvaluable, and sometimes sensitive information. Being able to access a system\nwithout passwords but using biometric details has been there for a while now,\nhowever, our proposed system has a different approach to taking the\ncredentials, processing, and combining the images, reformatting and producing\nthe output, and tracking. In addition, we ensure to enhance e-voting safety.\nWhile ML-based algorithms use different data sets and provide predictions in\nadvance.\n","authors":["G. M. I. K. Galagoda","W. M. C. A. Karunarathne","R. S. Bates","K. M. H. V. P. Gangathilaka","Kanishka Yapa","Erandika Gamage"],"pdf_url":"https://arxiv.org/pdf/2303.09045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09042v1","updated":"2023-03-16T02:25:51Z","published":"2023-03-16T02:25:51Z","title":"Embedding Theory of Reservoir Computing and Reducing Reservoir Network\n  Using Time Delays","summary":"  Reservoir computing (RC), a particular form of recurrent neural network, is\nunder explosive development due to its exceptional efficacy and high\nperformance in reconstruction or/and prediction of complex physical systems.\nHowever, the mechanism triggering such effective applications of RC is still\nunclear, awaiting deep and systematic exploration. Here, combining the delayed\nembedding theory with the generalized embedding theory, we rigorously prove\nthat RC is essentially a high dimensional embedding of the original input\nnonlinear dynamical system. Thus, using this embedding property, we unify into\na universal framework the standard RC and the time-delayed RC where we novelly\nintroduce time delays only into the network's output layer, and we further find\na trade-off relation between the time delays and the number of neurons in RC.\nBased on this finding, we significantly reduce the network size of RC for\nreconstructing and predicting some representative physical systems, and, more\nsurprisingly, only using a single neuron reservoir with time delays is\nsometimes sufficient for achieving those tasks.\n","authors":["Xing-Yue Duan","Xiong Ying","Si-Yang Leng","Jürgen Kurths","Wei Lin","Huan-Fei Ma"],"pdf_url":"https://arxiv.org/pdf/2303.09042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09041v1","updated":"2023-03-16T02:25:05Z","published":"2023-03-16T02:25:05Z","title":"A Multimodal Data-driven Framework for Anxiety Screening","summary":"  Early screening for anxiety and appropriate interventions are essential to\nreduce the incidence of self-harm and suicide in patients. Due to limited\nmedical resources, traditional methods that overly rely on physician expertise\nand specialized equipment cannot simultaneously meet the needs for high\naccuracy and model interpretability. Multimodal data can provide more objective\nevidence for anxiety screening to improve the accuracy of models. The large\namount of noise in multimodal data and the unbalanced nature of the data make\nthe model prone to overfitting. However, it is a non-differentiable problem\nwhen high-dimensional and multimodal feature combinations are used as model\ninputs and incorporated into model training. This causes existing anxiety\nscreening methods based on machine learning and deep learning to be\ninapplicable. Therefore, we propose a multimodal data-driven anxiety screening\nframework, namely MMD-AS, and conduct experiments on the collected health data\nof over 200 seafarers by smartphones. The proposed framework's feature\nextraction, dimension reduction, feature selection, and anxiety inference are\njointly trained to improve the model's performance. In the feature selection\nstep, a feature selection method based on the Improved Fireworks Algorithm is\nused to solve the non-differentiable problem of feature combination to remove\nredundant features and search for the ideal feature subset. The experimental\nresults show that our framework outperforms the comparison methods.\n","authors":["Haimiao Mo","Shuai Ding","Siu Cheung Hui"],"pdf_url":"https://arxiv.org/pdf/2303.09041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09033v1","updated":"2023-03-16T02:07:29Z","published":"2023-03-16T02:07:29Z","title":"Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling","summary":"  Most bandit algorithms assume that the reward variance or its upper bound is\nknown. While variance overestimation is usually safe and sound, it increases\nregret. On the other hand, an underestimated variance may lead to linear regret\ndue to committing early to a suboptimal arm. This motivated prior works on\nvariance-aware frequentist algorithms. We lay foundations for the Bayesian\nsetting. In particular, we study multi-armed bandits with known and\n\\emph{unknown heterogeneous reward variances}, and develop Thompson sampling\nalgorithms for both and bound their Bayes regret. Our regret bounds decrease\nwith lower reward variances, which make learning easier. The bound for unknown\nreward variances captures the effect of the prior on learning reward variances\nand is the first of its kind. Our experiments show the superiority of\nvariance-aware Bayesian algorithms and also highlight their robustness.\n","authors":["Aadirupa Saha","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2303.09033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09032v1","updated":"2023-03-16T02:05:16Z","published":"2023-03-16T02:05:16Z","title":"Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent\n  Reinforcement Learning","summary":"  Efficient exploration is critical in cooperative deep Multi-Agent\nReinforcement Learning (MARL). In this paper, we propose an exploration method\nthat efficiently encourages cooperative exploration based on the idea of the\ntheoretically justified tree search algorithm UCT (Upper Confidence bounds\napplied to Trees). The high-level intuition is that to perform optimism-based\nexploration, agents would achieve cooperative strategies if each agent's\noptimism estimate captures a structured dependency relationship with other\nagents. At each node (i.e., action) of the search tree, UCT performs\noptimism-based exploration using a bonus derived by conditioning on the\nvisitation count of its parent node. We provide a perspective to view MARL as\ntree search iterations and develop a method called Conditionally Optimistic\nExploration (COE). We assume agents take actions following a sequential order,\nand consider nodes at the same depth of the search tree as actions of one\nindividual agent. COE computes each agent's state-action value estimate with an\noptimistic bonus derived from the visitation count of the state and joint\nactions taken by agents up to the current agent. COE is adaptable to any value\ndecomposition method for centralized training with decentralized execution.\nExperiments across various cooperative MARL benchmarks show that COE\noutperforms current state-of-the-art exploration methods on hard-exploration\ntasks.\n","authors":["Xutong Zhao","Yangchen Pan","Chenjun Xiao","Sarath Chandar","Janarthanan Rajendran"],"pdf_url":"https://arxiv.org/pdf/2303.09032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09031v1","updated":"2023-03-16T02:02:18Z","published":"2023-03-16T02:02:18Z","title":"A Picture is Worth a Thousand Words: Language Models Plan from Pixels","summary":"  Planning is an important capability of artificial agents that perform\nlong-horizon tasks in real-world environments. In this work, we explore the use\nof pre-trained language models (PLMs) to reason about plan sequences from text\ninstructions in embodied visual environments. Prior PLM based approaches for\nplanning either assume observations are available in the form of text (e.g.,\nprovided by a captioning model), reason about plans from the instruction alone,\nor incorporate information about the visual environment in limited ways (such\nas a pre-trained affordance function). In contrast, we show that PLMs can\naccurately plan even when observations are directly encoded as input prompts\nfor the PLM. We show that this simple approach outperforms prior approaches in\nexperiments on the ALFWorld and VirtualHome benchmarks.\n","authors":["Anthony Z. Liu","Lajanugen Logeswaran","Sungryull Sohn","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2303.09031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01903v2","updated":"2023-03-16T01:49:29Z","published":"2023-03-03T13:05:15Z","title":"Prompting Large Language Models with Answer Heuristics for\n  Knowledge-based Visual Question Answering","summary":"  Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have sought to use a large language model (i.e.,\nGPT-3) as an implicit knowledge engine to acquire the necessary knowledge for\nanswering. Despite the encouraging results achieved by these methods, we argue\nthat they have not fully activated the capacity of GPT-3 as the provided input\ninformation is insufficient. In this paper, we present Prophet -- a\nconceptually simple framework designed to prompt GPT-3 with answer heuristics\nfor knowledge-based VQA. Specifically, we first train a vanilla VQA model on a\nspecific knowledge-based VQA dataset without external knowledge. After that, we\nextract two types of complementary answer heuristics from the model: answer\ncandidates and answer-aware examples. Finally, the two types of answer\nheuristics are encoded into the prompts to enable GPT-3 to better comprehend\nthe task thus enhancing its capacity. Prophet significantly outperforms all\nexisting state-of-the-art methods on two challenging knowledge-based VQA\ndatasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their\ntesting sets, respectively.\n","authors":["Zhenwei Shao","Zhou Yu","Meng Wang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2303.01903v2.pdf","comment":"Accepted to CVPR 2023, code available at\n  https://github.com/MILVLG/prophet"},{"id":"http://arxiv.org/abs/2303.09027v1","updated":"2023-03-16T01:43:18Z","published":"2023-03-16T01:43:18Z","title":"Learning Rewards to Optimize Global Performance Metrics in Deep\n  Reinforcement Learning","summary":"  When applying reinforcement learning (RL) to a new problem, reward\nengineering is a necessary, but often difficult and error-prone task a system\ndesigner has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL\nmethod that can optimize a global performance metric, which is supposed to be\navailable as part of the problem description. LR4GPM alternates between two\nphases: (1) learning a (possibly vector) reward function used to fit the\nperformance metric, and (2) training a policy to optimize an approximation of\nthis performance metric based on the learned rewards. Such RL training is not\nstraightforward since both the reward function and the policy are trained using\nnon-stationary data. To overcome this issue, we propose several training\ntricks. We demonstrate the efficiency of LR4GPM on several domains. Notably,\nLR4GPM outperforms the winner of a recent autonomous driving competition\norganized at DAI'2020.\n","authors":["Junqi Qian","Paul Weng","Chenmien Tan"],"pdf_url":"https://arxiv.org/pdf/2303.09027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06751v4","updated":"2023-03-16T01:29:57Z","published":"2023-02-13T23:25:55Z","title":"OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for\n  Experimental Science","summary":"  In many experiment-driven scientific domains, such as high-energy physics,\nmaterial science, and cosmology, high data rate experiments impose hard\nconstraints on data acquisition systems: collected data must either be\nindiscriminately stored for post-processing and analysis, thereby necessitating\nlarge storage capacity, or accurately filtered in real-time, thereby\nnecessitating low-latency processing. Deep neural networks, effective in other\nfiltering tasks, have not been widely employed in such data acquisition\nsystems, due to design and deployment difficulties. We present an open source,\nlightweight, compiler framework, without any proprietary dependencies, OpenHLS,\nbased on high-level synthesis techniques, for translating high-level\nrepresentations of deep neural networks to low-level representations, suitable\nfor deployment to near-sensor devices such as field-programmable gate arrays.\nWe evaluate OpenHLS on various workloads and present a case-study\nimplementation of a deep neural network for Bragg peak detection in the context\nof high-energy diffraction microscopy. We show OpenHLS is able to produce an\nimplementation of the network with a throughput 4.8 $\\mu$s/sample, which is\napproximately a 4$\\times$ improvement over the existing implementation\n","authors":["Maksim Levental","Arham Khan","Ryan Chard","Kazutomo Yoshii","Kyle Chard","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2302.06751v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08769v2","updated":"2023-03-16T01:19:06Z","published":"2023-02-17T23:25:57Z","title":"Prompting Large Language Models With the Socratic Method","summary":"  This paper presents a systematic approach to using the Socratic method in\ndeveloping prompt templates that effectively interact with large language\nmodels, including GPT-3. Various methods are examined, and those that yield\nprecise answers and justifications while fostering creativity and imagination\nto enhance creative writing are identified. Techniques such as {\\em\ndefinition}, {\\em elenchus}, {\\em dialectic}, {\\em maieutics}, {\\em\ngeneralization}, and {\\em counterfactual reasoning} are discussed for their\napplication in engineering prompt templates and their connections to inductive,\ndeductive, and abductive reasoning. Through examples, the effectiveness of\nthese dialogue and reasoning methods is demonstrated. An interesting\nobservation is made that when the task's goal and user intent are conveyed to\nGPT-3 via ChatGPT before the start of a dialogue, the large language model\nseems to connect to the external context expressed in the intent and perform\nmore effectively.\n","authors":["Edward Y. Chang"],"pdf_url":"https://arxiv.org/pdf/2303.08769v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.04910v2","updated":"2023-03-16T01:09:08Z","published":"2023-03-08T22:00:15Z","title":"Baldur: Whole-Proof Generation and Repair with Large Language Models","summary":"  Formally verifying software properties is a highly desirable but\nlabor-intensive task. Recent work has developed methods to automate formal\nverification using proof assistants, such as Coq and Isabelle/HOL, e.g., by\ntraining a model to predict one proof step at a time, and using that model to\nsearch through the space of possible proofs. This paper introduces a new method\nto automate formal verification: We use large language models, trained on\nnatural language text and code and fine-tuned on proofs, to generate whole\nproofs for theorems at once, rather than one step at a time. We combine this\nproof generation model with a fine-tuned repair model to repair generated\nproofs, further increasing proving power. As its main contributions, this paper\ndemonstrates for the first time that: (1) Whole-proof generation using\ntransformers is possible and is as effective as search-based techniques without\nrequiring costly search. (2) Giving the learned model additional context, such\nas a prior failed proof attempt and the ensuing error message, results in proof\nrepair and further improves automated proof generation. (3) We establish a new\nstate of the art for fully automated proof synthesis. We reify our method in a\nprototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL\ntheorems and their proofs. In addition to empirically showing the effectiveness\nof whole-proof generation, repair, and added context, we show that Baldur\nimproves on the state-of-the-art tool, Thor, by automatically generating proofs\nfor an additional 8.7% of the theorems. Together, Baldur and Thor can prove\n65.7% of the theorems fully automatically. This paper paves the way for new\nresearch into using large language models for automating formal verification.\n","authors":["Emily First","Markus N. Rabe","Talia Ringer","Yuriy Brun"],"pdf_url":"https://arxiv.org/pdf/2303.04910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09013v1","updated":"2023-03-16T00:58:50Z","published":"2023-03-16T00:58:50Z","title":"Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using\n  Deep Q-Network Reinforcement Learning","summary":"  For the purpose of inspecting power plants, autonomous robots can be built\nusing reinforcement learning techniques. The method replicates the environment\nand employs a simple reinforcement learning (RL) algorithm. This strategy might\nbe applied in several sectors, including the electricity generation sector. A\npre-trained model with perception, planning, and action is suggested by the\nresearch. To address optimization problems, such as the Unmanned Aerial Vehicle\n(UAV) navigation problem, Deep Q-network (DQN), a reinforcement learning-based\nframework that Deepmind launched in 2015, incorporates both deep learning and\nQ-learning. To overcome problems with current procedures, the research proposes\na power plant inspection system incorporating UAV autonomous navigation and DQN\nreinforcement learning. These training processes set reward functions with\nreference to states and consider both internal and external effect factors,\nwhich distinguishes them from other reinforcement learning training techniques\nnow in use. The key components of the reinforcement learning segment of the\ntechnique, for instance, introduce states such as the simulation of a wind\nfield, the battery charge level of an unmanned aerial vehicle, the height the\nUAV reached, etc. The trained model makes it more likely that the inspection\nstrategy will be applied in practice by enabling the UAV to move around on its\nown in difficult environments. The average score of the model converges to\n9,000. The trained model allowed the UAV to make the fewest number of rotations\nnecessary to go to the target point.\n","authors":["Haoran Guan"],"pdf_url":"https://arxiv.org/pdf/2303.09013v1.pdf","comment":"Submitted to the International Conference on Artificial Intelligence,\n  Robotics, and Control (AIRC 2023)"},{"id":"http://arxiv.org/abs/2303.09007v1","updated":"2023-03-16T00:43:46Z","published":"2023-03-16T00:43:46Z","title":"Machine Learning for Flow Cytometry Data Analysis","summary":"  Flow cytometry mainly used for detecting the characteristics of a number of\nbiochemical substances based on the expression of specific markers in cells. It\nis particularly useful for detecting membrane surface receptors, antigens,\nions, or during DNA/RNA expression. Not only can it be employed as a biomedical\nresearch tool for recognising distinctive types of cells in mixed populations,\nbut it can also be used as a diagnostic tool for classifying abnormal cell\npopulations connected with disease. Modern flow cytometers can rapidly analyse\ntens of thousands of cells at the same time while also measuring multiple\nparameters from a single cell. However, the rapid development of flow\ncytometers makes it challenging for conventional analysis methods to interpret\nflow cytometry data. Researchers need to be able to distinguish\ninteresting-looking cell populations manually in multi-dimensional data\ncollected from millions of cells. Thus, it is essential to find a robust\napproach for analysing flow cytometry data automatically, specifically in\nidentifying cell populations automatically. This thesis mainly concerns\ndiscover the potential shortcoming of current automated-gating algorithms in\nboth real datasets and synthetic datasets. Three representative automated\nclustering algorithms are selected to be applied, compared and evaluated by\ncompletely and partially automated gating. A subspace clustering ProClus also\nimplemented in this thesis. The performance of ProClus in flow cytometry is not\nwell, but it is still a useful algorithm to detect noise.\n","authors":["Yanhua Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09007v1.pdf","comment":"MSc thesis"},{"id":"http://arxiv.org/abs/2303.09005v1","updated":"2023-03-16T00:23:20Z","published":"2023-03-16T00:23:20Z","title":"Conditional Synthetic Food Image Generation","summary":"  Generative Adversarial Networks (GAN) have been widely investigated for image\nsynthesis based on their powerful representation learning ability. In this\nwork, we explore the StyleGAN and its application of synthetic food image\ngeneration. Despite the impressive performance of GAN for natural image\ngeneration, food images suffer from high intra-class diversity and inter-class\nsimilarity, resulting in overfitting and visual artifacts for synthetic images.\nTherefore, we aim to explore the capability and improve the performance of GAN\nmethods for food image generation. Specifically, we first choose StyleGAN3 as\nthe baseline method to generate synthetic food images and analyze the\nperformance. Then, we identify two issues that can cause performance\ndegradation on food images during the training phase: (1) inter-class feature\nentanglement during multi-food classes training and (2) loss of high-resolution\ndetail during image downsampling. To address both issues, we propose to train\none food category at a time to avoid feature entanglement and leverage image\npatches cropped from high-resolution datasets to retain fine details. We\nevaluate our method on the Food-101 dataset and show improved quality of\ngenerated synthetic food images compared with the baseline. Finally, we\ndemonstrate the great potential of improving the performance of downstream\ntasks, such as food image classification by including high-quality synthetic\ntraining samples in the data augmentation.\n","authors":["Wenjin Fu","Yue Han","Jiangpeng He","Sriram Baireddy","Mridul Gupta","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.09005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08996v1","updated":"2023-03-16T00:00:52Z","published":"2023-03-16T00:00:52Z","title":"Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion\n  Problems","summary":"  Effective investment planning decisions are crucial to ensure cyber-physical\ninfrastructures satisfy performance requirements over an extended time horizon.\nComputing these decisions often requires solving Capacity Expansion Problems\n(CEPs). In the context of regional-scale energy systems, these problems are\nprohibitively expensive to solve due to large network sizes, heterogeneous node\ncharacteristics, and a large number of operational periods. To maintain\ntractability, traditional approaches aggregate network nodes and/or select a\nset of representative time periods. Often, these reductions do not capture\nsupply-demand variations that crucially impact CEP costs and constraints,\nleading to suboptimal decisions. Here, we propose a novel graph convolutional\nautoencoder approach for spatio-temporal aggregation of a generic CEP with\nheterogeneous nodes (CEPHN). Our architecture leverages graph pooling to\nidentify nodes with similar characteristics and minimizes a multi-objective\nloss function. This loss function is tailored to induce desirable spatial and\ntemporal aggregations with regard to tractability and optimality. In\nparticular, the output of the graph pooling provides a spatial aggregation\nwhile clustering the low-dimensional encoded representations yields a temporal\naggregation. We apply our approach to generation expansion planning of a\ncoupled 88-node power and natural gas system in New England. The resulting\naggregation leads to a simpler CEPHN with 6 nodes and a small set of\nrepresentative days selected from one year. We evaluate aggregation outcomes\nover a range of hyperparameters governing the loss function and compare\nresulting upper bounds on the original problem with those obtained using\nbenchmark methods. We show that our approach provides upper bounds that are 33%\n(resp. 10%) lower those than obtained from benchmark spatial (resp. temporal)\naggregation approaches.\n","authors":["Aron Brenner","Rahman Khorramfar","Saurabh Amin"],"pdf_url":"https://arxiv.org/pdf/2303.08996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02110v3","updated":"2023-03-16T23:55:25Z","published":"2023-03-03T17:51:08Z","title":"Need for Objective Task-based Evaluation of Deep Learning-Based\n  Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT","summary":"  Artificial intelligence-based methods have generated substantial interest in\nnuclear medicine. An area of significant interest has been using deep-learning\n(DL)-based approaches for denoising images acquired with lower doses, shorter\nacquisition times, or both. Objective evaluation of these approaches is\nessential for clinical application. DL-based approaches for denoising\nnuclear-medicine images have typically been evaluated using fidelity-based\nfigures of merit (FoMs) such as RMSE and SSIM. However, these images are\nacquired for clinical tasks and thus should be evaluated based on their\nperformance in these tasks. Our objectives were to (1) investigate whether\nevaluation with these FoMs is consistent with objective clinical-task-based\nevaluation; (2) provide a theoretical analysis for determining the impact of\ndenoising on signal-detection tasks; (3) demonstrate the utility of virtual\nclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a\nDL-based method for denoising myocardial perfusion SPECT (MPS) images was\nconducted. The impact of DL-based denoising was evaluated using fidelity-based\nFoMs and AUC, which quantified performance on detecting perfusion defects in\nMPS images as obtained using a model observer with anthropomorphic channels.\nBased on fidelity-based FoMs, denoising using the considered DL-based method\nled to significantly superior performance. However, based on ROC analysis,\ndenoising did not improve, and in fact, often degraded detection-task\nperformance. The results motivate the need for objective task-based evaluation\nof DL-based denoising approaches. Further, this study shows how VCTs provide a\nmechanism to conduct such evaluations using VCTs. Finally, our theoretical\ntreatment reveals insights into the reasons for the limited performance of the\ndenoising approach.\n","authors":["Zitong Yu","Md Ashequr Rahman","Richard Laforest","Thomas H. Schindler","Robert J. Gropler","Richard L. Wahl","Barry A. Siegel","Abhinav K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.02110v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09564v1","updated":"2023-03-16T23:48:00Z","published":"2023-03-16T23:48:00Z","title":"TypeT5: Seq2seq Type Inference using Static Analysis","summary":"  There has been growing interest in automatically predicting missing type\nannotations in programs written in Python and JavaScript. While prior methods\nhave achieved impressive accuracy when predicting the most common types, they\noften perform poorly on rare or complex types. In this paper, we present a new\ntype inference method that treats type prediction as a code infilling task by\nleveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for\ncode. Our method uses static analysis to construct dynamic contexts for each\ncode element whose type signature is to be predicted by the model. We also\npropose an iterative decoding scheme that incorporates previous type\npredictions in the model's input context, allowing information exchange between\nrelated code elements. Our evaluation shows that the proposed approach, TypeT5,\nnot only achieves a higher overall accuracy (particularly on rare and complex\ntypes) but also produces more coherent results with fewer type errors -- while\nenabling easy user intervention.\n","authors":["Jiayi Wei","Greg Durrett","Isil Dillig"],"pdf_url":"https://arxiv.org/pdf/2303.09564v1.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09692v1","updated":"2023-03-16T23:36:57Z","published":"2023-03-16T23:36:57Z","title":"Probabilistic relations for modelling epistemic and aleatoric\n  uncertainty: its semantics and automated reasoning with theorem proving","summary":"  Probabilistic programming is a programming paradigm that combines general\ncomputer programming, statistical inference, and formal semantics to help\nsystems to made decisions when facing uncertainty. Probabilistic programs are\nubiquitous and believed to have a major impact on machine intelligence. While\nmany probabilistic algorithms have been used in practice in different domains,\ntheir automated verification based on formal semantics is still a relatively\nnew research area. In the last two decades, it has been attracting a lot of\ninterest. Many challenges, however, still remain. Our work presented in this\npaper, probabilistic relations, takes a step into our vision to tackle these\nchallenges.\n  Our work in essence is based on Hehner's predicative probabilistic\nprogramming, but there are several obstacles to the wider adoption of his work.\nOur contributions here include (1) the formalisation of its syntax and\nsemantics by introducing an Iverson bracket notation to separate relations from\narithmetic; (2) the formalisation of relations using Unifying Theories of\nProgramming (UTP) and probabilities outside the brackets using summation over\nthe topological space of the real numbers; (3) the constructive semantics for\nprobabilistic loops using the Kleene's fixed point theorem; (4) the enrichment\nof its semantics from distributions to subdistributions and superdistributions\nin order to deal with the constructive semantics; (5) the unique fixed point\ntheorem to largely simplify the reasoning about probabilistic loops; and (6)\nthe mechanisation of our theory in Isabelle/UTP, an implementation of UTP in\nIsabelle/HOL, for automated reasoning using theorem proving.\n  We demonstrate six interesting examples, and among them, one is about robot\nlocalisation, two are classification problems in machine learning, and two\ncontain probabilistic loops.\n","authors":["Kangfeng Ye","Jim Woodcock","Simon Foster"],"pdf_url":"https://arxiv.org/pdf/2303.09692v1.pdf","comment":"36 pages and 5 figures"},{"id":"http://arxiv.org/abs/2303.07321v2","updated":"2023-03-16T23:01:54Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03929v3","updated":"2023-03-16T22:57:25Z","published":"2022-11-08T00:59:05Z","title":"Comparative layer-wise analysis of self-supervised speech models","summary":"  Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive successes on downstream tasks, we still have a\nlimited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.\n","authors":["Ankita Pasad","Bowen Shi","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2211.03929v3.pdf","comment":"Accepted to ICASSP 2023. Code:\n  https://github.com/ankitapasad/layerwise-analysis"},{"id":"http://arxiv.org/abs/2303.09669v1","updated":"2023-03-16T22:08:41Z","published":"2023-03-16T22:08:41Z","title":"Predicting discrete-time bifurcations with deep learning","summary":"  Many natural and man-made systems are prone to critical transitions -- abrupt\nand potentially devastating changes in dynamics. Deep learning classifiers can\nprovide an early warning signal (EWS) for critical transitions by learning\ngeneric features of bifurcations (dynamical instabilities) from large simulated\ntraining data sets. So far, classifiers have only been trained to predict\ncontinuous-time bifurcations, ignoring rich dynamics unique to discrete-time\nbifurcations. Here, we train a deep learning classifier to provide an EWS for\nthe five local discrete-time bifurcations of codimension-1. We test the\nclassifier on simulation data from discrete-time models used in physiology,\neconomics and ecology, as well as experimental data of spontaneously beating\nchick-heart aggregates that undergo a period-doubling bifurcation. The\nclassifier outperforms commonly used EWS under a wide range of noise\nintensities and rates of approach to the bifurcation. It also predicts the\ncorrect bifurcation in most cases, with particularly high accuracy for the\nperiod-doubling, Neimark-Sacker and fold bifurcations. Deep learning as a tool\nfor bifurcation prediction is still in its nascence and has the potential to\ntransform the way we monitor systems for critical transitions.\n","authors":["Thomas M. Bury","Daniel Dylewsky","Chris T. Bauch","Madhur Anand","Leon Glass","Alvin Shrier","Gil Bub"],"pdf_url":"https://arxiv.org/pdf/2303.09669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09664v1","updated":"2023-03-16T21:47:48Z","published":"2023-03-16T21:47:48Z","title":"Tribe or Not? Critical Inspection of Group Differences Using TribalGram","summary":"  With the rise of AI and data mining techniques, group profiling and\ngroup-level analysis have been increasingly used in many domains including\npolicy making and direct marketing. In some cases, the statistics extracted\nfrom data may provide insights to a group's shared characteristics; in others,\nthe group-level analysis can lead to problems including stereotyping and\nsystematic oppression. How can analytic tools facilitate a more conscientious\nprocess in group analysis? In this work, we identify a set of accountable group\nanalytics design guidelines to explicate the needs for group differentiation\nand preventing overgeneralization of a group. Following the design guidelines,\nwe develop TribalGram, a visual analytic suite that leverages interpretable\nmachine learning algorithms and visualization to offer inference assessment,\nmodel explanation, data corroboration, and sense-making. Through the interviews\nwith domain experts, we showcase how our design and tools can bring a richer\nunderstanding of \"groups\" mined from the data.\n","authors":["Yongsu Ahn","Muheng Yan","Yu-Ru Lin","Wen-Ting Chung","Rebecca Hwa"],"pdf_url":"https://arxiv.org/pdf/2303.09664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05925v4","updated":"2023-03-16T21:32:56Z","published":"2022-12-08T20:40:57Z","title":"CausalEGM: a general causal inference framework by encoding generative\n  modeling","summary":"  Although understanding and characterizing causal effects have become\nessential in observational studies, it is challenging when the confounders are\nhigh-dimensional. In this article, we develop a general framework\n$\\textit{CausalEGM}$ for estimating causal effects by encoding generative\nmodeling, which can be applied in both binary and continuous treatment\nsettings. Under the potential outcome framework with unconfoundedness, we\nestablish a bidirectional transformation between the high-dimensional\nconfounders space and a low-dimensional latent space where the density is known\n(e.g., multivariate normal distribution). Through this, CausalEGM\nsimultaneously decouples the dependencies of confounders on both treatment and\noutcome and maps the confounders to the low-dimensional latent space. By\nconditioning on the low-dimensional latent features, CausalEGM can estimate the\ncausal effect for each individual or the average causal effect within a\npopulation. Our theoretical analysis shows that the excess risk for CausalEGM\ncan be bounded through empirical process theory. Under an assumption on\nencoder-decoder networks, the consistency of the estimate can be guaranteed. In\na series of experiments, CausalEGM demonstrates superior performance over\nexisting methods for both binary and continuous treatments. Specifically, we\nfind CausalEGM to be substantially more powerful than competing methods in the\npresence of large sample sizes and high dimensional confounders. The software\nof CausalEGM is freely available at https://github.com/SUwonglab/CausalEGM.\n","authors":["Qiao Liu","Zhongren Chen","Wing Hung Wong"],"pdf_url":"https://arxiv.org/pdf/2212.05925v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09658v1","updated":"2023-03-16T21:31:55Z","published":"2023-03-16T21:31:55Z","title":"Energy Management of Multi-mode Plug-in Hybrid Electric Vehicle using\n  Multi-agent Deep Reinforcement Learning","summary":"  The recently emerging multi-mode plug-in hybrid electric vehicle (PHEV)\ntechnology is one of the pathways making contributions to decarbonization, and\nits energy management requires multiple-input and multiple-output (MIMO)\ncontrol. At the present, the existing methods usually decouple the MIMO control\ninto single-output (MISO) control and can only achieve its local optimal\nperformance. To optimize the multi-mode vehicle globally, this paper studies a\nMIMO control method for energy management of the multi-mode PHEV based on\nmulti-agent deep reinforcement learning (MADRL). By introducing a relevance\nratio, a hand-shaking strategy is proposed to enable two learning agents to\nwork collaboratively under the MADRL framework using the deep deterministic\npolicy gradient (DDPG) algorithm. Unified settings for the DDPG agents are\nobtained through a sensitivity analysis of the influencing factors to the\nlearning performance. The optimal working mode for the hand-shaking strategy is\nattained through a parametric study on the relevance ratio. The advantage of\nthe proposed energy management method is demonstrated on a software-in-the-loop\ntesting platform. The result of the study indiates that learning rate of the\nDDPG agents is the greatest factor in learning performance. Using the unified\nDDPG settings and a relevance ratio of 0.2, the proposed MADRL method can save\nup to 4% energy compared to the single-agent method.\n","authors":["Min Hua","Cetengfei Zhang","Fanggang Zhang","Zhi Li","Xiaoli Yu","Hongming Xu","Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00055v2","updated":"2023-03-16T21:31:45Z","published":"2023-02-28T19:52:26Z","title":"Learning time-scales in two-layers neural networks","summary":"  Gradient-based learning in multi-layer neural networks displays a number of\nstriking features. In particular, the decrease rate of empirical risk is\nnon-monotone even after averaging over large batches. Long plateaus in which\none observes barely any progress alternate with intervals of rapid decrease.\nThese successive phases of learning often take place on very different time\nscales. Finally, models learnt in an early phase are typically `simpler' or\n`easier to learn' although in a way that is difficult to formalize.\n  Although theoretical explanations of these phenomena have been put forward,\neach of them captures at best certain specific regimes. In this paper, we study\nthe gradient flow dynamics of a wide two-layer neural network in\nhigh-dimension, when data are distributed according to a single-index model\n(i.e., the target function depends on a one-dimensional projection of the\ncovariates). Based on a mixture of new rigorous results, non-rigorous\nmathematical derivations, and numerical simulations, we propose a scenario for\nthe learning dynamics in this setting. In particular, the proposed evolution\nexhibits separation of timescales and intermittency. These behaviors arise\nnaturally because the population gradient flow can be recast as a singularly\nperturbed dynamical system.\n","authors":["Raphaël Berthier","Andrea Montanari","Kangjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.00055v2.pdf","comment":"54 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09657v1","updated":"2023-03-16T21:29:50Z","published":"2023-03-16T21:29:50Z","title":"ESCAPE: Countering Systematic Errors from Machine's Blind Spots via\n  Interactive Visual Analysis","summary":"  Classification models learn to generalize the associations between data\nsamples and their target classes. However, researchers have increasingly\nobserved that machine learning practice easily leads to systematic errors in AI\napplications, a phenomenon referred to as AI blindspots. Such blindspots arise\nwhen a model is trained with training samples (e.g., cat/dog classification)\nwhere important patterns (e.g., black cats) are missing or\nperiphery/undesirable patterns (e.g., dogs with grass background) are\nmisleading towards a certain class. Even more sophisticated techniques cannot\nguarantee to capture, reason about, and prevent the spurious associations. In\nthis work, we propose ESCAPE, a visual analytic system that promotes a\nhuman-in-the-loop workflow for countering systematic errors. By allowing human\nusers to easily inspect spurious associations, the system facilitates users to\nspontaneously recognize concepts associated misclassifications and evaluate\nmitigation strategies that can reduce biased associations. We also propose two\nstatistical approaches, relative concept association to better quantify the\nassociations between a concept and instances, and debias method to mitigate\nspurious associations. We demonstrate the utility of our proposed ESCAPE system\nand statistical measures through extensive evaluation including quantitative\nexperiments, usage scenarios, expert interviews, and controlled user\nexperiments.\n","authors":["Yongsu Ahn","Yu-Ru Lin","Panpan Xu","Zeng Dai"],"pdf_url":"https://arxiv.org/pdf/2303.09657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03008v2","updated":"2023-03-16T20:58:37Z","published":"2023-02-06T18:43:10Z","title":"LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease\n  Assessment from Fundus Images","summary":"  Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the\nleading cause of dementia. Early diagnosis is critical for patients to benefit\nfrom potential intervention and treatment. The retina has been hypothesized as\na diagnostic site for AD detection owing to its anatomical connection with the\nbrain. Developed AI models for this purpose have yet to provide a rational\nexplanation about the decision and neither infer the stage of disease's\nprogression. Along this direction, we propose a novel model-agnostic\nexplainable-AI framework, called Granular Neuron-level Explainer (LAVA), an\ninterpretation prototype that probes into intermediate layers of the\nConvolutional Neural Network (CNN) models to assess the AD continuum directly\nfrom the retinal imaging without longitudinal or clinical evaluation. This\nmethod is applied to validate the retinal vasculature as a biomarker and\ndiagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank\ncognitive tests and vascular morphological features suggest LAVA shows strong\npromise and effectiveness in identifying AD stages across the progression\ncontinuum.\n","authors":["Nooshin Yousefzadeh","Charlie Tran","Adolfo Ramirez-Zamora","Jinghua Chen","Ruogu Fang","My T. Thai"],"pdf_url":"https://arxiv.org/pdf/2302.03008v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.09642v1","updated":"2023-03-16T20:42:24Z","published":"2023-03-16T20:42:24Z","title":"SUD$^2$: Supervision by Denoising Diffusion Models for Image\n  Reconstruction","summary":"  Many imaging inverse problems$\\unicode{x2014}$such as image-dependent\nin-painting and dehazing$\\unicode{x2014}$are challenging because their forward\nmodels are unknown or depend on unknown latent parameters. While one can solve\nsuch problems by training a neural network with vast quantities of paired\ntraining data, such paired training data is often unavailable. In this paper,\nwe propose a generalized framework for training image reconstruction networks\nwhen paired training data is scarce. In particular, we demonstrate the ability\nof image denoising algorithms and, by extension, denoising diffusion models to\nsupervise network training in the absence of paired training data.\n","authors":["Matthew A. Chan","Sean I. Young","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2303.09642v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.09634v1","updated":"2023-03-16T20:28:36Z","published":"2023-03-16T20:28:36Z","title":"Causal Temporal Graph Convolutional Neural Networks (CTGCN)","summary":"  Many large-scale applications can be elegantly represented using graph\nstructures. Their scalability, however, is often limited by the domain\nknowledge required to apply them. To address this problem, we propose a novel\nCausal Temporal Graph Convolutional Neural Network (CTGCN). Our CTGCN\narchitecture is based on a causal discovery mechanism, and is capable of\ndiscovering the underlying causal processes. The major advantages of our\napproach stem from its ability to overcome computational scalability problems\nwith a divide and conquer technique, and from the greater explainability of\npredictions made using a causal model. We evaluate the scalability of our CTGCN\non two datasets to demonstrate that our method is applicable to large scale\nproblems, and show that the integration of causality into the TGCN architecture\nimproves prediction performance up to 40% over typical TGCN approach. Our\nresults are obtained without requiring additional domain knowledge, making our\napproach adaptable to various domains, specifically when little contextual\nknowledge is available.\n","authors":["Abigail Langbridge","Fearghal O'Donncha","Amadou Ba","Fabio Lorenzi","Christopher Lohse","Joern Ploennigs"],"pdf_url":"https://arxiv.org/pdf/2303.09634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05617v2","updated":"2023-03-16T20:28:12Z","published":"2023-03-09T23:11:52Z","title":"KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF\n  Grasp Synthesis on RGB-D input","summary":"  We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based\non keypoints. Keypoint-based grasp detector from image input has demonstrated\npromising results in the previous study, where the additional visual\ninformation provided by color images compensates for the noisy depth\nperception. However, it relies heavily on accurately predicting the location of\nkeypoints in the image space. In this paper, we devise a new grasp generation\nnetwork that reduces the dependency on precise keypoint estimation. Given an\nRGB-D input, our network estimates both the grasp pose from keypoint detection\nas well as scale towards the camera. We further re-design the keypoint output\nspace in order to mitigate the negative impact of keypoint prediction noise to\nPerspective-n-Point (PnP) algorithm. Experiments show that the proposed method\noutperforms the baseline by a large margin, validating the efficacy of our\napproach. Finally, despite trained on simple synthetic objects, our method\ndemonstrate sim-to-real capacity by showing competitive results in real-world\nrobot experiments.\n","authors":["Yiye Chen","Ruinian Xu","Yunzhi Lin","Hongyi Chen","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.05617v2.pdf","comment":"Submitted to IROS2023"},{"id":"http://arxiv.org/abs/2303.09630v1","updated":"2023-03-16T20:18:33Z","published":"2023-03-16T20:18:33Z","title":"Hyper-Reduced Autoencoders for Efficient and Accurate Nonlinear Model\n  Reductions","summary":"  Projection-based model order reduction on nonlinear manifolds has been\nrecently proposed for problems with slowly decaying Kolmogorov n-width such as\nadvection-dominated ones. These methods often use neural networks for manifold\nlearning and showcase improved accuracy over traditional linear\nsubspace-reduced order models. A disadvantage of the previously proposed\nmethods is the potential high computational costs of training the networks on\nhigh-fidelity solution snapshots. In this work, we propose and analyze a novel\nmethod that overcomes this disadvantage by training a neural network only on\nsubsampled versions of the high-fidelity solution snapshots. This method\ncoupled with collocation-based hyper-reduction and Gappy-POD allows for\nefficient and accurate surrogate models. We demonstrate the validity of our\napproach on a 2d Burgers problem.\n","authors":["Jorio Cocola","John Tencer","Francesco Rizzi","Eric Parish","Patrick Blonigan"],"pdf_url":"https://arxiv.org/pdf/2303.09630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09629v1","updated":"2023-03-16T20:16:45Z","published":"2023-03-16T20:16:45Z","title":"Online Reinforcement Learning in Periodic MDP","summary":"  We study learning in periodic Markov Decision Process (MDP), a special type\nof non-stationary MDP where both the state transition probabilities and reward\nfunctions vary periodically, under the average reward maximization setting. We\nformulate the problem as a stationary MDP by augmenting the state space with\nthe period index, and propose a periodic upper confidence bound reinforcement\nlearning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies\nlinearly with the period $N$ and as $\\mathcal{O}(\\sqrt{Tlog T})$ with the\nhorizon length $T$. Utilizing the information about the sparsity of transition\nmatrix of augmented MDP, we propose another algorithm PUCRLB which enhances\nupon PUCRL2, both in terms of regret ($O(\\sqrt{N})$ dependency on period) and\nempirical performance. Finally, we propose two other algorithms U-PUCRL2 and\nU-PUCRLB for extended uncertainty in the environment in which the period is\nunknown but a set of candidate periods are known. Numerical results demonstrate\nthe efficacy of all the algorithms.\n","authors":["Ayush Aniket","Arpan Chattopadhyay"],"pdf_url":"https://arxiv.org/pdf/2303.09629v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2207.12045"},{"id":"http://arxiv.org/abs/2303.09628v1","updated":"2023-03-16T20:09:47Z","published":"2023-03-16T20:09:47Z","title":"Efficient Learning of High Level Plans from Play","summary":"  Real-world robotic manipulation tasks remain an elusive challenge, since they\ninvolve both fine-grained environment interaction, as well as the ability to\nplan for long-horizon goals. Although deep reinforcement learning (RL) methods\nhave shown encouraging results when planning end-to-end in high-dimensional\nenvironments, they remain fundamentally limited by poor sample efficiency due\nto inefficient exploration, and by the complexity of credit assignment over\nlong horizons. In this work, we present Efficient Learning of High-Level Plans\nfrom Play (ELF-P), a framework for robotic learning that bridges motion\nplanning and deep RL to achieve long-horizon complex manipulation tasks. We\nleverage task-agnostic play data to learn a discrete behavioral prior over\nobject-centric primitives, modeling their feasibility given the current\ncontext. We then design a high-level goal-conditioned policy which (1) uses\nprimitives as building blocks to scaffold complex long-horizon tasks and (2)\nleverages the behavioral prior to accelerate learning. We demonstrate that\nELF-P has significantly better sample efficiency than relevant baselines over\nmultiple realistic manipulation tasks and learns policies that can be easily\ntransferred to physical hardware.\n","authors":["Núria Armengol Urpí","Marco Bagatella","Otmar Hilliges","Georg Martius","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2303.09628v1.pdf","comment":"Accepted to the International Conference on Robotics and Automation\n  2023"},{"id":"http://arxiv.org/abs/2208.07353v2","updated":"2023-03-16T19:58:23Z","published":"2022-08-15T17:42:27Z","title":"Easy Differentially Private Linear Regression","summary":"  Linear regression is a fundamental tool for statistical analysis. This has\nmotivated the development of linear regression methods that also satisfy\ndifferential privacy and thus guarantee that the learned model reveals little\nabout any one data point used to construct it. However, existing differentially\nprivate solutions assume that the end user can easily specify good data bounds\nand hyperparameters. Both present significant practical obstacles. In this\npaper, we study an algorithm which uses the exponential mechanism to select a\nmodel with high Tukey depth from a collection of non-private regression models.\nGiven $n$ samples of $d$-dimensional data used to train $m$ models, we\nconstruct an efficient analogue using an approximate Tukey depth that runs in\ntime $O(d^2n + dm\\log(m))$. We find that this algorithm obtains strong\nempirical performance in the data-rich setting with no data bounds or\nhyperparameter selection required.\n","authors":["Kareem Amin","Matthew Joseph","Mónica Ribero","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2208.07353v2.pdf","comment":"This version corresponds to the camera-ready at ICLR 2023"},{"id":"http://arxiv.org/abs/2302.10413v2","updated":"2023-03-16T19:52:19Z","published":"2023-02-21T02:53:37Z","title":"CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with\n  Clustered Aggregation and Knowledge DIStilled Regularization","summary":"  Federated learning enables edge devices to train a global model\ncollaboratively without exposing their data. Despite achieving outstanding\nadvantages in computing efficiency and privacy protection, federated learning\nfaces a significant challenge when dealing with non-IID data, i.e., data\ngenerated by clients that are typically not independent and identically\ndistributed. In this paper, we tackle a new type of Non-IID data, called\ncluster-skewed non-IID, discovered in actual data sets. The cluster-skewed\nnon-IID is a phenomenon in which clients can be grouped into clusters with\nsimilar data distributions. By performing an in-depth analysis of the behavior\nof a classification model's penultimate layer, we introduce a metric that\nquantifies the similarity between two clients' data distributions without\nviolating their privacy. We then propose an aggregation scheme that guarantees\nequality between clusters. In addition, we offer a novel local training\nregularization based on the knowledge-distillation technique that reduces the\noverfitting problem at clients and dramatically boosts the training scheme's\nperformance. We theoretically prove the superiority of the proposed aggregation\nover the benchmark FedAvg. Extensive experimental results on both standard\npublic datasets and our in-house real-world dataset demonstrate that the\nproposed approach improves accuracy by up to 16% compared to the FedAvg\nalgorithm.\n","authors":["Nang Hung Nguyen","Duc Long Nguyen","Trong Bang Nguyen","Thanh-Hung Nguyen","Huy Hieu Pham","Truong Thao Nguyen","Phi Le Nguyen"],"pdf_url":"https://arxiv.org/pdf/2302.10413v2.pdf","comment":"Accepted for presentation at the 23rd International Symposium on\n  Cluster, Cloud and Internet Computing (CCGrid 2023)"},{"id":"http://arxiv.org/abs/2303.09618v1","updated":"2023-03-16T19:47:41Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09611v1","updated":"2023-03-16T19:36:31Z","published":"2023-03-16T19:36:31Z","title":"Decentralized Riemannian natural gradient methods with Kronecker-product\n  approximations","summary":"  With a computationally efficient approximation of the second-order\ninformation, natural gradient methods have been successful in solving\nlarge-scale structured optimization problems. We study the natural gradient\nmethods for the large-scale decentralized optimization problems on Riemannian\nmanifolds, where the local objective function defined by the local dataset is\nof a log-probability type. By utilizing the structure of the Riemannian Fisher\ninformation matrix (RFIM), we present an efficient decentralized Riemannian\nnatural gradient descent (DRNGD) method. To overcome the communication issue of\nthe high-dimension RFIM, we consider a class of structured problems for which\nthe RFIM can be approximated by a Kronecker product of two low-dimension\nmatrices. By performing the communications over the Kronecker factors, a\nhigh-quality approximation of the RFIM can be obtained in a low cost. We prove\nthat DRNGD converges to a stationary point with the best-known rate of\n$\\mathcal{O}(1/K)$. Numerical experiments demonstrate the efficiency of our\nproposed method compared with the state-of-the-art ones. To the best of our\nknowledge, this is the first Riemannian second-order method for solving\ndecentralized manifold optimization problems.\n","authors":["Jiang Hu","Kangkang Deng","Na Li","Quanzheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.09611v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2207.05205v2","updated":"2023-03-16T19:21:56Z","published":"2022-07-11T21:45:54Z","title":"Scaling Novel Object Detection with Weakly Supervised Detection\n  Transformers","summary":"  A critical object detection task is finetuning an existing model to detect\nnovel objects, but the standard workflow requires bounding box annotations\nwhich are time-consuming and expensive to collect. Weakly supervised object\ndetection (WSOD) offers an appealing alternative, where object detectors can be\ntrained using image-level labels. However, the practical application of current\nWSOD models is limited, as they only operate at small data scales and require\nmultiple rounds of training and refinement. To address this, we propose the\nWeakly Supervised Detection Transformer, which enables efficient knowledge\ntransfer from a large-scale pretraining dataset to WSOD finetuning on hundreds\nof novel objects. Additionally, we leverage pretrained knowledge to improve the\nmultiple instance learning (MIL) framework often used in WSOD methods. Our\nexperiments show that our approach outperforms previous state-of-the-art models\non large-scale novel object detection datasets, and our scaling study reveals\nthat class quantity is more important than image quantity for WSOD pretraining.\n","authors":["Tyler LaBonte","Yale Song","Xin Wang","Vibhav Vineet","Neel Joshi"],"pdf_url":"https://arxiv.org/pdf/2207.05205v2.pdf","comment":"WACV 2023. Preliminary version appeared in CVPR 2022 Workshop on\n  Transformers for Vision"},{"id":"http://arxiv.org/abs/2303.09601v1","updated":"2023-03-16T19:01:29Z","published":"2023-03-16T19:01:29Z","title":"Psychotherapy AI Companion with Reinforcement Learning Recommendations\n  and Interpretable Policy Dynamics","summary":"  We introduce a Reinforcement Learning Psychotherapy AI Companion that\ngenerates topic recommendations for therapists based on patient responses. The\nsystem uses Deep Reinforcement Learning (DRL) to generate multi-objective\npolicies for four different psychiatric conditions: anxiety, depression,\nschizophrenia, and suicidal cases. We present our experimental results on the\naccuracy of recommended topics using three different scales of working alliance\nratings: task, bond, and goal. We show that the system is able to capture the\nreal data (historical topics discussed by the therapists) relatively well, and\nthat the best performing models vary by disorder and rating scale. To gain\ninterpretable insights into the learned policies, we visualize policy\ntrajectories in a 2D principal component analysis space and transition\nmatrices. These visualizations reveal distinct patterns in the policies trained\nwith different reward signals and trained on different clinical diagnoses. Our\nsystem's success in generating DIsorder-Specific Multi-Objective Policies\n(DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in\nproviding personalized and efficient therapeutic recommendations.\n","authors":["Baihan Lin","Guillermo Cecchi","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2303.09601v1.pdf","comment":"WWW 2023. This work supersede our prior work arxiv:2208.13077 by\n  studying the interpretability of RL-based therapy agents with policy\n  visualizations"},{"id":"http://arxiv.org/abs/2303.09599v1","updated":"2023-03-16T18:54:20Z","published":"2023-03-16T18:54:20Z","title":"cito: An R package for training neural networks using torch","summary":"  1. Deep neural networks (DNN) have become a central class of algorithms for\nregression and classification tasks. Although some packages exist that allow\nusers to specify DNN in R, those are rather limited in their functionality.\nMost current deep learning applications therefore rely on one of the major deep\nlearning frameworks, PyTorch or TensorFlow, to build and train DNN. However,\nusing these frameworks requires substantially more training and time than\ncomparable regression or machine learning packages in the R environment.\n  2. Here, we present cito, an user-friendly R package for deep learning. cito\nallows R users to specify deep neural networks in the familiar formula syntax\nused by most modeling functions in R. In the background, cito uses torch to fit\nthe models, taking advantage of all the numerical optimizations of the torch\nlibrary, including the ability to switch between training models on CPUs or\nGPUs. Moreover, cito includes many user-friendly functions for predictions and\nan explainable Artificial Intelligence (xAI) pipeline for the fitted models.\n  3. We showcase a typical analysis pipeline using cito, including its built-in\nxAI features to explore the trained DNN, by building a species distribution\nmodel of the African elephant.\n  4. In conclusion, cito provides a user-friendly R framework to specify,\ndeploy and interpret deep neural networks based on torch. The current stable\nCRAN version mainly supports fully connected DNNs, but it is planned that\nfuture versions will also include CNNs and RNNs.\n","authors":["Christian Amesoeder","Florian Hartig","Maximilian Pichler"],"pdf_url":"https://arxiv.org/pdf/2303.09599v1.pdf","comment":"8 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2211.05207v3","updated":"2023-03-16T18:32:52Z","published":"2022-11-09T21:33:40Z","title":"An Interpretable Machine Learning System to Identify EEG Patterns on the\n  Ictal-Interictal-Injury Continuum","summary":"  In many medical subfields, there is a call for greater interpretability in\nthe machine learning systems used for clinical work. In this paper, we design\nan interpretable deep learning model to predict the presence of 6 types of\nbrainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered\nin ICU EEG monitoring. Each prediction is accompanied by a high-quality\nexplanation delivered with the assistance of a specialized user interface. This\nnovel model architecture learns a set of prototypical examples (``prototypes'')\nand makes decisions by comparing a new EEG segment to these prototypes. These\nprototypes are either single-class (affiliated with only one class) or\ndual-class (affiliated with two classes).\n  We present three main ways of interpreting the model: 1) Using\nglobal-structure preserving methods, we map the 1275-dimensional cEEG latent\nfeatures to a 2D space to visualize the ictal-interictal-injury continuum and\ngain insight into its high-dimensional structure. 2) Predictions are made using\ncase-based reasoning, inherently providing explanations of the form ``this EEG\nlooks like that EEG.'' 3) We map the model decisions to a 2D space, allowing a\nuser to see how the current sample prediction compares to the distribution of\npredictions made by the model.\n  Our model performs better than the corresponding uninterpretable (black box)\nmodel with $p<0.01$ for discriminatory performance metrics AUROC (area under\nthe receiver operating characteristic curve) and AUPRC (area under the\nprecision-recall curve), as well as for task-specific interpretability metrics.\nWe provide videos of the user interface exploring the 2D embedded space,\nproviding the first global overview of the structure of ictal-interictal-injury\ncontinuum brainwave patterns. Our interpretable model and specialized user\ninterface can act as a reference for practitioners who work with cEEG patterns.\n","authors":["Alina Jade Barnett","Zhicheng Guo","Jin Jing","Wendong Ge","Cynthia Rudin","M. Brandon Westover"],"pdf_url":"https://arxiv.org/pdf/2211.05207v3.pdf","comment":"20 pages including appendices, 7 figures, submitted for peer review"},{"id":"http://arxiv.org/abs/2303.09590v1","updated":"2023-03-16T18:31:18Z","published":"2023-03-16T18:31:18Z","title":"Visual Analytics of Multivariate Networks with Representation Learning\n  and Composite Variable Construction","summary":"  Multivariate networks are commonly found in real-world data-driven\napplications. Uncovering and understanding the relations of interest in\nmultivariate networks is not a trivial task. This paper presents a visual\nanalytics workflow for studying multivariate networks to extract associations\nbetween different structural and semantic characteristics of the networks\n(e.g., what are the combinations of attributes largely relating to the density\nof a social network?). The workflow consists of a neural-network-based learning\nphase to classify the data based on the chosen input and output attributes, a\ndimensionality reduction and optimization phase to produce a simplified set of\nresults for examination, and finally an interpreting phase conducted by the\nuser through an interactive visualization interface. A key part of our design\nis a composite variable construction step that remodels nonlinear features\nobtained by neural networks into linear features that are intuitive to\ninterpret. We demonstrate the capabilities of this workflow with multiple case\nstudies on networks derived from social media usage and also evaluate the\nworkflow through an expert interview.\n","authors":["Hsiao-Ying Lu","Takanori Fujiwara","Ming-Yi Chang","Yang-chih Fu","Anders Ynnerman","Kwan-Liu Ma"],"pdf_url":"https://arxiv.org/pdf/2303.09590v1.pdf","comment":"To appear in Visual Informatics"},{"id":"http://arxiv.org/abs/2209.08752v3","updated":"2023-03-16T18:10:43Z","published":"2022-09-19T04:23:20Z","title":"Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the\n  Monocular RGB-D input","summary":"  Great success has been achieved in the 6-DoF grasp learning from the point\ncloud input, yet the computational cost due to the point set orderlessness\nremains a concern. Alternatively, we explore the grasp generation from the\nRGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects\nthe projection of the gripper keypoints in the image space and then recover the\nSE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive\nshape and the grasp family is constructed to examine our idea. Metric-based\nevaluation reveals that our method outperforms the baselines in terms of the\ngrasp proposal accuracy, diversity, and the time cost. Finally, robot\nexperiments show high success rate, demonstrating the potential of the idea in\nthe real-world applications.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio Vela"],"pdf_url":"https://arxiv.org/pdf/2209.08752v3.pdf","comment":"Accepted by ICRA2023. Final version"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.07347v2","updated":"2023-03-16T11:26:39Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v2.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07863v2","updated":"2023-03-16T08:34:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v2.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2211.11337v2","updated":"2023-03-16T07:38:02Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via\n  Contrastive Prompt-Tuning","summary":"  Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n  To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09112v1","updated":"2023-03-16T06:57:51Z","published":"2023-03-16T06:57:51Z","title":"SigVIC: Spatial Importance Guided Variable-Rate Image Compression","summary":"  Variable-rate mechanism has improved the flexibility and efficiency of\nlearning-based image compression that trains multiple models for different\nrate-distortion tradeoffs. One of the most common approaches for variable-rate\nis to channel-wisely or spatial-uniformly scale the internal features. However,\nthe diversity of spatial importance is instructive for bit allocation of image\ncompression. In this paper, we introduce a Spatial Importance Guided\nVariable-rate Image Compression (SigVIC), in which a spatial gating unit (SGU)\nis designed for adaptively learning a spatial importance mask. Then, a spatial\nscaling network (SSN) takes the spatial importance mask to guide the feature\nscaling and bit allocation for variable-rate. Moreover, to improve the quality\nof decoded image, Top-K shallow features are selected to refine the decoded\nfeatures through a shallow feature fusion module (SFFM). Experiments show that\nour method outperforms other learning-based methods (whether variable-rate or\nnot) and traditional codecs, with storage saving and high flexibility.\n","authors":["Jiaming Liang","Meiqin Liu","Chao Yao","Chunyu Lin","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.09112v1.pdf","comment":"Accepted by IEEE ICASSP2023 (Camera Ready)"},{"id":"http://arxiv.org/abs/2303.09048v1","updated":"2023-03-16T02:36:02Z","published":"2023-03-16T02:36:02Z","title":"Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP\n  Platforms","summary":"  In this paper, we present a method for fine-tuning models trained on the Deep\nNoise Suppression (DNS) 2020 Challenge to improve their performance on Voice\nover Internet Protocol (VoIP) applications. Our approach involves adapting the\nDNS 2020 models to the specific acoustic characteristics of VoIP\ncommunications, which includes distortion and artifacts caused by compression,\ntransmission, and platform-specific processing. To this end, we propose a\nmulti-task learning framework for VoIP-DNS that jointly optimizes noise\nsuppression and VoIP-specific acoustics for speech enhancement. We evaluate our\napproach on a diverse VoIP scenarios and show that it outperforms both industry\nperformance and state-of-the-art methods for speech enhancement on VoIP\napplications. Our results demonstrate the potential of models trained on\nDNS-2020 to be improved and tailored to different VoIP platforms using\nVoIP-DNS, whose findings have important applications in areas such as speech\nrecognition, voice assistants, and telecommunication.\n","authors":["Joseph Konan","Ojas Bhargave","Shikhar Agnihotri","Hojeong Lee","Ankit Shah","Shuo Han","Yunyang Zeng","Amanda Shu","Haohui Liu","Xuankai Chang","Hamza Khalid","Minseon Gwak","Kawon Lee","Minjeong Kim","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2303.09048v1.pdf","comment":"Under review at European Association for Signal Processing. 5 pages"}]},"2023-03-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.10128v1","updated":"2023-03-17T17:12:18Z","published":"2023-03-17T17:12:18Z","title":"Direct and indirect evidence of compression of word lengths. Zipf's law\n  of abbreviation revisited","summary":"  Zipf's law of abbreviation, the tendency of more frequent words to be\nshorter, is one of the most solid candidates for a linguistic universal, in the\nsense that it has the potential for being exceptionless or with a number of\nexceptions that is vanishingly small compared to the number of languages on\nEarth. Since Zipf's pioneering research, this law has been viewed as a\nmanifestation of a universal principle of communication, i.e. the minimization\nof word lengths, to reduce the effort of communication. Here we revisit the\nconcordance of written language with the law of abbreviation. Crucially, we\nprovide wider evidence that the law holds also in speech (when word length is\nmeasured in time), in particular in 46 languages from 14 linguistic families.\nAgreement with the law of abbreviation provides indirect evidence of\ncompression of languages via the theoretical argument that the law of\nabbreviation is a prediction of optimal coding. Motivated by the need of direct\nevidence of compression, we derive a simple formula for a random baseline\nindicating that word lengths are systematically below chance, across linguistic\nfamilies and writing systems, and independently of the unit of measurement\n(length in characters or duration in time). Our work paves the way to measure\nand compare the degree of optimality of word lengths in languages.\n","authors":["Sonia Petrini","Antoni Casas-i-Muñoz","Jordi Cluet-i-Martinell","Mengxue Wang","Chris Bentz","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2303.10128v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2208.10384"},{"id":"http://arxiv.org/abs/2210.10723v2","updated":"2023-03-17T16:36:53Z","published":"2022-10-19T17:08:13Z","title":"TabLLM: Few-shot Classification of Tabular Data with Large Language\n  Models","summary":"  We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.\n","authors":["Stefan Hegselmann","Alejandro Buendia","Hunter Lang","Monica Agrawal","Xiaoyi Jiang","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2210.10723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06924v3","updated":"2023-03-17T16:18:20Z","published":"2022-06-14T15:43:44Z","title":"The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity","summary":"  The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping\n$\\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers\nthat maximizes $D(G)=\\sum_{uv\\in E(G)}|\\pi(u) - \\pi(v)|$. In this setting,\nvertices are considered to lie on a horizontal line and edges are drawn as\nsemicircles above the line. There exist variants of MaxLA in which the\narrangements are constrained. In the planar variant, edge crossings are\nforbidden. In the projective variant for rooted trees, arrangements are planar\nand the root cannot be covered by any edge. Here we present $O(n)$-time and\n$O(n)$-space algorithms that solve planar and projective MaxLA for trees. We\nalso prove several properties of maximum projective and planar arrangements,\nand show that caterpillar trees maximize planar MaxLA over all trees of a fixed\nsize thereby generalizing a previous extremal result on trees.\n","authors":["Lluís Alemany-Puig","Juan Luis Esteban","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2206.06924v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14228v5","updated":"2023-03-17T16:14:55Z","published":"2022-11-25T16:41:59Z","title":"GPT-3-driven pedagogical agents for training children's curious\n  question-asking skills","summary":"  In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.\n","authors":["Rania Abdelghani","Yen-Hsiang Wang","Xingdi Yuan","Tong Wang","Pauline Lucas","Hélène Sauzéon","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2211.14228v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10093v1","updated":"2023-03-17T16:14:37Z","published":"2023-03-17T16:14:37Z","title":"Enhancing the Role of Context in Region-Word Alignment for Object\n  Detection","summary":"  Vision-language pretraining to learn a fine-grained, region-word alignment\nbetween image-caption pairs has propelled progress in open-vocabulary object\ndetection. We observe that region-word alignment methods are typically used in\ndetection with respect to only object nouns, and the impact of other rich\ncontext in captions, such as attributes, is unclear. In this study, we explore\nhow language context affects downstream object detection and propose to enhance\nthe role of context. In particular, we show how to strategically contextualize\nthe grounding pretraining objective for improved alignment. We further hone in\non attributes as especially useful object context and propose a novel adjective\nand noun-based negative sampling strategy for increasing their focus in\ncontrastive learning. Overall, our methods enhance object detection when\ncompared to the state-of-the-art in region-word pretraining. We also highlight\nthe fine-grained utility of an attribute-sensitive model through text-region\nretrieval and phrase grounding analysis.\n","authors":["Kyle Buettner","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2303.10093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09306v2","updated":"2023-03-17T15:13:01Z","published":"2023-03-16T13:31:31Z","title":"BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition","summary":"  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n","authors":["HAZ Sameen Shahgir","Ramisa Alam","Md. Zarif Ul Alam"],"pdf_url":"https://arxiv.org/pdf/2303.09306v2.pdf","comment":"Winning Solution for the Bangla Complex Named Entity Recognition\n  Challenge"},{"id":"http://arxiv.org/abs/2211.14563v2","updated":"2023-03-17T15:12:13Z","published":"2022-11-26T13:33:42Z","title":"Who are you referring to? Coreference resolution in image narrations","summary":"  Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.\n","authors":["Arushi Goel","Basura Fernando","Frank Keller","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2211.14563v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2302.10186v4","updated":"2023-03-17T15:04:27Z","published":"2023-02-16T21:45:35Z","title":"E2E Spoken Entity Extraction for Virtual Agents","summary":"  This paper reimagines some aspects of speech processing using speech\nencoders, specifically about extracting entities directly from speech, with no\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, postal addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech, ignoring the superfluous portions such\nas carrier phrases and spellings of entities. In the context of dialogs from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step cascade of first generating lexical transcriptions followed\nby text-based entity extraction for identifying spoken entities.\n","authors":["Karan Singla","Yeon-Jun Kim","Srinivas Bangalore"],"pdf_url":"https://arxiv.org/pdf/2302.10186v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09015v2","updated":"2023-03-17T14:25:43Z","published":"2022-08-18T18:31:40Z","title":"Treeformer: Dense Gradient Trees for Efficient Attention Computation","summary":"  Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.\n","authors":["Lovish Madaan","Srinadh Bhojanapalli","Himanshu Jain","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2208.09015v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2212.08489v2","updated":"2023-03-17T13:26:14Z","published":"2022-12-16T14:01:42Z","title":"Effectiveness of Text, Acoustic, and Lattice-based representations in\n  Spoken Language Understanding tasks","summary":"  In this paper, we perform an exhaustive evaluation of different\nrepresentations to address the intent classification problem in a Spoken\nLanguage Understanding (SLU) setup. We benchmark three types of systems to\nperform the SLU intent detection task: 1) text-based, 2) lattice-based, and a\nnovel 3) multimodal approach. Our work provides a comprehensive analysis of\nwhat could be the achievable performance of different state-of-the-art SLU\nsystems under different circumstances, e.g., automatically- vs.\nmanually-generated transcripts. We evaluate the systems on the publicly\navailable SLURP spoken language resource corpus. Our results indicate that\nusing richer forms of Automatic Speech Recognition (ASR) outputs, namely\nword-consensus-networks, allows the SLU system to improve in comparison to the\n1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,\nlearning from acoustic and text embeddings, obtains performance similar to the\noracle setup, a relative improvement of 17.8% over the 1-best configuration,\nbeing a recommended alternative to overcome the limitations of working with\nautomatically generated transcripts.\n","authors":["Esaú Villatoro-Tello","Srikanth Madikeri","Juan Zuluaga-Gomez","Bidisha Sharma","Seyyed Saeed Sarfjoo","Iuliia Nigmatulina","Petr Motlicek","Alexei V. Ivanov","Aravind Ganapathiraju"],"pdf_url":"https://arxiv.org/pdf/2212.08489v2.pdf","comment":"Accepted in ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.09905v1","updated":"2023-03-17T11:43:08Z","published":"2023-03-17T11:43:08Z","title":"More Robust Schema-Guided Dialogue State Tracking via Tree-Based\n  Paraphrase Ranking","summary":"  The schema-guided paradigm overcomes scalability issues inherent in building\ntask-oriented dialogue (TOD) agents with static ontologies. Instead of\noperating on dialogue context alone, agents have access to hierarchical schemas\ncontaining task-relevant natural language descriptions. Fine-tuned language\nmodels excel at schema-guided dialogue state tracking (DST) but are sensitive\nto the writing style of the schemas. We explore methods for improving the\nrobustness of DST models. We propose a framework for generating synthetic\nschemas which uses tree-based ranking to jointly optimise lexical diversity and\nsemantic faithfulness. The generalisation of strong baselines is improved when\naugmenting their training data with prompts generated by our framework, as\ndemonstrated by marked improvements in average joint goal accuracy (JGA) and\nschema sensitivity (SS) on the SGD-X benchmark.\n","authors":["A. Coca","B. H. Tseng","W. Lin","B. Byrne"],"pdf_url":"https://arxiv.org/pdf/2303.09905v1.pdf","comment":"Accepted at EACL (Findings) 2023"},{"id":"http://arxiv.org/abs/2303.09901v1","updated":"2023-03-17T11:33:06Z","published":"2023-03-17T11:33:06Z","title":"mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive\n  Pre-Training of Transformers for Few- and Zero-shot Framing Detection","summary":"  This paper presents the winning system for the zero-shot Spanish framing\ndetection task, which also achieves competitive places in eight additional\nlanguages. The challenge of the framing detection task lies in identifying a\nset of 14 frames when only a few or zero samples are available, i.e., a\nmultilingual multi-label few- or zero-shot setting. Our developed solution\nemploys a pre-training procedure based on multilingual Transformers using a\nlabel-aware contrastive loss function. In addition to describing the system, we\nperform an embedding space analysis and ablation study to demonstrate how our\npre-training procedure supports framing detection to advance computational\nframing analysis.\n","authors":["Markus Reiter-Haas","Alexander Ertl","Kevin Innerhofer","Elisabeth Lex"],"pdf_url":"https://arxiv.org/pdf/2303.09901v1.pdf","comment":"Manuscript submitted for publication at SemEval'23"},{"id":"http://arxiv.org/abs/2303.09892v1","updated":"2023-03-17T11:13:30Z","published":"2023-03-17T11:13:30Z","title":"Memotion 3: Dataset on sentiment and emotion analysis of codemixed\n  Hindi-English Memes","summary":"  Memes are the new-age conveyance mechanism for humor on social media sites.\nMemes often include an image and some text. Memes can be used to promote\ndisinformation or hatred, thus it is crucial to investigate in details. We\nintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other\nprevalent datasets in the domain, including prior iterations of Memotion,\nMemotion 3 introduces Hindi-English Codemixed memes while prior works in the\narea were limited to only the English memes. We describe the Memotion task, the\ndata collection and the dataset creation methodologies. We also provide a\nbaseline for the task. The baseline code and dataset will be made available at\nhttps://github.com/Shreyashm16/Memotion-3.0\n","authors":["Shreyash Mishra","S Suryavardan","Parth Patwa","Megha Chakraborty","Anku Rani","Aishwarya Reganti","Aman Chadha","Amitava Das","Amit Sheth","Manoj Chinnakotla","Asif Ekbal","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.09892v1.pdf","comment":"Defactify2 @AAAI"},{"id":"http://arxiv.org/abs/2303.09859v1","updated":"2023-03-17T09:53:33Z","published":"2023-03-17T09:53:33Z","title":"Trained on 100 million words and still in shape: BERT meets British\n  National Corpus","summary":"  While modern masked language models (LMs) are trained on ever larger corpora,\nwe here explore the effects of down-scaling training to a modestly-sized but\nrepresentative, well-balanced, and publicly available English text source --\nthe British National Corpus. We show that pre-training on this carefully\ncurated corpus can reach better performance than the original BERT model. We\nargue that this type of corpora has great potential as a language modeling\nbenchmark. To showcase this potential, we present fair, reproducible and\ndata-efficient comparative studies of LMs, in which we evaluate several\ntraining objectives and model architectures and replicate previous empirical\nresults in a systematic way. We propose an optimized LM architecture called\nLTG-BERT.\n","authors":["David Samuel","Andrey Kutuzov","Lilja Øvrelid","Erik Velldal"],"pdf_url":"https://arxiv.org/pdf/2303.09859v1.pdf","comment":"Accepted to EACL 2023"},{"id":"http://arxiv.org/abs/2109.01636v3","updated":"2023-03-17T09:32:37Z","published":"2021-09-03T17:28:04Z","title":"Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding","summary":"  With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n","authors":["Xin Chen","Qi Zhao","Xinyang Liu"],"pdf_url":"https://arxiv.org/pdf/2109.01636v3.pdf","comment":"Want to correct"},{"id":"http://arxiv.org/abs/2303.09827v1","updated":"2023-03-17T08:12:36Z","published":"2023-03-17T08:12:36Z","title":"DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through\n  Dependency Parsing","summary":"  We present our work on Track 2 in the Dialog System Technology Challenges 11\n(DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot,\ncross-domain, intent-set induction. In the absence of in-domain training\ndataset, robust utterance representation that can be used across domains is\nnecessary to induce users' intentions. To achieve this, we leveraged a\nmulti-domain dialogue dataset to fine-tune the language model and proposed\nextracting Verb-Object pairs to remove the artifacts of unnecessary\ninformation. Furthermore, we devised the method that generates each cluster's\nname for the explainability of clustered results. Our approach achieved 3rd\nplace in the precision score and showed superior accuracy and normalized mutual\ninformation (NMI) score than the baseline model on various domain datasets.\n","authors":["Jihyun Lee","Seungyeon Seo","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2303.09827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09823v1","updated":"2023-03-17T08:02:54Z","published":"2023-03-17T08:02:54Z","title":"Transformers and Ensemble methods: A solution for Hate Speech Detection\n  in Arabic languages","summary":"  This paper describes our participation in the shared task of hate speech\ndetection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our\nexperiments evaluate the performance of six transformer models and their\ncombination using 2 ensemble approaches. The best results on the training set,\nin a five-fold cross validation scenario, were obtained by using the ensemble\napproach based on the majority vote. The evaluation of this approach on the\ntest set resulted in an F1-score of 0.60 and an Accuracy of 0.86.\n","authors":["Angel Felipe Magnossão de Paula","Imene Bensalem","Paolo Rosso","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2303.09823v1.pdf","comment":"7 pages, 3 tables"},{"id":"http://arxiv.org/abs/2212.02021v3","updated":"2023-03-17T06:46:27Z","published":"2022-12-05T04:37:22Z","title":"Analysis of Utterance Embeddings and Clustering Methods Related to\n  Intent Induction for Task-Oriented Dialogue","summary":"  The focus of this work is to investigate unsupervised approaches to overcome\nquintessential challenges in designing task-oriented dialog schema: assigning\nintent labels to each dialog turn (intent clustering) and generating a set of\nintents based on the intent clustering methods (intent induction). We postulate\nthere are two salient factors for automatic induction of intents: (1)\nclustering algorithm for intent labeling and (2) user utterance embedding\nspace. We compare existing off-the-shelf clustering models and embeddings based\non DSTC11 evaluation. Our extensive experiments demonstrate that the combined\nselection of utterance embedding and clustering method in the intent induction\ntask should be carefully considered. We also present that pretrained MiniLM\nwith Agglomerative clustering shows significant improvement in NMI, ARI, F1,\naccuracy and example coverage in intent induction tasks. The source codes are\navailable at https://github.com/Jeiyoon/dstc11-track2.\n","authors":["Jeiyoon Park","Yoonna Jang","Chanhee Lee","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2212.02021v3.pdf","comment":"The Eleventh Dialog System Technology Challenge (DSTC11)"},{"id":"http://arxiv.org/abs/2303.09752v1","updated":"2023-03-17T03:28:17Z","published":"2023-03-17T03:28:17Z","title":"CoLT5: Faster Long-Range Transformers with Conditional Computation","summary":"  Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n","authors":["Joshua Ainslie","Tao Lei","Michiel de Jong","Santiago Ontañón","Siddhartha Brahma","Yury Zemlyanskiy","David Uthus","Mandy Guo","James Lee-Thorp","Yi Tay","Yun-Hsuan Sung","Sumit Sanghai"],"pdf_url":"https://arxiv.org/pdf/2303.09752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08954v2","updated":"2023-03-17T02:26:52Z","published":"2023-03-15T21:51:13Z","title":"PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented\n  Dialogs","summary":"  Research interest in task-oriented dialogs has increased as systems such as\nGoogle Assistant, Alexa and Siri have become ubiquitous in everyday life.\nHowever, the impact of academic research in this area has been limited by the\nlack of datasets that realistically capture the wide array of user pain points.\nTo enable research on some of the more challenging aspects of parsing realistic\nconversations, we introduce PRESTO, a public dataset of over 550K contextual\nmultilingual conversations between humans and virtual assistants. PRESTO\ncontains a diverse array of challenges that occur in real-world NLU tasks such\nas disfluencies, code-switching, and revisions. It is the only large scale\nhuman generated conversational parsing dataset that provides structured context\nsuch as a user's contacts and lists for each example. Our mT5 model based\nbaselines demonstrate that the conversational phenomenon present in PRESTO are\nchallenging to model, which is further pronounced in a low-resource setup.\n","authors":["Rahul Goel","Waleed Ammar","Aditya Gupta","Siddharth Vashishtha","Motoki Sano","Faiz Surani","Max Chang","HyunJeong Choe","David Greene","Kyle He","Rattima Nitisaroj","Anna Trukhina","Shachi Paul","Pararth Shah","Rushin Shah","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2303.08954v2.pdf","comment":"PRESTO v1 Release"},{"id":"http://arxiv.org/abs/2211.00795v2","updated":"2023-03-17T01:56:10Z","published":"2022-11-02T00:18:25Z","title":"InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss","summary":"  This paper presents InterMPL, a semi-supervised learning method of end-to-end\nautomatic speech recognition (ASR) that performs pseudo-labeling (PL) with\nintermediate supervision. Momentum PL (MPL) trains a connectionist temporal\nclassification (CTC)-based model on unlabeled data by continuously generating\npseudo-labels on the fly and improving their quality. In contrast to\nautoregressive formulations, such as the attention-based encoder-decoder and\ntransducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in\ngeneral, owing to its simple/fast inference algorithm and robustness against\ngenerating collapsed labels. However, CTC generally yields inferior performance\nthan the autoregressive models due to the conditional independence assumption,\nthereby limiting the performance of MPL. We propose to enhance MPL by\nintroducing intermediate loss, inspired by the recent advances in CTC-based\nmodeling. Specifically, we focus on self-conditional and hierarchical\nconditional CTC, that apply auxiliary CTC losses to intermediate layers such\nthat the conditional independence assumption is explicitly relaxed. We also\nexplore how pseudo-labels should be generated and used as supervision for\nintermediate losses. Experimental results in different semi-supervised settings\ndemonstrate that the proposed approach outperforms MPL and improves an ASR\nmodel by up to a 12.1% absolute performance gain. In addition, our detailed\nanalysis validates the importance of the intermediate loss.\n","authors":["Yosuke Higuchi","Tetsuji Ogawa","Tetsunori Kobayashi","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2211.00795v2.pdf","comment":"Accepted to ICASSP2023"},{"id":"http://arxiv.org/abs/2211.00792v2","updated":"2023-03-17T01:52:50Z","published":"2022-11-02T00:10:43Z","title":"BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder","summary":"  We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.\n","authors":["Yosuke Higuchi","Tetsuji Ogawa","Tetsunori Kobayashi","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2211.00792v2.pdf","comment":"Accepted to ICASSP2023"},{"id":"http://arxiv.org/abs/2301.08771v3","updated":"2023-03-17T01:32:34Z","published":"2023-01-20T19:13:09Z","title":"Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt\n  Learning for Automatic Scoring in Science Education","summary":"  Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models (PLMs)\ncan be adapted to downstream tasks without fine-tuning with prompts. However,\nno research has employed such a prompt approach in science education. As\nstudent responses are presented with natural language, aligning the scoring\nprocedure as the next sentence prediction task using prompts can skip the\ncostly fine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.\n","authors":["Xuansheng Wu","Xinyu He","Tianming Liu","Ninghao Liu","Xiaoming Zhai"],"pdf_url":"https://arxiv.org/pdf/2301.08771v3.pdf","comment":"10+3 pages"},{"id":"http://arxiv.org/abs/2303.09719v1","updated":"2023-03-17T01:26:39Z","published":"2023-03-17T01:26:39Z","title":"Learning towards Selective Data Augmentation for Dialogue Generation","summary":"  As it is cumbersome and expensive to acquire a huge amount of data for\ntraining neural dialog models, data augmentation is proposed to effectively\nutilize existing training samples. However, current data augmentation\ntechniques on the dialog generation task mostly augment all cases in the\ntraining dataset without considering the intrinsic attributes between different\ncases. We argue that not all cases are beneficial for augmentation task, and\nthe cases suitable for augmentation should obey the following two attributes:\n(1) low-quality (the dialog model cannot generate a high-quality response for\nthe case), (2) representative (the case should represent the property of the\nwhole dataset). Herein, we explore this idea by proposing a Selective Data\nAugmentation framework (SDA) for the response generation task. SDA employs a\ndual adversarial network to select the lowest quality and most representative\ndata points for augmentation in one stage. Extensive experiments conducted on\ntwo publicly available datasets, i.e., DailyDialog and OpenSubtitles, show that\nour framework can improve the response generation performance with respect to\nvarious metrics.\n","authors":["Xiuying Chen","Mingzhe Li","Jiayi Zhang","Xiaoqiang Xia","Chen Wei","Jianwei Cui","Xin Gao","Xiangliang Zhang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2303.09719v1.pdf","comment":"9 pages, 4 figures, AAAI 2023"},{"id":"http://arxiv.org/abs/2303.09713v1","updated":"2023-03-17T01:10:33Z","published":"2023-03-17T01:10:33Z","title":"CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos","summary":"  Visual information is central to conversation: body gestures and facial\nexpressions, for example, contribute to meaning that transcends words alone. To\ndate, however, most neural conversational models are limited to just text. We\nintroduce CHAMPAGNE, a generative model of conversations that can account for\nvisual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a\nlarge-scale corpus of 18M video-based dialogues. YTD-18M is constructed from\nweb videos: crucial to our data collection pipeline is a pretrained language\nmodel that converts error-prone automatic transcripts to a cleaner dialogue\nformat while maintaining meaning. Human evaluation reveals that YTD-18M is more\nsensible and specific than prior resources (MMDialog, 1M dialogues), while\nmaintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE\nlearns to conduct conversation from YTD-18M; and 2) when fine-tuned, it\nachieves state-of-the-art results on four vision-language tasks focused on\nreal-world conversations. We release data, models, and code at\nhttps://seungjuhan.me/champagne.\n","authors":["Seungju Han","Jack Hessel","Nouha Dziri","Yejin Choi","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2303.09713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15042v2","updated":"2023-03-17T00:55:42Z","published":"2022-10-26T21:18:31Z","title":"EW-Tune: A Framework for Privately Fine-Tuning Large Language Models\n  with Differential Privacy","summary":"  Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n","authors":["Rouzbeh Behnia","Mohamamdreza Ebrahimi","Jason Pacheco","balaji Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2210.15042v2.pdf","comment":"Publised at IEEE ICDM Workshop on Machine Learning for Cybersecurity\n  (MLC) 2022"},{"id":"http://arxiv.org/abs/2210.12353v3","updated":"2023-03-17T00:52:56Z","published":"2022-10-22T05:04:54Z","title":"Leveraging Large Language Models for Multiple Choice Question Answering","summary":"  While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.\n","authors":["Joshua Robinson","Christopher Michael Rytting","David Wingate"],"pdf_url":"https://arxiv.org/pdf/2210.12353v3.pdf","comment":"Accepted for ICLR 2023"},{"id":"http://arxiv.org/abs/2302.02123v3","updated":"2023-03-17T00:08:53Z","published":"2023-02-04T07:44:23Z","title":"Greedy Ordering of Layer Weight Matrices in Transformers Improves\n  Translation","summary":"  Prior work has attempted to understand the internal structures and\nfunctionalities of Transformer-based encoder-decoder architectures on the level\nof multi-head attention and feed-forward sublayers. Interpretations have\nfocused on the encoder and decoder, along with the combinatorial possibilities\nof the self-attention, cross-attention, and feed-forward sublayers. However,\nwithout examining the low-level structures, one gains limited understanding of\nthe motivation behind sublayer reordering. Could we dive into the sublayer\nabstraction and permute layer weight matrices to improve the quality of\ntranslation? We propose AEIUOrder to greedily reorder layer weight matrices in\nthe encoder by their well-trainedness, as measured by Heavy-Tailed\nSelf-Regularization (HT-SR) metrics, and order the decoder matrices\ncorrespondingly. Our results suggest that greedily reordering layer weight\nmatrices to maximize Total well-trainedness facilitates the model to learn\nrepresentations and generate translations more effectively.\n","authors":["Elicia Ye"],"pdf_url":"https://arxiv.org/pdf/2302.02123v3.pdf","comment":"The paper contains an error in the implementation of the algorithm"},{"id":"http://arxiv.org/abs/2203.05659v2","updated":"2023-03-17T22:21:50Z","published":"2022-03-10T21:58:33Z","title":"NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of\n  Misinformation in News Articles","summary":"  In this paper, we present the fifth installment of the NELA-GT datasets,\nNELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between\nJanuary 1st, 2022 and December 31st, 2022. Just as in past releases of the\ndataset, NELA-GT-2022 includes outlet-level veracity labels from Media\nBias/Fact Check and tweets embedded in collected news articles. The\nNELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H\n","authors":["Maurício Gruppi","Benjamin D. Horne","Sibel Adalı"],"pdf_url":"https://arxiv.org/pdf/2203.05659v2.pdf","comment":"Technical report documenting the NELA-GT recent update\n  (NELA-GT-2022). arXiv admin note: substantial text overlap with\n  arXiv:2102.04567"},{"id":"http://arxiv.org/abs/2303.10255v1","updated":"2023-03-17T21:39:33Z","published":"2023-03-17T21:39:33Z","title":"Feedback Effect in User Interaction with Intelligent Assistants: Delayed\n  Engagement, Adaption and Drop-out","summary":"  With the growing popularity of intelligent assistants (IAs), evaluating IA\nquality becomes an increasingly active field of research. This paper identifies\nand quantifies the feedback effect, a novel component in IA-user interactions:\nhow the capabilities and limitations of the IA influence user behavior over\ntime. First, we demonstrate that unhelpful responses from the IA cause users to\ndelay or reduce subsequent interactions in the short term via an observational\nstudy. Next, we expand the time horizon to examine behavior changes and show\nthat as users discover the limitations of the IA's understanding and functional\ncapabilities, they learn to adjust the scope and wording of their requests to\nincrease the likelihood of receiving a helpful response from the IA. Our\nfindings highlight the impact of the feedback effect at both the micro and meso\nlevels. We further discuss its macro-level consequences: unsatisfactory\ninteractions continuously reduce the likelihood and diversity of future user\nengagements in a feedback loop.\n","authors":["Zidi Xiu","Kai-Chen Cheng","David Q. Sun","Jiannan Lu","Hadas Kotek","Yuhan Zhang","Paul McCarthy","Christopher Klein","Stephen Pulman","Jason D. Williams"],"pdf_url":"https://arxiv.org/pdf/2303.10255v1.pdf","comment":"PAKDD 2023"},{"id":"http://arxiv.org/abs/2303.10227v1","updated":"2023-03-17T19:50:51Z","published":"2023-03-17T19:50:51Z","title":"Conversational Tree Search: A New Hybrid Dialog Task","summary":"  Conversational interfaces provide a flexible and easy way for users to seek\ninformation that may otherwise be difficult or inconvenient to obtain. However,\nexisting interfaces generally fall into one of two categories: FAQs, where\nusers must have a concrete question in order to retrieve a general answer, or\ndialogs, where users must follow a predefined path but may receive a\npersonalized answer. In this paper, we introduce Conversational Tree Search\n(CTS) as a new task that bridges the gap between FAQ-style information\nretrieval and task-oriented dialog, allowing domain-experts to define dialog\ntrees which can then be converted to an efficient dialog policy that learns\nonly to ask the questions necessary to navigate a user to their goal. We\ncollect a dataset for the travel reimbursement domain and demonstrate a\nbaseline as well as a novel deep Reinforcement Learning architecture for this\ntask. Our results show that the new architecture combines the positive aspects\nof both the FAQ and dialog system used in the baseline and achieves higher goal\ncompletion while skipping unnecessary questions.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2303.10227v1.pdf","comment":"EACL 2023"},{"id":"http://arxiv.org/abs/2303.00908v2","updated":"2023-03-17T18:25:24Z","published":"2023-03-02T01:57:17Z","title":"Interactive Text Generation","summary":"  Users interact with text, image, code, or other editors on a daily basis.\nHowever, machine learning models are rarely trained in the settings that\nreflect the interactivity between users and their editor. This is\nunderstandable as training AI models with real users is not only slow and\ncostly, but what these models learn may be specific to user interface design\nchoices. Unfortunately, this means most of the research on text, code, and\nimage generation has focused on non-interactive settings, whereby the model is\nexpected to get everything right without accounting for any input from a user\nwho may be willing to help.\n  We introduce a new Interactive Text Generation task that allows training\ngeneration models interactively without the costs of involving real users, by\nusing user simulators that provide edits that guide the model towards a given\ntarget text. We train our interactive models using Imitation Learning, and our\nexperiments against competitive non-interactive generation models show that\nmodels trained interactively are superior to their non-interactive\ncounterparts, even when all models are given the same budget of user inputs or\nedits.\n","authors":["Felix Faltings","Michel Galley","Baolin Peng","Kianté Brantley","Weixin Cai","Yizhe Zhang","Jianfeng Gao","Bill Dolan"],"pdf_url":"https://arxiv.org/pdf/2303.00908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09038v2","updated":"2023-03-17T18:00:44Z","published":"2023-03-16T02:21:39Z","title":"Translating Radiology Reports into Plain Language using ChatGPT and\n  GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential","summary":"  The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n","authors":["Qing Lyu","Josh Tan","Michael E. Zapadka","Janardhana Ponnatapura","Chuang Niu","Ge Wang","Christopher T. Whitlow"],"pdf_url":"https://arxiv.org/pdf/2303.09038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11156v1","updated":"2023-03-17T17:53:19Z","published":"2023-03-17T17:53:19Z","title":"Can AI-Generated Text be Reliably Detected?","summary":"  The rapid progress of Large Language Models (LLMs) has made them capable of\nperforming astonishingly well on various tasks including document completion\nand question answering. The unregulated use of these models, however, can\npotentially lead to malicious consequences such as plagiarism, generating fake\nnews, spamming, etc. Therefore, reliable detection of AI-generated text can be\ncritical to ensure the responsible use of LLMs. Recent works attempt to tackle\nthis problem either using certain model signatures present in the generated\ntext outputs or by applying watermarking techniques that imprint specific\npatterns onto them. In this paper, both empirically and theoretically, we show\nthat these detectors are not reliable in practical scenarios. Empirically, we\nshow that paraphrasing attacks, where a light paraphraser is applied on top of\nthe generative text model, can break a whole range of detectors, including the\nones using the watermarking schemes as well as neural network-based detectors\nand zero-shot classifiers. We then provide a theoretical impossibility result\nindicating that for a sufficiently good language model, even the best-possible\ndetector can only perform marginally better than a random classifier. Finally,\nwe show that even LLMs protected by watermarking schemes can be vulnerable\nagainst spoofing attacks where adversarial humans can infer hidden watermarking\nsignatures and add them to their generated text to be detected as text\ngenerated by the LLMs, potentially causing reputational damages to their\ndevelopers. We believe these results can open an honest conversation in the\ncommunity regarding the ethical and reliable use of AI-generated text.\n","authors":["Vinu Sankar Sadasivan","Aounon Kumar","Sriram Balasubramanian","Wenxiao Wang","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2303.11156v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.10151v1","updated":"2023-03-17T17:40:32Z","published":"2023-03-17T17:40:32Z","title":"Toward Super-Resolution for Appearance-Based Gaze Estimation","summary":"  Gaze tracking is a valuable tool with a broad range of applications in\nvarious fields, including medicine, psychology, virtual reality, marketing, and\nsafety. Therefore, it is essential to have gaze tracking software that is\ncost-efficient and high-performing. Accurately predicting gaze remains a\ndifficult task, particularly in real-world situations where images are affected\nby motion blur, video compression, and noise. Super-resolution has been shown\nto improve image quality from a visual perspective. This work examines the\nusefulness of super-resolution for improving appearance-based gaze tracking. We\nshow that not all SR models preserve the gaze direction. We propose a two-step\nframework based on SwinIR super-resolution model. The proposed method\nconsistently outperforms the state-of-the-art, particularly in scenarios\ninvolving low-resolution or degraded images. Furthermore, we examine the use of\nsuper-resolution through the lens of self-supervised learning for gaze\nprediction. Self-supervised learning aims to learn from unlabelled data to\nreduce the amount of required labeled data for downstream tasks. We propose a\nnovel architecture called SuperVision by fusing an SR backbone network to a\nResNet18 (with some skip connections). The proposed SuperVision method uses 5x\nless labeled data and yet outperforms, by 15%, the state-of-the-art method of\nGazeTR which uses 100% of training data.\n","authors":["Galen O'Shea","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2303.10151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10147v1","updated":"2023-03-17T17:31:55Z","published":"2023-03-17T17:31:55Z","title":"CoDEPS: Online Continual Learning for Depth Estimation and Panoptic\n  Segmentation","summary":"  Operating a robot in the open world requires a high level of robustness with\nrespect to previously unseen environments. Optimally, the robot is able to\nadapt by itself to new conditions without human supervision, e.g.,\nautomatically adjusting its perception system to changing lighting conditions.\nIn this work, we address the task of continual learning for deep learning-based\nmonocular depth estimation and panoptic segmentation in new environments in an\nonline manner. We introduce CoDEPS to perform continual learning involving\nmultiple real-world domains while mitigating catastrophic forgetting by\nleveraging experience replay. In particular, we propose a novel domain-mixing\nstrategy to generate pseudo-labels to adapt panoptic segmentation. Furthermore,\nwe explicitly address the limited storage capacity of robotic systems by\nproposing sampling strategies for constructing a fixed-size replay buffer based\non rare semantic class sampling and image diversity. We perform extensive\nevaluations of CoDEPS on various real-world datasets demonstrating that it\nsuccessfully adapts to unseen environments without sacrificing performance on\nprevious domains while achieving state-of-the-art results. The code of our work\nis publicly available at http://codeps.cs.uni-freiburg.de.\n","authors":["Niclas Vödisch","Kürsat Petek","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2303.10147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00462v3","updated":"2023-03-17T17:31:40Z","published":"2023-03-01T12:41:12Z","title":"Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision","summary":"  This work proposes a novel approach to 4D radar-based scene flow estimation\nvia cross-modal learning. Our approach is motivated by the co-located sensing\nredundancy in modern autonomous vehicles. Such redundancy implicitly provides\nvarious forms of supervision cues to the radar scene flow estimation.\nSpecifically, we introduce a multi-task model architecture for the identified\ncross-modal learning problem and propose loss functions to opportunistically\nengage scene flow estimation using multiple cross-modal constraints for\neffective model training. Extensive experiments show the state-of-the-art\nperformance of our method and demonstrate the effectiveness of cross-modal\nsupervised learning to infer more accurate 4D radar scene flow. We also show\nits usefulness to two subtasks - motion segmentation and ego-motion estimation.\nOur source code will be available on https://github.com/Toytiny/CMFlow.\n","authors":["Fangqiang Ding","Andras Palffy","Dariu M. Gavrila","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.00462v3.pdf","comment":"10 pages, 7 figures. Accepted by CVPR 2023. See our code at\n  https://github.com/Toytiny/CMFlow. Supplementary materials can be found at\n  https://drive.google.com/file/d/1Iewcqnjzecge2ePBM8k2tg-85LX5xs3N/view"},{"id":"http://arxiv.org/abs/2303.10145v1","updated":"2023-03-17T17:30:42Z","published":"2023-03-17T17:30:42Z","title":"Spectrum-inspired Low-light Image Translation for Saliency Detection","summary":"  Saliency detection methods are central to several real-world applications\nsuch as robot navigation and satellite imagery. However, the performance of\nexisting methods deteriorate under low-light conditions because training\ndatasets mostly comprise of well-lit images. One possible solution is to\ncollect a new dataset for low-light conditions. This involves pixel-level\nannotations, which is not only tedious and time-consuming but also infeasible\nif a huge training corpus is required. We propose a technique that performs\nclassical band-pass filtering in the Fourier space to transform well-lit images\nto low-light images and use them as a proxy for real low-light images. Unlike\npopular deep learning approaches which require learning thousands of parameters\nand enormous amounts of training data, the proposed transformation is fast and\nsimple and easy to extend to other tasks such as low-light depth estimation.\nOur experiments show that the state-of-the-art saliency detection and depth\nestimation networks trained on our proxy low-light images perform significantly\nbetter on real low-light images than networks trained using existing\nstrategies.\n","authors":["Kitty Varghese","Sudarshan Rajagopalan","Mohit Lamba","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2303.10145v1.pdf","comment":"Presented at The Indian Conference on Computer Vision, Graphics and\n  Image Processing (ICVGIP) 2022"},{"id":"http://arxiv.org/abs/2212.11565v2","updated":"2023-03-17T17:28:04Z","published":"2022-12-22T09:43:36Z","title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for\n  Text-to-Video Generation","summary":"  To replicate the success of text-to-image (T2I) generation, recent works\nemploy large-scale video datasets to train a text-to-video (T2V) generator.\nDespite their promising results, such paradigm is computationally expensive. In\nthis work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot\nVideo Tuning, where only one text-video pair is presented. Our model is built\non state-of-the-art T2I diffusion models pre-trained on massive image data. We\nmake two key observations: 1) T2I models can generate still images that\nrepresent verb terms; 2) extending T2I models to generate multiple images\nconcurrently exhibits surprisingly good content consistency. To further learn\ncontinuous motion, we introduce Tune-A-Video, which involves a tailored\nspatio-temporal attention mechanism and an efficient one-shot tuning strategy.\nAt inference, we employ DDIM inversion to provide structure guidance for\nsampling. Extensive qualitative and numerical experiments demonstrate the\nremarkable ability of our method across various applications.\n","authors":["Jay Zhangjie Wu","Yixiao Ge","Xintao Wang","Weixian Lei","Yuchao Gu","Yufei Shi","Wynne Hsu","Ying Shan","Xiaohu Qie","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2212.11565v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.10137v1","updated":"2023-03-17T17:25:10Z","published":"2023-03-17T17:25:10Z","title":"A Recipe for Watermarking Diffusion Models","summary":"  Recently, diffusion models (DMs) have demonstrated their advantageous\npotential for generative tasks. Widespread interest exists in incorporating DMs\ninto downstream applications, such as producing or editing photorealistic\nimages. However, practical deployment and unprecedented power of DMs raise\nlegal issues, including copyright protection and monitoring of generated\ncontent. In this regard, watermarking has been a proven solution for copyright\nprotection and content monitoring, but it is underexplored in the DMs\nliterature. Specifically, DMs generate samples from longer tracks and may have\nnewly designed multimodal structures, necessitating the modification of\nconventional watermarking pipelines. To this end, we conduct comprehensive\nanalyses and derive a recipe for efficiently watermarking state-of-the-art DMs\n(e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe\nis straightforward but involves empirically ablated implementation details,\nproviding a solid foundation for future research on watermarking DMs. Our Code:\nhttps://github.com/yunqing-me/WatermarkDM.\n","authors":["Yunqing Zhao","Tianyu Pang","Chao Du","Xiao Yang","Ngai-Man Cheung","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10137v1.pdf","comment":"23 pages, 22 figures"},{"id":"http://arxiv.org/abs/2303.10136v1","updated":"2023-03-17T17:24:57Z","published":"2023-03-17T17:24:57Z","title":"MassNet: A Deep Learning Approach for Body Weight Extraction from A\n  Single Pressure Image","summary":"  Body weight, as an essential physiological trait, is of considerable\nsignificance in many applications like body management, rehabilitation, and\ndrug dosing for patient-specific treatments. Previous works on the body weight\nestimation task are mainly vision-based, using 2D/3D, depth, or infrared\nimages, facing problems in illumination, occlusions, and especially privacy\nissues. The pressure mapping mattress is a non-invasive and privacy-preserving\ntool to obtain the pressure distribution image over the bed surface, which\nstrongly correlates with the body weight of the lying person. To extract the\nbody weight from this image, we propose a deep learning-based model, including\na dual-branch network to extract the deep features and pose features\nrespectively. A contrastive learning module is also combined with the\ndeep-feature branch to help mine the mutual factors across different postures\nof every single subject. The two groups of features are then concatenated for\nthe body weight regression task. To test the model's performance over different\nhardware and posture settings, we create a pressure image dataset of 10\nsubjects and 23 postures, using a self-made pressure-sensing bedsheet. This\ndataset, which is made public together with this paper, together with a public\ndataset, are used for the validation. The results show that our model\noutperforms the state-of-the-art algorithms over both 2 datasets. Our research\nconstitutes an important step toward fully automatic weight estimation in both\nclinical and at-home practice. Our dataset is available for research purposes\nat: https://github.com/USTCWzy/MassEstimation.\n","authors":["Ziyu Wu","Quan Wan","Mingjie Zhao","Yi Ke","Yiran Fang","Zhen Liang","Fangting Xie","Jingyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.10136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10126v1","updated":"2023-03-17T17:07:36Z","published":"2023-03-17T17:07:36Z","title":"IRGen: Generative Modeling for Image Retrieval","summary":"  While generative modeling has been ubiquitous in natural language processing\nand computer vision, its application to image retrieval remains unexplored. In\nthis paper, we recast image retrieval as a form of generative modeling by\nemploying a sequence-to-sequence model, contributing to the current unified\ntheme. Our framework, IRGen, is a unified model that enables end-to-end\ndifferentiable search, thus achieving superior performance thanks to direct\noptimization. While developing IRGen we tackle the key technical challenge of\nconverting an image into quite a short sequence of semantic units in order to\nenable efficient and effective retrieval. Empirical experiments demonstrate\nthat our model yields significant improvement over three commonly used\nbenchmarks, for example, 22.9\\% higher than the best baseline method in\nprecision@10 on In-shop dataset with comparable recall@10 score.\n","authors":["Yidan Zhang","Ting Zhang","Dong Chen","Yujing Wang","Qi Chen","Xing Xie","Hao Sun","Weiwei Deng","Qi Zhang","Fan Yang","Mao Yang","Qingmin Liao","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2303.10126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12849v2","updated":"2023-03-17T16:36:26Z","published":"2022-09-26T16:58:00Z","title":"AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft\n  Detection and Tracking","summary":"  Detect-and-Avoid (DAA) capabilities are critical for safe operations of\nunmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time\nvision-only detect and tracking framework that respects the size, weight, and\npower (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios\n(SNR) of far away aircraft, we propose using full resolution images in a deep\nlearning framework that aligns successive images to remove ego-motion. The\naligned images are then used downstream in cascaded primary and secondary\nclassifiers to improve detection and tracking performance on multiple metrics.\nWe show that AirTrack outperforms state-of-the art baselines on the Amazon\nAirborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a\nCessna 182 interacting with general aviation traffic and additional\nnear-collision flight tests with a Bell helicopter flying towards a UAS in a\ncontrolled setting showcase that the proposed approach satisfies the newly\nintroduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that\nour system has a probability of track of more than 95% up to a range of 700m.\nVideo available at https://youtu.be/H3lL_Wjxjpw .\n","authors":["Sourish Ghosh","Jay Patrikar","Brady Moon","Milad Moghassem Hamidi","and Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2209.12849v2.pdf","comment":"7 pages, 5 figures, ICRA 2023"},{"id":"http://arxiv.org/abs/2303.10103v1","updated":"2023-03-17T16:26:20Z","published":"2023-03-17T16:26:20Z","title":"Image comparison and scaling via nonlinear elasticity","summary":"  A nonlinear elasticity model for comparing images is formulated and analyzed,\nin which optimal transformations between images are sought as minimizers of an\nintegral functional. The existence of minimizers in a suitable class of\nhomeomorphisms between image domains is established under natural hypotheses.\nWe investigate whether for linearly related images the minimization algorithm\ndelivers the linear transformation as the unique minimizer.\n","authors":["John M. Ball","Christopher L. Horner"],"pdf_url":"https://arxiv.org/pdf/2303.10103v1.pdf","comment":"SSVM2023 Proceedings to appear"},{"id":"http://arxiv.org/abs/2303.10100v1","updated":"2023-03-17T16:23:36Z","published":"2023-03-17T16:23:36Z","title":"Unified Mask Embedding and Correspondence Learning for Self-Supervised\n  Video Segmentation","summary":"  The objective of this paper is self-supervised learning of video object\nsegmentation. We develop a unified framework which simultaneously models\ncross-frame dense correspondence for locally discriminative feature learning\nand embeds object-level context for target-mask decoding. As a result, it is\nable to directly learn to perform mask-guided sequential segmentation from\nunlabeled videos, in contrast to previous efforts usually relying on an oblique\nsolution - cheaply \"copying\" labels according to pixel-wise correlations.\nConcretely, our algorithm alternates between i) clustering video pixels for\ncreating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo\nlabels to learn mask encoding and decoding for VOS. Unsupervised correspondence\nlearning is further incorporated into this self-taught, mask embedding scheme,\nso as to ensure the generic nature of the learnt representation and avoid\ncluster degeneracy. Our algorithm sets state-of-the-arts on two standard\nbenchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and\nfully-supervised VOS, in terms of both performance and network architecture\ndesign.\n","authors":["Liulei Li","Wenguan Wang","Tianfei Zhou","Jianwu Li","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10100v1.pdf","comment":"CVPR 2023. code: https://github.com/0liliulei/Mask-VOS"},{"id":"http://arxiv.org/abs/2303.10096v1","updated":"2023-03-17T16:16:36Z","published":"2023-03-17T16:16:36Z","title":"Efficient Neural Generation of 4K Masks for Homogeneous Diffusion\n  Inpainting","summary":"  With well-selected data, homogeneous diffusion inpainting can reconstruct\nimages from sparse data with high quality. While 4K colour images of size 3840\nx 2160 can already be inpainted in real time, optimising the known data for\napplications like image compression remains challenging: Widely used stochastic\nstrategies can take days for a single 4K image. Recently, a first neural\napproach for this so-called mask optimisation problem offered high speed and\ngood quality for small images. It trains a mask generation network with the\nhelp of a neural inpainting surrogate. However, these mask networks can only\noutput masks for the resolution and mask density they were trained for. We\nsolve these problems and enable mask optimisation for high-resolution images\nthrough a neuroexplicit coarse-to-fine strategy. Additionally, we improve the\ntraining and interpretability of mask networks by including a numerical\ninpainting solver directly into the network. This allows to generate masks for\n4K images in around 0.6 seconds while exceeding the quality of stochastic\nmethods on practically relevant densities. Compared to popular existing\napproaches, this is an acceleration of up to four orders of magnitude.\n","authors":["Karl Schrader","Pascal Peter","Niklas Kämper","Joachim Weickert"],"pdf_url":"https://arxiv.org/pdf/2303.10096v1.pdf","comment":"To appear in L. Calatroni, M. Donatelli, S. Morigi, M. Prato, M.\n  Santavesaria (Eds.): Scale Space and Variational Methods in Computer Vision.\n  Lecture Notes in Computer Science, Springer, Cham, 2023"},{"id":"http://arxiv.org/abs/2303.10093v1","updated":"2023-03-17T16:14:37Z","published":"2023-03-17T16:14:37Z","title":"Enhancing the Role of Context in Region-Word Alignment for Object\n  Detection","summary":"  Vision-language pretraining to learn a fine-grained, region-word alignment\nbetween image-caption pairs has propelled progress in open-vocabulary object\ndetection. We observe that region-word alignment methods are typically used in\ndetection with respect to only object nouns, and the impact of other rich\ncontext in captions, such as attributes, is unclear. In this study, we explore\nhow language context affects downstream object detection and propose to enhance\nthe role of context. In particular, we show how to strategically contextualize\nthe grounding pretraining objective for improved alignment. We further hone in\non attributes as especially useful object context and propose a novel adjective\nand noun-based negative sampling strategy for increasing their focus in\ncontrastive learning. Overall, our methods enhance object detection when\ncompared to the state-of-the-art in region-word pretraining. We also highlight\nthe fine-grained utility of an attribute-sensitive model through text-region\nretrieval and phrase grounding analysis.\n","authors":["Kyle Buettner","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2303.10093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10087v1","updated":"2023-03-17T16:10:50Z","published":"2023-03-17T16:10:50Z","title":"Refinement for Absolute Pose Regression with Neural Feature Synthesis","summary":"  Absolute Pose Regression (APR) methods use deep neural networks to directly\nregress camera poses from RGB images. Despite their advantages in inference\nspeed and simplicity, these methods still fall short of the accuracy achieved\nby geometry-based techniques. To address this issue, we propose a new model\ncalled the Neural Feature Synthesizer (NeFeS). Our approach encodes 3D\ngeometric features during training and renders dense novel view features at\ntest time to refine estimated camera poses from arbitrary APR methods. Unlike\nprevious APR works that require additional unlabeled training data, our method\nleverages implicit geometric constraints during test time using a robust\nfeature field. To enhance the robustness of our NeFeS network, we introduce a\nfeature fusion module and a progressive training strategy. Our proposed method\nimproves the state-of-the-art single-image APR accuracy by as much as 54.9% on\nindoor and outdoor benchmark datasets without additional time-consuming\nunlabeled data training.\n","authors":["Shuai Chen","Yash Bhalgat","Xinghui Li","Jiawang Bian","Kejie Li","Zirui Wang","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2303.10087v1.pdf","comment":"Paper Website: http://nefes.active.vision"},{"id":"http://arxiv.org/abs/2303.10083v1","updated":"2023-03-17T16:08:56Z","published":"2023-03-17T16:08:56Z","title":"$α$Surf: Implicit Surface Reconstruction for Semi-Transparent and\n  Thin Objects with Decoupled Geometry and Opacity","summary":"  Implicit surface representations such as the signed distance function (SDF)\nhave emerged as a promising approach for image-based surface reconstruction.\nHowever, existing optimization methods assume solid surfaces and are therefore\nunable to properly reconstruct semi-transparent surfaces and thin structures,\nwhich also exhibit low opacity due to the blending effect with the background.\nWhile neural radiance field (NeRF) based methods can model semi-transparency\nand achieve photo-realistic quality in synthesized novel views, their\nvolumetric geometry representation tightly couples geometry and opacity, and\ntherefore cannot be easily converted into surfaces without introducing\nartifacts. We present $\\alpha$Surf, a novel surface representation with\ndecoupled geometry and opacity for the reconstruction of semi-transparent and\nthin surfaces where the colors mix. Ray-surface intersections on our\nrepresentation can be found in closed-form via analytical solutions of cubic\npolynomials, avoiding Monte-Carlo sampling and is fully differentiable by\nconstruction. Our qualitative and quantitative evaluations show that our\napproach can accurately reconstruct surfaces with semi-transparent and thin\nparts with fewer artifacts, achieving better reconstruction quality than\nstate-of-the-art SDF and NeRF methods. Website: https://alphasurf.netlify.app/\n","authors":["Tianhao Wu","Hanxue Liang","Fangcheng Zhong","Gernot Riegler","Shimon Vainer","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2303.10083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10078v1","updated":"2023-03-17T16:00:18Z","published":"2023-03-17T16:00:18Z","title":"Fuzziness-tuned: Improving the Transferability of Adversarial Examples","summary":"  With the development of adversarial attacks, adversairal examples have been\nwidely used to enhance the robustness of the training models on deep neural\nnetworks. Although considerable efforts of adversarial attacks on improving the\ntransferability of adversarial examples have been developed, the attack success\nrate of the transfer-based attacks on the surrogate model is much higher than\nthat on victim model under the low attack strength (e.g., the attack strength\n$\\epsilon=8/255$). In this paper, we first systematically investigated this\nissue and found that the enormous difference of attack success rates between\nthe surrogate model and victim model is caused by the existence of a special\narea (known as fuzzy domain in our paper), in which the adversarial examples in\nthe area are classified wrongly by the surrogate model while correctly by the\nvictim model. Then, to eliminate such enormous difference of attack success\nrates for improving the transferability of generated adversarial examples, a\nfuzziness-tuned method consisting of confidence scaling mechanism and\ntemperature scaling mechanism is proposed to ensure the generated adversarial\nexamples can effectively skip out of the fuzzy domain. The confidence scaling\nmechanism and the temperature scaling mechanism can collaboratively tune the\nfuzziness of the generated adversarial examples through adjusting the gradient\ndescent weight of fuzziness and stabilizing the update direction, respectively.\nSpecifically, the proposed fuzziness-tuned method can be effectively integrated\nwith existing adversarial attacks to further improve the transferability of\nadverarial examples without changing the time complexity. Extensive experiments\ndemonstrated that fuzziness-tuned method can effectively enhance the\ntransferability of adversarial examples in the latest transfer-based attacks.\n","authors":["Xiangyuan Yang","Jie Lin","Hanlin Zhang","Xinyu Yang","Peng Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10076v1","updated":"2023-03-17T15:57:14Z","published":"2023-03-17T15:57:14Z","title":"A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving","summary":"  The task of estimating 3D occupancy from surrounding view images is an\nexciting development in the field of autonomous driving, following the success\nof Birds Eye View (BEV) perception.This task provides crucial 3D attributes of\nthe driving environment, enhancing the overall understanding and perception of\nthe surrounding space. However, there is still a lack of a baseline to define\nthe task, such as network design, optimization, and evaluation. In this work,\nwe present a simple attempt for 3D occupancy estimation, which is a CNN-based\nframework designed to reveal several key factors for 3D occupancy estimation.\nIn addition, we explore the relationship between 3D occupancy estimation and\nother related tasks, such as monocular depth estimation, stereo matching, and\nBEV perception (3D object detection and map segmentation), which could advance\nthe study on 3D occupancy estimation. For evaluation, we propose a simple\nsampling strategy to define the metric for occupancy evaluation, which is\nflexible for current public datasets. Moreover, we establish a new benchmark in\nterms of the depth estimation metric, where we compare our proposed method with\nmonocular depth estimation methods on the DDAD and Nuscenes datasets.The\nrelevant code will be available in\nhttps://github.com/GANWANSHUI/SimpleOccupancy\n","authors":["Wanshui Gan","Ningkai Mo","Hongbin Xu","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2303.10076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10073v1","updated":"2023-03-17T15:54:30Z","published":"2023-03-17T15:54:30Z","title":"DialogPaint: A Dialog-based Image Editing Model","summary":"  We present DialogPaint, an innovative framework that employs an interactive\nconversational approach for image editing. The framework comprises a pretrained\ndialogue model (Blenderbot) and a diffusion model (Stable Diffusion). The\ndialogue model engages in conversation with users to understand their\nrequirements and generates concise instructions based on the dialogue.\nSubsequently, the Stable Diffusion model employs these instructions, along with\nthe input image, to produce the desired output. Due to the difficulty of\nacquiring fine-tuning data for such models, we leverage multiple large-scale\nmodels to generate simulated dialogues and corresponding image pairs. After\nfine-tuning our framework with the synthesized data, we evaluate its\nperformance in real application scenes. The results demonstrate that\nDialogPaint excels in both objective and subjective evaluation metrics\neffectively handling ambiguous instructions and performing tasks such as object\nreplacement, style transfer, color modification. Moreover, our framework\nsupports multi-round editing, allowing for the completion of complicated\nediting tasks.\n","authors":["Jingxuan Wei","Shiyu Wu","Xin Jiang","Yequan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10073v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.10070v1","updated":"2023-03-17T15:52:45Z","published":"2023-03-17T15:52:45Z","title":"A Unified Continual Learning Framework with General Parameter-Efficient\n  Tuning","summary":"  The \"pre-training $\\rightarrow$ downstream adaptation\" presents both new\nopportunities and challenges for Continual Learning (CL). Although the recent\nstate-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET)\nadaptation paradigm, only prompt has been explored, limiting its application to\nTransformers only. In this paper, we position prompting as one instantiation of\nPET, and propose a unified CL framework with general PET, dubbed as\nLearning-Accumulation-Ensemble (LAE). PET, e.g., using Adapter, LoRA, or\nPrefix, can adapt a pre-trained model to downstream tasks with fewer parameters\nand resources. Given a PET method, our LAE framework incorporates it for CL\nwith three novel designs. 1) Learning: the pre-trained model adapts to the new\ntask by tuning an online PET module, along with our adaptation speed\ncalibration to align different PET modules, 2) Accumulation: the task-specific\nknowledge learned by the online PET module is accumulated into an offline PET\nmodule through momentum update, 3) Ensemble: During inference, we respectively\nconstruct two experts with online/offline PET modules (which are favored by the\nnovel/historical tasks) for prediction ensemble. We show that LAE is compatible\nwith a battery of PET methods and gains strong CL capability. For example, LAE\nwith Adaptor PET surpasses the prior state-of-the-art by 1.3% and 3.6% in\nlast-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively.\n","authors":["Qiankun Gao","Chen Zhao","Yifan Sun","Teng Xi","Gang Zhang","Bernard Ghanem","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10070v1.pdf","comment":"Tech Report. Code is available at https://github.com/gqk/LAE"},{"id":"http://arxiv.org/abs/2303.02735v2","updated":"2023-03-17T15:49:09Z","published":"2023-03-05T18:02:54Z","title":"Scalable Object Detection on Embedded Devices Using Weight Pruning and\n  Singular Value Decomposition","summary":"  This paper presents a method for optimizing object detection models by\ncombining weight pruning and singular value decomposition (SVD). The proposed\nmethod was evaluated on a custom dataset of street work images obtained from\nhttps://universe.roboflow.com/roboflow-100/street-work. The dataset consists of\n611 training images, 175 validation images, and 87 test images with 7 classes.\nWe compared the performance of the optimized models with the original\nunoptimized model in terms of frame rate, mean average precision (mAP@50), and\nweight size. The results show that the weight pruning + SVD model achieved a\n0.724 mAP@50 with a frame rate of 1.48 FPS and a weight size of 12.1 MB,\noutperforming the original model (0.717 mAP@50, 1.50 FPS, and 12.3 MB).\nPrecision-recall curves were also plotted for all models. Our work demonstrates\nthat the proposed method can effectively optimize object detection models while\nbalancing accuracy, speed, and model size.\n","authors":["Dohyun Ham","Jaeyeop Jeong","June-Kyoo Park","Raehyeon Jeong","Seungmin Jeon","Hyeongjun Jeon","Yewon Lim"],"pdf_url":"https://arxiv.org/pdf/2303.02735v2.pdf","comment":"8 pages, 3 figures. A report of the project done as part of the\n  Yonsei-Roboin project for the 2nd semester, 2022"},{"id":"http://arxiv.org/abs/2303.10062v1","updated":"2023-03-17T15:44:44Z","published":"2023-03-17T15:44:44Z","title":"Confidence-aware 3D Gaze Estimation and Evaluation Metric","summary":"  Deep learning appearance-based 3D gaze estimation is gaining popularity due\nto its minimal hardware requirements and being free of constraint. Unreliable\nand overconfident inferences, however, still limit the adoption of this gaze\nestimation method. To address the unreliable and overconfident issues, we\nintroduce a confidence-aware model that predicts uncertainties together with\ngaze angle estimations. We also introduce a novel effectiveness evaluation\nmethod based on the causality between eye feature degradation and the rise in\ninference uncertainty to assess the uncertainty estimation. Our\nconfidence-aware model demonstrates reliable uncertainty estimations while\nproviding angular estimation accuracies on par with the state-of-the-art.\nCompared with the existing statistical uncertainty-angular-error evaluation\nmetric, the proposed effectiveness evaluation approach can more effectively\njudge inferred uncertainties' performance at each prediction.\n","authors":["Qiaojie Zheng","Jiucai Zhang","Amy Zhang","Xiaoli Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10062v1.pdf","comment":"9 pages 12 figures"},{"id":"http://arxiv.org/abs/2303.10058v1","updated":"2023-03-17T15:38:39Z","published":"2023-03-17T15:38:39Z","title":"No Fear of Classifier Biases: Neural Collapse Inspired Federated\n  Learning with Synthetic and Fixed Classifier","summary":"  Data heterogeneity is an inherent challenge that hinders the performance of\nfederated learning (FL). Recent studies have identified the biased classifiers\nof local models as the key bottleneck. Previous attempts have used classifier\ncalibration after FL training, but this approach falls short in improving the\npoor feature representations caused by training-time classifier biases.\nResolving the classifier bias dilemma in FL requires a full understanding of\nthe mechanisms behind the classifier. Recent advances in neural collapse have\nshown that the classifiers and feature prototypes under perfect training\nscenarios collapse into an optimal structure called simplex equiangular tight\nframe (ETF). Building on this neural collapse insight, we propose a solution to\nthe FL's classifier bias problem by utilizing a synthetic and fixed ETF\nclassifier during training. The optimal classifier structure enables all\nclients to learn unified and optimal feature representations even under\nextremely heterogeneous data. We devise several effective modules to better\nadapt the ETF structure in FL, achieving both high generalization and\npersonalization. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet.\n","authors":["Zexi Li","Xinyi Shang","Rui He","Tao Lin","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.10058v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.10056v1","updated":"2023-03-17T15:37:07Z","published":"2023-03-17T15:37:07Z","title":"GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation","summary":"  Text-to-image (T2I) models based on diffusion processes have achieved\nremarkable success in controllable image generation using user-provided\ncaptions. However, the tight coupling between the current text encoder and\nimage decoder in T2I models makes it challenging to replace or upgrade. Such\nchanges often require massive fine-tuning or even training from scratch with\nthe prohibitive expense. To address this problem, we propose GlueGen, which\napplies a newly proposed GlueNet model to align features from single-modal or\nmulti-modal encoders with the latent space of an existing T2I model. The\napproach introduces a new training objective that leverages parallel corpora to\nalign the representation spaces of different encoders. Empirical results show\nthat GlueNet can be trained efficiently and enables various capabilities beyond\nprevious state-of-the-art models: 1) multilingual language models such as\nXLM-Roberta can be aligned with existing T2I models, allowing for the\ngeneration of high-quality images from captions beyond English; 2) GlueNet can\nalign multi-modal encoders such as AudioCLIP with the Stable Diffusion model,\nenabling sound-to-image generation; 3) it can also upgrade the current text\nencoder of the latent diffusion model for challenging case generation. By the\nalignment of various feature representations, the GlueNet allows for flexible\nand efficient integration of new functionality into existing T2I models and\nsheds light on X-to-image (X2I) generation.\n","authors":["Can Qin","Ning Yu","Chen Xing","Shu Zhang","Zeyuan Chen","Stefano Ermon","Yun Fu","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.10056v1.pdf","comment":"26 pages, 23 figures"},{"id":"http://arxiv.org/abs/2303.10049v1","updated":"2023-03-17T15:23:15Z","published":"2023-03-17T15:23:15Z","title":"Uncertainty-informed Mutual Learning for Joint Medical Image\n  Classification and Segmentation","summary":"  Classification and segmentation are crucial in medical image analysis as they\nenable accurate diagnosis and disease monitoring. However, current methods\noften prioritize the mutual learning features and shared model parameters,\nwhile neglecting the reliability of features and performances. In this paper,\nwe propose a novel Uncertainty-informed Mutual Learning (UML) framework for\nreliable and interpretable medical image analysis. Our UML introduces\nreliability to joint classification and segmentation tasks, leveraging mutual\nlearning with uncertainty to improve performance. To achieve this, we first use\nevidential deep learning to provide image-level and pixel-wise confidences.\nThen, an Uncertainty Navigator Decoder is constructed for better using mutual\nfeatures and generating segmentation results. Besides, an Uncertainty\nInstructor is proposed to screen reliable masks for classification. Overall,\nUML could produce confidence estimation in features and performance for each\nlink (classification and segmentation). The experiments on the public datasets\ndemonstrate that our UML outperforms existing methods in terms of both accuracy\nand robustness. Our UML has the potential to explore the development of more\nreliable and explainable medical image analysis models. We will release the\ncodes for reproduction after acceptance.\n","authors":["Kai Ren","Ke Zou","Xianjie Liu","Yidi Chen","Xuedong Yuan","Xiaojing Shen","Meng Wang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.10049v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.10042v1","updated":"2023-03-17T15:12:25Z","published":"2023-03-17T15:12:25Z","title":"ShaRPy: Shape Reconstruction and Hand Pose Estimation from RGB-D with\n  Uncertainty","summary":"  Despite their potential, markerless hand tracking technologies are not yet\napplied in practice to the diagnosis or monitoring of the activity in\ninflammatory musculoskeletal diseases. One reason is that the focus of most\nmethods lies in the reconstruction of coarse, plausible poses for gesture\nrecognition or AR/VR applications, whereas in the clinical context, accurate,\ninterpretable, and reliable results are required. Therefore, we propose ShaRPy,\nthe first RGB-D Shape Reconstruction and hand Pose tracking system, which\nprovides uncertainty estimates of the computed pose to guide clinical\ndecision-making. Our method requires only a light-weight setup with a single\nconsumer-level RGB-D camera yet it is able to distinguish similar poses with\nonly small joint angle deviations. This is achieved by combining a data-driven\ndense correspondence predictor with traditional energy minimization, optimizing\nfor both, pose and hand shape parameters. We evaluate ShaRPy on a keypoint\ndetection benchmark and show qualitative results on recordings of a patient.\n","authors":["Vanessa Wirth","Anna-Maria Liphardt","Birte Coppers","Johanna Bräunig","Simon Heinrich","Arnd Kleyer","Georg Schett","Martin Vossiek","Bernhard Egger","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2303.10042v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.14563v2","updated":"2023-03-17T15:12:13Z","published":"2022-11-26T13:33:42Z","title":"Who are you referring to? Coreference resolution in image narrations","summary":"  Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.\n","authors":["Arushi Goel","Basura Fernando","Frank Keller","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2211.14563v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.10033v1","updated":"2023-03-17T15:03:58Z","published":"2023-03-17T15:03:58Z","title":"Multi-modal Expression Recognition with Ensemble Method","summary":"  This paper presents our submission to the Expression Classification Challenge\nof the fifth Affective Behavior Analysis in-the-wild (ABAW) Competition. In our\nmethod, multimodal feature combinations extracted by several different\npre-trained models are applied to capture more effective emotional information.\nFor these combinations of visual and audio modal features, we utilize two\ntemporal encoders to explore the temporal contextual information in the data.\nIn addition, we employ several ensemble strategies for different experimental\nsettings to obtain the most accurate expression recognition results. Our system\nachieves the average F1 Score of 0.45774 on the validation set.\n","authors":["Chuanhe Liu","Xinjie Zhang","Xiaolong Liu","Tenggan Zhang","Liyu Meng","Yuchen Liu","Yuanyuan Deng","Wenqiang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.10033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07677v2","updated":"2023-03-17T14:48:57Z","published":"2023-03-14T07:26:55Z","title":"SR-init: An interpretable layer pruning method","summary":"  Despite the popularization of deep neural networks (DNNs) in many fields, it\nis still challenging to deploy state-of-the-art models to resource-constrained\ndevices due to high computational overhead. Model pruning provides a feasible\nsolution to the aforementioned challenges. However, the interpretation of\nexisting pruning criteria is always overlooked. To counter this issue, we\npropose a novel layer pruning method by exploring the Stochastic\nRe-initialization. Our SR-init method is inspired by the discovery that the\naccuracy drop due to stochastic re-initialization of layer parameters differs\nin various layers. On the basis of this observation, we come up with a layer\npruning criterion, i.e., those layers that are not sensitive to stochastic\nre-initialization (low accuracy drop) produce less contribution to the model\nand could be pruned with acceptable loss. Afterward, we experimentally verify\nthe interpretability of SR-init via feature visualization. The visual\nexplanation demonstrates that SR-init is theoretically feasible, thus we\ncompare it with state-of-the-art methods to further evaluate its\npracticability. As for ResNet56 on CIFAR-10 and CIFAR-100, SR-init achieves a\ngreat reduction in parameters (63.98% and 37.71%) with an ignorable drop in\ntop-1 accuracy (-0.56% and 0.8%). With ResNet50 on ImageNet, we achieve a\n15.59% FLOPs reduction by removing 39.29% of the parameters, with only a drop\nof 0.6% in top-1 accuracy. Our code is available at\nhttps://github.com/huitang-zjut/SR-init.\n","authors":["Hui Tang","Yao Lu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2303.07677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08888v2","updated":"2023-03-17T14:40:14Z","published":"2023-03-15T19:16:47Z","title":"Stochastic Segmentation with Conditional Categorical Diffusion Models","summary":"  Semantic segmentation has made significant progress in recent years thanks to\ndeep neural networks, but the common objective of generating a single\nsegmentation output that accurately matches the image's content may not be\nsuitable for safety-critical domains such as medical diagnostics and autonomous\ndriving. Instead, multiple possible correct segmentation maps may be required\nto reflect the true distribution of annotation maps. In this context,\nstochastic semantic segmentation methods must learn to predict conditional\ndistributions of labels given the image, but this is challenging due to the\ntypically multimodal distributions, high-dimensional output spaces, and limited\nannotation data. To address these challenges, we propose a conditional\ncategorical diffusion model (CCDM) for semantic segmentation based on Denoising\nDiffusion Probabilistic Models. Our model is conditioned to the input image,\nenabling it to generate multiple segmentation label maps that account for the\naleatoric uncertainty arising from divergent ground truth annotations. Our\nexperimental results show that CCDM achieves state-of-the-art performance on\nLIDC, a stochastic semantic segmentation dataset, and outperforms established\nbaselines on the classical segmentation dataset Cityscapes.\n","authors":["Lukas Zbinden","Lars Doorenbos","Theodoros Pissas","Raphael Sznitman","Pablo Márquez-Neila"],"pdf_url":"https://arxiv.org/pdf/2303.08888v2.pdf","comment":"Code available at\n  https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation"},{"id":"http://arxiv.org/abs/2209.00776v2","updated":"2023-03-17T14:33:59Z","published":"2022-09-02T01:34:14Z","title":"WOC: A Handy Webcam-based 3D Online Chatroom","summary":"  We develop WOC, a webcam-based 3D virtual online chatroom for multi-person\ninteraction, which captures the 3D motion of users and drives their individual\n3D virtual avatars in real-time. Compared to the existing wearable\nequipment-based solution, WOC offers convenient and low-cost 3D motion capture\nwith a single camera. To promote the immersive chat experience, WOC provides\nhigh-fidelity virtual avatar manipulation, which also supports the user-defined\ncharacters. With the distributed data flow service, the system delivers highly\nsynchronized motion and voice for all users. Deployed on the website and no\ninstallation required, users can freely experience the virtual online chat at\nhttps://yanch.cloud.\n","authors":["Chuanhang Yan","Yu Sun","Qian Bao","Jinhui Pang","Wu Liu","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2209.00776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13093v2","updated":"2023-03-17T14:25:29Z","published":"2022-11-23T16:25:49Z","title":"Autonomous Marker-less Rapid Aerial Grasping","summary":"  In a future with autonomous robots, visual and spatial perception is of\nutmost importance for robotic systems. Particularly for aerial robotics, there\nare many applications where utilizing visual perception is necessary for any\nreal-world scenarios. Robotic aerial grasping using drones promises fast\npick-and-place solutions with a large increase in mobility over other robotic\nsolutions. Utilizing Mask R-CNN scene segmentation (detectron2), we propose a\nvision-based system for autonomous rapid aerial grasping which does not rely on\nmarkers for object localization and does not require the appearence of the\nobject to be previously known. Combining segmented images with spatial\ninformation from a depth camera, we generate a dense point cloud of the\ndetected objects and perform geometry-based grasp planning to determine\ngrasping points on the objects. In real-world experiments on a dynamically\ngrasping aerial platform, we show that our system can replicate the performance\nof a motion capture system for object localization up to 94.5% of the baseline\ngrasping success rate. With our results, we show the first use of\ngeometry-based grasping techniques with a flying platform and aim to increase\nthe autonomy of existing aerial manipulation platforms, bringing them further\ntowards real-world applications in warehouses and similar environments.\n","authors":["Erik Bauer","Barnabas Gavin Cangan","Robert K. Katzschmann"],"pdf_url":"https://arxiv.org/pdf/2211.13093v2.pdf","comment":"8 pages, 12 figures, preprint of submission to IEEE/RSJ International\n  Conference on Intelligent Robots and Systems 2023"},{"id":"http://arxiv.org/abs/2303.09998v1","updated":"2023-03-17T14:20:28Z","published":"2023-03-17T14:20:28Z","title":"TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint\n  Perception and Prediction in Vision-Centric Autonomous Driving","summary":"  Vision-centric joint perception and prediction (PnP) has become an emerging\ntrend in autonomous driving research. It predicts the future states of the\ntraffic participants in the surrounding environment from raw RGB images.\nHowever, it is still a critical challenge to synchronize features obtained at\nmultiple camera views and timestamps due to inevitable geometric distortions\nand further exploit those spatial-temporal features. To address this issue, we\npropose a temporal bird's-eye-view pyramid transformer (TBP-Former) for\nvision-centric PnP, which includes two novel designs. First, a\npose-synchronized BEV encoder is proposed to map raw image inputs with any\ncamera pose at any time to a shared and synchronized BEV space for better\nspatial-temporal synchronization. Second, a spatial-temporal pyramid\ntransformer is introduced to comprehensively extract multi-scale BEV features\nand predict future BEV states with the support of spatial-temporal priors.\nExtensive experiments on nuScenes dataset show that our proposed framework\noverall outperforms all state-of-the-art vision-based prediction methods.\n","authors":["Shaoheng Fang","Zi Wang","Yiqi Zhong","Junhao Ge","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09998v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09992v1","updated":"2023-03-17T14:07:55Z","published":"2023-03-17T14:07:55Z","title":"LION: Implicit Vision Prompt Tuning","summary":"  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n","authors":["Haixin Wang","Jianlong Chang","Xiao Luo","Jinan Sun","Zhouchen Lin","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09988v1","updated":"2023-03-17T14:03:49Z","published":"2023-03-17T14:03:49Z","title":"Star-Net: Improving Single Image Desnowing Model With More Efficient\n  Connection and Diverse Feature Interaction","summary":"  Compared to other severe weather image restoration tasks, single image\ndesnowing is a more challenging task. This is mainly due to the diversity and\nirregularity of snow shape, which makes it extremely difficult to restore\nimages in snowy scenes. Moreover, snow particles also have a veiling effect\nsimilar to haze or mist. Although current works can effectively remove snow\nparticles with various shapes, they also bring distortion to the restored\nimage. To address these issues, we propose a novel single image desnowing\nnetwork called Star-Net. First, we design a Star type Skip Connection (SSC) to\nestablish information channels for all different scale features, which can deal\nwith the complex shape of snow particles.Second, we present a Multi-Stage\nInteractive Transformer (MIT) as the base module of Star-Net, which is designed\nto better understand snow particle shapes and to address image distortion by\nexplicitly modeling a variety of important image recovery features. Finally, we\npropose a Degenerate Filter Module (DFM) to filter the snow particle and snow\nfog residual in the SSC on the spatial and channel domains. Extensive\nexperiments show that our Star-Net achieves state-of-the-art snow removal\nperformances on three standard snow removal datasets and retains the original\nsharpness of the images.\n","authors":["Jiawei Mao","Yuanqi Chang","Xuesong Yin","Binling Nie"],"pdf_url":"https://arxiv.org/pdf/2303.09988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09987v1","updated":"2023-03-17T14:03:40Z","published":"2023-03-17T14:03:40Z","title":"Breast Cancer Histopathology Image based Gene Expression Prediction\n  using Spatial Transcriptomics data and Deep Learning","summary":"  Tumour heterogeneity in breast cancer poses challenges in predicting outcome\nand response to therapy. Spatial transcriptomics technologies may address these\nchallenges, as they provide a wealth of information about gene expression at\nthe cell level, but they are expensive, hindering their use in large-scale\nclinical oncology studies. Predicting gene expression from hematoxylin and\neosin stained histology images provides a more affordable alternative for such\nstudies. Here we present BrST-Net, a deep learning framework for predicting\ngene expression from histopathology images using spatial transcriptomics data.\nUsing this framework, we trained and evaluated 10 state-of-the-art deep\nlearning models without utilizing pretrained weights for the prediction of 250\ngenes. To enhance the generalisation performance of the main network, we\nintroduce an auxiliary network into the framework. Our methodology outperforms\nprevious studies, with 237 genes identified with positive correlation,\nincluding 24 genes with a median correlation coefficient greater than 0.50.\nThis is a notable improvement over previous studies, which could predict only\n102 genes with positive correlation, with the highest correlation values\nranging from 0.29 to 0.34.\n","authors":["Md Mamunur Rahaman","Ewan K. A. Millar","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2303.09987v1.pdf","comment":"14 pages, 6 tables, 6 figures"},{"id":"http://arxiv.org/abs/2303.07130v3","updated":"2023-03-17T13:58:52Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) based scoring method is used to identify the extent of\nlung involvement observed on a CT scan. This paper presents a domain\nknowledge-based pipeline for extracting regions of infection in COVID-19\npatients using a combination of image-processing algorithms and a pre-trained\nUNET model. The severity of the infection is then classified into different\ncategories using an ensemble of three machine-learning models: Extreme Gradient\nBoosting, Extremely Randomized Trees, and Support Vector Machine. The proposed\nsystem was evaluated on a validation dataset in the AI-Enabled Medical Image\nAnalysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and\nachieved a macro F1 score of 64%. These results demonstrate the potential of\ncombining domain knowledge with machine learning techniques for accurate\nCOVID-19 diagnosis using CT scans. The implementation of the proposed system\nfor severity analysis is available at\n\\textit{https://github.com/aanandt/Enhancing-COVID-19-Severity-Analysis-through-Ensemble-Methods.git\n}\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09977v1","updated":"2023-03-17T13:50:18Z","published":"2023-03-17T13:50:18Z","title":"Semantic Scene Completion with Cleaner Self","summary":"  Semantic Scene Completion (SSC) transforms an image of single-view depth\nand/or RGB 2D pixels into 3D voxels, each of whose semantic labels are\npredicted. SSC is a well-known ill-posed problem as the prediction model has to\n\"imagine\" what is behind the visible surface, which is usually represented by\nTruncated Signed Distance Function (TSDF). Due to the sensory imperfection of\nthe depth camera, most existing methods based on the noisy TSDF estimated from\ndepth values suffer from 1) incomplete volumetric predictions and 2) confused\nsemantic labels. To this end, we use the ground-truth 3D voxels to generate a\nperfect visible surface, called TSDF-CAD, and then train a \"cleaner\" SSC model.\nAs the model is noise-free, it is expected to focus more on the \"imagination\"\nof unseen voxels. Then, we propose to distill the intermediate \"cleaner\"\nknowledge into another model with noisy TSDF input. In particular, we use the\n3D occupancy feature and the semantic relations of the \"cleaner self\" to\nsupervise the counterparts of the \"noisy self\" to respectively address the\nabove two incorrect predictions. Experimental results validate that our method\nimproves the noisy counterparts with 3.1% IoU and 2.2% mIoU for measuring scene\ncompletion and SSC, and also achieves new state-of-the-art accuracy on the\npopular NYU dataset.\n","authors":["Fengyun Wang","Dong Zhang","Hanwang Zhang","Jinhui Tang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2303.09977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v1","updated":"2023-03-17T13:48:17Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.09972v1","updated":"2023-03-17T13:44:52Z","published":"2023-03-17T13:44:52Z","title":"Neighborhood Averaging for Improving Outlier Detectors","summary":"  We hypothesize that similar objects should have similar outlier scores. To\nour knowledge, all existing outlier detectors calculate the outlier score for\neach object independently regardless of the outlier scores of the other\nobjects. Therefore, they do not guarantee that similar objects have similar\noutlier scores. To verify our proposed hypothesis, we propose an outlier score\npost-processing technique for outlier detectors, called neighborhood\naveraging(NA), which pays attention to objects and their neighbors and\nguarantees them to have more similar outlier scores than their original scores.\nGiven an object and its outlier score from any outlier detector, NA modifies\nits outlier score by combining it with its k nearest neighbors' scores. We\ndemonstrate the effectivity of NA by using the well-known k-nearest neighbors\n(k-NN). Experimental results show that NA improves all 10 tested baseline\ndetectors by 13% (from 0.70 to 0.79 AUC) on average evaluated on nine\nreal-world datasets. Moreover, even outlier detectors that are already based on\nk-NN are also improved. The experiments also show that in some applications,\nthe choice of detector is no more significant when detectors are jointly used\nwith NA, which may pose a challenge to the generally considered idea that the\ndata model is the most important factor. We open our code on www.outlierNet.com\nfor reproducibility.\n","authors":["Jiawei Yang","Susanto Rahardja","Pasi Franti"],"pdf_url":"https://arxiv.org/pdf/2303.09972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09962v1","updated":"2023-03-17T13:34:38Z","published":"2023-03-17T13:34:38Z","title":"Adversarial Counterfactual Visual Explanations","summary":"  Counterfactual explanations and adversarial attacks have a related goal:\nflipping output labels with minimal perturbations regardless of their\ncharacteristics. Yet, adversarial attacks cannot be used directly in a\ncounterfactual explanation perspective, as such perturbations are perceived as\nnoise and not as actionable and understandable image modifications. Building on\nthe robust learning literature, this paper proposes an elegant method to turn\nadversarial attacks into semantically meaningful perturbations, without\nmodifying the classifiers to explain. The proposed approach hypothesizes that\nDenoising Diffusion Probabilistic Models are excellent regularizers for\navoiding high-frequency and out-of-distribution perturbations when generating\nadversarial attacks. The paper's key idea is to build attacks through a\ndiffusion model to polish them. This allows studying the target model\nregardless of its robustification level. Extensive experimentation shows the\nadvantages of our counterfactual explanation approach over current\nState-of-the-Art in multiple testbeds.\n","authors":["Guillaume Jeanneret","Loïc Simon","Frédéric Jurie"],"pdf_url":"https://arxiv.org/pdf/2303.09962v1.pdf","comment":"CVPR 2023 camera-ready; Main manuscript + supplementary material"},{"id":"http://arxiv.org/abs/2206.12126v2","updated":"2023-03-17T13:25:42Z","published":"2022-06-24T07:43:50Z","title":"Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive\n  Learning","summary":"  Spatiotemporal predictive learning aims to generate future frames by learning\nfrom historical frames. In this paper, we investigate existing methods and\npresent a general framework of spatiotemporal predictive learning, in which the\nspatial encoder and decoder capture intra-frame features and the middle\ntemporal module catches inter-frame correlations. While the mainstream methods\nemploy recurrent units to capture long-term temporal dependencies, they suffer\nfrom low computational efficiency due to their unparallelizable architectures.\nTo parallelize the temporal module, we propose the Temporal Attention Unit\n(TAU), which decomposes the temporal attention into intra-frame statical\nattention and inter-frame dynamical attention. Moreover, while the mean squared\nerror loss focuses on intra-frame errors, we introduce a novel differential\ndivergence regularization to take inter-frame variations into account.\nExtensive experiments demonstrate that the proposed method enables the derived\nmodel to achieve competitive performance on various spatiotemporal prediction\nbenchmarks.\n","authors":["Cheng Tan","Zhangyang Gao","Lirong Wu","Yongjie Xu","Jun Xia","Siyuan Li","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2206.12126v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09956v1","updated":"2023-03-17T13:25:29Z","published":"2023-03-17T13:25:29Z","title":"GNNFormer: A Graph-based Framework for Cytopathology Report Generation","summary":"  Cytopathology report generation is a necessary step for the standardized\nexamination of pathology images. However, manually writing detailed reports\nbrings heavy workloads for pathologists. To improve efficiency, some existing\nworks have studied automatic generation of cytopathology reports, mainly by\napplying image caption generation frameworks with visual encoders originally\nproposed for natural images. A common weakness of these works is that they do\nnot explicitly model the structural information among cells, which is a key\nfeature of pathology images and provides significant information for making\ndiagnoses. In this paper, we propose a novel graph-based framework called\nGNNFormer, which seamlessly integrates graph neural network (GNN) and\nTransformer into the same framework, for cytopathology report generation. To\nthe best of our knowledge, GNNFormer is the first report generation method that\nexplicitly models the structural information among cells in pathology images.\nIt also effectively fuses structural information among cells, fine-grained\nmorphology features of cells and background features to generate high-quality\nreports. Experimental results on the NMI-WSI dataset show that GNNFormer can\noutperform other state-of-the-art baselines.\n","authors":["Yang-Fan Zhou","Kai-Lang Yao","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2303.09956v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.13869v2","updated":"2023-03-17T13:25:21Z","published":"2023-02-27T15:17:01Z","title":"EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View\n  Identification in Pediatric Echocardiography","summary":"  An efficient decoupled masked autoencoder (EDMAE), which is a novel\nself-supervised method is proposed for standard view recognition in pediatric\nechocardiography in this paper. The proposed EDMAE based on the encoder-decoder\nstructure forms a new proxy task. The decoder of EDMAE consists of a teacher\nencoder and a student encoder, in which the teacher encoder extracts the latent\nrepresentation of the masked image blocks, while the student encoder extracts\nthe latent representation of the visible image blocks. A loss is calculated\nbetween the feature maps output from two encoders to ensure consistency in the\nlatent representations they extracted. EDMAE replaces the VIT structure in the\nencoder of traditional MAE with pure convolution operation to improve training\nefficiency. EDMAE is pre-trained in a self-supervised manner on a large-scale\nprivate dataset of pediatric echocardiography, and then fine-tuned on the\ndownstream task of standard view recognition. The high classification accuracy\nis achieved in 27 standard views of pediatric echocardiography. To further\nvalidate the effectiveness of the proposed method, another downstream task of\ncardiac ultrasound segmentation is performed on a public dataset CAMUS. The\nexperiments show that the proposed method not only can surpass some recent\nsupervised methods but also has more competitiveness on different downstream\ntasks.\n","authors":["Yiman Liu","Xiaoxiang Han","Tongtong Liang","Qiaohong Liu","Qingli Li","Yuqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.13869v2.pdf","comment":"13 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.09952v1","updated":"2023-03-17T13:19:25Z","published":"2023-03-17T13:19:25Z","title":"Single-view Neural Radiance Fields with Depth Teacher","summary":"  Neural Radiance Fields (NeRF) have been proposed for photorealistic novel\nview rendering. However, it requires many different views of one scene for\ntraining. Moreover, it has poor generalizations to new scenes and requires\nretraining or fine-tuning on each scene. In this paper, we develop a new NeRF\nmodel for novel view synthesis using only a single image as input. We propose\nto combine the (coarse) planar rendering and the (fine) volume rendering to\nachieve higher rendering quality and better generalizations. We also design a\ndepth teacher net that predicts dense pseudo depth maps to supervise the joint\nrendering mechanism and boost the learning of consistent 3D geometry. We\nevaluate our method on three challenging datasets. It outperforms\nstate-of-the-art single-view NeRFs by achieving 5$\\sim$20\\% improvements in\nPSNR and reducing 20$\\sim$50\\% of the errors in the depth rendering. It also\nshows excellent generalization abilities to unseen data without the need to\nfine-tune on each new scene.\n","authors":["Yurui Chen","Chun Gu","Feihu Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09950v1","updated":"2023-03-17T13:15:03Z","published":"2023-03-17T13:15:03Z","title":"Deep Graph-based Spatial Consistency for Robust Non-rigid Point Cloud\n  Registration","summary":"  We study the problem of outlier correspondence pruning for non-rigid point\ncloud registration. In rigid registration, spatial consistency has been a\ncommonly used criterion to discriminate outliers from inliers. It measures the\ncompatibility of two correspondences by the discrepancy between the respective\ndistances in two point clouds. However, spatial consistency no longer holds in\nnon-rigid cases and outlier rejection for non-rigid registration has not been\nwell studied. In this work, we propose Graph-based Spatial Consistency Network\n(GraphSCNet) to filter outliers for non-rigid registration. Our method is based\non the fact that non-rigid deformations are usually locally rigid, or local\nshape preserving. We first design a local spatial consistency measure over the\ndeformation graph of the point cloud, which evaluates the spatial compatibility\nonly between the correspondences in the vicinity of a graph node. An\nattention-based non-rigid correspondence embedding module is then devised to\nlearn a robust representation of non-rigid correspondences from local spatial\nconsistency. Despite its simplicity, GraphSCNet effectively improves the\nquality of the putative correspondences and attains state-of-the-art\nperformance on three challenging benchmarks. Our code and models are available\nat https://github.com/qinzheng93/GraphSCNet.\n","authors":["Zheng Qin","Hao Yu","Changjian Wang","Yuxing Peng","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09950v1.pdf","comment":"Accepted by CVPR 2023. Our code and models are available at\n  https://github.com/qinzheng93/GraphSCNet"},{"id":"http://arxiv.org/abs/2303.09375v2","updated":"2023-03-17T13:08:26Z","published":"2023-03-16T15:04:10Z","title":"DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human\n  Avatars","summary":"  We present DINAR, an approach for creating realistic rigged fullbody avatars\nfrom single RGB images. Similarly to previous works, our method uses neural\ntextures combined with the SMPL-X body model to achieve photo-realistic quality\nof avatars while keeping them easy to animate and fast to infer. To restore the\ntexture, we use a latent diffusion model and show how such model can be trained\nin the neural texture space. The use of the diffusion model allows us to\nrealistically reconstruct large unseen regions such as the back of a person\ngiven the frontal view. The models in our pipeline are trained using 2D images\nand videos only. In the experiments, our approach achieves state-of-the-art\nrendering quality and good generalization to new poses and viewpoints. In\nparticular, the approach improves state-of-the-art on the SnapshotPeople public\nbenchmark.\n","authors":["David Svitov","Dmitrii Gudkov","Renat Bashirov","Victor Lempitsky"],"pdf_url":"https://arxiv.org/pdf/2303.09375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03770v3","updated":"2023-03-17T12:59:22Z","published":"2023-03-07T10:04:55Z","title":"Guiding Pseudo-labels with Uncertainty Estimation for Source-free\n  Unsupervised Domain Adaptation","summary":"  Standard Unsupervised Domain Adaptation (UDA) methods assume the availability\nof both source and target data during the adaptation. In this work, we\ninvestigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific\ncase of UDA where a model is adapted to a target domain without access to\nsource data. We propose a novel approach for the SF-UDA setting based on a loss\nreweighting strategy that brings robustness against the noise that inevitably\naffects the pseudo-labels. The classification loss is reweighted based on the\nreliability of the pseudo-labels that is measured by estimating their\nuncertainty. Guided by such reweighting strategy, the pseudo-labels are\nprogressively refined by aggregating knowledge from neighbouring samples.\nFurthermore, a self-supervised contrastive framework is leveraged as a target\nspace regulariser to enhance such knowledge aggregation. A novel negative pairs\nexclusion strategy is proposed to identify and exclude negative pairs made of\nsamples sharing the same class, even in presence of some noise in the\npseudo-labels. Our method outperforms previous methods on three major\nbenchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C\nand DomainNet with a performance gain of +1.8% on both benchmarks and on PACS\nwith +12.3% in the single-source setting and +6.6% in multi-target adaptation.\nAdditional analyses demonstrate that the proposed approach is robust to the\nnoise, which results in significantly more accurate pseudo-labels compared to\nstate-of-the-art approaches.\n","authors":["Mattia Litrico","Alessio Del Bue","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2303.03770v3.pdf","comment":"To be published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.09941v1","updated":"2023-03-17T12:55:22Z","published":"2023-03-17T12:55:22Z","title":"Leaping Into Memories: Space-Time Deep Feature Synthesis","summary":"  The success of deep learning models has led to their adaptation and adoption\nby prominent video understanding methods. The majority of these approaches\nencode features in a joint space-time modality for which the inner workings and\nlearned representations are difficult to visually interpret. We propose LEArned\nPreconscious Synthesis (LEAPS), an architecture-agnostic method for\nsynthesizing videos from the internal spatiotemporal representations of models.\nUsing a stimulus video and a target class, we prime a fixed space-time model\nand iteratively optimize a video initialized with random noise. We incorporate\nadditional regularizers to improve the feature diversity of the synthesized\nvideos as well as the cross-frame temporal coherence of motions. We\nquantitatively and qualitatively evaluate the applicability of LEAPS by\ninverting a range of spatiotemporal convolutional and attention-based\narchitectures trained on Kinetics-400, which to the best of our knowledge has\nnot been previously accomplished.\n","authors":["Alexandros Stergiou","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2303.09941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07275v2","updated":"2023-03-17T12:44:18Z","published":"2022-12-14T15:17:46Z","title":"PhoMoH: Implicit Photorealistic 3D Models of Human Heads","summary":"  We present PhoMoH, a neural network methodology to construct generative\nmodels of photo-realistic 3D geometry and appearance of human heads including\nhair, beards, an oral cavity, and clothing. In contrast to prior work, PhoMoH\nmodels the human head using neural fields, thus supporting complex topology.\nInstead of learning a head model from scratch, we propose to augment an\nexisting expressive head model with new features. Concretely, we learn a highly\ndetailed geometry network layered on top of a mid-resolution head model\ntogether with a detailed, local geometry-aware, and disentangled color field.\nOur proposed architecture allows us to learn photo-realistic human head models\nfrom relatively little data. The learned generative geometry and appearance\nnetworks can be sampled individually and enable the creation of diverse and\nrealistic human heads. Extensive experiments validate our method qualitatively\nand across different metrics.\n","authors":["Mihai Zanfir","Thiemo Alldieck","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2212.07275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06175v2","updated":"2023-03-17T12:38:53Z","published":"2023-02-13T08:23:35Z","title":"Learning and Aggregating Lane Graphs for Urban Automated Driving","summary":"  Lane graph estimation is an essential and highly challenging task in\nautomated driving and HD map learning. Existing methods using either onboard or\naerial imagery struggle with complex lane topologies, out-of-distribution\nscenarios, or significant occlusions in the image space. Moreover, merging\noverlapping lane graphs to obtain consistent large-scale graphs remains\ndifficult. To overcome these challenges, we propose a novel bottom-up approach\nto lane graph estimation from aerial imagery that aggregates multiple\noverlapping graphs into a single consistent graph. Due to its modular design,\nour method allows us to address two complementary tasks: predicting\nego-respective successor lane graphs from arbitrary vehicle positions using a\ngraph neural network and aggregating these predictions into a consistent global\nlane graph. Extensive experiments on a large-scale lane graph dataset\ndemonstrate that our approach yields highly accurate lane graphs, even in\nregions with severe occlusions. The presented approach to graph aggregation\nproves to eliminate inconsistent predictions while increasing the overall graph\nquality. We make our large-scale urban lane graph dataset and code publicly\navailable at http://urbanlanegraph.cs.uni-freiburg.de.\n","authors":["Martin Büchner","Jannik Zürn","Ion-George Todoran","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2302.06175v2.pdf","comment":"22 pages, 17 figures"},{"id":"http://arxiv.org/abs/2303.09930v1","updated":"2023-03-17T12:38:28Z","published":"2023-03-17T12:38:28Z","title":"Robust Semi-Supervised Learning for Histopathology Images through\n  Self-Supervision Guided Out-of-Distribution Scoring","summary":"  Semi-supervised learning (semi-SL) is a promising alternative to supervised\nlearning for medical image analysis when obtaining good quality supervision for\nmedical imaging is difficult. However, semi-SL assumes that the underlying\ndistribution of unaudited data matches that of the few labeled samples, which\nis often violated in practical settings, particularly in medical images. The\npresence of out-of-distribution (OOD) samples in the unlabeled training pool of\nsemi-SL is inevitable and can reduce the efficiency of the algorithm. Common\npreprocessing methods to filter out outlier samples may not be suitable for\nmedical images that involve a wide range of anatomical structures and rare\nmorphologies. In this paper, we propose a novel pipeline for addressing\nopen-set supervised learning challenges in digital histology images. Our\npipeline efficiently estimates an OOD score for each unlabelled data point\nbased on self-supervised learning to calibrate the knowledge needed for a\nsubsequent semi-SL framework. The outlier score derived from the OOD detector\nis used to modulate sample selection for the subsequent semi-SL stage, ensuring\nthat samples conforming to the distribution of the few labeled samples are more\nfrequently exposed to the subsequent semi-SL framework. Our framework is\ncompatible with any semi-SL framework, and we base our experiments on the\npopular Mixmatch semi-SL framework. We conduct extensive studies on two digital\npathology datasets, Kather colorectal histology dataset and a dataset derived\nfrom TCGA-BRCA whole slide images, and establish the effectiveness of our\nmethod by comparing with popular methods and frameworks in semi-SL algorithms\nthrough various experiments.\n","authors":["Nikhil Cherian Kurian","Varsha S","Abhijit Patil","Shashikant Khade","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2303.09930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09919v1","updated":"2023-03-17T12:12:41Z","published":"2023-03-17T12:12:41Z","title":"Dual Memory Aggregation Network for Event-Based Object Detection with\n  Learnable Representation","summary":"  Event-based cameras are bio-inspired sensors that capture brightness change\nof every pixel in an asynchronous manner. Compared with frame-based sensors,\nevent cameras have microsecond-level latency and high dynamic range, hence\nshowing great potential for object detection under high-speed motion and poor\nillumination conditions. Due to sparsity and asynchronism nature with event\nstreams, most of existing approaches resort to hand-crafted methods to convert\nevent data into 2D grid representation. However, they are sub-optimal in\naggregating information from event stream for object detection. In this work,\nwe propose to learn an event representation optimized for event-based object\ndetection. Specifically, event streams are divided into grids in the x-y-t\ncoordinates for both positive and negative polarity, producing a set of pillars\nas 3D tensor representation. To fully exploit information with event streams to\ndetect objects, a dual-memory aggregation network (DMANet) is proposed to\nleverage both long and short memory along event streams to aggregate effective\ninformation for object detection. Long memory is encoded in the hidden state of\nadaptive convLSTMs while short memory is modeled by computing spatial-temporal\ncorrelation between event pillars at neighboring time intervals. Extensive\nexperiments on the recently released event-based automotive detection dataset\ndemonstrate the effectiveness of the proposed method.\n","authors":["Dongsheng Wang","Xu Jia","Yang Zhang","Xinyu Zhang","Yaoyuan Wang","Ziyang Zhang","Dong Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.09919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09915v1","updated":"2023-03-17T12:08:47Z","published":"2023-03-17T12:08:47Z","title":"Privacy-preserving Pedestrian Tracking using Distributed 3D LiDARs","summary":"  The growing demand for intelligent environments unleashes an extraordinary\ncycle of privacy-aware applications that makes individuals' life more\ncomfortable and safe. Examples of these applications include pedestrian\ntracking systems in large areas. Although the ubiquity of camera-based systems,\nthey are not a preferable solution due to the vulnerability of leaking the\nprivacy of pedestrians.In this paper, we introduce a novel privacy-preserving\nsystem for pedestrian tracking in smart environments using multiple distributed\nLiDARs of non-overlapping views. The system is designed to leverage LiDAR\ndevices to track pedestrians in partially covered areas due to practical\nconstraints, e.g., occlusion or cost. Therefore, the system uses the point\ncloud captured by different LiDARs to extract discriminative features that are\nused to train a metric learning model for pedestrian matching purposes. To\nboost the system's robustness, we leverage a probabilistic approach to model\nand adapt the dynamic mobility patterns of individuals and thus connect their\nsub-trajectories.We deployed the system in a large-scale testbed with 70\ncolorless LiDARs and conducted three different experiments. The evaluation\nresult at the entrance hall confirms the system's ability to accurately track\nthe pedestrians with a 0.98 F-measure even with zero-covered areas. This result\nhighlights the promise of the proposed system as the next generation of\nprivacy-preserving tracking means in smart environments.\n","authors":["Masakazu Ohno","Riki Ukyo","Tatsuya Amano","Hamada Rizk","Hirozumi Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.09915v1.pdf","comment":"Accepted to the 21st International Conference on Pervasive Computing\n  and Communications (PerCom 2023)"},{"id":"http://arxiv.org/abs/2212.07207v3","updated":"2023-03-17T11:59:55Z","published":"2022-12-14T13:10:27Z","title":"MAELi $\\unicode{x2013}$ Masked Autoencoder for Large-Scale LiDAR Point\n  Clouds","summary":"  We demonstrate how the often overlooked inherent properties of large-scale\nLiDAR point clouds can be effectively utilized for self-supervised\nrepresentation learning. In pursuit of this goal, we design a highly\ndata-efficient feature pre-training backbone that considerably reduces the need\nfor tedious 3D annotations to train state-of-the-art object detectors. We\npropose Masked AutoEncoder for LiDAR point clouds (MAELi) that intuitively\nleverages the sparsity of LiDAR point clouds in both the encoder and decoder\nduring reconstruction. Our approach results in more expressive and useful\nfeatures, which can be directly applied to downstream perception tasks, such as\n3D object detection for autonomous driving. In a novel reconstruction schema,\nMAELi distinguishes between free and occluded space and employs a new masking\nstrategy that targets the LiDAR's inherent spherical projection. To demonstrate\nthe potential of MAELi, we pre-train one of the most widely-used 3D backbones\nin an end-to-end manner and show the effectiveness of our unsupervised\npre-trained features on various 3D object detection architectures. Our method\nachieves significant performance improvements when only a small fraction of\nlabeled frames is available for fine-tuning object detectors. For instance,\nwith ~800 labeled frames, MAELi features enhance a SECOND model by\n+10.79APH/LEVEL 2 on Waymo Vehicles.\n","authors":["Georg Krispel","David Schinagl","Christian Fruhwirth-Reisinger","Horst Possegger","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2212.07207v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2203.06431v2","updated":"2023-03-17T11:43:18Z","published":"2022-03-12T13:14:13Z","title":"Deep learning-based conditional inpainting for restoration of\n  artifact-affected 4D CT images","summary":"  4D CT imaging is an essential component of radiotherapy of thoracic/abdominal\ntumors. 4D CT images are, however, often affected by artifacts that compromise\ntreatment planning quality. In this work, deep learning (DL)-based conditional\ninpainting is proposed to restore anatomically correct image information of\nartifact-affected areas. The restoration approach consists of a two-stage\nprocess: DL-based detection of common interpolation (INT) and double structure\n(DS) artifacts, followed by conditional inpainting applied to the artifact\nareas. In this context, conditional refers to a guidance of the inpainting\nprocess by patient-specific image data to ensure anatomically reliable results.\nThe study is based on 65 in-house 4D CT images of lung cancer patients (48 with\nonly slight artifacts, 17 with pronounced artifacts) and two publicly available\n4D CT data sets that serve as independent external test sets. Automated\nartifact detection revealed a ROC-AUC of 0.99 for INT and of 0.97 for DS\nartifacts (in-house data). The proposed inpainting method decreased the average\nroot mean squared error (RMSE) by 52%(INT) and 59% (DS) for the in-house data.\nFor the external test data sets, the RMSE improvement is similar (50% and 59 %,\nrespectively). Applied to 4D CT data with pronounced artifacts (not part of the\ntraining set), 72% of the detectable artifacts were removed. The results\nhighlight the potential of DL-based inpainting for restoration of\nartifact-affected 4D CT data. Compared to recent 4D CT inpainting and\nrestoration approaches, the proposed methodology illustrates the advantages of\nexploiting patient-specific prior image information.\n","authors":["Frederic Madesta","Thilo Sentker","Tobias Gauer","Rene Werner"],"pdf_url":"https://arxiv.org/pdf/2203.06431v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2201.11316v2","updated":"2023-03-17T11:10:31Z","published":"2022-01-27T04:22:25Z","title":"Transformer Module Networks for Systematic Generalization in Visual\n  Question Answering","summary":"  Transformers achieve great performance on Visual Question Answering (VQA).\nHowever, their systematic generalization capabilities, i.e., handling novel\ncombinations of known concepts, is unclear. We reveal that Neural Module\nNetworks (NMNs), i.e., question-specific compositions of modules that tackle a\nsub-task, achieve better or similar systematic generalization performance than\nthe conventional Transformers, even though NMNs' modules are CNN-based. In\norder to address this shortcoming of Transformers with respect to NMNs, in this\npaper we investigate whether and how modularity can bring benefits to\nTransformers. Namely, we introduce Transformer Module Network (TMN), a novel\nNMN based on compositions of Transformer modules. TMNs achieve state-of-the-art\nsystematic generalization performance in three VQA datasets, improving more\nthan 30% over standard Transformers for novel compositions of sub-tasks. We\nshow that not only the module composition but also the module specialization\nfor each sub-task are the key of such performance gain.\n","authors":["Moyuru Yamada","Vanessa D'Amario","Kentaro Takemoto","Xavier Boix","Tomotake Sasaki"],"pdf_url":"https://arxiv.org/pdf/2201.11316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.04248v2","updated":"2023-03-17T11:07:16Z","published":"2021-10-08T16:58:20Z","title":"Observations on K-image Expansion of Image-Mixing Augmentation for\n  Classification","summary":"  Image-mixing augmentations (e.g., Mixup and CutMix), which typically involve\nmixing two images, have become the de-facto training techniques for image\nclassification. Despite their huge success in image classification, the number\nof images to be mixed has not been elucidated in the literature: only the naive\nK-image expansion has been shown to lead to performance degradation. This study\nderives a new K-image mixing augmentation based on the stick-breaking process\nunder Dirichlet prior distribution. We demonstrate the superiority of our\nK-image expansion augmentation over conventional two-image mixing augmentation\nmethods through extensive experiments and analyses: (1) more robust and\ngeneralized classifiers; (2) a more desirable loss landscape shape; (3) better\nadversarial robustness. Moreover, we show that our probabilistic model can\nmeasure the sample-wise uncertainty and boost the efficiency for network\narchitecture search by achieving a 7-fold reduction in the search time. Code\nwill be available at https://github.com/yjyoo3312/DCutMix-PyTorch.git.\n","authors":["Joonhyun Jeong","Sungmin Cha","Youngjoon Yoo","Sangdoo Yun","Taesup Moon","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2110.04248v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2212.07766v2","updated":"2023-03-17T11:04:53Z","published":"2022-12-15T12:36:49Z","title":"DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients","summary":"  Line segments are ubiquitous in our human-made world and are increasingly\nused in vision tasks. They are complementary to feature points thanks to their\nspatial extent and the structural information they provide. Traditional line\ndetectors based on the image gradient are extremely fast and accurate, but lack\nrobustness in noisy images and challenging conditions. Their learned\ncounterparts are more repeatable and can handle challenging images, but at the\ncost of a lower accuracy and a bias towards wireframe lines. We propose to\ncombine traditional and learned approaches to get the best of both worlds: an\naccurate and robust line detector that can be trained in the wild without\nground truth lines. Our new line segment detector, DeepLSD, processes images\nwith a deep network to generate a line attraction field, before converting it\nto a surrogate image gradient magnitude and angle, which is then fed to any\nexisting handcrafted line detector. Additionally, we propose a new optimization\ntool to refine line segments based on the attraction field and vanishing\npoints. This refinement improves the accuracy of current deep detectors by a\nlarge margin. We demonstrate the performance of our method on low-level line\ndetection metrics, as well as on several downstream tasks using multiple\nchallenging datasets. The source code and models are available at\nhttps://github.com/cvg/DeepLSD.\n","authors":["Rémi Pautrat","Daniel Barath","Viktor Larsson","Martin R. Oswald","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2212.07766v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08340v2","updated":"2023-03-17T10:54:16Z","published":"2023-03-15T03:14:30Z","title":"VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow\n  Estimation","summary":"  We introduce VideoFlow, a novel optical flow estimation framework for videos.\nIn contrast to previous methods that learn to estimate optical flow from two\nframes, VideoFlow concurrently estimates bi-directional optical flows for\nmultiple frames that are available in videos by sufficiently exploiting\ntemporal cues. We first propose a TRi-frame Optical Flow (TROF) module that\nestimates bi-directional optical flows for the center frame in a three-frame\nmanner. The information of the frame triplet is iteratively fused onto the\ncenter frame. To extend TROF for handling more frames, we further propose a\nMOtion Propagation (MOP) module that bridges multiple TROFs and propagates\nmotion features between adjacent TROFs. With the iterative flow estimation\nrefinement, the information fused in individual TROFs can be propagated into\nthe whole sequence via MOP. By effectively exploiting video information,\nVideoFlow presents extraordinary performance, ranking 1st on all public\nbenchmarks. On the Sintel benchmark, VideoFlow achieves 1.649 and 0.991 average\nend-point-error (AEPE) on the final and clean passes, a 15.1% and 7.6% error\nreduction from the best published results (1.943 and 1.073 from FlowFormer++).\nOn the KITTI-2015 benchmark, VideoFlow achieves an F1-all error of 3.65%, a\n19.2% error reduction from the best published result (4.52% from FlowFormer++).\n","authors":["Xiaoyu Shi","Zhaoyang Huang","Weikang Bian","Dasong Li","Manyuan Zhang","Ka Chun Cheung","Simon See","Hongwei Qin","Jifeng Dai","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.08340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09877v1","updated":"2023-03-17T10:51:38Z","published":"2023-03-17T10:51:38Z","title":"On the Effects of Self-supervision and Contrastive Alignment in Deep\n  Multi-view Clustering","summary":"  Self-supervised learning is a central component in recent approaches to deep\nmulti-view clustering (MVC). However, we find large variations in the\ndevelopment of self-supervision-based methods for deep MVC, potentially slowing\nthe progress of the field. To address this, we present DeepMVC, a unified\nframework for deep MVC that includes many recent methods as instances. We\nleverage our framework to make key observations about the effect of\nself-supervision, and in particular, drawbacks of aligning representations with\ncontrastive learning. Further, we prove that contrastive alignment can\nnegatively influence cluster separability, and that this effect becomes worse\nwhen the number of views increases. Motivated by our findings, we develop\nseveral new DeepMVC instances with new forms of self-supervision. We conduct\nextensive experiments and find that (i) in line with our theoretical findings,\ncontrastive alignments decreases performance on datasets with many views; (ii)\nall methods benefit from some form of self-supervision; and (iii) our new\ninstances outperform previous methods on several datasets. Based on our\nresults, we suggest several promising directions for future research. To\nenhance the openness of the field, we provide an open-source implementation of\nDeepMVC, including recent models and our new instances. Our implementation\nincludes a consistent evaluation protocol, facilitating fair and accurate\nevaluation of methods and components.\n","authors":["Daniel J. Trosten","Sigurd Løkse","Robert Jenssen","Michael C. Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2303.09877v1.pdf","comment":"CVPR 2023. Code available at https://github.com/DanielTrosten/DeepMVC"},{"id":"http://arxiv.org/abs/2301.07147v2","updated":"2023-03-17T10:47:07Z","published":"2023-01-17T19:23:54Z","title":"COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM","summary":"  Collaborative SLAM is at the core of perception in multi-robot systems as it\nenables the co-localization of the team of robots in a common reference frame,\nwhich is of vital importance for any coordination amongst them. The paradigm of\na centralized architecture is well established, with the robots (i.e. agents)\nrunning Visual-Inertial Odometry (VIO) onboard while communicating relevant\ndata, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), which\nthen merges and optimizes the joint maps of the agents. While these frameworks\nhave proven to be successful, their capability and performance are highly\ndependent on the choice of the VIO front-end, thus limiting their flexibility.\nIn this work, we present COVINS-G, a generalized back-end building upon the\nCOVINS framework, enabling the compatibility of the server-back-end with any\narbitrary VIO front-end, including, for example, off-the-shelf cameras with\nodometry capabilities, such as the Realsense T265. The COVINS-G back-end\ndeploys a multi-camera relative pose estimation algorithm for computing the\nloop-closure constraints allowing the system to work purely on 2D image data.\nIn the experimental evaluation, we show on-par accuracy with state-of-the-art\nmulti-session and collaborative SLAM systems, while demonstrating the\nflexibility and generality of our approach by employing different front-ends\nonboard collaborating agents within the same mission. The COVINS-G codebase\nalong with a generalized front-end wrapper to allow any existing VIO front-end\nto be readily used in combination with the proposed collaborative back-end is\nopen-sourced. Video: https://youtu.be/FoJfXCfaYDw\n","authors":["Manthan Patel","Marco Karrer","Philipp Bänninger","Margarita Chli"],"pdf_url":"https://arxiv.org/pdf/2301.07147v2.pdf","comment":"6+1 Pages, 5 Figures, 3 Tables, Accepted at ICRA 2023, London"},{"id":"http://arxiv.org/abs/2204.11700v2","updated":"2023-03-17T10:45:32Z","published":"2022-04-25T14:43:15Z","title":"ClusterGNN: Cluster-based Coarse-to-Fine Graph Neural Network for\n  Efficient Feature Matching","summary":"  Graph Neural Networks (GNNs) with attention have been successfully applied\nfor learning visual feature matching. However, current methods learn with\ncomplete graphs, resulting in a quadratic complexity in the number of features.\nMotivated by a prior observation that self- and cross- attention matrices\nconverge to a sparse representation, we propose ClusterGNN, an attentional GNN\narchitecture which operates on clusters for learning the feature matching task.\nUsing a progressive clustering module we adaptively divide keypoints into\ndifferent subgraphs to reduce redundant connectivity, and employ a\ncoarse-to-fine paradigm for mitigating miss-classification within images. Our\napproach yields a 59.7% reduction in runtime and 58.4% reduction in memory\nconsumption for dense detection, compared to current state-of-the-art GNN-based\nmatching, while achieving a competitive performance on various computer vision\ntasks.\n","authors":["Yan Shi","Jun-Xiong Cai","Yoli Shavit","Tai-Jiang Mu","Wensen Feng","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2204.11700v2.pdf","comment":"Has been accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition 2022,(modified some typos)"},{"id":"http://arxiv.org/abs/2303.09875v1","updated":"2023-03-17T10:42:05Z","published":"2023-03-17T10:42:05Z","title":"A Dynamic Multi-Scale Voxel Flow Network for Video Prediction","summary":"  The performance of video prediction has been greatly boosted by advanced deep\nneural networks. However, most of the current methods suffer from large model\nsizes and require extra inputs, e.g., semantic/depth maps, for promising\nperformance. For efficiency consideration, in this paper, we propose a Dynamic\nMulti-scale Voxel Flow Network (DMVFN) to achieve better video prediction\nperformance at lower computational costs with only RGB images, than previous\nmethods. The core of our DMVFN is a differentiable routing module that can\neffectively perceive the motion scales of video frames. Once trained, our DMVFN\nselects adaptive sub-networks for different inputs at the inference stage.\nExperiments on several benchmarks demonstrate that our DMVFN is an order of\nmagnitude faster than Deep Voxel Flow and surpasses the state-of-the-art\niterative-based OPT on generated image quality. Our code and demo are available\nat https://huxiaotaostasy.github.io/DMVFN/.\n","authors":["Xiaotao Hu","Zhewei Huang","Ailin Huang","Jun Xu","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09875v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09874v1","updated":"2023-03-17T10:38:27Z","published":"2023-03-17T10:38:27Z","title":"Disentangling the Link Between Image Statistics and Human Perception","summary":"  In the 1950s Horace Barlow and Fred Attneave suggested a connection between\nsensory systems and how they are adapted to the environment: early vision\nevolved to maximise the information it conveys about incoming signals.\nFollowing Shannon's definition, this information was described using the\nprobability of the images taken from natural scenes. Previously, direct\naccurate predictions of image probabilities were not possible due to\ncomputational limitations. Despite the exploration of this idea being indirect,\nmainly based on oversimplified models of the image density or on system design\nmethods, these methods had success in reproducing a wide range of physiological\nand psychophysical phenomena. In this paper, we directly evaluate the\nprobability of natural images and analyse how it may determine perceptual\nsensitivity. We employ image quality metrics that correlate well with human\nopinion as a surrogate of human vision, and an advanced generative model to\ndirectly estimate the probability. Specifically, we analyse how the sensitivity\nof full-reference image quality metrics can be predicted from quantities\nderived directly from the probability distribution of natural images. First, we\ncompute the mutual information between a wide range of probability surrogates\nand the sensitivity of the metrics and find that the most influential factor is\nthe probability of the noisy image. Then we explore how these probability\nsurrogates can be combined using a simple model to predict the metric\nsensitivity, giving an upper bound for the correlation of 0.85 between the\nmodel predictions and the actual perceptual sensitivity. Finally, we explore\nhow to combine the probability surrogates using simple expressions, and obtain\ntwo functional forms (using one or two surrogates) that can be used to predict\nthe sensitivity of the human visual system given a particular pair of images.\n","authors":["Alexander Hepburn","Valero Laparra","Raul Santos-Rodriguez","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2303.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09871v1","updated":"2023-03-17T10:28:31Z","published":"2023-03-17T10:28:31Z","title":"Fluid Dynamics Network: Topology-Agnostic 4D Reconstruction via Fluid\n  Dynamics Priors","summary":"  Representing 3D surfaces as level sets of continuous functions over\n$\\mathbb{R}^3$ is the common denominator of neural implicit representations,\nwhich recently enabled remarkable progress in geometric deep learning and\ncomputer vision tasks. In order to represent 3D motion within this framework,\nit is often assumed (either explicitly or implicitly) that the transformations\nwhich a surface may undergo are homeomorphic: this is not necessarily true, for\ninstance, in the case of fluid dynamics. In order to represent more general\nclasses of deformations, we propose to apply this theoretical framework as\nregularizers for the optimization of simple 4D implicit functions (such as\nsigned distance fields). We show that our representation is capable of\ncapturing both homeomorphic and topology-changing deformations, while also\ndefining correspondences over the continuously-reconstructed surfaces.\n","authors":["Daniele Baieri","Stefano Esposito","Filippo Maggioli","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2303.09871v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.09870v1","updated":"2023-03-17T10:15:13Z","published":"2023-03-17T10:15:13Z","title":"TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation","summary":"  Most recent test-time adaptation methods focus on only classification tasks,\nuse specialized network architectures, destroy model calibration or rely on\nlightweight information from the source domain. To tackle these issues, this\npaper proposes a novel Test-time Self-Learning method with automatic\nAdversarial augmentation dubbed TeSLA for adapting a pre-trained source model\nto the unlabeled streaming test data. In contrast to conventional self-learning\nmethods based on cross-entropy, we introduce a new test-time loss function\nthrough an implicitly tight connection with the mutual information and online\nknowledge distillation. Furthermore, we propose a learnable efficient\nadversarial augmentation module that further enhances online knowledge\ndistillation by simulating high entropy augmented images. Our method achieves\nstate-of-the-art classification and segmentation results on several benchmarks\nand types of domain shifts, particularly on challenging measurement shifts of\nmedical images. TeSLA also benefits from several desirable properties compared\nto competing methods in terms of calibration, uncertainty metrics,\ninsensitivity to model architectures, and source training strategies, all\nsupported by extensive ablations. Our code and models are available on GitHub.\n","authors":["Devavrat Tomar","Guillaume Vray","Behzad Bozorgtabar","Jean-Philippe Thiran"],"pdf_url":"https://arxiv.org/pdf/2303.09870v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.11596v2","updated":"2023-03-17T10:11:52Z","published":"2022-12-22T10:45:08Z","title":"Deformable Surface Reconstruction via Riemannian Metric Preservation","summary":"  Estimating the pose of an object from a monocular image is an inverse problem\nfundamental in computer vision. The ill-posed nature of this problem requires\nincorporating deformation priors to solve it. In practice, many materials do\nnot perceptibly shrink or extend when manipulated, constituting a powerful and\nwell-known prior. Mathematically, this translates to the preservation of the\nRiemannian metric. Neural networks offer the perfect playground to solve the\nsurface reconstruction problem as they can approximate surfaces with arbitrary\nprecision and allow the computation of differential geometry quantities. This\npaper presents an approach to inferring continuous deformable surfaces from a\nsequence of images, which is benchmarked against several techniques and obtains\nstate-of-the-art performance without the need for offline training.\n","authors":["Oriol Barbany","Adrià Colomé","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2212.11596v2.pdf","comment":"This paper is under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2211.16961v3","updated":"2023-03-17T10:10:08Z","published":"2022-11-30T13:11:46Z","title":"Pattern Attention Transformer with Doughnut Kernel","summary":"  We present in this paper a new architecture, the Pattern Attention\nTransformer (PAT), that is composed of the new doughnut kernel. Compared with\ntokens in the NLP field, Transformer in computer vision has the problem of\nhandling the high resolution of pixels in images. In ViT, an image is cut into\nsquare-shaped patches. As the follow-up of ViT, Swin Transformer proposes an\nadditional step of shifting to decrease the existence of fixed boundaries,\nwhich also incurs 'two connected Swin Transformer blocks' as the minimum unit\nof the model. Inheriting the patch/window idea, our doughnut kernel enhances\nthe design of patches further. It replaces the line-cut boundaries with two\ntypes of areas: sensor and updating, which is based on the comprehension of\nself-attention (named QKVA grid). The doughnut kernel also brings a new topic\nabout the shape of kernels beyond square. To verify its performance on image\nclassification, PAT is designed with Transformer blocks of regular octagon\nshape doughnut kernels. Its architecture is lighter: the minimum pattern\nattention layer is only one for each stage. Under similar complexity of\ncomputation, its performances on ImageNet 1K reach higher throughput (+10%) and\nsurpass Swin Transformer (+0.1 acc1).\n","authors":["WenYuan Sheng"],"pdf_url":"https://arxiv.org/pdf/2211.16961v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09867v1","updated":"2023-03-17T10:07:19Z","published":"2023-03-17T10:07:19Z","title":"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model","summary":"  Existing text-video retrieval solutions are, in essence, discriminant models\nfocused on maximizing the conditional likelihood, i.e., p(candidates|query).\nWhile straightforward, this de facto paradigm overlooks the underlying data\ndistribution p(query), which makes it challenging to identify\nout-of-distribution data. To address this limitation, we creatively tackle this\ntask from a generative viewpoint and model the correlation between the text and\nthe video as their joint probability p(candidates,query). This is accomplished\nthrough a diffusion-based text-video retrieval framework (DiffusionRet), which\nmodels the retrieval task as a process of gradually generating joint\ndistribution from noise. During training, DiffusionRet is optimized from both\nthe generation and discrimination perspectives, with the generator being\noptimized by generation loss and the feature extractor trained with contrastive\nloss. In this way, DiffusionRet cleverly leverages the strengths of both\ngenerative and discriminative methods. Extensive experiments on five commonly\nused text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD,\nActivityNet Captions, and DiDeMo, with superior performances, justify the\nefficacy of our method. More encouragingly, without any modification,\nDiffusionRet even performs well in out-domain retrieval settings. We believe\nthis work brings fundamental insights into the related fields. Code will be\navailable at https://github.com/jpthu17/DiffusionRet.\n","authors":["Peng Jin","Hao Li","Zesen Cheng","Kehan Li","Xiangyang Ji","Chang Liu","Li Yuan","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09858v1","updated":"2023-03-17T09:37:41Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zha","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09857v1","updated":"2023-03-17T09:37:07Z","published":"2023-03-17T09:37:07Z","title":"Dual-path Adaptation from Image to Video Transformers","summary":"  In this paper, we efficiently transfer the surpassing representation power of\nthe vision foundation models, such as ViT and Swin, for video understanding\nwith only a few trainable parameters. Previous adaptation methods have\nsimultaneously considered spatial and temporal modeling with a unified\nlearnable module but still suffered from fully leveraging the representative\ncapabilities of image transformers. We argue that the popular dual-path\n(two-stream) architecture in video models can mitigate this problem. We propose\na novel DualPath adaptation separated into spatial and temporal adaptation\npaths, where a lightweight bottleneck adapter is employed in each transformer\nblock. Especially for temporal dynamic modeling, we incorporate consecutive\nframes into a grid-like frameset to precisely imitate vision transformers'\ncapability that extrapolates relationships between tokens. In addition, we\nextensively investigate the multiple baselines from a unified perspective in\nvideo understanding and compare them with DualPath. Experimental results on\nfour action recognition benchmarks prove that pretrained image transformers\nwith DualPath can be effectively generalized beyond the data domain.\n","authors":["Jungin Park","Jiyoung Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2303.09857v1.pdf","comment":"CVPR 2023. Code is available at\n  https://github.com/park-jungin/DualPath"},{"id":"http://arxiv.org/abs/2303.09199v2","updated":"2023-03-17T09:26:52Z","published":"2023-03-16T10:17:33Z","title":"A Generative Model for Digital Camera Noise Synthesis","summary":"  Noise synthesis is a challenging low-level vision task aiming to generate\nrealistic noise given a clean image along with the camera settings. To this\nend, we propose an effective generative model which utilizes clean features as\nguidance followed by noise injections into the network. Specifically, our\ngenerator follows a UNet-like structure with skip connections but without\ndownsampling and upsampling layers. Firstly, we extract deep features from a\nclean image as the guidance and concatenate a Gaussian noise map to the\ntransition point between the encoder and decoder as the noise source. Secondly,\nwe propose noise synthesis blocks in the decoder in each of which we inject\nGaussian noise to model the noise characteristics. Thirdly, we propose to\nutilize an additional Style Loss and demonstrate that this allows better noise\ncharacteristics supervision in the generator. Through a number of new\nexperiments, we evaluate the temporal variance and the spatial correlation of\nthe generated noise which we hope can provide meaningful insights for future\nworks. Finally, we show that our proposed approach outperforms existing methods\nfor synthesizing camera noise.\n","authors":["Mingyang Song","Yang Zhang","Tunç O. Aydın","Elham Amin Mansour","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2303.09199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09849v1","updated":"2023-03-17T09:09:48Z","published":"2023-03-17T09:09:48Z","title":"Exploiting Semantic Attributes for Transductive Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) aims to recognize unseen classes by generalizing the\nrelation between visual features and semantic attributes learned from the seen\nclasses. A recent paradigm called transductive zero-shot learning further\nleverages unlabeled unseen data during training and has obtained impressive\nresults. These methods always synthesize unseen features from attributes\nthrough a generative adversarial network to mitigate the bias towards seen\nclasses. However, they neglect the semantic information in the unlabeled unseen\ndata and thus fail to generate high-fidelity attribute-consistent unseen\nfeatures. To address this issue, we present a novel transductive ZSL method\nthat produces semantic attributes of the unseen data and imposes them on the\ngenerative process. In particular, we first train an attribute decoder that\nlearns the mapping from visual features to semantic attributes. Then, from the\nattribute decoder, we obtain pseudo-attributes of unlabeled data and integrate\nthem into the generative model, which helps capture the detailed differences\nwithin unseen classes so as to synthesize more discriminative features.\nExperiments on five standard benchmarks show that our method yields\nstate-of-the-art results for zero-shot learning.\n","authors":["Zhengbo Wang","Jian Liang","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.09849v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.09843v1","updated":"2023-03-17T08:56:27Z","published":"2023-03-17T08:56:27Z","title":"DUDES: Deep Uncertainty Distillation using Ensembles for Semantic\n  Segmentation","summary":"  Deep neural networks lack interpretability and tend to be overconfident,\nwhich poses a serious problem in safety-critical applications like autonomous\ndriving, medical imaging, or machine vision tasks with high demands on\nreliability. Quantifying the predictive uncertainty is a promising endeavour to\nopen up the use of deep neural networks for such applications. Unfortunately,\ncurrent available methods are computationally expensive. In this work, we\npresent a novel approach for efficient and reliable uncertainty estimation\nwhich we call Deep Uncertainty Distillation using Ensembles for Segmentation\n(DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to\naccurately approximate predictive uncertainties with a single forward pass\nwhile maintaining simplicity and adaptability. Experimentally, DUDES accurately\ncaptures predictive uncertainties without sacrificing performance on the\nsegmentation task and indicates impressive capabilities of identifying wrongly\nclassified pixels and out-of-domain samples on the Cityscapes dataset. With\nDUDES, we manage to simultaneously simplify and outperform previous work on\nDeep Ensemble-based Uncertainty Distillation.\n","authors":["Steven Landgraf","Kira Wursthorn","Markus Hillemann","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2303.09843v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.09833v1","updated":"2023-03-17T08:38:33Z","published":"2023-03-17T08:38:33Z","title":"FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model","summary":"  Recently, conditional diffusion models have gained popularity in numerous\napplications due to their exceptional generation ability. However, many\nexisting methods are training-required. They need to train a time-dependent\nclassifier or a condition-dependent score estimator, which increases the cost\nof constructing conditional diffusion models and is inconvenient to transfer\nacross different conditions. Some current works aim to overcome this limitation\nby proposing training-free solutions, but most can only be applied to a\nspecific category of tasks and not to more general conditions. In this work, we\npropose a training-Free conditional Diffusion Model (FreeDoM) used for various\nconditions. Specifically, we leverage off-the-shelf pre-trained networks, such\nas a face detection model, to construct time-independent energy functions,\nwhich guide the generation process without requiring training. Furthermore,\nbecause the construction of the energy function is very flexible and adaptable\nto various conditions, our proposed FreeDoM has a broader range of applications\nthan existing training-free methods. FreeDoM is advantageous in its simplicity,\neffectiveness, and low cost. Experiments demonstrate that FreeDoM is effective\nfor various conditions and suitable for diffusion models of diverse data\ndomains, including image and latent code domains.\n","authors":["Jiwen Yu","Yinhuai Wang","Chen Zhao","Bernard Ghanem","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09831v1","updated":"2023-03-17T08:35:17Z","published":"2023-03-17T08:35:17Z","title":"MODIFY: Model-driven Face Stylization without Style Images","summary":"  Existing face stylization methods always acquire the presence of the target\n(style) domain during the translation process, which violates privacy\nregulations and limits their applicability in real-world systems. To address\nthis issue, we propose a new method called MODel-drIven Face stYlization\n(MODIFY), which relies on the generative model to bypass the dependence of the\ntarget images. Briefly, MODIFY first trains a generative model in the target\ndomain and then translates a source input to the target domain via the provided\nstyle model. To preserve the multimodal style information, MODIFY further\nintroduces an additional remapping network, mapping a known continuous\ndistribution into the encoder's embedding space. During translation in the\nsource domain, MODIFY fine-tunes the encoder module within the target\nstyle-persevering model to capture the content of the source input as precisely\nas possible. Our method is extremely simple and satisfies versatile training\nmodes for face stylization. Experimental results on several different datasets\nvalidate the effectiveness of MODIFY for unsupervised face stylization.\n","authors":["Yuhe Ding","Jian Liang","Jie Cao","Aihua Zheng","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09830v1","updated":"2023-03-17T08:29:54Z","published":"2023-03-17T08:29:54Z","title":"Prototype Knowledge Distillation for Medical Segmentation with Missing\n  Modality","summary":"  Multi-modality medical imaging is crucial in clinical treatment as it can\nprovide complementary information for medical image segmentation. However,\ncollecting multi-modal data in clinical is difficult due to the limitation of\nthe scan time and other clinical situations. As such, it is clinically\nmeaningful to develop an image segmentation paradigm to handle this missing\nmodality problem. In this paper, we propose a prototype knowledge distillation\n(ProtoKD) method to tackle the challenging problem, especially for the toughest\nscenario when only single modal data can be accessed. Specifically, our ProtoKD\ncan not only distillate the pixel-wise knowledge of multi-modality data to\nsingle-modality data but also transfer intra-class and inter-class feature\nvariations, such that the student model could learn more robust feature\nrepresentation from the teacher model and inference with only one single\nmodality data. Our method achieves state-of-the-art performance on BraTS\nbenchmark.\n","authors":["Shuai Wang","Zipei Yan","Daoan Zhang","Haining Wei","Zhongsen Li","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2303.09830v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.01979v2","updated":"2023-03-17T08:26:32Z","published":"2023-03-02T08:02:45Z","title":"ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud\n  Completion","summary":"  Point cloud completion addresses filling in the missing parts of a partial\npoint cloud obtained from depth sensors and generating a complete point cloud.\nAlthough there has been steep progress in the supervised methods on the\nsynthetic point cloud completion task, it is hardly applicable in real-world\nscenarios due to the domain gap between the synthetic and real-world datasets\nor the requirement of prior information. To overcome these limitations, we\npropose a novel self-supervised framework ACL-SPC for point cloud completion to\ntrain and test on the same data. ACL-SPC takes a single partial input and\nattempts to output the complete point cloud using an adaptive closed-loop (ACL)\nsystem that enforces the output same for the variation of an input. We evaluate\nour proposed ACL-SPC on various datasets to prove that it can successfully\nlearn to complete a partial point cloud as the first self-supervised scheme.\nResults show that our method is comparable with unsupervised methods and\nachieves superior performance on the real-world dataset compared to the\nsupervised methods trained on the synthetic dataset. Extensive experiments\njustify the necessity of self-supervised learning and the effectiveness of our\nproposed method for the real-world point cloud completion task. The code is\npublicly available from https://github.com/Sangminhong/ACL-SPC_PyTorch\n","authors":["Sangmin Hong","Mohsen Yavartanoo","Reyhaneh Neshatavar","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2303.01979v2.pdf","comment":"Published at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09826v1","updated":"2023-03-17T08:11:14Z","published":"2023-03-17T08:11:14Z","title":"Learning Data-Driven Vector-Quantized Degradation Model for Animation\n  Video Super-Resolution","summary":"  Existing real-world video super-resolution (VSR) methods focus on designing a\ngeneral degradation pipeline for open-domain videos while ignoring data\nintrinsic characteristics which strongly limit their performance when applying\nto some specific domains (e.g. animation videos). In this paper, we thoroughly\nexplore the characteristics of animation videos and leverage the rich priors in\nreal-world animation data for a more practical animation VSR model. In\nparticular, we propose a multi-scale Vector-Quantized Degradation model for\nanimation video Super-Resolution (VQD-SR) to decompose the local details from\nglobal structures and transfer the degradation priors in real-world animation\nvideos to a learned vector-quantized codebook for degradation modeling. A\nrich-content Real Animation Low-quality (RAL) video dataset is collected for\nextracting the priors. We further propose a data enhancement strategy for\nhigh-resolution (HR) training videos based on our observation that existing HR\nvideos are mostly collected from the Web which contains conspicuous compression\nartifacts. The proposed strategy is valid to lift the upper bound of animation\nVSR performance, regardless of the specific VSR model. Experimental results\ndemonstrate the superiority of the proposed VQD-SR over state-of-the-art\nmethods, through extensive quantitative and qualitative evaluations of the\nlatest animation video super-resolution benchmark.\n","authors":["Zixi Tuo","Huan Yang","Jianlong Fu","Yujie Dun","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2303.09826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09825v1","updated":"2023-03-17T08:07:56Z","published":"2023-03-17T08:07:56Z","title":"LCE-Calib: Automatic LiDAR-Frame/Event Camera Extrinsic Calibration With\n  A Globally Optimal Solution","summary":"  The combination of LiDARs and cameras enables a mobile robot to perceive\nenvironments with multi-modal data, becoming a key factor in achieving robust\nperception. Traditional frame cameras are sensitive to changing illumination\nconditions, motivating us to introduce novel event cameras to make LiDAR-camera\nfusion more complete and robust. However, to jointly exploit these sensors, the\nchallenging extrinsic calibration problem should be addressed. This paper\nproposes an automatic checkerboard-based approach to calibrate extrinsics\nbetween a LiDAR and a frame/event camera, where four contributions are\npresented. Firstly, we present an automatic feature extraction and checkerboard\ntracking method from LiDAR's point clouds. Secondly, we reconstruct realistic\nframe images from event streams, applying traditional corner detectors to event\ncameras. Thirdly, we propose an initialization-refinement procedure to estimate\nextrinsics using point-to-plane and point-to-line constraints in a\ncoarse-to-fine manner. Fourthly, we introduce a unified and globally optimal\nsolution to address two optimization problems in calibration. Our approach has\nbeen validated with extensive experiments on 19 simulated and real-world\ndatasets and outperforms the state-of-the-art.\n","authors":["Jianhao Jiao","Feiyi Chen","Hexiang Wei","Jin Wu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.09825v1.pdf","comment":"11 pages, 13 figures, accepted to IEEE/ASME Transactions on\n  Mechatronics"},{"id":"http://arxiv.org/abs/2106.07217v2","updated":"2023-03-17T07:56:13Z","published":"2021-06-14T08:04:18Z","title":"Over-Fit: Noisy-Label Detection based on the Overfitted Model Property","summary":"  Deep neural network can easily overfit to even noisy labels due to its high\ncapacity, which degrades the generalization performance of a model. To overcome\nthis issue, we propose a new approach for learning from noisy labels (LNL) via\npost-training, which can significantly improve the generalization performance\nof any pre-trained model on noisy label data. To this end, we rather exploit\nthe overfitting property of a trained model to identify mislabeled samples.\nSpecifically, our post-training approach gradually removes samples with high\ninfluence on the decision boundary and refines the decision boundary to improve\ngeneralization performance. Our post-training approach creates great synergies\nwhen combined with the existing LNL methods. Experimental results on various\nreal-world and synthetic benchmark datasets demonstrate the validity of our\napproach in diverse realistic scenarios.\n","authors":["Seulki Park","Hwanjun Song","Daeho Um","Dae Ung Jo","Sangdoo Yun","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2106.07217v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.09817v1","updated":"2023-03-17T07:53:18Z","published":"2023-03-17T07:53:18Z","title":"Hospital Length of Stay Prediction Based on Multi-modal Data towards\n  Trustworthy Human-AI Collaboration in Radiomics","summary":"  To what extent can the patient's length of stay in a hospital be predicted\nusing only an X-ray image? We answer this question by comparing the performance\nof machine learning survival models on a novel multi-modal dataset created from\n1235 images with textual radiology reports annotated by humans. Although\nblack-box models predict better on average than interpretable ones, like Cox\nproportional hazards, they are not inherently understandable. To overcome this\ntrust issue, we introduce time-dependent model explanations into the human-AI\ndecision making process. Explaining models built on both: human-annotated and\nalgorithm-extracted radiomics features provides valuable insights for\nphysicians working in a hospital. We believe the presented approach to be\ngeneral and widely applicable to other time-to-event medical use cases. For\nreproducibility, we open-source code and the TLOS dataset at\nhttps://github.com/mi2datalab/xlungs-trustworthy-los-prediction.\n","authors":["Hubert Baniecki","Bartlomiej Sobieski","Przemysław Bombiński","Patryk Szatkowski","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2303.09817v1.pdf","comment":"Accepted at International Conference on Artificial Intelligence in\n  Medicine (AIME 2023)"},{"id":"http://arxiv.org/abs/2303.09813v1","updated":"2023-03-17T07:47:55Z","published":"2023-03-17T07:47:55Z","title":"DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery","summary":"  Learning from a large corpus of data, pre-trained models have achieved\nimpressive progress nowadays. As popular generative pre-training, diffusion\nmodels capture both low-level visual knowledge and high-level semantic\nrelations. In this paper, we propose to exploit such knowledgeable diffusion\nmodels for mainstream discriminative tasks, i.e., unsupervised object\ndiscovery: saliency segmentation and object localization. However, the\nchallenges exist as there is one structural difference between generative and\ndiscriminative models, which limits the direct use. Besides, the lack of\nexplicitly labeled data significantly limits performance in unsupervised\nsettings. To tackle these issues, we introduce DiffusionSeg, one novel\nsynthesis-exploitation framework containing two-stage strategies. To alleviate\ndata insufficiency, we synthesize abundant images, and propose a novel\ntraining-free AttentionCut to obtain masks in the first synthesis stage. In the\nsecond exploitation stage, to bridge the structural gap, we use the inversion\ntechnique, to map the given image back to diffusion features. These features\ncan be directly used by downstream architectures. Extensive experiments and\nablation studies demonstrate the superiority of adapting diffusion for\nunsupervised object discovery.\n","authors":["Chaofan Ma","Yuhuan Yang","Chen Ju","Fei Zhang","Jinxiang Liu","Yu Wang","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11375v2","updated":"2023-03-17T07:43:42Z","published":"2022-12-21T21:32:36Z","title":"Semi-supervised Bladder Tissue Classification in Multi-Domain Endoscopic\n  Images","summary":"  Objective: Accurate visual classification of bladder tissue during\nTrans-Urethral Resection of Bladder Tumor (TURBT) procedures is essential to\nimprove early cancer diagnosis and treatment. During TURBT interventions, White\nLight Imaging (WLI) and Narrow Band Imaging (NBI) techniques are used for\nlesion detection. Each imaging technique provides diverse visual information\nthat allows clinicians to identify and classify cancerous lesions. Computer\nvision methods that use both imaging techniques could improve endoscopic\ndiagnosis. We address the challenge of tissue classification when annotations\nare available only in one domain, in our case WLI, and the endoscopic images\ncorrespond to an unpaired dataset, i.e. there is no exact equivalent for every\nimage in both NBI and WLI domains. Method: We propose a semi-surprised\nGenerative Adversarial Network (GAN)-based method composed of three main\ncomponents: a teacher network trained on the labeled WLI data; a\ncycle-consistency GAN to perform unpaired image-to-image translation, and a\nmulti-input student network. To ensure the quality of the synthetic images\ngenerated by the proposed GAN we perform a detailed quantitative, and\nqualitative analysis with the help of specialists. Conclusion: The overall\naverage classification accuracy, precision, and recall obtained with the\nproposed method for tissue classification are 0.90, 0.88, and 0.89\nrespectively, while the same metrics obtained in the unlabeled domain (NBI) are\n0.92, 0.64, and 0.94 respectively. The quality of the generated images is\nreliable enough to deceive specialists. Significance: This study shows the\npotential of using semi-supervised GAN-based bladder tissue classification when\nannotations are limited in multi-domain data. The dataset is available at\nhttps://zenodo.org/record/7741476#.ZBQUK7TMJ6k\n","authors":["Jorge F. Lazo","Benoit Rosa","Michele Catellani","Matteo Fontana","Francesco A. Mistretta","Gennaro Musi","Ottavio de Cobelli","Michel de Mathelin","Elena De Momi"],"pdf_url":"https://arxiv.org/pdf/2212.11375v2.pdf","comment":"Title and abstract updated. Typos corrected"},{"id":"http://arxiv.org/abs/2303.09412v2","updated":"2023-03-17T07:34:26Z","published":"2023-03-16T15:44:31Z","title":"NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing\n  Diverse Intrinsic and Extrinsic Camera Parameters","summary":"  Novel view synthesis using neural radiance fields (NeRF) is the\nstate-of-the-art technique for generating high-quality images from novel\nviewpoints. Existing methods require a priori knowledge about extrinsic and\nintrinsic camera parameters. This limits their applicability to synthetic\nscenes, or real-world scenarios with the necessity of a preprocessing step.\nCurrent research on the joint optimization of camera parameters and NeRF\nfocuses on refining noisy extrinsic camera parameters and often relies on the\npreprocessing of intrinsic camera parameters. Further approaches are limited to\ncover only one single camera intrinsic. To address these limitations, we\npropose a novel end-to-end trainable approach called NeRFtrinsic Four. We\nutilize Gaussian Fourier features to estimate extrinsic camera parameters and\ndynamically predict varying intrinsic camera parameters through the supervision\nof the projection error. Our approach outperforms existing joint optimization\nmethods on LLFF and BLEFF. In addition to these existing datasets, we introduce\na new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic\nFour is a step forward in joint optimization NeRF-based view synthesis and\nenables more realistic and flexible rendering in real-world scenarios with\nvarying camera parameters.\n","authors":["Hannah Schieber","Fabian Deuser","Bernhard Egger","Norbert Oswald","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02498v2","updated":"2023-03-17T07:28:18Z","published":"2022-09-06T13:42:17Z","title":"MMV_Im2Im: An Open Source Microscopy Machine Vision Toolbox for\n  Image-to-Image Transformation","summary":"  Over the past decade, deep learning (DL) research in computer vision has been\ngrowing rapidly, with many advances in DL-based image analysis methods for\nbiomedical problems. In this work, we introduce MMV_Im2Im, a new open-source\npython package for image-to-image transformation in bioimaging applications.\nMMV_Im2Im is designed with a generic image-to-image transformation framework\nthat can be used for a wide range of tasks, including semantic segmentation,\ninstance segmentation, image restoration, and image generation, etc.. Our\nimplementation takes advantage of state-of-the-art machine learning engineering\ntechniques, allowing researchers to focus on their research without worrying\nabout engineering details. We demonstrate the effectiveness of MMV_Im2Im on\nmore than ten different biomedical problems, showcasing its general potentials\nand applicabilities. For computational biomedical researchers, MMV_Im2Im\nprovides a starting point for developing new biomedical image analysis or\nmachine learning algorithms, where they can either reuse the code in this\npackage or fork and extend this package to facilitate the development of new\nmethods. Experimental biomedical researchers can benefit from this work by\ngaining a comprehensive view of the image-to-image transformation concept\nthrough diversified examples and use cases. We hope this work can give the\ncommunity inspirations on how DL-based image-to-image transformation can be\nintegrated into the assay development process, enabling new biomedical studies\nthat cannot be done only with traditional experimental assays. To help\nresearchers get started, we have provided source code, documentation, and\ntutorials for MMV_Im2Im at https://github.com/MMV-Lab/mmv_im2im under MIT\nlicense.\n","authors":["Justin Sonneck","Jianxu Chen"],"pdf_url":"https://arxiv.org/pdf/2209.02498v2.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2212.05993v2","updated":"2023-03-17T07:27:15Z","published":"2022-12-12T15:50:00Z","title":"RGBD2: Generative Scene Synthesis via Incremental View Inpainting using\n  RGBD Diffusion Models","summary":"  We address the challenge of recovering an underlying scene geometry and\ncolors from a sparse set of RGBD view observations. In this work, we present a\nnew solution termed RGBD$^2$ that sequentially generates novel RGBD views along\na camera trajectory, and the scene geometry is simply the fusion result of\nthese views. More specifically, we maintain an intermediate surface mesh used\nfor rendering new RGBD views, which subsequently becomes complete by an\ninpainting network; each rendered RGBD view is later back-projected as a\npartial surface and is supplemented into the intermediate mesh. The use of\nintermediate mesh and camera projection helps solve the tough problem of\nmulti-view inconsistency. We practically implement the RGBD inpainting network\nas a versatile RGBD diffusion model, which is previously used for 2D generative\nmodeling; we make a modification to its reverse diffusion process to enable our\nuse. We evaluate our approach on the task of 3D scene synthesis from sparse\nRGBD inputs; extensive experiments on the ScanNet dataset demonstrate the\nsuperiority of our approach over existing ones. Project page:\nhttps://jblei.site/proj/rgbd-diffusion.\n","authors":["Jiabao Lei","Jiapeng Tang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2212.05993v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09807v1","updated":"2023-03-17T07:26:16Z","published":"2023-03-17T07:26:16Z","title":"TKN: Transformer-based Keypoint Prediction Network For Real-time Video\n  Prediction","summary":"  Video prediction is a complex time-series forecasting task with great\npotential in many use cases. However, conventional methods overemphasize\naccuracy while ignoring the slow prediction speed caused by complicated model\nstructures that learn too much redundant information with excessive GPU memory\nconsumption. Furthermore, conventional methods mostly predict frames\nsequentially (frame-by-frame) and thus are hard to accelerate. Consequently,\nvaluable use cases such as real-time danger prediction and warning cannot\nachieve fast enough inference speed to be applicable in reality. Therefore, we\npropose a transformer-based keypoint prediction neural network (TKN), an\nunsupervised learning method that boost the prediction process via constrained\ninformation extraction and parallel prediction scheme. TKN is the first\nreal-time video prediction solution to our best knowledge, while significantly\nreducing computation costs and maintaining other performance. Extensive\nexperiments on KTH and Human3.6 datasets demonstrate that TKN predicts 11 times\nfaster than existing methods while reducing memory consumption by 17.4% and\nachieving state-of-the-art prediction performance on average.\n","authors":["Haoran Li","Pengyuan Zhou","Yihang Lin","Yanbin Hao","Haiyong Xie","Yong Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03630v2","updated":"2023-03-17T07:12:37Z","published":"2023-03-07T03:24:54Z","title":"No One Left Behind: Improving the Worst Categories in Long-Tailed\n  Learning","summary":"  Unlike the case when using a balanced training dataset, the per-class recall\n(i.e., accuracy) of neural networks trained with an imbalanced dataset are\nknown to vary a lot from category to category. The convention in long-tailed\nrecognition is to manually split all categories into three subsets and report\nthe average accuracy within each subset. We argue that under such an evaluation\nsetting, some categories are inevitably sacrificed. On one hand, focusing on\nthe average accuracy on a balanced test set incurs little penalty even if some\nworst performing categories have zero accuracy. On the other hand, classes in\nthe \"Few\" subset do not necessarily perform worse than those in the \"Many\" or\n\"Medium\" subsets. We therefore advocate to focus more on improving the lowest\nrecall among all categories and the harmonic mean of all recall values.\nSpecifically, we propose a simple plug-in method that is applicable to a wide\nrange of methods. By simply re-training the classifier of an existing\npre-trained model with our proposed loss function and using an optional\nensemble trick that combines the predictions of the two classifiers, we achieve\na more uniform distribution of recall values across categories, which leads to\na higher harmonic mean accuracy while the (arithmetic) average accuracy is\nstill high. The effectiveness of our method is justified on widely used\nbenchmark datasets.\n","authors":["Yingxiao Du","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2303.03630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09801v1","updated":"2023-03-17T07:07:17Z","published":"2023-03-17T07:07:17Z","title":"Adaptive Graph Convolution Module for Salient Object Detection","summary":"  Salient object detection (SOD) is a task that involves identifying and\nsegmenting the most visually prominent object in an image. Existing solutions\ncan accomplish this use a multi-scale feature fusion mechanism to detect the\nglobal context of an image. However, as there is no consideration of the\nstructures in the image nor the relations between distant pixels, conventional\nmethods cannot deal with complex scenes effectively. In this paper, we propose\nan adaptive graph convolution module (AGCM) to overcome these limitations.\nPrototype features are initially extracted from the input image using a\nlearnable region generation layer that spatially groups features in the image.\nThe prototype features are then refined by propagating information between them\nbased on a graph architecture, where each feature is regarded as a node.\nExperimental results show that the proposed AGCM dramatically improves the SOD\nperformance both quantitatively and quantitatively.\n","authors":["Yongwoo Lee","Minhyeok Lee","Suhwan Cho","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2303.09801v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.02871v2","updated":"2023-03-17T07:07:10Z","published":"2022-09-30T02:25:12Z","title":"Self-Distillation for Further Pre-training of Transformers","summary":"  Pre-training a large transformer model on a massive amount of unlabeled data\nand fine-tuning it on labeled datasets for diverse downstream tasks has proven\nto be a successful strategy, for a variety of vision and natural language\nprocessing tasks. However, direct fine-tuning of the pre-trained model may be\nsuboptimal if there exist large discrepancies across data domains for\npre-training and fine-tuning. To tackle this issue, several previous studies\nhave proposed further pre-training strategies, where we continue to pre-train\nthe model on the target unlabeled dataset before fine-tuning. However, all of\nthem solely focus on language models and we empirically find that a Vision\nTransformer is vulnerable to overfitting as we continue to pretrain the model\non target unlabeled data. In order to tackle this limitation, we propose\nself-distillation as a regularization for a further pre-training stage.\nSpecifically, we first further pre-train the initial pre-trained model on the\ntarget unlabeled data and then consider it as a teacher for self-distillation.\nThen we take the same initial pre-trained model as a student and enforce its\nhidden representations to be close to those of the teacher while optimizing the\nstudent with a masked auto-encoding objective. We empirically validate the\nefficacy of self-distillation on a variety of benchmark datasets for image and\ntext classification tasks. Experimentally, we show that our proposed method\noutperforms all the relevant baselines. Theoretically, we analyze the proposed\nmethod with a simplified model to understand how self-distillation for further\npre-training can potentially help improve the performance of the downstream\ntasks.\n","authors":["Seanie Lee","Minki Kang","Juho Lee","Sung Ju Hwang","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2210.02871v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09800v1","updated":"2023-03-17T07:05:04Z","published":"2023-03-17T07:05:04Z","title":"GOOD: General Optimization-based Fusion for 3D Object Detection via\n  LiDAR-Camera Object Candidates","summary":"  3D object detection serves as the core basis of the perception tasks in\nautonomous driving. Recent years have seen the rapid progress of multi-modal\nfusion strategies for more robust and accurate 3D object detection. However,\ncurrent researches for robust fusion are all learning-based frameworks, which\ndemand a large amount of training data and are inconvenient to implement in new\nscenes. In this paper, we propose GOOD, a general optimization-based fusion\nframework that can achieve satisfying detection without training additional\nmodels and is available for any combinations of 2D and 3D detectors to improve\nthe accuracy and robustness of 3D detection. First we apply the mutual-sided\nnearest-neighbor probability model to achieve the 3D-2D data association. Then\nwe design an optimization pipeline that can optimize different kinds of\ninstances separately based on the matching result. Apart from this, the 3D MOT\nmethod is also introduced to enhance the performance aided by previous frames.\nTo the best of our knowledge, this is the first optimization-based late fusion\nframework for multi-modal 3D object detection which can be served as a baseline\nfor subsequent research. Experiments on both nuScenes and KITTI datasets are\ncarried out and the results show that GOOD outperforms by 9.1\\% on mAP score\ncompared with PointPillars and achieves competitive results with the\nlearning-based late fusion CLOCs.\n","authors":["Bingqi Shen","Shuwei Dai","Yuyin Chen","Rong Xiong","Yue Wang","Yanmei Jiao"],"pdf_url":"https://arxiv.org/pdf/2303.09800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09799v1","updated":"2023-03-17T07:02:59Z","published":"2023-03-17T07:02:59Z","title":"Style Transfer for 2D Talking Head Animation","summary":"  Audio-driven talking head animation is a challenging research topic with many\nreal-world applications. Recent works have focused on creating photo-realistic\n2D animation, while learning different talking or singing styles remains an\nopen problem. In this paper, we present a new method to generate talking head\nanimation with learnable style references. Given a set of style reference\nframes, our framework can reconstruct 2D talking head animation based on a\nsingle input image and an audio stream. Our method first produces facial\nlandmarks motion from the audio stream and constructs the intermediate style\npatterns from the style reference images. We then feed both outputs into a\nstyle-aware image generator to generate the photo-realistic and fidelity 2D\nanimation. In practice, our framework can extract the style information of a\nspecific character and transfer it to any new static image for talking head\nanimation. The intensive experimental results show that our method achieves\nbetter results than recent state-of-the-art approaches qualitatively and\nquantitatively.\n","authors":["Trong-Thang Pham","Nhat Le","Tuong Do","Hung Nguyen","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.09799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09797v1","updated":"2023-03-17T06:43:08Z","published":"2023-03-17T06:43:08Z","title":"MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D\n  Face Animation","summary":"  Audio-Driven Face Animation is an eagerly anticipated technique for\napplications such as VR/AR, games, and movie making. With the rapid development\nof 3D engines, there is an increasing demand for driving 3D faces with audio.\nHowever, currently available 3D face animation datasets are either\nscale-limited or quality-unsatisfied, which hampers further developments of\naudio-driven 3D face animation. To address this challenge, we propose MMFace4D,\na large-scale multi-modal 4D (3D sequence) face dataset consisting of 431\nidentities, 35,904 sequences, and 3.9 million frames. MMFace4D has three\nappealing characteristics: 1) highly diversified subjects and corpus, 2)\nsynchronized audio and 3D mesh sequence with high-resolution face details, and\n3) low storage cost with a new efficient compression algorithm on 3D mesh\nsequences. These characteristics enable the training of high-fidelity,\nexpressive, and generalizable face animation models. Upon MMFace4D, we\nconstruct a challenging benchmark of audio-driven 3D face animation with a\nstrong baseline, which enables non-autoregressive generation with fast\ninference speed and outperforms the state-of-the-art autoregressive method. The\nwhole benchmark will be released.\n","authors":["Haozhe Wu","Jia Jia","Junliang Xing","Hongwei Xu","Xiangyuan Wang","Jelo Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09026v2","updated":"2023-03-17T06:33:52Z","published":"2023-03-16T01:39:11Z","title":"Commonsense Knowledge Assisted Deep Learning for Resource-constrained\n  and Fine-grained Object Detection","summary":"  In this paper, we consider fine-grained image object detection in\nresource-constrained cases such as edge computing. Deep learning (DL), namely\nlearning with deep neural networks (DNNs), has become the dominating approach\nto object detection. To achieve accurate fine-grained detection, one needs to\nemploy a large enough DNN model and a vast amount of data annotations, which\nbrings a challenge for using modern DL object detectors in resource-constrained\ncases. To this end, we propose an approach, which leverages commonsense\nknowledge to assist a coarse-grained object detector to get accurate\nfine-grained detection results. Specifically, we introduce a commonsense\nknowledge inference module (CKIM) to process coarse-grained lables given by a\nbenchmark DL detector to produce fine-grained lables. We consider both\ncrisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to\nhandle ambiguity in the target semantic labels. We implement our method based\non several modern DL detectors, namely YOLOv4, Mobilenetv3-SSD and YOLOv7-tiny.\nExperiment results show that our approach outperforms benchmark detectors\nremarkably in terms of accuracy, model size and processing latency.\n","authors":["Pu Zhang","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.09026v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.09794v1","updated":"2023-03-17T06:31:06Z","published":"2023-03-17T06:31:06Z","title":"Revisiting Image Reconstruction for Semi-supervised Semantic\n  Segmentation","summary":"  Autoencoding, which aims to reconstruct the input images through a bottleneck\nlatent representation, is one of the classic feature representation learning\nstrategies. It has been shown effective as an auxiliary task for\nsemi-supervised learning but has become less popular as more sophisticated\nmethods have been proposed in recent years. In this paper, we revisit the idea\nof using image reconstruction as the auxiliary task and incorporate it with a\nmodern semi-supervised semantic segmentation framework. Surprisingly, we\ndiscover that such an old idea in semi-supervised learning can produce results\ncompetitive with state-of-the-art semantic segmentation algorithms. By\nvisualizing the intermediate layer activations of the image reconstruction\nmodule, we show that the feature map channel could correlate well with the\nsemantic concept, which explains why joint training with the reconstruction\ntask is helpful for the segmentation task. Motivated by our observation, we\nfurther proposed a modification to the image reconstruction task, aiming to\nfurther disentangle the object clue from the background patterns. From\nexperiment evaluation on various datasets, we show that using reconstruction as\nauxiliary loss can lead to consistent improvements in various datasets and\nmethods. The proposed method can further lead to significant improvement in\nobject-centric segmentation tasks.\n","authors":["Yuhao Lin","Haiming Xu","Lingqiao Liu","Jinan Zou","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2303.09794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09792v1","updated":"2023-03-17T06:26:55Z","published":"2023-03-17T06:26:55Z","title":"Exploring Sparse Visual Prompt for Cross-domain Semantic Segmentation","summary":"  Visual Domain Prompts (VDP) have shown promising potential in addressing\nvisual cross-domain problems. Existing methods adopt VDP in classification\ndomain adaptation (DA), such as tuning image-level or feature-level prompts for\ntarget domains. Since the previous dense prompts are opaque and mask out\ncontinuous spatial details in the prompt regions, it will suffer from\ninaccurate contextual information extraction and insufficient domain-specific\nfeature transferring when dealing with the dense prediction (i.e. semantic\nsegmentation) DA problems. Therefore, we propose a novel Sparse Visual Domain\nPrompts (SVDP) approach tailored for addressing domain shift problems in\nsemantic segmentation, which holds minimal discrete trainable parameters (e.g.\n10\\%) of the prompt and reserves more spatial information. To better apply\nSVDP, we propose Domain Prompt Placement (DPP) method to adaptively distribute\nseveral SVDP on regions with large data distribution distance based on\nuncertainty guidance. It aims to extract more local domain-specific knowledge\nand realizes efficient cross-domain learning. Furthermore, we design a Domain\nPrompt Updating (DPU) method to optimize prompt parameters differently for each\ntarget domain sample with different degrees of domain shift, which helps SVDP\nto better fit target domain knowledge. Experiments, which are conducted on the\nwidely-used benchmarks (Cityscapes, Foggy-Cityscapes, and ACDC), show that our\nproposed method achieves state-of-the-art performances on the source-free\nadaptations, including six Test Time Adaptation and one Continual Test-Time\nAdaptation in semantic segmentation.\n","authors":["Senqiao Yang","Jiarui Wu","Jiaming Liu","Xiaoqi Li","Qizhe Zhang","Mingjie Pan","Mingjie Pan","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09790v1","updated":"2023-03-17T06:18:16Z","published":"2023-03-17T06:18:16Z","title":"Reliable Multimodality Eye Disease Screening via Mixture of Student's t\n  Distributions","summary":"  Multimodality eye disease screening is crucial in ophthalmology as it\nintegrates information from diverse sources to complement their respective\nperformances. However, the existing methods are weak in assessing the\nreliability of each unimodality, and directly fusing an unreliable modality may\ncause screening errors. To address this issue, we introduce a novel\nmultimodality evidential fusion pipeline for eye disease screening, EyeMoS$t$,\nwhich provides a measure of confidence for unimodality and elegantly integrates\nthe multimodality information from a multi-distribution fusion perspective.\nSpecifically, our model estimates both local uncertainty for unimodality and\nglobal uncertainty for the fusion modality to produce reliable classification\nresults. More importantly, the proposed mixture of Student's $t$ distributions\nadaptively integrates different modalities to endow the model with heavy-tailed\nproperties, increasing robustness and reliability. Our experimental findings on\nboth public and in-house datasets show that our model is more reliable than\ncurrent methods. Additionally, EyeMos$t$ has the potential ability to serve as\na data quality discriminator, enabling reliable decision-making for\nmultimodality eye disease screening.\n","authors":["Ke Zou","Tian Lin","Xuedong Yuan","Haoyu Chen","Xiaojing Shen","Meng Wang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09790v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/1905.10575v3","updated":"2023-03-17T06:10:49Z","published":"2019-05-25T11:33:02Z","title":"Deep Image Feature Learning with Fuzzy Rules","summary":"  The methods of extracting image features are the key to many image processing\ntasks. At present, the most popular method is the deep neural network which can\nautomatically extract robust features through end-to-end training instead of\nhand-crafted feature extraction. However, the deep neural network currently\nfaces many challenges: 1) its effectiveness is heavily dependent on large\ndatasets, so the computational complexity is very high; 2) it is usually\nregarded as a black box model with poor interpretability. To meet the above\nchallenges, a more interpretable and scalable feature learning method, i.e.,\ndeep image feature learning with fuzzy rules (DIFL-FR), is proposed in the\npaper, which combines the rule-based fuzzy modeling technique and the deep\nstacked learning strategy. The method progressively learns image features\nthrough a layer-by-layer manner based on fuzzy rules, so the feature learning\nprocess can be better explained by the generated rules. More importantly, the\nlearning process of the method is only based on forward propagation without\nback propagation and iterative learning, which results in the high learning\nefficiency. In addition, the method is under the settings of unsupervised\nlearning and can be easily extended to scenes of supervised and semi-supervised\nlearning. Extensive experiments are conducted on image datasets of different\nscales. The results obviously show the effectiveness of the proposed method.\n","authors":["Xiang Ma","Liangzhe Chen","Zhaohong Deng","Peng Xu","Qisheng Yan","Kup-Sze Choi","Shitong Wang"],"pdf_url":"https://arxiv.org/pdf/1905.10575v3.pdf","comment":"Accepted by IEEE Trans. Emerging Topics in Computational Intelligence"},{"id":"http://arxiv.org/abs/2303.09789v1","updated":"2023-03-17T06:03:49Z","published":"2023-03-17T06:03:49Z","title":"Urban Regional Function Guided Traffic Flow Prediction","summary":"  The prediction of traffic flow is a challenging yet crucial problem in\nspatial-temporal analysis, which has recently gained increasing interest. In\naddition to spatial-temporal correlations, the functionality of urban areas\nalso plays a crucial role in traffic flow prediction. However, the exploration\nof regional functional attributes mainly focuses on adding additional\ntopological structures, ignoring the influence of functional attributes on\nregional traffic patterns. Different from the existing works, we propose a\nnovel module named POI-MetaBlock, which utilizes the functionality of each\nregion (represented by Point of Interest distribution) as metadata to further\nmine different traffic characteristics in areas with different functions.\nSpecifically, the proposed POI-MetaBlock employs a self-attention architecture\nand incorporates POI and time information to generate dynamic attention\nparameters for each region, which enables the model to fit different traffic\npatterns of various areas at different times. Furthermore, our lightweight\nPOI-MetaBlock can be easily integrated into conventional traffic flow\nprediction models. Extensive experiments demonstrate that our module\nsignificantly improves the performance of traffic flow prediction and\noutperforms state-of-the-art methods that use metadata.\n","authors":["Kuo Wang","Lingbo Liu","Yang Liu","Guanbin Li","Fan Zhou","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.09789v1.pdf","comment":"15 pages, 8 figures.This work has been submitted to the Information\n  Sciences for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2303.09785v1","updated":"2023-03-17T06:01:04Z","published":"2023-03-17T06:01:04Z","title":"ABAW : Facial Expression Recognition in the wild","summary":"  The fifth Affective Behavior Analysis in-the-wild (ABAW) competition has\nmultiple challenges such as Valence-Arousal Estimation Challenge, Expression\nClassification Challenge, Action Unit Detection Challenge, Emotional Reaction\nIntensity Estimation Challenge. In this paper we have dealt only expression\nclassification challenge using multiple approaches such as fully supervised,\nsemi-supervised and noisy label approach. Our approach using noise aware model\nhas performed better than baseline model by 10.46% and semi supervised model\nhas performed better than baseline model by 9.38% and the fully supervised\nmodel has performed better than the baseline by 9.34%\n","authors":["Darshan Gera","Badveeti Naveen Siva Kumar","Bobbili Veerendra Raj Kumar","S Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2303.09785v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2303.00952v2","updated":"2023-03-17T05:55:02Z","published":"2023-03-02T04:12:53Z","title":"MuscleMap: Towards Video-based Activated Muscle Group Estimation","summary":"  In this paper, we tackle the new task of video-based Activated Muscle Group\nEstimation (AMGE) aiming at identifying active muscle regions during physical\nactivity. To this intent, we provide the MuscleMap136 dataset featuring >15K\nvideo clips with 136 different activities and 20 labeled muscle groups. This\ndataset opens the vistas to multiple video-based applications in sports and\nrehabilitation medicine. We further complement the main MuscleMap136 dataset,\nwhich specifically targets physical exercise, with Muscle-UCF90 and\nMuscle-HMDB41, which are new variants of the well-known activity recognition\nbenchmarks extended with AMGE annotations. To make the AMGE model applicable in\nreal-life situations, it is crucial to ensure that the model can generalize\nwell to types of physical activities not present during training and involving\nnew combinations of activated muscles. To achieve this, our benchmark also\ncovers an evaluation setting where the model is exposed to activity types\nexcluded from the training set. Our experiments reveal that generalizability of\nexisting architectures adapted for the AMGE task remains a challenge.\nTherefore, we also propose a new approach, TransM3E, which employs a\ntransformer-based model with cross-modal multi-label knowledge distillation and\nsurpasses all popular video classification models when dealing with both,\npreviously seen and new types of physical activities. The datasets and code\nwill be publicly available at https://github.com/KPeng9510/MuscleMap.\n","authors":["Kunyu Peng","David Schneider","Alina Roitberg","Kailun Yang","Jiaming Zhang","M. Saquib Sarfraz","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.00952v2.pdf","comment":"The datasets and code will be publicly available at\n  https://github.com/KPeng9510/MuscleMap"},{"id":"http://arxiv.org/abs/2303.09782v1","updated":"2023-03-17T05:48:26Z","published":"2023-03-17T05:48:26Z","title":"High Accurate and Explainable Multi-Pill Detection Framework with Graph\n  Neural Network-Assisted Multimodal Data Fusion","summary":"  Due to the significant resemblance in visual appearance, pill misuse is\nprevalent and has become a critical issue, responsible for one-third of all\ndeaths worldwide. Pill identification, thus, is a crucial concern needed to be\ninvestigated thoroughly. Recently, several attempts have been made to exploit\ndeep learning to tackle the pill identification problem. However, most\npublished works consider only single-pill identification and fail to\ndistinguish hard samples with identical appearances. Also, most existing pill\nimage datasets only feature single pill images captured in carefully controlled\nenvironments under ideal lighting conditions and clean backgrounds. In this\nwork, we are the first to tackle the multi-pill detection problem in real-world\nsettings, aiming at localizing and identifying pills captured by users in a\npill intake. Moreover, we also introduce a multi-pill image dataset taken in\nunconstrained conditions. To handle hard samples, we propose a novel method for\nconstructing heterogeneous a priori graphs incorporating three forms of\ninter-pill relationships, including co-occurrence likelihood, relative size,\nand visual semantic correlation. We then offer a framework for integrating a\npriori with pills' visual features to enhance detection accuracy. Our\nexperimental results have proved the robustness, reliability, and\nexplainability of the proposed framework. Experimentally, it outperforms all\ndetection benchmarks in terms of all evaluation metrics. Specifically, our\nproposed framework improves COCO mAP metrics by 9.4% over Faster R-CNN and\n12.0% compared to vanilla YOLOv5. Our study opens up new opportunities for\nprotecting patients from medication errors using an AI-based pill\nidentification solution.\n","authors":["Anh Duy Nguyen","Huy Hieu Pham","Huynh Thanh Trung","Quoc Viet Hung Nguyen","Thao Nguyen Truong","Phi Le Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.09782v1.pdf","comment":"Under review by Plos ONE journal"},{"id":"http://arxiv.org/abs/2303.09780v1","updated":"2023-03-17T05:27:16Z","published":"2023-03-17T05:27:16Z","title":"Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox\n  Spread","summary":"  The challenge on forestalling monkeypox (Mpox) spread is the timely,\nconvenient and accurate diagnosis for earlystage infected individuals. Here, we\npropose a remote and realtime online visualization strategy, called \"Super\nMonitoring\" to construct a low cost, convenient, timely and unspecialized\ndiagnosis of early-stage Mpox. Such AI-mediated \"Super Monitoring\" (Mpox-AISM)\ninvokes a framework assembled by deep learning, data augmentation and\nself-supervised learning, as well as professionally classifies four subtypes\naccording to dataset characteristics and evolution trend of Mpox and seven\nother types of dermatopathya with high similarity, hence these features\ntogether with reasonable program interface and threshold setting ensure that\nits Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%.\nAs a result, with the help of cloud service on Internet and communication\nterminal, this strategy can be potentially utilized for the real-time detection\nof earlystage Mpox in various scenarios including entry-exit inspection in\nairport, family doctor, rural area in underdeveloped region and wild to\neffectively shorten the window period of Mpox spread.\n","authors":["Yubiao Yue","Zhenzhang Li","Xinyue Zhang","Jialong Xu","Jinbao Liu","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2303.09780v1.pdf","comment":"7pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.09779v1","updated":"2023-03-17T05:22:44Z","published":"2023-03-17T05:22:44Z","title":"Bidirectional Domain Mixup for Domain Adaptive Semantic Segmentation","summary":"  Mixup provides interpolated training samples and allows the model to obtain\nsmoother decision boundaries for better generalization. The idea can be\nnaturally applied to the domain adaptation task, where we can mix the source\nand target samples to obtain domain-mixed samples for better adaptation.\nHowever, the extension of the idea from classification to segmentation (i.e.,\nstructured output) is nontrivial. This paper systematically studies the impact\nof mixup under the domain adaptaive semantic segmentation task and presents a\nsimple yet effective mixup strategy called Bidirectional Domain Mixup (BDM). In\nspecific, we achieve domain mixup in two-step: cut and paste. Given the warm-up\nmodel trained from any adaptation techniques, we forward the source and target\nsamples and perform a simple threshold-based cut out of the unconfident regions\n(cut). After then, we fill-in the dropped regions with the other domain region\npatches (paste). In doing so, we jointly consider class distribution, spatial\nstructure, and pseudo label confidence. Based on our analysis, we found that\nBDM leaves domain transferable regions by cutting, balances the dataset-level\nclass distribution while preserving natural scene context by pasting. We\ncoupled our proposal with various state-of-the-art adaptation models and\nobserve significant improvement consistently. We also provide extensive\nablation experiments to empirically verify our main components of the\nframework. Visit our project page with the code at\nhttps://sites.google.com/view/bidirectional-domain-mixup\n","authors":["Daehan Kim","Minseok Seo","Kwanyong Park","Inkyu Shin","Sanghyun Woo","In-So Kweon","Dong-Geol Choi"],"pdf_url":"https://arxiv.org/pdf/2303.09779v1.pdf","comment":"10 pages, 3 figures, Accepted on AAAI 2023"},{"id":"http://arxiv.org/abs/2210.05174v2","updated":"2023-03-17T05:17:43Z","published":"2022-10-11T06:23:30Z","title":"BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised\n  Instance Segmentation","summary":"  Labeling objects with pixel-wise segmentation requires a huge amount of human\nlabor compared to bounding boxes. Most existing methods for weakly supervised\ninstance segmentation focus on designing heuristic losses with priors from\nbounding boxes. While, we find that box-supervised methods can produce some\nfine segmentation masks and we wonder whether the detectors could learn from\nthese fine masks while ignoring low-quality masks. To answer this question, we\npresent BoxTeacher, an efficient and end-to-end training framework for\nhigh-performance weakly supervised instance segmentation, which leverages a\nsophisticated teacher to generate high-quality masks as pseudo labels.\nConsidering the massive noisy masks hurt the training, we present a mask-aware\nconfidence score to estimate the quality of pseudo masks and propose the\nnoise-aware pixel loss and noise-reduced affinity loss to adaptively optimize\nthe student with pseudo masks. Extensive experiments can demonstrate the\neffectiveness of the proposed BoxTeacher. Without bells and whistles,\nBoxTeacher remarkably achieves 35.0 mask AP and 36.5 mask AP with ResNet-50 and\nResNet-101 respectively on the challenging COCO dataset, which outperforms the\nprevious state-of-the-art methods by a significant margin and bridges the gap\nbetween box-supervised and mask-supervised methods. The code and models will be\navailable at https://github.com/hustvl/BoxTeacher.\n","authors":["Tianheng Cheng","Xinggang Wang","Shaoyu Chen","Qian Zhang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2210.05174v2.pdf","comment":"Accepted to CVPR 2023. Code and models:\n  https://github.com/hustvl/BoxTeacher"},{"id":"http://arxiv.org/abs/2303.09170v2","updated":"2023-03-17T05:10:48Z","published":"2023-03-16T09:27:40Z","title":"NLUT: Neural-based 3D Lookup Tables for Video Photorealistic Style\n  Transfer","summary":"  Video photorealistic style transfer is desired to generate videos with a\nsimilar photorealistic style to the style image while maintaining temporal\nconsistency. However, existing methods obtain stylized video sequences by\nperforming frame-by-frame photorealistic style transfer, which is inefficient\nand does not ensure the temporal consistency of the stylized video. To address\nthis issue, we use neural network-based 3D Lookup Tables (LUTs) for the\nphotorealistic transfer of videos, achieving a balance between efficiency and\neffectiveness. We first train a neural network for generating photorealistic\nstylized 3D LUTs on a large-scale dataset; then, when performing photorealistic\nstyle transfer for a specific video, we select a keyframe and style image in\nthe video as the data source and fine-turn the neural network; finally, we\nquery the 3D LUTs generated by the fine-tuned neural network for the colors in\nthe video, resulting in a super-fast photorealistic style transfer, even\nprocessing 8K video takes less than 2 millisecond per frame. The experimental\nresults show that our method not only realizes the photorealistic style\ntransfer of arbitrary style images but also outperforms the existing methods in\nterms of visual quality and consistency. Project\npage:https://semchan.github.io/NLUT_Project.\n","authors":["Yaosen Chen","Han Yang","Yuexin Yang","Yuegen Liu","Wei Wang","Xuming Wen","Chaoping Xie"],"pdf_url":"https://arxiv.org/pdf/2303.09170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09233v2","updated":"2023-03-17T04:50:05Z","published":"2023-03-16T11:16:02Z","title":"SwinVFTR: A Novel Volumetric Feature-learning Transformer for 3D OCT\n  Fluid Segmentation","summary":"  Accurately segmenting fluid in 3D volumetric optical coherence tomography\n(OCT) images is a crucial yet challenging task for detecting eye diseases.\nTraditional autoencoding-based segmentation approaches have limitations in\nextracting fluid regions due to successive resolution loss in the encoding\nphase and the inability to recover lost information in the decoding phase.\nAlthough current transformer-based models for medical image segmentation\naddresses this limitation, they are not designed to be applied out-of-the-box\nfor 3D OCT volumes, which have a wide-ranging channel-axis size based on\ndifferent vendor device and extraction technique. To address these issues, we\npropose SwinVFTR, a new transformer-based architecture designed for precise\nfluid segmentation in 3D volumetric OCT images. We first utilize a channel-wise\nvolumetric sampling for training on OCT volumes with varying depths (B-scans).\nNext, the model uses a novel shifted window transformer block in the encoder to\nachieve better localization and segmentation of fluid regions. Additionally, we\npropose a new volumetric attention block for spatial and depth-wise attention,\nwhich improves upon traditional residual skip connections. Consequently,\nutilizing multi-class dice loss, the proposed architecture outperforms other\nexisting architectures on the three publicly available vendor-specific OCT\ndatasets, namely Spectralis, Cirrus, and Topcon, with mean dice scores of 0.72,\n0.59, and 0.68, respectively. Additionally, SwinVFTR outperforms other\narchitectures in two additional relevant metrics, mean intersection-over-union\n(Mean-IOU) and structural similarity measure (SSIM).\n","authors":["Sharif Amit Kamran","Khondker Fariha Hossain","Alireza Tavakkoli","Salah A. Baker","Stewart Lee Zuckerbrod"],"pdf_url":"https://arxiv.org/pdf/2303.09233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09773v1","updated":"2023-03-17T04:42:27Z","published":"2023-03-17T04:42:27Z","title":"Progressive Content-aware Coded Hyperspectral Compressive Imaging","summary":"  Hyperspectral imaging plays a pivotal role in a wide range of applications,\nlike remote sensing, medicine, and cytology. By acquiring 3D hyperspectral\nimages (HSIs) via 2D sensors, the coded aperture snapshot spectral imaging\n(CASSI) has achieved great success due to its hardware-friendly implementation\nand fast imaging speed. However, for some less spectrally sparse scenes, single\nsnapshot and unreasonable coded aperture design tend to make HSI recovery more\nill-posed and yield poor spatial and spectral fidelity. In this paper, we\npropose a novel Progressive Content-Aware CASSI framework, dubbed PCA-CASSI,\nwhich captures HSIs with multiple optimized content-aware coded apertures and\nfuses all the snapshots for reconstruction progressively. Simultaneously, by\nmapping the Range-Null space Decomposition (RND) into a deep network with\nseveral phases, an RND-HRNet is proposed for HSI recovery. Each recovery phase\ncan fully exploit the hidden physical information in the coded apertures via\nexplicit $\\mathcal{R}$$-$$\\mathcal{N}$ decomposition and explore the\nspatial-spectral correlation by dual transformer blocks. Our method is\nvalidated to surpass other state-of-the-art methods on both multiple- and\nsingle-shot HSI imaging tasks by large margins.\n","authors":["Xuanyu Zhang","Bin Chen","Wenzhen Zou","Shuai Liu","Yongbing Zhang","Ruiqin Xiong","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09773v1.pdf","comment":"a novel hyperspectral snapshot compressive imaging and restoration\n  framework"},{"id":"http://arxiv.org/abs/2212.00648v4","updated":"2023-03-17T04:40:59Z","published":"2022-12-01T16:49:53Z","title":"One-shot recognition of any material anywhere using contrastive learning\n  with physics-based rendering","summary":"  Visual recognition of materials and their states is essential for\nunderstanding most aspects of the world, from determining whether food is\ncooked, metal is rusted, or a chemical reaction has occurred. However, current\nimage recognition methods are limited to specific classes and properties and\ncan't handle the vast number of material states in the world. To address this,\nwe present MatSim: the first dataset and benchmark for computer vision-based\nrecognition of similarities and transitions between materials and textures,\nfocusing on identifying any material under any conditions using one or a few\nexamples. The dataset contains synthetic and natural images. The synthetic\nimages were rendered using giant collections of textures, objects, and\nenvironments generated by computer graphics artists. We use mixtures and\ngradual transitions between materials to allow the system to learn cases with\nsmooth transitions between states (like gradually cooked food). We also render\nimages with materials inside transparent containers to support beverage and\nchemistry lab use cases. We use this dataset to train a siamese net that\nidentifies the same material in different objects, mixtures, and environments.\nThe descriptor generated by this net can be used to identify the states of\nmaterials and their subclasses using a single image. We also present the first\nfew-shot material recognition benchmark with images from a wide range of\nfields, including the state of foods and drinks, types of grounds, and many\nother use cases. We show that a net trained on the MatSim synthetic dataset\noutperforms state-of-the-art models like Clip on the benchmark and also\nachieves good results on other unsupervised material classification tasks.\n","authors":["Manuel S. Drehwald","Sagi Eppel","Jolina Li","Han Hao","Alan Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2212.00648v4.pdf","comment":"for associated code and dataset, see\n  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or\n  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX\n  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF"},{"id":"http://arxiv.org/abs/2303.09535v2","updated":"2023-03-17T04:28:36Z","published":"2023-03-16T17:51:13Z","title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing","summary":"  The diffusion-based generative models have achieved remarkable success in\ntext-based image generation. However, since it contains enormous randomness in\ngeneration progress, it is still challenging to apply such models for\nreal-world visual content editing, especially in videos. In this paper, we\npropose FateZero, a zero-shot text-based editing method on real-world videos\nwithout per-prompt training or use-specific mask. To edit videos consistently,\nwe propose several techniques based on the pre-trained models. Firstly, in\ncontrast to the straightforward DDIM inversion technique, our approach captures\nintermediate attention maps during inversion, which effectively retain both\nstructural and motion information. These maps are directly fused in the editing\nprocess rather than generated during denoising. To further minimize semantic\nleakage of the source video, we then fuse self-attentions with a blending mask\nobtained by cross-attention features from the source prompt. Furthermore, we\nhave implemented a reform of the self-attention mechanism in denoising UNet by\nintroducing spatial-temporal attention to ensure frame consistency. Yet\nsuccinct, our method is the first one to show the ability of zero-shot\ntext-driven video style and local attribute editing from the trained\ntext-to-image model. We also have a better zero-shot shape-aware editing\nability based on the text-to-video model. Extensive experiments demonstrate our\nsuperior temporal consistency and editing capability than previous works.\n","authors":["Chenyang Qi","Xiaodong Cun","Yong Zhang","Chenyang Lei","Xintao Wang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09535v2.pdf","comment":"Project page: https://fate-zero-edit.github.io ; Github repository:\n  https://github.com/ChenyangQiQi/FateZero"},{"id":"http://arxiv.org/abs/2303.09769v1","updated":"2023-03-17T04:20:47Z","published":"2023-03-17T04:20:47Z","title":"Denoising Diffusion Autoencoders are Unified Self-supervised Learners","summary":"  Inspired by recent advances in diffusion models, which are reminiscent of\ndenoising autoencoders, we investigate whether they can acquire discriminative\nrepresentations for classification via generative pre-training. This paper\nshows that the networks in diffusion models, namely denoising diffusion\nautoencoders (DDAE), are unified self-supervised learners: by pre-training on\nunconditional image generation, DDAE has already learned strongly\nlinear-separable representations at its intermediate layers without auxiliary\nencoders, thus making diffusion pre-training emerge as a general approach for\nself-supervised generative and discriminative learning. To verify this, we\nperform linear probe and fine-tuning evaluations on multi-class datasets. Our\ndiffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on\nCIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked\nautoencoders and contrastive learning for the first time. Additionally,\ntransfer learning from ImageNet confirms DDAE's suitability for latent-space\nVision Transformers, suggesting the potential for scaling DDAEs as unified\nfoundation models.\n","authors":["Weilai Xiang","Hongyu Yang","Di Huang","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14710v2","updated":"2023-03-17T04:16:45Z","published":"2022-11-27T03:36:32Z","title":"3DPPE: 3D Point Positional Encoding for Multi-Camera 3D Object Detection\n  Transformers","summary":"  Transformer-based methods have swept the benchmarks on 2D and 3D detection on\nimages. Because tokenization before the attention mechanism drops the spatial\ninformation, positional encoding becomes critical for those methods. Recent\nworks found that encodings based on samples of the 3D viewing rays can\nsignificantly improve the quality of multi-camera 3D object detection. We\nhypothesize that 3D point locations can provide more information than rays.\nTherefore, we introduce 3D point positional encoding, 3DPPE, to the 3D\ndetection Transformer decoder. Although 3D measurements are not available at\nthe inference time of monocular 3D object detection, 3DPPE uses predicted depth\nto approximate the real point positions. Our hybriddepth module combines direct\nand categorical depth to estimate the refined depth of each pixel. Despite the\napproximation, 3DPPE achieves 46.0 mAP and 51.4 NDS on the competitive nuScenes\ndataset, significantly outperforming encodings based on ray samples. We will\nmake the codes available for further investigation.\n","authors":["Changyong Shu","JIajun Deng","Fisher Yu","Yifan Liu"],"pdf_url":"https://arxiv.org/pdf/2211.14710v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.09758v1","updated":"2023-03-17T03:45:00Z","published":"2023-03-17T03:45:00Z","title":"Hierarchical Prior Mining for Non-local Multi-View Stereo","summary":"  As a fundamental problem in computer vision, multi-view stereo (MVS) aims at\nrecovering the 3D geometry of a target from a set of 2D images. Recent advances\nin MVS have shown that it is important to perceive non-local structured\ninformation for recovering geometry in low-textured areas. In this work, we\npropose a Hierarchical Prior Mining for Non-local Multi-View Stereo (HPM-MVS).\nThe key characteristics are the following techniques that exploit non-local\ninformation to assist MVS: 1) A Non-local Extensible Sampling Pattern (NESP),\nwhich is able to adaptively change the size of sampled areas without becoming\nsnared in locally optimal solutions. 2) A new approach to leverage non-local\nreliable points and construct a planar prior model based on K-Nearest Neighbor\n(KNN), to obtain potential hypotheses for the regions where prior construction\nis challenging. 3) A Hierarchical Prior Mining (HPM) framework, which is used\nto mine extensive non-local prior information at different scales to assist 3D\nmodel recovery, this strategy can achieve a considerable balance between the\nreconstruction of details and low-textured areas. Experimental results on the\nETH3D and Tanks \\& Temples have verified the superior performance and strong\ngeneralization capability of our method. Our code will be released.\n","authors":["Chunlin Ren","Qingshan Xu","Shikun Zhang","Jiaqi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09757v1","updated":"2023-03-17T03:44:17Z","published":"2023-03-17T03:44:17Z","title":"Video Dehazing via a Multi-Range Temporal Alignment Network with\n  Physical Prior","summary":"  Video dehazing aims to recover haze-free frames with high visibility and\ncontrast. This paper presents a novel framework to effectively explore the\nphysical haze priors and aggregate temporal information. Specifically, we\ndesign a memory-based physical prior guidance module to encode the\nprior-related features into long-range memory. Besides, we formulate a\nmulti-range scene radiance recovery module to capture space-time dependencies\nin multiple space-time ranges, which helps to effectively aggregate temporal\ninformation from adjacent frames. Moreover, we construct the first large-scale\noutdoor video dehazing benchmark dataset, which contains videos in various\nreal-world scenarios. Experimental results on both synthetic and real\nconditions show the superiority of our proposed method.\n","authors":["Jiaqi Xu","Xiaowei Hu","Lei Zhu","Qi Dou","Jifeng Dai","Yu Qiao","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.09757v1.pdf","comment":"CVPR 2023; Code: https://github.com/jiaqixuac/MAP-Net"},{"id":"http://arxiv.org/abs/2303.09756v1","updated":"2023-03-17T03:44:15Z","published":"2023-03-17T03:44:15Z","title":"Video Action Recognition with Attentive Semantic Units","summary":"  Visual-Language Models (VLMs) have significantly advanced action video\nrecognition. Supervised by the semantics of action labels, recent works adapt\nthe visual branch of VLMs to learn video representations. Despite the\neffectiveness proved by these works, we believe that the potential of VLMs has\nyet to be fully harnessed. In light of this, we exploit the semantic units (SU)\nhiding behind the action labels and leverage their correlations with\nfine-grained items in frames for more accurate action recognition. SUs are\nentities extracted from the language descriptions of the entire action set,\nincluding body parts, objects, scenes, and motions. To further enhance the\nalignments between visual contents and the SUs, we introduce a multi-region\nmodule (MRA) to the visual branch of the VLM. The MRA allows the perception of\nregion-aware visual features beyond the original global feature. Our method\nadaptively attends to and selects relevant SUs with visual features of frames.\nWith a cross-modal decoder, the selected SUs serve to decode spatiotemporal\nvideo representations. In summary, the SUs as the medium can boost\ndiscriminative ability and transferability. Specifically, in fully-supervised\nlearning, our method achieved 87.8\\% top-1 accuracy on Kinetics-400. In K=2\nfew-shot experiments, our method surpassed the previous state-of-the-art by\n+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.\n","authors":["Yifei Chen","Dapeng Chen","Ruijin Liu","Hao Li","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.09756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10156v4","updated":"2023-03-17T03:20:06Z","published":"2022-11-17T13:35:11Z","title":"DETRDistill: A Universal Knowledge Distillation Framework for\n  DETR-families","summary":"  Transformer-based detectors (DETRs) are becoming popular for their simple\nframework, but the large model size and heavy time consumption hinder their\ndeployment in the real world. While knowledge distillation (KD) can be an\nappealing technique to compress giant detectors into small ones for comparable\ndetection performance and low inference cost. Since DETRs formulate object\ndetection as a set prediction problem, existing KD methods designed for classic\nconvolution-based detectors may not be directly applicable. In this paper, we\npropose DETRDistill, a novel knowledge distillation method dedicated to\nDETR-families. Specifically, we first design a Hungarian-matching logits\ndistillation to encourage the student model to have the exact predictions as\nthat of teacher DETRs. Next, we propose a target-aware feature distillation to\nhelp the student model learn from the object-centric features of the teacher\nmodel. Finally, in order to improve the convergence rate of the student DETR,\nwe introduce a query-prior assignment distillation to speed up the student\nmodel learning from well-trained queries and stable assignment of the teacher\nmodel. Extensive experimental results on the COCO dataset validate the\neffectiveness of our approach. Notably, DETRDistill consistently improves\nvarious DETRs by more than 2.0 mAP, even surpassing their teacher models.\n","authors":["Jiahao Chang","Shuo Wang","Haiming Xu","Zehui Chen","Chenhongyi Yang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2211.10156v4.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.03010v3","updated":"2023-03-17T03:12:33Z","published":"2022-12-06T14:32:55Z","title":"GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds","summary":"  Despite the tremendous progress of Masked Autoencoders (MAE) in developing\nvision tasks such as image and video, exploring MAE in large-scale 3D point\nclouds remains challenging due to the inherent irregularity. In contrast to\nprevious 3D MAE frameworks, which either design a complex decoder to infer\nmasked information from maintained regions or adopt sophisticated masking\nstrategies, we instead propose a much simpler paradigm. The core idea is to\napply a \\textbf{G}enerative \\textbf{D}ecoder for MAE (GD-MAE) to automatically\nmerges the surrounding context to restore the masked geometric knowledge in a\nhierarchical fusion manner. In doing so, our approach is free from introducing\nthe heuristic design of decoders and enjoys the flexibility of exploring\nvarious masking strategies. The corresponding part costs less than\n\\textbf{12\\%} latency compared with conventional methods, while achieving\nbetter performance. We demonstrate the efficacy of the proposed method on\nseveral large-scale benchmarks: Waymo, KITTI, and ONCE. Consistent improvement\non downstream detection tasks illustrates strong robustness and generalization\ncapability. Not only our method reveals state-of-the-art results, but\nremarkably, we achieve comparable accuracy even with \\textbf{20\\%} of the\nlabeled data on the Waymo dataset. Code will be released at\nhttps://github.com/Nightmare-n/GD-MAE.\n","authors":["Honghui Yang","Tong He","Jiaheng Liu","Hua Chen","Boxi Wu","Binbin Lin","Xiaofei He","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2212.03010v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.09746v1","updated":"2023-03-17T02:55:08Z","published":"2023-03-17T02:55:08Z","title":"Detecting Out-of-distribution Examples via Class-conditional Impressions\n  Reappearing","summary":"  Out-of-distribution (OOD) detection aims at enhancing standard deep neural\nnetworks to distinguish anomalous inputs from original training data. Previous\nprogress has introduced various approaches where the in-distribution training\ndata and even several OOD examples are prerequisites. However, due to privacy\nand security, auxiliary data tends to be impractical in a real-world scenario.\nIn this paper, we propose a data-free method without training on natural data,\ncalled Class-Conditional Impressions Reappearing (C2IR), which utilizes image\nimpressions from the fixed model to recover class-conditional feature\nstatistics. Based on that, we introduce Integral Probability Metrics to\nestimate layer-wise class-conditional deviations and obtain layer weights by\nMeasuring Gradient-based Importance (MGI). The experiments verify the\neffectiveness of our method and indicate that C2IR outperforms other post-hoc\nmethods and reaches comparable performance to the full access (ID and OOD)\ndetection method, especially in the far-OOD dataset (SVHN).\n","authors":["Jinggang Chen","Xiaoyang Qu","Junjie Li","Jianzong Wang","Jiguang Wan","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.09746v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.12977v2","updated":"2023-03-17T02:52:25Z","published":"2022-12-26T00:19:39Z","title":"SMMix: Self-Motivated Image Mixing for Vision Transformers","summary":"  CutMix is a vital augmentation strategy that determines the performance and\ngeneralization ability of vision transformers (ViTs). However, the\ninconsistency between the mixed images and the corresponding labels harms its\nefficacy. Existing CutMix variants tackle this problem by generating more\nconsistent mixed images or more precise mixed labels, but inevitably introduce\nheavy training overhead or require extra information, undermining ease of use.\nTo this end, we propose an novel and effective Self-Motivated image Mixing\nmethod (SMMix), which motivates both image and label enhancement by the model\nunder training itself. Specifically, we propose a max-min attention region\nmixing approach that enriches the attention-focused objects in the mixed\nimages. Then, we introduce a fine-grained label assignment technique that\nco-trains the output tokens of mixed images with fine-grained supervision.\nMoreover, we devise a novel feature consistency constraint to align features\nfrom mixed and unmixed images. Due to the subtle designs of the self-motivated\nparadigm, our SMMix is significant in its smaller training overhead and better\nperformance than other CutMix variants. In particular, SMMix improves the\naccuracy of DeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L by more than +1% on\nImageNet-1k. The generalization capability of our method is also demonstrated\non downstream tasks and out-of-distribution datasets. Our project is\nanonymously available at https://github.com/ChenMnZ/SMMix.\n","authors":["Mengzhao Chen","Mingbao Lin","ZhiHang Lin","Yuxin Zhang","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2212.12977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09736v1","updated":"2023-03-17T02:38:53Z","published":"2023-03-17T02:38:53Z","title":"Dynamic Structure Pruning for Compressing CNNs","summary":"  Structure pruning is an effective method to compress and accelerate neural\nnetworks. While filter and channel pruning are preferable to other structure\npruning methods in terms of realistic acceleration and hardware compatibility,\npruning methods with a finer granularity, such as intra-channel pruning, are\nexpected to be capable of yielding more compact and computationally efficient\nnetworks. Typical intra-channel pruning methods utilize a static and\nhand-crafted pruning granularity due to a large search space, which leaves room\nfor improvement in their pruning performance. In this work, we introduce a\nnovel structure pruning method, termed as dynamic structure pruning, to\nidentify optimal pruning granularities for intra-channel pruning. In contrast\nto existing intra-channel pruning methods, the proposed method automatically\noptimizes dynamic pruning granularities in each layer while training deep\nneural networks. To achieve this, we propose a differentiable group learning\nmethod designed to efficiently learn a pruning granularity based on\ngradient-based learning of filter groups. The experimental results show that\ndynamic structure pruning achieves state-of-the-art pruning performance and\nbetter realistic acceleration on a GPU compared with channel pruning. In\nparticular, it reduces the FLOPs of ResNet50 by 71.85% without accuracy\ndegradation on the ImageNet dataset. Our code is available at\nhttps://github.com/irishev/DSP.\n","authors":["Jun-Hyung Park","Yeachan Kim","Junho Kim","Joon-Young Choi","SangKeun Lee"],"pdf_url":"https://arxiv.org/pdf/2303.09736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09735v1","updated":"2023-03-17T02:38:44Z","published":"2023-03-17T02:38:44Z","title":"SRFormer: Permuted Self-Attention for Single Image Super-Resolution","summary":"  Previous works have shown that increasing the window size for\nTransformer-based image super-resolution models (e.g., SwinIR) can\nsignificantly improve the model performance but the computation overhead is\nalso considerable. In this paper, we present SRFormer, a simple but novel\nmethod that can enjoy the benefit of large window self-attention but introduces\neven less computational burden. The core of our SRFormer is the permuted\nself-attention (PSA), which strikes an appropriate balance between the channel\nand spatial information for self-attention. Our PSA is simple and can be easily\napplied to existing super-resolution networks based on window self-attention.\nWithout any bells and whistles, we show that our SRFormer achieves a 33.86dB\nPSNR score on the Urban100 dataset, which is 0.46dB higher than that of SwinIR\nbut uses fewer parameters and computations. We hope our simple and effective\napproach can serve as a useful tool for future research in super-resolution\nmodel design.\n","authors":["Yupeng Zhou","Zhen Li","Chun-Le Guo","Song Bai","Ming-Ming Cheng","Qibin Hou"],"pdf_url":"https://arxiv.org/pdf/2303.09735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09065v2","updated":"2023-03-17T02:34:18Z","published":"2023-03-16T03:45:46Z","title":"Maximum Margin Learning of t-SPNs for Cell Classification with Filtering","summary":"  An algorithm based on a deep probabilistic architecture referred to as a\ntree-structured sum-product network (t-SPN) is considered for cell\nclassification. The t-SPN is constructed such that the unnormalized probability\nis represented as conditional probabilities of a subset of most similar cell\nclasses. The constructed t-SPN architecture is learned by maximizing the\nmargin, which is the difference in the conditional probability between the true\nand the most competitive false label. To enhance the generalization ability of\nthe architecture, L2-regularization (REG) is considered along with the maximum\nmargin (MM) criterion in the learning process. To highlight cell features, this\npaper investigates the effectiveness of two generic high-pass filters: ideal\nhigh-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both\nHEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on\nthe max-margin criterion with regularization produced the highest accuracy rate\ncompared to other state-of-the-art algorithms that include convolutional neural\nnetwork (CNN) based algorithms. The ideal high-pass filter was more effective\non the HEp-2 dataset, which is based on immunofluorescence staining, while the\nLOG was more effective on the Feulgen dataset, which is based on Feulgen\nstaining.\n","authors":["Haeyong Kang","Chang D. Yoo","Yongcheon Na"],"pdf_url":"https://arxiv.org/pdf/2303.09065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07050v2","updated":"2023-03-17T02:32:06Z","published":"2022-12-14T06:04:18Z","title":"Significantly Improving Zero-Shot X-ray Pathology Classification via\n  Fine-tuning Pre-trained Image-Text Encoders","summary":"  Deep neural networks have been successfully adopted to diverse domains\nincluding pathology classification based on medical images. However,\nlarge-scale and high-quality data to train powerful neural networks are rare in\nthe medical domain as the labeling must be done by qualified experts.\nResearchers recently tackled this problem with some success by taking advantage\nof models pre-trained on large-scale general domain data. Specifically,\nresearchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it\nwith chest X-ray images and paired reports to perform zero-shot pathology\nclassification, thus completely removing the need for pathology-annotated\nimages to train a classification model. Existing studies, however, fine-tuned\nthe pre-trained model with the same contrastive learning objective, and failed\nto exploit the multi-labeled nature of medical image-report pairs. In this\npaper, we propose a new fine-tuning strategy based on sentence sampling and\npositive pair loss relaxation for improving the downstream zero-shot pathology\nclassification performance, which can be applied to any pre-trained contrastive\nimage-text encoders. Our method consistently showed dramatically improved\nzero-shot pathology classification performance on four different chest X-ray\ndatasets and 3 different pre-trained models (5.77% average AUROC increase). In\nparticular, fine-tuning CLIP with our method showed much comparable or\nmarginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1\nscore and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent\ndiseases from the CheXpert dataset.\n","authors":["Jongseong Jang","Daeun Kyung","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2212.07050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09733v1","updated":"2023-03-17T02:27:59Z","published":"2023-03-17T02:27:59Z","title":"Scribble-Supervised RGB-T Salient Object Detection","summary":"  Salient object detection segments attractive objects in scenes. RGB and\nthermal modalities provide complementary information and scribble annotations\nalleviate large amounts of human labor. Based on the above facts, we propose a\nscribble-supervised RGB-T salient object detection model. By a four-step\nsolution (expansion, prediction, aggregation, and supervision), label-sparse\nchallenge of scribble-supervised method is solved. To expand scribble\nannotations, we collect the superpixels that foreground scribbles pass through\nin RGB and thermal images, respectively. The expanded multi-modal labels\nprovide the coarse object boundary. To further polish the expanded labels, we\npropose a prediction module to alleviate the sharpness of boundary. To play the\ncomplementary roles of two modalities, we combine the two into aggregated\npseudo labels. Supervised by scribble annotations and pseudo labels, our model\nachieves the state-of-the-art performance on the relabeled RGBT-S dataset.\nFurthermore, the model is applied to RGB-D and video scribble-supervised\napplications, achieving consistently excellent performance.\n","authors":["Zhengyi Liu","Xiaoshen Huang","Guanghui Zhang","Xianyong Fang","Linbo Wang","Bin Tang"],"pdf_url":"https://arxiv.org/pdf/2303.09733v1.pdf","comment":"ICME2023"},{"id":"http://arxiv.org/abs/2303.09730v1","updated":"2023-03-17T02:19:28Z","published":"2023-03-17T02:19:28Z","title":"ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision\n  Transformer on Diverse Mobile Devices","summary":"  Neural Architecture Search (NAS) has shown promising performance in the\nautomatic design of vision transformers (ViT) exceeding 1G FLOPs. However,\ndesigning lightweight and low-latency ViT models for diverse mobile devices\nremains a big challenge. In this work, we propose ElasticViT, a two-stage NAS\napproach that trains a high-quality ViT supernet over a very large search space\nthat supports a wide range of mobile devices, and then searches an optimal\nsub-network (subnet) for direct deployment. However, prior supernet training\nmethods that rely on uniform sampling suffer from the gradient conflict issue:\nthe sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G\nFLOPs), leading to different optimization directions and inferior performance.\nTo address this challenge, we propose two novel sampling techniques:\ncomplexity-aware sampling and performance-aware sampling. Complexity-aware\nsampling limits the FLOPs difference among the subnets sampled across adjacent\ntraining steps, while covering different-sized subnets in the search space.\nPerformance-aware sampling further selects subnets that have good accuracy,\nwhich can reduce gradient conflicts and improve supernet quality. Our\ndiscovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to\n80.0% on ImageNet from 60M to 800M FLOPs without extra retraining,\noutperforming all prior CNNs and ViTs in terms of accuracy and latency. Our\ntiny and small models are also the first ViT models that surpass\nstate-of-the-art CNNs with significantly lower latency on mobile devices. For\ninstance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher\naccuracy.\n","authors":["Chen Tang","Li Lyna Zhang","Huiqiang Jiang","Jiahang Xu","Ting Cao","Quanlu Zhang","Yuqing Yang","Zhi Wang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09728v1","updated":"2023-03-17T02:01:11Z","published":"2023-03-17T02:01:11Z","title":"The Cascaded Forward Algorithm for Neural Network Training","summary":"  Backpropagation algorithm has been widely used as a mainstream learning\nprocedure for neural networks in the past decade, and has played a significant\nrole in the development of deep learning. However, there exist some limitations\nassociated with this algorithm, such as getting stuck in local minima and\nexperiencing vanishing/exploding gradients, which have led to questions about\nits biological plausibility. To address these limitations, alternative\nalgorithms to backpropagation have been preliminarily explored, with the\nForward-Forward (FF) algorithm being one of the most well-known. In this paper\nwe propose a new learning framework for neural networks, namely Cascaded\nForward (CaFo) algorithm, which does not rely on BP optimization as that in FF.\nUnlike FF, our framework directly outputs label distributions at each cascaded\nblock, which does not require generation of additional negative samples and\nthus leads to a more efficient process at both training and testing. Moreover,\nin our framework each block can be trained independently, so it can be easily\ndeployed into parallel acceleration systems. The proposed method is evaluated\non four public image classification benchmarks, and the experimental results\nillustrate significant improvement in prediction accuracy in comparison with\nthe baseline.\n","authors":["Gongpei Zhao","Tao Wang","Yidong Li","Yi Jin","Congyan Lang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.09728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06550v2","updated":"2023-03-17T01:56:25Z","published":"2023-03-12T03:25:01Z","title":"Spatial Correspondence between Graph Neural Network-Segmented Images","summary":"  Graph neural networks (GNNs) have been proposed for medical image\nsegmentation, by predicting anatomical structures represented by graphs of\nvertices and edges. One such type of graph is predefined with fixed size and\nconnectivity to represent a reference of anatomical regions of interest, thus\nknown as templates. This work explores the potentials in these GNNs with common\ntopology for establishing spatial correspondence, implicitly maintained during\nsegmenting two or more images. With an example application of registering local\nvertebral sub-regions found in CT images, our experimental results showed that\nthe GNN-based segmentation is capable of accurate and reliable localization of\nthe same interventionally interesting structures between images, not limited to\nthe segmentation classes. The reported average target registration errors of\n2.2$\\pm$1.3 mm and 2.7$\\pm$1.4 mm, for aligning holdout test images with a\nreference and for aligning two test images, respectively, were by a\nconsiderable margin lower than those from the tested non-learning and\nlearning-based registration algorithms. Further ablation studies assess the\ncontributions towards the registration performance, from individual components\nin the originally segmentation-purposed network and its training algorithm. The\nresults highlight that the proposed segmentation-in-lieu-of-registration\napproach shares methodological similarities with existing registration methods,\nsuch as the use of displacement smoothness constraint and point distance\nminimization albeit on non-grid graphs, which interestingly yielded benefits\nfor both segmentation and registration. We, therefore, conclude that the\ntemplate-based GNN segmentation can effectively establish spatial\ncorrespondence in our application, without any other dedicated registration\nalgorithms.\n","authors":["Qian Li","Yunguan Fu","Qianye Yang","Zhijiang Du","Hongjian Yu","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06550v2.pdf","comment":"Accepted at MIDL 2023 (The Medical Imaging with Deep Learning\n  conference, 2023)"},{"id":"http://arxiv.org/abs/2211.00313v2","updated":"2023-03-17T01:55:07Z","published":"2022-11-01T07:41:03Z","title":"RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection","summary":"  Purpose: Self-supervised learning is rapidly advancing computer-aided\ndiagnosis in the medical field. Masked image modeling (MIM) is one of the\nself-supervised learning methods that masks a subset of input pixels and\nattempts to predict the masked pixels. Traditional MIM methods often employ a\nrandom masking strategy. In comparison to ordinary images, medical images often\nhave a small region of interest for disease detection. Consequently, we focus\non fixing the problem in this work, which is evaluated by automatic COVID-19\nidentification. Methods: In this study, we propose a novel region-guided masked\nimage modeling method (RGMIM) for COVID-19 detection in this paper. In our\nmethod, we devise a new masking strategy that employed lung mask information to\nidentify valid regions to learn more useful information for COVID-19 detection.\nThe proposed method was contrasted with five self-supervised learning\ntechniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative\nevaluation of open COVID-19 CXR datasets as well as masking ratio\nhyperparameter studies. Results: When using the entire training set, RGMIM\noutperformed other comparable methods, achieving 0.962 detection accuracy.\nSpecifically, RGMIM significantly improved COVID-19 detection in small data\nvolumes, such as 5% and 10% of the training set (846 and 1,693 images) compared\nto other methods, and achieved 0.957 detection accuracy even when only 50% of\nthe training set was used. Conclusions: RGMIM can mask more valid lung-related\nregions, facilitating the learning of discriminative representations and the\nsubsequent high-accuracy COVID-19 detection. RGMIM outperforms other\nstate-of-the-art self-supervised learning methods in experiments, particularly\nwhen limited training data is used. RGMIM also has the potential to be applied\nto other medical images.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2211.00313v2.pdf","comment":"Submitted as a journal paper at Springer IJCARS"},{"id":"http://arxiv.org/abs/2210.16380v2","updated":"2023-03-17T01:11:23Z","published":"2022-10-28T19:39:14Z","title":"An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble\n  Modeling, Normalized Distributions of Annotations, and Entropic Measures of\n  Uncertainty","summary":"  Performing classification on noisy, crowdsourced image datasets can prove\nchallenging even for the best neural networks. Two issues which complicate the\nproblem on such datasets are class imbalance and ground-truth uncertainty in\nlabeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped,\nindividual characters from images of ancient Greek papyri -- are strongly\naffected by both issues. The application of ensemble modeling to such datasets\ncan help identify images where the ground-truth is questionable and quantify\nthe trustworthiness of those samples. As such, we apply stacked generalization\nconsisting of nearly identical ResNets with different loss functions: one\nutilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence\n(KLD). Both networks use labels drawn from the crowdsourced consensus. For the\nsecond network, the KLD is calculated with respect to the proposed Normalized\nDistribution of Annotations (NDA). For our ensemble model, we apply a k-nearest\nneighbors model to the outputs of the CXE and KLD networks. Individually, the\nResNet models have approximately 93% accuracy, while the ensemble model\nachieves an accuracy of > 95%. We also perform an analysis of the Shannon\nentropy of the various models' output distributions to measure classification\nuncertainty. Our results suggest that entropy is useful for predicting model\nmisclassifications.\n","authors":["Graham West","Matthew I. Swindall","Ben Keener","Timothy Player","Alex C. Williams","James H. Brusuelas","John F. Wallin"],"pdf_url":"https://arxiv.org/pdf/2210.16380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08162v3","updated":"2023-03-17T01:06:39Z","published":"2022-09-16T20:30:45Z","title":"Uncertainty Quantification of Collaborative Detection for Self-Driving","summary":"  Sharing information between connected and autonomous vehicles (CAVs)\nfundamentally improves the performance of collaborative object detection for\nself-driving. However, CAVs still have uncertainties on object detection due to\npractical challenges, which will affect the later modules in self-driving such\nas planning and control. Hence, uncertainty quantification is crucial for\nsafety-critical systems such as CAVs. Our work is the first to estimate the\nuncertainty of collaborative object detection. We propose a novel uncertainty\nquantification method, called Double-M Quantification, which tailors a moving\nblock bootstrap (MBB) algorithm with direct modeling of the multivariant\nGaussian distribution of each corner of the bounding box. Our method captures\nboth the epistemic uncertainty and aleatoric uncertainty with one inference\npass based on the offline Double-M training process. And it can be used with\ndifferent collaborative object detectors. Through experiments on the\ncomprehensive collaborative perception dataset, we show that our Double-M\nmethod achieves more than 4X improvement on uncertainty score and more than 3%\naccuracy improvement, compared with the state-of-the-art uncertainty\nquantification methods. Our code is public on\nhttps://coperception.github.io/double-m-quantification.\n","authors":["Sanbao Su","Yiming Li","Sihong He","Songyang Han","Chen Feng","Caiwen Ding","Fei Miao"],"pdf_url":"https://arxiv.org/pdf/2209.08162v3.pdf","comment":"This paper has been accepted by the 2023 IEEE International\n  Conference on Robotics and Automation (ICRA 2023)"},{"id":"http://arxiv.org/abs/2211.01513v2","updated":"2023-03-17T00:49:21Z","published":"2022-11-02T23:18:14Z","title":"Optimizing Fiducial Marker Placement for Improved Visual Localization","summary":"  Adding fiducial markers to a scene is a well-known strategy for making visual\nlocalization algorithms more robust. Traditionally, these marker locations are\nselected by humans who are familiar with visual localization techniques. This\npaper explores the problem of automatic marker placement within a scene.\nSpecifically, given a predetermined set of markers and a scene model, we\ncompute optimized marker positions within the scene that can improve accuracy\nin visual localization. Our main contribution is a novel framework for modeling\ncamera localizability that incorporates both natural scene features and\nartificial fiducial markers added to the scene. We present optimized marker\nplacement (OMP), a greedy algorithm that is based on the camera localizability\nframework. We have also designed a simulation framework for testing marker\nplacement algorithms on 3D models and images generated from synthetic scenes.\nWe have evaluated OMP within this testbed and demonstrate an improvement in the\nlocalization rate by up to 20 percent on four different scenes.\n","authors":["Qiangqiang Huang","Joseph DeGol","Victor Fragoso","Sudipta N. Sinha","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2211.01513v2.pdf","comment":"Extended technical report for publication in IEEE Robotics and\n  Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2209.13476v5","updated":"2023-03-17T00:41:23Z","published":"2022-09-27T15:50:31Z","title":"Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with\n  Extremely Limited Labels","summary":"  Recent studies on contrastive learning have achieved remarkable performance\nsolely by leveraging few labels in the context of medical image segmentation.\nExisting methods mainly focus on instance discrimination and invariant mapping.\nHowever, they face three common pitfalls: (1) tailness: medical image data\nusually follows an implicit long-tail class distribution. Blindly leveraging\nall pixels in training hence can lead to the data imbalance issues, and cause\ndeteriorated performance; (2) consistency: it remains unclear whether a\nsegmentation model has learned meaningful and yet consistent anatomical\nfeatures due to the intra-class variations between different anatomical\nfeatures; and (3) diversity: the intra-slice correlations within the entire\ndataset have received significantly less attention. This motivates us to seek a\nprincipled approach for strategically making use of the dataset itself to\ndiscover similar yet distinct samples from different anatomical views. In this\npaper, we introduce a novel semi-supervised 2D medical image segmentation\nframework termed Mine yOur owN Anatomy (MONA), and make three contributions.\nFirst, prior work argues that every pixel equally matters to the model\ntraining; we observe empirically that this alone is unlikely to define\nmeaningful anatomical features, mainly due to lacking the supervision signal.\nWe show two simple solutions towards learning invariances - through the use of\nstronger data augmentations and nearest neighbors. Second, we construct a set\nof objectives that encourage the model to be capable of decomposing medical\nimages into a collection of anatomical features in an unsupervised manner.\nLastly, we both empirically and theoretically, demonstrate the efficacy of our\nMONA on three benchmark datasets with different labeled settings, achieving new\nstate-of-the-art under different labeled semi-supervised settings\n","authors":["Chenyu You","Weicheng Dai","Fenglin Liu","Yifei Min","Haoran Su","Xiaoran Zhang","Xiaoxiao Li","David A. Clifton","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2209.13476v5.pdf","comment":"In this version: Add theoretical analysis and correct some typos"},{"id":"http://arxiv.org/abs/2303.09706v1","updated":"2023-03-17T00:28:33Z","published":"2023-03-17T00:28:33Z","title":"Unsupervised Self-Driving Attention Prediction via Uncertainty Mining\n  and Knowledge Embedding","summary":"  Predicting attention regions of interest is an important yet challenging task\nfor self-driving systems. Existing methodologies rely on large-scale labeled\ntraffic datasets that are labor-intensive to obtain. Besides, the huge domain\ngap between natural scenes and traffic scenes in current datasets also limits\nthe potential for model training. To address these challenges, we are the first\nto introduce an unsupervised way to predict self-driving attention by\nuncertainty modeling and driving knowledge integration. Our approach's\nUncertainty Mining Branch (UMB) discovers commonalities and differences from\nmultiple generated pseudo-labels achieved from models pre-trained on natural\nscenes by actively measuring the uncertainty. Meanwhile, our Knowledge\nEmbedding Block (KEB) bridges the domain gap by incorporating driving knowledge\nto adaptively refine the generated pseudo-labels. Quantitative and qualitative\nresults with equivalent or even more impressive performance compared to\nfully-supervised state-of-the-art approaches across all three public datasets\ndemonstrate the effectiveness of the proposed method and the potential of this\ndirection. The code will be made publicly available.\n","authors":["Pengfei Zhu","Mengshi Qi","Xia Li","Weijian Li","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.09706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09695v1","updated":"2023-03-17T00:03:38Z","published":"2023-03-17T00:03:38Z","title":"PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point\n  Clouds","summary":"  Garment pattern design aims to convert a 3D garment to the corresponding 2D\npanels and their sewing structure. Existing methods rely either on template\nfitting with heuristics and prior assumptions, or on model learning with\ncomplicated shape parameterization. Importantly, both approaches do not allow\nfor personalization of the output garment, which today has increasing demands.\nTo fill this demand, we introduce PersonalTailor: a personalized 2D pattern\ndesign method, where the user can input specific constraints or demands (in\nlanguage or sketch) for personal 2D panel fabrication from 3D point clouds.\nPersonalTailor first learns a multi-modal panel embeddings based on\nunsupervised cross-modal association and attentive fusion. It then predicts a\nbinary panel masks individually using a transformer encoder-decoder framework.\nExtensive experiments show that our PersonalTailor excels on both personalized\nand standard pattern fabrication tasks.\n","authors":["Anran Qi","Sauradip Nag","Xiatian Zhu","Ariel Shamir"],"pdf_url":"https://arxiv.org/pdf/2303.09695v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2203.08213v2","updated":"2023-03-17T00:00:54Z","published":"2022-03-15T19:26:29Z","title":"HUMUS-Net: Hybrid unrolled multi-scale network architecture for\n  accelerated MRI reconstruction","summary":"  In accelerated MRI reconstruction, the anatomy of a patient is recovered from\na set of under-sampled and noisy measurements. Deep learning approaches have\nbeen proven to be successful in solving this ill-posed inverse problem and are\ncapable of producing very high quality reconstructions. However, current\narchitectures heavily rely on convolutions, that are content-independent and\nhave difficulties modeling long-range dependencies in images. Recently,\nTransformers, the workhorse of contemporary natural language processing, have\nemerged as powerful building blocks for a multitude of vision tasks. These\nmodels split input images into non-overlapping patches, embed the patches into\nlower-dimensional tokens and utilize a self-attention mechanism that does not\nsuffer from the aforementioned weaknesses of convolutional architectures.\nHowever, Transformers incur extremely high compute and memory cost when 1) the\ninput image resolution is high and 2) when the image needs to be split into a\nlarge number of patches to preserve fine detail information, both of which are\ntypical in low-level vision problems such as MRI reconstruction, having a\ncompounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid\narchitecture that combines the beneficial implicit bias and efficiency of\nconvolutions with the power of Transformer blocks in an unrolled and\nmulti-scale network. HUMUS-Net extracts high-resolution features via\nconvolutional blocks and refines low-resolution features via a novel\nTransformer-based multi-scale feature extractor. Features from both levels are\nthen synthesized into a high-resolution output reconstruction. Our network\nestablishes new state of the art on the largest publicly available MRI dataset,\nthe fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two\nother popular MRI datasets and perform fine-grained ablation studies to\nvalidate our design.\n","authors":["Zalan Fabian","Berk Tinaz","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2203.08213v2.pdf","comment":"18 pages, 11 figures, NeurIPS 2022"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.10094v1","updated":"2023-03-17T16:15:13Z","published":"2023-03-17T16:15:13Z","title":"Stat-weight: Improving the Estimator of Interleaved Methods Outcomes\n  with Statistical Hypothesis Testing","summary":"  Interleaving is an online evaluation approach for information retrieval\nsystems that compares the effectiveness of ranking functions in interpreting\nthe users' implicit feedback. Previous work such as Hofmann et al (2011) has\nevaluated the most promising interleaved methods at the time, on uniform\ndistributions of queries. In the real world, ordinarily, there is an unbalanced\ndistribution of repeated queries that follows a long-tailed users' search\ndemand curve. The more a query is executed, by different users (or in different\nsessions), the higher the probability of collecting implicit feedback\n(interactions/clicks) on the related search results. This paper first aims to\nreplicate the Team Draft Interleaving accuracy evaluation on uniform query\ndistributions and then focuses on assessing how this method generalizes to\nlong-tailed real-world scenarios. The reproducibility work raised interesting\nconsiderations on how the winning ranking function for each query should impact\nthe overall winner for the entire evaluation. Based on what was observed, we\npropose that not all the queries should contribute to the final decision in\nequal proportion. As a result of these insights, we designed two variations of\nthe $\\Delta_{AB}$ score winner estimator that assign to each query a credit\nbased on statistical hypothesis testing. To replicate, reproduce and extend the\noriginal work, we have developed from scratch a system that simulates a search\nengine and users' interactions from datasets from the industry. Our experiments\nconfirm our intuition and show that our methods are promising in terms of\naccuracy, sensitivity, and robustness to noise.\n","authors":["Alessandro Benedetti","Anna Ruggero"],"pdf_url":"https://arxiv.org/pdf/2303.10094v1.pdf","comment":"This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Advances in Information Retrieval 45th European\n  Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April, 2023,\n  Proceedings, Part III, and is available online at\n  https://doi.org/10.1007/978-3-031-28241-6_2"},{"id":"http://arxiv.org/abs/2303.09999v1","updated":"2023-03-17T14:20:34Z","published":"2023-03-17T14:20:34Z","title":"STIXnet: A Novel and Modular Solution for Extracting All STIX Objects in\n  CTI Reports","summary":"  The automatic extraction of information from Cyber Threat Intelligence (CTI)\nreports is crucial in risk management. The increased frequency of the\npublications of these reports has led researchers to develop new systems for\nautomatically recovering different types of entities and relations from textual\ndata. Most state-of-the-art models leverage Natural Language Processing (NLP)\ntechniques, which perform greatly in extracting a few types of entities at a\ntime but cannot detect heterogeneous data or their relations. Furthermore,\nseveral paradigms, such as STIX, have become de facto standards in the CTI\ncommunity and dictate a formal categorization of different entities and\nrelations to enable organizations to share data consistently. This paper\npresents STIXnet, the first solution for the automated extraction of all STIX\nentities and relationships in CTI reports. Through the use of NLP techniques\nand an interactive Knowledge Base (KB) of entities, our approach obtains F1\nscores comparable to state-of-the-art models for entity extraction (0.916) and\nrelation extraction (0.724) while considering significantly more types of\nentities and relations. Moreover, STIXnet constitutes a modular and extensible\nframework that manages and coordinates different modules to merge their\ncontributions uniquely and exhaustively. With our approach, researchers and\norganizations can extend their Information Extraction (IE) capabilities by\nintegrating the efforts of several techniques without needing to develop new\ntools from scratch.\n","authors":["Francesco Marchiori","Mauro Conti","Nino Vincenzo Verde"],"pdf_url":"https://arxiv.org/pdf/2303.09999v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.09957v1","updated":"2023-03-17T13:26:33Z","published":"2023-03-17T13:26:33Z","title":"A Benchmark of PDF Information Extraction Tools using a Multi-Task and\n  Multi-Domain Evaluation Framework for Academic Documents","summary":"  Extracting information from academic PDF documents is crucial for numerous\nindexing, retrieval, and analysis use cases. Choosing the best tool to extract\nspecific content elements is difficult because many, technically diverse tools\nare available, but recent performance benchmarks are rare. Moreover, such\nbenchmarks typically cover only a few content elements like header metadata or\nbibliographic references and use smaller datasets from specific academic\ndisciplines. We provide a large and diverse evaluation framework that supports\nmore extraction tasks than most related datasets. Our framework builds upon\nDocBank, a multi-domain dataset of 1.5M annotated content elements extracted\nfrom 500K pages of research papers on arXiv. Using the new framework, we\nbenchmark ten freely available tools in extracting document metadata,\nbibliographic references, tables, and other content elements from academic PDF\ndocuments. GROBID achieves the best metadata and reference extraction results,\nfollowed by CERMINE and Science Parse. For table extraction, Adobe Extract\noutperforms other tools, even though the performance is much lower than for\nother content elements. All tools struggle to extract lists, footers, and\nequations. We conclude that more research on improving and combining tools is\nnecessary to achieve satisfactory extraction quality for most content elements.\nEvaluation datasets and frameworks like the one we present support this line of\nresearch. We make our data and code publicly available to contribute toward\nthis goal.\n","authors":["Norman Meuschke","Apurva Jagdale","Timo Spinde","Jelena Mitrović","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.09957v1.pdf","comment":"iConference 2023"},{"id":"http://arxiv.org/abs/2303.09902v1","updated":"2023-03-17T11:39:35Z","published":"2023-03-17T11:39:35Z","title":"Contrastive Self-supervised Learning in Recommender Systems: A Survey","summary":"  Deep learning-based recommender systems have achieved remarkable success in\nrecent years. However, these methods usually heavily rely on labeled data\n(i.e., user-item interactions), suffering from problems such as data sparsity\nand cold-start. Self-supervised learning, an emerging paradigm that extracts\ninformation from unlabeled data, provides insights into addressing these\nproblems. Specifically, contrastive self-supervised learning, due to its\nflexibility and promising performance, has attracted considerable interest and\nrecently become a dominant branch in self-supervised learning-based\nrecommendation methods. In this survey, we provide an up-to-date and\ncomprehensive review of current contrastive self-supervised learning-based\nrecommendation methods. Firstly, we propose a unified framework for these\nmethods. We then introduce a taxonomy based on the key components of the\nframework, including view generation strategy, contrastive task, and\ncontrastive objective. For each component, we provide detailed descriptions and\ndiscussions to guide the choice of the appropriate method. Finally, we outline\nopen issues and promising directions for future research.\n","authors":["Mengyuan Jing","Yanmin Zhu","Tianzi Zang","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09855v1","updated":"2023-03-17T09:30:42Z","published":"2023-03-17T09:30:42Z","title":"High-Dimensional Approximate Nearest Neighbor Search: with Reliable and\n  Efficient Distance Comparison Operations","summary":"  Approximate K nearest neighbor (AKNN) search is a fundamental and challenging\nproblem. We observe that in high-dimensional space, the time consumption of\nnearly all AKNN algorithms is dominated by that of the distance comparison\noperations (DCOs). For each operation, it scans full dimensions of an object\nand thus, runs in linear time wrt the dimensionality. To speed it up, we\npropose a randomized algorithm named ADSampling which runs in logarithmic time\nwrt to the dimensionality for the majority of DCOs and succeeds with high\nprobability. In addition, based on ADSampling we develop one general and two\nalgorithm-specific techniques as plugins to enhance existing AKNN algorithms.\nBoth theoretical and empirical studies confirm that: (1) our techniques\nintroduce nearly no accuracy loss and (2) they consistently improve the\nefficiency.\n","authors":["Jianyang Gao","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2303.09855v1.pdf","comment":"This paper has been accepted by SIGMOD 2023"},{"id":"http://arxiv.org/abs/2303.10230v1","updated":"2023-03-17T20:08:59Z","published":"2023-03-17T20:08:59Z","title":"ITM-Rec: An Open Data Set for Educational Recommender Systems","summary":"  With the development of recommender systems (RS), several promising systems\nhave emerged, such as context-aware RS, multi-criteria RS, and group RS.\nHowever, the education domain may not benefit from these developments due to\nmissing information, such as contexts and multiple criteria, in educational\ndata sets. In this paper, we announce and release an open data set for\neducational recommender systems. This data set includes not only traditional\nrating entries, but also enriched information, e.g., contexts, user preferences\nin multiple criteria, group compositions and preferences, etc. It provides a\ntestbed and enables more opportunities to develop and examine various\neducational recommender systems.\n","authors":["Yong Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.10230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11172v1","updated":"2023-03-17T02:34:05Z","published":"2023-03-17T02:34:05Z","title":"Explaining the Performance of Collaborative Filtering Methods With\n  Optimal Data Characteristics","summary":"  The performance of a Collaborative Filtering (CF) method is based on the\nproperties of a User-Item Rating Matrix (URM). And the properties or Rating\nData Characteristics (RDC) of a URM are constantly changing. Recent studies\nsignificantly explained the variation in the performances of CF methods\nresulted due to the change in URM using six or more RDC. Here, we found that\nthe significant proportion of variation in the performances of different CF\ntechniques can be accounted to two RDC only. The two RDC are the number of\nratings per user or Information per User (IpU) and the number of ratings per\nitem or Information per Item (IpI). And the performances of CF algorithms are\nquadratic to IpU (or IpI) for a square URM. The findings of this study are\nbased on seven well-established CF methods and three popular public recommender\ndatasets: 1M MovieLens, 25M MovieLens, and Yahoo! Music Rating datasets\n","authors":["Samin Poudel","Marwan Bikdash"],"pdf_url":"https://arxiv.org/pdf/2303.11172v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.10167v1","updated":"2023-03-17T17:54:25Z","published":"2023-03-17T17:54:25Z","title":"Generalized partitioned local depth","summary":"  In this paper we provide a generalization of the concept of cohesion as\nintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the National\nAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on the\ntechnique of partitioned local depth by distilling two key probabilistic\nconcepts: local relevance and support division. Earlier results are extended\nwithin the new context, and examples of applications to revealing communities\nin data with uncertainty are included.\n","authors":["Kenneth S. Berenhaut","John D. Foley","Liangdongsheng Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.10167v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.14446v2","updated":"2023-03-17T17:53:32Z","published":"2023-02-28T09:46:44Z","title":"Reproducing kernel Hilbert spaces in the mean field limit","summary":"  Kernel methods, being supported by a well-developed theory and coming with\nefficient algorithms, are among the most popular and successful machine\nlearning techniques. From a mathematical point of view, these methods rest on\nthe concept of kernels and function spaces generated by kernels, so called\nreproducing kernel Hilbert spaces. Motivated by recent developments of learning\napproaches in the context of interacting particle systems, we investigate\nkernel methods acting on data with many measurement variables. We show the\nrigorous mean field limit of kernels and provide a detailed analysis of the\nlimiting reproducing kernel Hilbert space. Furthermore, several examples of\nkernels, that allow a rigorous mean field limit, are presented.\n","authors":["Christian Fiedler","Michael Herty","Michael Rom","Chiara Segala","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2302.14446v2.pdf","comment":"Updated author email addresses"},{"id":"http://arxiv.org/abs/2303.10165v1","updated":"2023-03-17T17:53:28Z","published":"2023-03-17T17:53:28Z","title":"Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs","summary":"  We study reward-free reinforcement learning (RL) with linear function\napproximation, where the agent works in two phases: (1) in the exploration\nphase, the agent interacts with the environment but cannot access the reward;\nand (2) in the planning phase, the agent is given a reward function and is\nexpected to find a near-optimal policy based on samples collected in the\nexploration phase. The sample complexities of existing reward-free algorithms\nhave a polynomial dependence on the planning horizon, which makes them\nintractable for long planning horizon RL problems. In this paper, we propose a\nnew reward-free algorithm for learning linear mixture Markov decision processes\n(MDPs), where the transition probability can be parameterized as a linear\ncombination of known feature mappings. At the core of our algorithm is\nuncertainty-weighted value-targeted regression with exploration-driven\npseudo-reward and a high-order moment estimator for the aleatoric and epistemic\nuncertainties. When the total reward is bounded by $1$, we show that our\nalgorithm only needs to explore $\\tilde O( d^2\\varepsilon^{-2})$ episodes to\nfind an $\\varepsilon$-optimal policy, where $d$ is the dimension of the feature\nmapping. The sample complexity of our algorithm only has a polylogarithmic\ndependence on the planning horizon and therefore is ``horizon-free''. In\naddition, we provide an $\\Omega(d^2\\varepsilon^{-2})$ sample complexity lower\nbound, which matches the sample complexity of our algorithm up to logarithmic\nfactors, suggesting that our algorithm is optimal.\n","authors":["Junkai Zhang","Weitong Zhang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2303.10165v1.pdf","comment":"37 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2303.10158v1","updated":"2023-03-17T17:44:56Z","published":"2023-03-17T17:44:56Z","title":"Data-centric Artificial Intelligence: A Survey","summary":"  Artificial Intelligence (AI) is making a profound impact in almost every\ndomain. A vital enabler of its great success is the availability of abundant\nand high-quality data for building machine learning models. Recently, the role\nof data in AI has been significantly magnified, giving rise to the emerging\nconcept of data-centric AI. The attention of researchers and practitioners has\ngradually shifted from advancing model design to enhancing the quality and\nquantity of the data. In this survey, we discuss the necessity of data-centric\nAI, followed by a holistic view of three general data-centric goals (training\ndata development, inference data development, and data maintenance) and the\nrepresentative methods. We also organize the existing literature from\nautomation and collaboration perspectives, discuss the challenges, and tabulate\nthe benchmarks for various tasks. We believe this is the first comprehensive\nsurvey that provides a global view of a spectrum of tasks across various stages\nof the data lifecycle. We hope it can help the readers efficiently grasp a\nbroad picture of this field, and equip them with the techniques and further\nresearch ideas to systematically engineer data for building AI systems. A\ncompanion list of data-centric AI resources will be regularly updated on\nhttps://github.com/daochenzha/data-centric-AI\n","authors":["Daochen Zha","Zaid Pervaiz Bhat","Kwei-Herng Lai","Fan Yang","Zhimeng Jiang","Shaochen Zhong","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10158v1.pdf","comment":"38 pages, 6 figues, 5 tables. A companion list of data-centric AI\n  resources is available at https://github.com/daochenzha/data-centric-AI"},{"id":"http://arxiv.org/abs/2303.00462v3","updated":"2023-03-17T17:31:40Z","published":"2023-03-01T12:41:12Z","title":"Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision","summary":"  This work proposes a novel approach to 4D radar-based scene flow estimation\nvia cross-modal learning. Our approach is motivated by the co-located sensing\nredundancy in modern autonomous vehicles. Such redundancy implicitly provides\nvarious forms of supervision cues to the radar scene flow estimation.\nSpecifically, we introduce a multi-task model architecture for the identified\ncross-modal learning problem and propose loss functions to opportunistically\nengage scene flow estimation using multiple cross-modal constraints for\neffective model training. Extensive experiments show the state-of-the-art\nperformance of our method and demonstrate the effectiveness of cross-modal\nsupervised learning to infer more accurate 4D radar scene flow. We also show\nits usefulness to two subtasks - motion segmentation and ego-motion estimation.\nOur source code will be available on https://github.com/Toytiny/CMFlow.\n","authors":["Fangqiang Ding","Andras Palffy","Dariu M. Gavrila","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.00462v3.pdf","comment":"10 pages, 7 figures. Accepted by CVPR 2023. See our code at\n  https://github.com/Toytiny/CMFlow. Supplementary materials can be found at\n  https://drive.google.com/file/d/1Iewcqnjzecge2ePBM8k2tg-85LX5xs3N/view"},{"id":"http://arxiv.org/abs/2301.00810v3","updated":"2023-03-17T17:29:11Z","published":"2023-01-02T18:59:59Z","title":"SIRL: Similarity-based Implicit Representation Learning","summary":"  When robots learn reward functions using high capacity models that take raw\nstate directly as input, they need to both learn a representation for what\nmatters in the task -- the task ``features\" -- as well as how to combine these\nfeatures into a single objective. If they try to do both at once from input\ndesigned to teach the full reward function, it is easy to end up with a\nrepresentation that contains spurious correlations in the data, which fails to\ngeneralize to new settings. Instead, our ultimate goal is to enable robots to\nidentify and isolate the causal features that people actually care about and\nuse when they represent states and behavior. Our idea is that we can tune into\nthis representation by asking users what behaviors they consider similar:\nbehaviors will be similar if the features that matter are similar, even if\nlow-level behavior is different; conversely, behaviors will be different if\neven one of the features that matter differs. This, in turn, is what enables\nthe robot to disambiguate between what needs to go into the representation\nversus what is spurious, as well as what aspects of behavior can be compressed\ntogether versus not. The notion of learning representations based on similarity\nhas a nice parallel in contrastive learning, a self-supervised representation\nlearning technique that maps visually similar data points to similar\nembeddings, where similarity is defined by a designer through data augmentation\nheuristics. By contrast, in order to learn the representations that people use,\nso we can learn their preferences and objectives, we use their definition of\nsimilarity. In simulation as well as in a user study, we show that learning\nthrough such similarity queries leads to representations that, while far from\nperfect, are indeed more generalizable than self-supervised and task-input\nalternatives.\n","authors":["Andreea Bobu","Yi Liu","Rohin Shah","Daniel S. Brown","Anca D. Dragan"],"pdf_url":"https://arxiv.org/pdf/2301.00810v3.pdf","comment":"12 pages, 6 figures, HRI 2023"},{"id":"http://arxiv.org/abs/2303.10144v1","updated":"2023-03-17T17:29:02Z","published":"2023-03-17T17:29:02Z","title":"Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting","summary":"  Early stopping based on the validation set performance is a popular approach\nto find the right balance between under- and overfitting in the context of\nsupervised learning. However, in reinforcement learning, even for supervised\nsub-problems such as world model learning, early stopping is not applicable as\nthe dataset is continually evolving. As a solution, we propose a new general\nmethod that dynamically adjusts the update to data (UTD) ratio during training\nbased on under- and overfitting detection on a small subset of the continuously\ncollected experience not used for training. We apply our method to DreamerV2, a\nstate-of-the-art model-based reinforcement learning algorithm, and evaluate it\non the DeepMind Control Suite and the Atari $100$k benchmark. The results\ndemonstrate that one can better balance under- and overestimation by adjusting\nthe UTD ratio with our approach compared to the default setting in DreamerV2\nand that it is competitive with an extensive hyperparameter search which is not\nfeasible for many applications. Our method eliminates the need to set the UTD\nhyperparameter by hand and even leads to a higher robustness with regard to\nother learning-related hyperparameters further reducing the amount of necessary\ntuning.\n","authors":["Nicolai Dorka","Tim Welschehold","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.10144v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.10140v1","updated":"2023-03-17T17:27:47Z","published":"2023-03-17T17:27:47Z","title":"Geometric Deep Learning for Molecular Crystal Structure Prediction","summary":"  We develop and test new machine learning strategies for accelerating\nmolecular crystal structure ranking and crystal property prediction using tools\nfrom geometric deep learning on molecular graphs. Leveraging developments in\ngraph-based learning and the availability of large molecular crystal datasets,\nwe train models for density prediction and stability ranking which are\naccurate, fast to evaluate, and applicable to molecules of widely varying size\nand composition. Our density prediction model, MolXtalNet-D, achieves state of\nthe art performance, with lower than 2% mean absolute error on a large and\ndiverse test dataset. Our crystal ranking tool, MolXtalNet-S, correctly\ndiscriminates experimental samples from synthetically generated fakes and is\nfurther validated through analysis of the submissions to the Cambridge\nStructural Database Blind Tests 5 and 6. Our new tools are computationally\ncheap and flexible enough to be deployed within an existing crystal structure\nprediction pipeline both to reduce the search space and score/filter crystal\ncandidates.\n","authors":["Michael Kilgour","Jutta Rogal","Mark Tuckerman"],"pdf_url":"https://arxiv.org/pdf/2303.10140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10139v1","updated":"2023-03-17T17:27:18Z","published":"2023-03-17T17:27:18Z","title":"Distill n' Explain: explaining graph neural networks using simple\n  surrogates","summary":"  Explaining node predictions in graph neural networks (GNNs) often boils down\nto finding graph substructures that preserve predictions. Finding these\nstructures usually implies back-propagating through the GNN, bonding the\ncomplexity (e.g., number of layers) of the GNN to the cost of explaining it.\nThis naturally begs the question: Can we break this bond by explaining a\nsimpler surrogate GNN? To answer the question, we propose Distill n' Explain\n(DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX\nextracts node or edge-level explanations by solving a simple convex program. We\nalso propose FastDnX, a faster version of DnX that leverages the linear\ndecomposition of our surrogate model. Experiments show that DnX and FastDnX\noften outperform state-of-the-art GNN explainers while being orders of\nmagnitude faster. Additionally, we support our empirical findings with\ntheoretical results linking the quality of the surrogate model (i.e.,\ndistillation error) to the faithfulness of explanations.\n","authors":["Tamara Pereira","Erik Nasciment","Lucas E. Resck","Diego Mesquita","Amauri Souza"],"pdf_url":"https://arxiv.org/pdf/2303.10139v1.pdf","comment":"To appear in AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.10138v1","updated":"2023-03-17T17:26:56Z","published":"2023-03-17T17:26:56Z","title":"Generate, Transform, Answer: Question Specific Tool Synthesis for\n  Tabular Data","summary":"  Tabular question answering (TQA) presents a challenging setting for neural\nsystems by requiring joint reasoning of natural language with large amounts of\nsemi-structured data. Unlike humans who use programmatic tools like filters to\ntransform data before processing, language models in TQA process tables\ndirectly, resulting in information loss as table size increases. In this paper\nwe propose ToolWriter to generate query specific programs and detect when to\napply them to transform tables and align them with the TQA model's\ncapabilities. Focusing ToolWriter to generate row-filtering tools improves the\nstate-of-the-art for WikiTableQuestions and WikiSQL with the most performance\ngained on long tables. By investigating headroom, our work highlights the\nbroader potential for programmatic tools combined with neural components to\nmanipulate large amounts of structured data.\n","authors":["Carlos Gemmell","Jeffrey Dalton"],"pdf_url":"https://arxiv.org/pdf/2303.10138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10137v1","updated":"2023-03-17T17:25:10Z","published":"2023-03-17T17:25:10Z","title":"A Recipe for Watermarking Diffusion Models","summary":"  Recently, diffusion models (DMs) have demonstrated their advantageous\npotential for generative tasks. Widespread interest exists in incorporating DMs\ninto downstream applications, such as producing or editing photorealistic\nimages. However, practical deployment and unprecedented power of DMs raise\nlegal issues, including copyright protection and monitoring of generated\ncontent. In this regard, watermarking has been a proven solution for copyright\nprotection and content monitoring, but it is underexplored in the DMs\nliterature. Specifically, DMs generate samples from longer tracks and may have\nnewly designed multimodal structures, necessitating the modification of\nconventional watermarking pipelines. To this end, we conduct comprehensive\nanalyses and derive a recipe for efficiently watermarking state-of-the-art DMs\n(e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe\nis straightforward but involves empirically ablated implementation details,\nproviding a solid foundation for future research on watermarking DMs. Our Code:\nhttps://github.com/yunqing-me/WatermarkDM.\n","authors":["Yunqing Zhao","Tianyu Pang","Chao Du","Xiao Yang","Ngai-Man Cheung","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10137v1.pdf","comment":"23 pages, 22 figures"},{"id":"http://arxiv.org/abs/2303.10136v1","updated":"2023-03-17T17:24:57Z","published":"2023-03-17T17:24:57Z","title":"MassNet: A Deep Learning Approach for Body Weight Extraction from A\n  Single Pressure Image","summary":"  Body weight, as an essential physiological trait, is of considerable\nsignificance in many applications like body management, rehabilitation, and\ndrug dosing for patient-specific treatments. Previous works on the body weight\nestimation task are mainly vision-based, using 2D/3D, depth, or infrared\nimages, facing problems in illumination, occlusions, and especially privacy\nissues. The pressure mapping mattress is a non-invasive and privacy-preserving\ntool to obtain the pressure distribution image over the bed surface, which\nstrongly correlates with the body weight of the lying person. To extract the\nbody weight from this image, we propose a deep learning-based model, including\na dual-branch network to extract the deep features and pose features\nrespectively. A contrastive learning module is also combined with the\ndeep-feature branch to help mine the mutual factors across different postures\nof every single subject. The two groups of features are then concatenated for\nthe body weight regression task. To test the model's performance over different\nhardware and posture settings, we create a pressure image dataset of 10\nsubjects and 23 postures, using a self-made pressure-sensing bedsheet. This\ndataset, which is made public together with this paper, together with a public\ndataset, are used for the validation. The results show that our model\noutperforms the state-of-the-art algorithms over both 2 datasets. Our research\nconstitutes an important step toward fully automatic weight estimation in both\nclinical and at-home practice. Our dataset is available for research purposes\nat: https://github.com/USTCWzy/MassEstimation.\n","authors":["Ziyu Wu","Quan Wan","Mingjie Zhao","Yi Ke","Yiran Fang","Zhen Liang","Fangting Xie","Jingyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.10136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10135v1","updated":"2023-03-17T17:23:14Z","published":"2023-03-17T17:23:14Z","title":"Efficient and Feasible Robotic Assembly Sequence Planning via Graph\n  Representation Learning","summary":"  Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve\nproductivity and resilience in modern manufacturing along with the growing need\nfor greater product customization. One of the main challenges in realizing such\nautomation resides in efficiently finding solutions from a growing number of\npotential sequences for increasingly complex assemblies. Besides, costly\nfeasibility checks are always required for the robotic system. To address this,\nwe propose a holistic graphical approach including a graph representation\ncalled Assembly Graph for product assemblies and a policy architecture, Graph\nAssembly Processing Network, dubbed GRACE for assembly sequence generation.\nSecondly, we use GRACE to extract meaningful information from the graph input\nand predict assembly sequences in a step-by-step manner. In experiments, we\nshow that our approach can predict feasible assembly sequences across product\nvariants of aluminum profiles based on data collected in simulation of a\ndual-armed robotic system. We further demonstrate that our method is capable of\ndetecting infeasible assemblies, substantially alleviating the undesirable\nimpacts from false predictions, and hence facilitating real-world deployment\nsoon. Code and training data will be open-sourced.\n","authors":["Matan Atad","Jianxiang Feng","Ismael Rodríguez","Maximilian Durner","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.10135v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.05497v2","updated":"2023-03-17T17:18:33Z","published":"2023-03-09T18:50:15Z","title":"Learning Stationary Markov Processes with Contrastive Adjustment","summary":"  We introduce a new optimization algorithm, termed contrastive adjustment, for\nlearning Markov transition kernels whose stationary distribution matches the\ndata distribution. Contrastive adjustment is not restricted to a particular\nfamily of transition distributions and can be used to model data in both\ncontinuous and discrete state spaces. Inspired by recent work on noise-annealed\nsampling, we propose a particular transition operator, the noise kernel, that\ncan trade mixing speed for sample fidelity. We show that contrastive adjustment\nis highly valuable in human-computer design processes, as the stationarity of\nthe learned Markov chain enables local exploration of the data manifold and\nmakes it possible to iteratively refine outputs by human feedback. We compare\nthe performance of noise kernels trained with contrastive adjustment to current\nstate-of-the-art generative models and demonstrate promising results on a\nvariety of image synthesis tasks.\n","authors":["Ludvig Bergenstråhle","Jens Lagergren","Joakim Lundeberg"],"pdf_url":"https://arxiv.org/pdf/2303.05497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10131v1","updated":"2023-03-17T17:16:53Z","published":"2023-03-17T17:16:53Z","title":"She Elicits Requirements and He Tests: Software Engineering Gender Bias\n  in Large Language Models","summary":"  Implicit gender bias in software development is a well-documented issue, such\nas the association of technical roles with men. To address this bias, it is\nimportant to understand it in more detail. This study uses data mining\ntechniques to investigate the extent to which 56 tasks related to software\ndevelopment, such as assigning GitHub issues and testing, are affected by\nimplicit gender bias embedded in large language models. We systematically\ntranslated each task from English into a genderless language and back, and\ninvestigated the pronouns associated with each task. Based on translating each\ntask 100 times in different permutations, we identify a significant disparity\nin the gendered pronoun associations with different tasks. Specifically,\nrequirements elicitation was associated with the pronoun \"he\" in only 6% of\ncases, while testing was associated with \"he\" in 100% of cases. Additionally,\ntasks related to helping others had a 91% association with \"he\" while the same\nassociation for tasks related to asking coworkers was only 52%. These findings\nreveal a clear pattern of gender bias related to software development tasks and\nhave important implications for addressing this issue both in the training of\nlarge language models and in broader society.\n","authors":["Christoph Treude","Hideaki Hata"],"pdf_url":"https://arxiv.org/pdf/2303.10131v1.pdf","comment":"6 pages, MSR 2023"},{"id":"http://arxiv.org/abs/2303.08874v2","updated":"2023-03-17T16:59:46Z","published":"2023-03-15T18:37:41Z","title":"Bayesian Quadrature for Neural Ensemble Search","summary":"  Ensembling can improve the performance of Neural Networks, but existing\napproaches struggle when the architecture likelihood surface has dispersed,\nnarrow peaks. Furthermore, existing methods construct equally weighted\nensembles, and this is likely to be vulnerable to the failure modes of the\nweaker architectures. By viewing ensembling as approximately marginalising over\narchitectures we construct ensembles using the tools of Bayesian Quadrature --\ntools which are well suited to the exploration of likelihood surfaces with\ndispersed, narrow peaks. Additionally, the resulting ensembles consist of\narchitectures weighted commensurate with their performance. We show empirically\n-- in terms of test likelihood, accuracy, and expected calibration error --\nthat our method outperforms state-of-the-art baselines, and verify via ablation\nstudies that its components do so independently.\n","authors":["Saad Hamid","Xingchen Wan","Martin Jørgensen","Binxin Ru","Michael Osborne"],"pdf_url":"https://arxiv.org/pdf/2303.08874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06015v2","updated":"2023-03-17T16:49:18Z","published":"2022-06-13T10:13:51Z","title":"No-Regret Learning in Games with Noisy Feedback: Faster Rates and\n  Adaptivity via Learning Rate Separation","summary":"  We examine the problem of regret minimization when the learner is involved in\na continuous game with other optimizing agents: in this case, if all players\nfollow a no-regret algorithm, it is possible to achieve significantly lower\nregret relative to fully adversarial environments. We study this problem in the\ncontext of variationally stable games (a class of continuous games which\nincludes all convex-concave and monotone games), and when the players only have\naccess to noisy estimates of their individual payoff gradients. If the noise is\nadditive, the game-theoretic and purely adversarial settings enjoy similar\nregret guarantees; however, if the noise is multiplicative, we show that the\nlearners can, in fact, achieve constant regret. We achieve this faster rate via\nan optimistic gradient scheme with learning rate separation -- that is, the\nmethod's extrapolation and update steps are tuned to different schedules,\ndepending on the noise profile. Subsequently, to eliminate the need for\ndelicate hyperparameter tuning, we propose a fully adaptive method that attains\nnearly the same guarantees as its non-adapted counterpart, while operating\nwithout knowledge of either the game or of the noise profile.\n","authors":["Yu-Guan Hsieh","Kimon Antonakopoulos","Volkan Cevher","Panayotis Mertikopoulos"],"pdf_url":"https://arxiv.org/pdf/2206.06015v2.pdf","comment":"In Advances in Neural Information Processing Systems 35 (NeurIPS\n  2022)"},{"id":"http://arxiv.org/abs/2303.10112v1","updated":"2023-03-17T16:45:01Z","published":"2023-03-17T16:45:01Z","title":"Causal Discovery from Temporal Data: An Overview and New Perspectives","summary":"  Temporal data, representing chronological observations of complex systems,\nhas always been a typical data structure that can be widely generated by many\ndomains, such as industry, medicine and finance. Analyzing this type of data is\nextremely valuable for various applications. Thus, different temporal data\nanalysis tasks, eg, classification, clustering and prediction, have been\nproposed in the past decades. Among them, causal discovery, learning the causal\nrelations from temporal data, is considered an interesting yet critical task\nand has attracted much research attention. Existing casual discovery works can\nbe divided into two highly correlated categories according to whether the\ntemporal data is calibrated, ie, multivariate time series casual discovery, and\nevent sequence casual discovery. However, most previous surveys are only\nfocused on the time series casual discovery and ignore the second category. In\nthis paper, we specify the correlation between the two categories and provide a\nsystematical overview of existing solutions. Furthermore, we provide public\ndatasets, evaluation metrics and new perspectives for temporal data casual\ndiscovery.\n","authors":["Chang Gong","Di Yao","Chuzhe Zhang","Wenbin Li","Jingping Bi"],"pdf_url":"https://arxiv.org/pdf/2303.10112v1.pdf","comment":"50 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10108v1","updated":"2023-03-17T16:39:21Z","published":"2023-03-17T16:39:21Z","title":"Data-Centric Learning from Unlabeled Graphs with Diffusion Model","summary":"  Graph property prediction tasks are important and numerous. While each task\noffers a small size of labeled examples, unlabeled graphs have been collected\nfrom various sources and at a large scale. A conventional approach is training\na model with the unlabeled graphs on self-supervised tasks and then fine-tuning\nthe model on the prediction tasks. However, the self-supervised task knowledge\ncould not be aligned or sometimes conflicted with what the predictions needed.\nIn this paper, we propose to extract the knowledge underlying the large set of\nunlabeled graphs as a specific set of useful data points to augment each\nproperty prediction model. We use a diffusion model to fully utilize the\nunlabeled graphs and design two new objectives to guide the model's denoising\nprocess with each task's labeled data to generate task-specific graph examples\nand their labels. Experiments demonstrate that our data-centric approach\nperforms significantly better than fourteen existing various methods on fifteen\ntasks. The performance improvement brought by unlabeled data is visible as the\ngenerated labeled examples unlike self-supervised learning.\n","authors":["Gang Liu","Eric Inae","Tong Zhao","Jiaxin Xu","Tengfei Luo","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.10108v1.pdf","comment":"Preprint. 18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2206.04113v2","updated":"2023-03-17T16:38:10Z","published":"2022-06-08T18:18:18Z","title":"Push--Pull with Device Sampling","summary":"  We consider decentralized optimization problems in which a number of agents\ncollaborate to minimize the average of their local functions by exchanging over\nan underlying communication graph. Specifically, we place ourselves in an\nasynchronous model where only a random portion of nodes perform computation at\neach iteration, while the information exchange can be conducted between all the\nnodes and in an asymmetric fashion. For this setting, we propose an algorithm\nthat combines gradient tracking with a network-level variance reduction (in\ncontrast to variance reduction within each node). This enables each node to\ntrack the average of the gradients of the objective functions. Our theoretical\nanalysis shows that the algorithm converges linearly, when the local objective\nfunctions are strongly convex, under mild connectivity conditions on the\nexpected mixing matrices. In particular, our result does not require the mixing\nmatrices to be doubly stochastic. In the experiments, we investigate a\nbroadcast mechanism that transmits information from computing nodes to their\nneighbors, and confirm the linear convergence of our method on both synthetic\nand real-world datasets.\n","authors":["Yu-Guan Hsieh","Yassine Laguel","Franck Iutzeler","Jérôme Malick"],"pdf_url":"https://arxiv.org/pdf/2206.04113v2.pdf","comment":"In IEEE Transactions on Automatic Control"},{"id":"http://arxiv.org/abs/2209.12849v2","updated":"2023-03-17T16:36:26Z","published":"2022-09-26T16:58:00Z","title":"AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft\n  Detection and Tracking","summary":"  Detect-and-Avoid (DAA) capabilities are critical for safe operations of\nunmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time\nvision-only detect and tracking framework that respects the size, weight, and\npower (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios\n(SNR) of far away aircraft, we propose using full resolution images in a deep\nlearning framework that aligns successive images to remove ego-motion. The\naligned images are then used downstream in cascaded primary and secondary\nclassifiers to improve detection and tracking performance on multiple metrics.\nWe show that AirTrack outperforms state-of-the art baselines on the Amazon\nAirborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a\nCessna 182 interacting with general aviation traffic and additional\nnear-collision flight tests with a Bell helicopter flying towards a UAS in a\ncontrolled setting showcase that the proposed approach satisfies the newly\nintroduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that\nour system has a probability of track of more than 95% up to a range of 700m.\nVideo available at https://youtu.be/H3lL_Wjxjpw .\n","authors":["Sourish Ghosh","Jay Patrikar","Brady Moon","Milad Moghassem Hamidi","and Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2209.12849v2.pdf","comment":"7 pages, 5 figures, ICRA 2023"},{"id":"http://arxiv.org/abs/2210.16114v5","updated":"2023-03-17T16:35:00Z","published":"2022-10-28T13:21:28Z","title":"Towards Reliable Neural Specifications","summary":"  Having reliable specifications is an unavoidable challenge in achieving\nverifiable correctness, robustness, and interpretability of AI systems.\nExisting specifications for neural networks are in the paradigm of data as\nspecification. That is, the local neighborhood centering around a reference\ninput is considered to be correct (or robust). While existing specifications\ncontribute to verifying adversarial robustness, a significant problem in many\nresearch domains, our empirical study shows that those verified regions are\nsomewhat tight, and thus fail to allow verification of test set inputs, making\nthem impractical for some real-world applications. To this end, we propose a\nnew family of specifications called neural representation as specification,\nwhich uses the intrinsic information of neural networks - neural activation\npatterns (NAPs), rather than input data to specify the correctness and/or\nrobustness of neural network predictions. We present a simple statistical\napproach to mining neural activation patterns. To show the effectiveness of\ndiscovered NAPs, we formally verify several important properties, such as\nvarious types of misclassifications will never happen for a given NAP, and\nthere is no ambiguity between different NAPs. We show that by using NAP, we can\nverify a significant region of the input space, while still recalling 84% of\nthe data on MNIST. Moreover, we can push the verifiable bound to 10 times\nlarger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be\nused as a more reliable and extensible specification for neural network\nverification.\n","authors":["Chuqin Geng","Nham Le","Xiaojie Xu","Zhaoyue Wang","Arie Gurfinkel","Xujie Si"],"pdf_url":"https://arxiv.org/pdf/2210.16114v5.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2211.14158v2","updated":"2023-03-17T16:25:15Z","published":"2022-11-25T15:03:37Z","title":"An Isolation-Aware Online Virtual Network Embedding via Deep\n  Reinforcement Learning","summary":"  Virtualization technologies are the foundation of modern ICT infrastructure,\nenabling service providers to create dedicated virtual networks (VNs) that can\nsupport a wide range of smart city applications. These VNs continuously\ngenerate massive amounts of data, necessitating stringent reliability and\nsecurity requirements. In virtualized network environments, however, multiple\nVNs may coexist on the same physical infrastructure and, if not properly\nisolated, may interfere with or provide unauthorized access to one another. The\nformer causes performance degradation, while the latter compromises the\nsecurity of VNs. Service assurance for infrastructure providers becomes\nsignificantly more complicated when a specific VN violates the isolation\nrequirement.\n  In an effort to address the isolation issue, this paper proposes isolation\nduring virtual network embedding (VNE), the procedure of allocating VNs onto\nphysical infrastructure. We define a simple abstracted concept of isolation\nlevels to capture the variations in isolation requirements and then formulate\nisolation-aware VNE as an optimization problem with resource and isolation\nconstraints. A deep reinforcement learning (DRL)-based VNE algorithm\nISO-DRL_VNE, is proposed that considers resource and isolation constraints and\nis compared to the existing three state-of-the-art algorithms: NodeRank, Global\nResource Capacity (GRC), and Mote-Carlo Tree Search (MCTS). Evaluation results\nshow that the ISO-DRL_VNE algorithm outperforms others in acceptance ratio,\nlong-term average revenue, and long-term average revenue-to-cost ratio by 6%,\n13%, and 15%.\n","authors":["Ali Gohar","Chunming Rong","Sanghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2211.14158v2.pdf","comment":"7 pages, 9 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2303.10096v1","updated":"2023-03-17T16:16:36Z","published":"2023-03-17T16:16:36Z","title":"Efficient Neural Generation of 4K Masks for Homogeneous Diffusion\n  Inpainting","summary":"  With well-selected data, homogeneous diffusion inpainting can reconstruct\nimages from sparse data with high quality. While 4K colour images of size 3840\nx 2160 can already be inpainted in real time, optimising the known data for\napplications like image compression remains challenging: Widely used stochastic\nstrategies can take days for a single 4K image. Recently, a first neural\napproach for this so-called mask optimisation problem offered high speed and\ngood quality for small images. It trains a mask generation network with the\nhelp of a neural inpainting surrogate. However, these mask networks can only\noutput masks for the resolution and mask density they were trained for. We\nsolve these problems and enable mask optimisation for high-resolution images\nthrough a neuroexplicit coarse-to-fine strategy. Additionally, we improve the\ntraining and interpretability of mask networks by including a numerical\ninpainting solver directly into the network. This allows to generate masks for\n4K images in around 0.6 seconds while exceeding the quality of stochastic\nmethods on practically relevant densities. Compared to popular existing\napproaches, this is an acceleration of up to four orders of magnitude.\n","authors":["Karl Schrader","Pascal Peter","Niklas Kämper","Joachim Weickert"],"pdf_url":"https://arxiv.org/pdf/2303.10096v1.pdf","comment":"To appear in L. Calatroni, M. Donatelli, S. Morigi, M. Prato, M.\n  Santavesaria (Eds.): Scale Space and Variational Methods in Computer Vision.\n  Lecture Notes in Computer Science, Springer, Cham, 2023"},{"id":"http://arxiv.org/abs/2303.10093v1","updated":"2023-03-17T16:14:37Z","published":"2023-03-17T16:14:37Z","title":"Enhancing the Role of Context in Region-Word Alignment for Object\n  Detection","summary":"  Vision-language pretraining to learn a fine-grained, region-word alignment\nbetween image-caption pairs has propelled progress in open-vocabulary object\ndetection. We observe that region-word alignment methods are typically used in\ndetection with respect to only object nouns, and the impact of other rich\ncontext in captions, such as attributes, is unclear. In this study, we explore\nhow language context affects downstream object detection and propose to enhance\nthe role of context. In particular, we show how to strategically contextualize\nthe grounding pretraining objective for improved alignment. We further hone in\non attributes as especially useful object context and propose a novel adjective\nand noun-based negative sampling strategy for increasing their focus in\ncontrastive learning. Overall, our methods enhance object detection when\ncompared to the state-of-the-art in region-word pretraining. We also highlight\nthe fine-grained utility of an attribute-sensitive model through text-region\nretrieval and phrase grounding analysis.\n","authors":["Kyle Buettner","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2303.10093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10085v1","updated":"2023-03-17T16:10:06Z","published":"2023-03-17T16:10:06Z","title":"Robust probabilistic inference via a constrained transport metric","summary":"  Flexible Bayesian models are typically constructed using limits of large\nparametric models with a multitude of parameters that are often\nuninterpretable. In this article, we offer a novel alternative by constructing\nan exponentially tilted empirical likelihood carefully designed to concentrate\nnear a parametric family of distributions of choice with respect to a novel\nvariant of the Wasserstein metric, which is then combined with a prior\ndistribution on model parameters to obtain a robustified posterior. The\nproposed approach finds applications in a wide variety of robust inference\nproblems, where we intend to perform inference on the parameters associated\nwith the centering distribution in presence of outliers. Our proposed transport\nmetric enjoys great computational simplicity, exploiting the Sinkhorn\nregularization for discrete optimal transport problems, and being inherently\nparallelizable. We demonstrate superior performance of our methodology when\ncompared against state-of-the-art robust Bayesian inference methods. We also\ndemonstrate equivalence of our approach with a nonparametric Bayesian\nformulation under a suitable asymptotic framework, testifying to its\nflexibility. The constrained entropy maximization that sits at the heart of our\nlikelihood formulation finds its utility beyond robust Bayesian inference; an\nillustration is provided in a trustworthy machine learning application.\n","authors":["Abhisek Chakraborty","Anirban Bhattacharya","Debdeep Pati"],"pdf_url":"https://arxiv.org/pdf/2303.10085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2005.12900v7","updated":"2023-03-17T16:08:00Z","published":"2020-05-26T17:53:18Z","title":"Breaking the Sample Size Barrier in Model-Based Reinforcement Learning\n  with a Generative Model","summary":"  This paper is concerned with the sample efficiency of reinforcement learning,\nassuming access to a generative model (or simulator). We first consider\n$\\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with\nstate space $\\mathcal{S}$ and action space $\\mathcal{A}$. Despite a number of\nprior works tackling this problem, a complete picture of the trade-offs between\nsample complexity and statistical accuracy is yet to be determined. In\nparticular, all prior results suffer from a severe sample size barrier, in the\nsense that their claimed statistical guarantees hold only when the sample size\nexceeds at least $\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^2}$. The current\npaper overcomes this barrier by certifying the minimax optimality of two\nalgorithms -- a perturbed model-based algorithm and a conservative model-based\nalgorithm -- as soon as the sample size exceeds the order of\n$\\frac{|\\mathcal{S}||\\mathcal{A}|}{1-\\gamma}$ (modulo some log factor). Moving\nbeyond infinite-horizon MDPs, we further study time-inhomogeneous\nfinite-horizon MDPs, and prove that a plain model-based planning algorithm\nsuffices to achieve minimax-optimal sample complexity given any target accuracy\nlevel. To the best of our knowledge, this work delivers the first\nminimax-optimal guarantees that accommodate the entire range of sample sizes\n(beyond which finding a meaningful policy is information theoretically\ninfeasible).\n","authors":["Gen Li","Yuting Wei","Yuejie Chi","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2005.12900v7.pdf","comment":"accepted Operations Research"},{"id":"http://arxiv.org/abs/2303.09949v1","updated":"2023-03-17T16:03:10Z","published":"2023-03-17T16:03:10Z","title":"Towards a Foundation Model for Neural Network Wavefunctions","summary":"  Deep neural networks have become a highly accurate and powerful wavefunction\nansatz in combination with variational Monte Carlo methods for solving the\nelectronic Schr\\\"odinger equation. However, despite their success and favorable\nscaling, these methods are still computationally too costly for wide adoption.\nA significant obstacle is the requirement to optimize the wavefunction from\nscratch for each new system, thus requiring long optimization. In this work, we\npropose a novel neural network ansatz, which effectively maps uncorrelated,\ncomputationally cheap Hartree-Fock orbitals, to correlated, high-accuracy\nneural network orbitals. This ansatz is inherently capable of learning a single\nwavefunction across multiple compounds and geometries, as we demonstrate by\nsuccessfully transferring a wavefunction model pre-trained on smaller fragments\nto larger compounds. Furthermore, we provide ample experimental evidence to\nsupport the idea that extensive pre-training of a such a generalized\nwavefunction model across different compounds and geometries could lead to a\nfoundation wavefunction model. Such a model could yield high-accuracy ab-initio\nenergies using only minimal computational effort for fine-tuning and evaluation\nof observables.\n","authors":["Michael Scherbela","Leon Gerard","Philipp Grohs"],"pdf_url":"https://arxiv.org/pdf/2303.09949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10078v1","updated":"2023-03-17T16:00:18Z","published":"2023-03-17T16:00:18Z","title":"Fuzziness-tuned: Improving the Transferability of Adversarial Examples","summary":"  With the development of adversarial attacks, adversairal examples have been\nwidely used to enhance the robustness of the training models on deep neural\nnetworks. Although considerable efforts of adversarial attacks on improving the\ntransferability of adversarial examples have been developed, the attack success\nrate of the transfer-based attacks on the surrogate model is much higher than\nthat on victim model under the low attack strength (e.g., the attack strength\n$\\epsilon=8/255$). In this paper, we first systematically investigated this\nissue and found that the enormous difference of attack success rates between\nthe surrogate model and victim model is caused by the existence of a special\narea (known as fuzzy domain in our paper), in which the adversarial examples in\nthe area are classified wrongly by the surrogate model while correctly by the\nvictim model. Then, to eliminate such enormous difference of attack success\nrates for improving the transferability of generated adversarial examples, a\nfuzziness-tuned method consisting of confidence scaling mechanism and\ntemperature scaling mechanism is proposed to ensure the generated adversarial\nexamples can effectively skip out of the fuzzy domain. The confidence scaling\nmechanism and the temperature scaling mechanism can collaboratively tune the\nfuzziness of the generated adversarial examples through adjusting the gradient\ndescent weight of fuzziness and stabilizing the update direction, respectively.\nSpecifically, the proposed fuzziness-tuned method can be effectively integrated\nwith existing adversarial attacks to further improve the transferability of\nadverarial examples without changing the time complexity. Extensive experiments\ndemonstrated that fuzziness-tuned method can effectively enhance the\ntransferability of adversarial examples in the latest transfer-based attacks.\n","authors":["Xiangyuan Yang","Jie Lin","Hanlin Zhang","Xinyu Yang","Peng Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10067v1","updated":"2023-03-17T15:50:00Z","published":"2023-03-17T15:50:00Z","title":"Deep Author Name Disambiguation using DBLP Data","summary":"  In the academic world, the number of scientists grows every year and so does\nthe number of authors sharing the same names. Consequently, it challenging to\nassign newly published papers to their respective authors. Therefore, Author\nName Ambiguity (ANA) is considered a critical open problem in digital\nlibraries. This paper proposes an Author Name Disambiguation (AND) approach\nthat links author names to their real-world entities by leveraging their\nco-authors and domain of research. To this end, we use data collected from the\nDBLP repository that contains more than 5 million bibliographic records\nauthored by around 2.6 million co-authors. Our approach first groups authors\nwho share the same last names and same first name initials. The author within\neach group is identified by capturing the relation with his/her co-authors and\narea of research, represented by the titles of the validated publications of\nthe corresponding author. To this end, we train a neural network model that\nlearns from the representations of the co-authors and titles. We validated the\neffectiveness of our approach by conducting extensive experiments on a large\ndataset.\n","authors":["Zeyd Boukhers","Nagaraj Bahubali Asundi"],"pdf_url":"https://arxiv.org/pdf/2303.10067v1.pdf","comment":"Accepted for publication in the International Journal on Digital\n  Libraries. arXiv admin note: substantial text overlap with arXiv:2207.04772"},{"id":"http://arxiv.org/abs/2303.10058v1","updated":"2023-03-17T15:38:39Z","published":"2023-03-17T15:38:39Z","title":"No Fear of Classifier Biases: Neural Collapse Inspired Federated\n  Learning with Synthetic and Fixed Classifier","summary":"  Data heterogeneity is an inherent challenge that hinders the performance of\nfederated learning (FL). Recent studies have identified the biased classifiers\nof local models as the key bottleneck. Previous attempts have used classifier\ncalibration after FL training, but this approach falls short in improving the\npoor feature representations caused by training-time classifier biases.\nResolving the classifier bias dilemma in FL requires a full understanding of\nthe mechanisms behind the classifier. Recent advances in neural collapse have\nshown that the classifiers and feature prototypes under perfect training\nscenarios collapse into an optimal structure called simplex equiangular tight\nframe (ETF). Building on this neural collapse insight, we propose a solution to\nthe FL's classifier bias problem by utilizing a synthetic and fixed ETF\nclassifier during training. The optimal classifier structure enables all\nclients to learn unified and optimal feature representations even under\nextremely heterogeneous data. We devise several effective modules to better\nadapt the ETF structure in FL, achieving both high generalization and\npersonalization. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet.\n","authors":["Zexi Li","Xinyi Shang","Rui He","Tao Lin","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.10058v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.10057v1","updated":"2023-03-17T15:38:11Z","published":"2023-03-17T15:38:11Z","title":"Posterior Estimation Using Deep Learning: A Simulation Study of\n  Compartmental Modeling in Dynamic PET","summary":"  Background: In medical imaging, images are usually treated as deterministic,\nwhile their uncertainties are largely underexplored. Purpose: This work aims at\nusing deep learning to efficiently estimate posterior distributions of imaging\nparameters, which in turn can be used to derive the most probable parameters as\nwell as their uncertainties. Methods: Our deep learning-based approaches are\nbased on a variational Bayesian inference framework, which is implemented using\ntwo different deep neural networks based on conditional variational\nauto-encoder (CVAE), CVAE-dual-encoder and CVAE-dual-decoder. The conventional\nCVAE framework, i.e., CVAE-vanilla, can be regarded as a simplified case of\nthese two neural networks. We applied these approaches to a simulation study of\ndynamic brain PET imaging using a reference region-based kinetic model.\nResults: In the simulation study, we estimated posterior distributions of PET\nkinetic parameters given a measurement of time-activity curve. Our proposed\nCVAE-dual-encoder and CVAE-dual-decoder yield results that are in good\nagreement with the asymptotically unbiased posterior distributions sampled by\nMarkov Chain Monte Carlo (MCMC). The CVAE-vanilla can also be used for\nestimating posterior distributions, although it has an inferior performance to\nboth CVAE-dual-encoder and CVAE-dual-decoder. Conclusions: We have evaluated\nthe performance of our deep learning approaches for estimating posterior\ndistributions in dynamic brain PET. Our deep learning approaches yield\nposterior distributions, which are in good agreement with unbiased\ndistributions estimated by MCMC. All these neural networks have different\ncharacteristics and can be chosen by the user for specific applications. The\nproposed methods are general and can be adapted to other problems.\n","authors":["Xiaofeng Liu","Thibault Marin","Tiss Amal","Jonghye Woo","Georges El Fakhri","Jinsong Ouyang"],"pdf_url":"https://arxiv.org/pdf/2303.10057v1.pdf","comment":"Published in Medical Physics"},{"id":"http://arxiv.org/abs/2205.15213v3","updated":"2023-03-17T15:33:16Z","published":"2022-05-30T16:17:09Z","title":"Backpropagation through Combinatorial Algorithms: Identity with\n  Projection Works","summary":"  Embedding discrete solvers as differentiable layers has given modern deep\nlearning architectures combinatorial expressivity and discrete reasoning\ncapabilities. The derivative of these solvers is zero or undefined, therefore a\nmeaningful replacement is crucial for effective gradient-based learning. Prior\nworks rely on smoothing the solver with input perturbations, relaxing the\nsolver to continuous problems, or interpolating the loss landscape with\ntechniques that typically require additional solver calls, introduce extra\nhyper-parameters, or compromise performance. We propose a principled approach\nto exploit the geometry of the discrete solution space to treat the solver as a\nnegative identity on the backward pass and further provide a theoretical\njustification. Our experiments demonstrate that such a straightforward\nhyper-parameter-free approach is able to compete with previous more complex\nmethods on numerous experiments such as backpropagation through discrete\nsamplers, deep graph matching, and image retrieval. Furthermore, we substitute\nthe previously proposed problem-specific and label-dependent margin with a\ngeneric regularization procedure that prevents cost collapse and increases\nrobustness.\n","authors":["Subham Sekhar Sahoo","Anselm Paulus","Marin Vlastelica","Vít Musil","Volodymyr Kuleshov","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2205.15213v3.pdf","comment":"ICLR 2023 conference paper. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.10035v1","updated":"2023-03-17T15:04:57Z","published":"2023-03-17T15:04:57Z","title":"A Policy Iteration Approach for Flock Motion Control","summary":"  The flocking motion control is concerned with managing the possible conflicts\nbetween local and team objectives of multi-agent systems. The overall control\nprocess guides the agents while monitoring the flock-cohesiveness and\nlocalization. The underlying mechanisms may degrade due to overlooking the\nunmodeled uncertainties associated with the flock dynamics and formation. On\nanother side, the efficiencies of the various control designs rely on how\nquickly they can adapt to different dynamic situations in real-time. An online\nmodel-free policy iteration mechanism is developed here to guide a flock of\nagents to follow an independent command generator over a time-varying graph\ntopology. The strength of connectivity between any two agents or the graph edge\nweight is decided using a position adjacency dependent function. An online\nrecursive least squares approach is adopted to tune the guidance strategies\nwithout knowing the dynamics of the agents or those of the command generator.\nIt is compared with another reinforcement learning approach from the literature\nwhich is based on a value iteration technique. The simulation results of the\npolicy iteration mechanism revealed fast learning and convergence behaviors\nwith less computational effort.\n","authors":["Shuzheng Qu","Mohammed Abouheaf","Wail Gueaieb","Davide Spinello"],"pdf_url":"https://arxiv.org/pdf/2303.10035v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.10030v1","updated":"2023-03-17T14:59:00Z","published":"2023-03-17T14:59:00Z","title":"How robust is randomized blind deconvolution via nuclear norm\n  minimization against adversarial noise?","summary":"  In this paper, we study the problem of recovering two unknown signals from\ntheir convolution, which is commonly referred to as blind deconvolution.\nReformulation of blind deconvolution as a low-rank recovery problem has led to\nmultiple theoretical recovery guarantees in the past decade due to the success\nof the nuclear norm minimization heuristic. In particular, in the absence of\nnoise, exact recovery has been established for sufficiently incoherent signals\ncontained in lower-dimensional subspaces. However, if the convolution is\ncorrupted by additive bounded noise, the stability of the recovery problem\nremains much less understood. In particular, existing reconstruction bounds\ninvolve large dimension factors and therefore fail to explain the empirical\nevidence for dimension-independent robustness of nuclear norm minimization.\nRecently, theoretical evidence has emerged for ill-posed behavior of low-rank\nmatrix recovery for sufficiently small noise levels. In this work, we develop\nimproved recovery guarantees for blind deconvolution with adversarial noise\nwhich exhibit square-root scaling in the noise level. Hence, our results are\nconsistent with existing counterexamples which speak against linear scaling in\nthe noise level as demonstrated for related low-rank matrix recovery problems.\n","authors":["Julia Kostin","Felix Krahmer","Dominik Stöger"],"pdf_url":"https://arxiv.org/pdf/2303.10030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10022v1","updated":"2023-03-17T14:50:51Z","published":"2023-03-17T14:50:51Z","title":"Hierarchical-Hyperplane Kernels for Actively Learning Gaussian Process\n  Models of Nonstationary Systems","summary":"  Learning precise surrogate models of complex computer simulations and\nphysical machines often require long-lasting or expensive experiments.\nFurthermore, the modeled physical dependencies exhibit nonlinear and\nnonstationary behavior. Machine learning methods that are used to produce the\nsurrogate model should therefore address these problems by providing a scheme\nto keep the number of queries small, e.g. by using active learning and be able\nto capture the nonlinear and nonstationary properties of the system. One way of\nmodeling the nonstationarity is to induce input-partitioning, a principle that\nhas proven to be advantageous in active learning for Gaussian processes.\nHowever, these methods either assume a known partitioning, need to introduce\ncomplex sampling schemes or rely on very simple geometries. In this work, we\npresent a simple, yet powerful kernel family that incorporates a partitioning\nthat: i) is learnable via gradient-based methods, ii) uses a geometry that is\nmore flexible than previous ones, while still being applicable in the low data\nregime. Thus, it provides a good prior for active learning procedures. We\nempirically demonstrate excellent performance on various active learning tasks.\n","authors":["Matthias Bitzer","Mona Meister","Christoph Zimmer"],"pdf_url":"https://arxiv.org/pdf/2303.10022v1.pdf","comment":"Accepted at AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.10019v1","updated":"2023-03-17T14:47:55Z","published":"2023-03-17T14:47:55Z","title":"Multivariate Probabilistic CRPS Learning with an Application to\n  Day-Ahead Electricity Prices","summary":"  This paper presents a new method for combining (or aggregating or ensembling)\nmultivariate probabilistic forecasts, taking into account dependencies between\nquantiles and covariates through a smoothing procedure that allows for online\nlearning. Two smoothing methods are discussed: dimensionality reduction using\nBasis matrices and penalized smoothing. The new online learning algorithm\ngeneralizes the standard CRPS learning framework into multivariate dimensions.\nIt is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic\nlearning properties. We provide an in-depth discussion on possible extensions\nof the algorithm and several nested cases related to the existing literature on\nonline forecast combination. The methodology is applied to forecasting\nday-ahead electricity prices, which are 24-dimensional distributional\nforecasts. The proposed method yields significant improvements over uniform\ncombination in terms of continuous ranked probability score (CRPS). We discuss\nthe temporal evolution of the weights and hyperparameters and present the\nresults of reduced versions of the preferred model. A fast C++ implementation\nof all discussed methods is provided in the R-Package profoc.\n","authors":["Jonathan Berrisch","Florian Ziel"],"pdf_url":"https://arxiv.org/pdf/2303.10019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09015v2","updated":"2023-03-17T14:25:43Z","published":"2022-08-18T18:31:40Z","title":"Treeformer: Dense Gradient Trees for Efficient Attention Computation","summary":"  Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.\n","authors":["Lovish Madaan","Srinadh Bhojanapalli","Himanshu Jain","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2208.09015v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09994v1","updated":"2023-03-17T14:10:52Z","published":"2023-03-17T14:10:52Z","title":"A Data-Driven Model-Reference Adaptive Control Approach Based on\n  Reinforcement Learning","summary":"  Model-reference adaptive systems refer to a consortium of techniques that\nguide plants to track desired reference trajectories. Approaches based on\ntheories like Lyapunov, sliding surfaces, and backstepping are typically\nemployed to advise adaptive control strategies. The resulting solutions are\noften challenged by the complexity of the reference model and those of the\nderived control strategies. Additionally, the explicit dependence of the\ncontrol strategies on the process dynamics and reference dynamical models may\ncontribute in degrading their efficiency in the face of uncertain or unknown\ndynamics. A model-reference adaptive solution is developed here for autonomous\nsystems where it solves the Hamilton-Jacobi-Bellman equation of an error-based\nstructure. The proposed approach describes the process with an integral\ntemporal difference equation and solves it using an integral reinforcement\nlearning mechanism. This is done in real-time without knowing or employing the\ndynamics of either the process or reference model in the control strategies. A\nclass of aircraft is adopted to validate the proposed technique.\n","authors":["Mohammed Abouheaf","Wail Gueaieb","Davide Spinello","Salah Al-Sharhan"],"pdf_url":"https://arxiv.org/pdf/2303.09994v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.09989v1","updated":"2023-03-17T14:04:51Z","published":"2023-03-17T14:04:51Z","title":"Finding Competence Regions in Domain Generalization","summary":"  We propose a \"learning to reject\" framework to address the problem of silent\nfailures in Domain Generalization (DG), where the test distribution differs\nfrom the training distribution. Assuming a mild distribution shift, we wish to\naccept out-of-distribution (OOD) data whenever a model's estimated competence\nforesees trustworthy responses, instead of rejecting OOD data outright.\nTrustworthiness is then predicted via a proxy incompetence score that is\ntightly linked to the performance of a classifier. We present a comprehensive\nexperimental evaluation of incompetence scores for classification and highlight\nthe resulting trade-offs between rejection rate and accuracy gain. For\ncomparability with prior work, we focus on standard DG benchmarks and consider\nthe effect of measuring incompetence via different learned representations in a\nclosed versus an open world setting. Our results suggest that increasing\nincompetence scores are indeed predictive of reduced accuracy, leading to\nsignificant improvements of the average accuracy below a suitable incompetence\nthreshold. However, the scores are not yet good enough to allow for a favorable\naccuracy/rejection trade-off in all tested domains. Surprisingly, our results\nalso indicate that classifiers optimized for DG robustness do not outperform a\nnaive Empirical Risk Minimization (ERM) baseline in the competence region, that\nis, where test samples elicit low incompetence scores.\n","authors":["Jens Müller","Stefan T. Radev","Robert Schmier","Felix Draxler","Carsten Rother","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2303.09989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09986v1","updated":"2023-03-17T14:02:35Z","published":"2023-03-17T14:02:35Z","title":"Towards AI-controlled FES-restoration of movements: Learning cycling\n  stimulation pattern with reinforcement learning","summary":"  Functional electrical stimulation (FES) has been increasingly integrated with\nother rehabilitation devices, including robots. FES cycling is one of the\ncommon FES applications in rehabilitation, which is performed by stimulating\nleg muscles in a certain pattern. The appropriate pattern varies across\nindividuals and requires manual tuning which can be time-consuming and\nchallenging for the individual user. Here, we present an AI-based method for\nfinding the patterns, which requires no extra hardware or sensors. Our method\nhas two phases, starting with finding model-based patterns using reinforcement\nlearning and detailed musculoskeletal models. The models, built using\nopen-source software, can be customised through our automated script and can be\ntherefore used by non-technical individuals without extra cost. Next, our\nmethod fine-tunes the pattern using real cycling data. We test our both in\nsimulation and experimentally on a stationary tricycle. In the simulation test,\nour method can robustly deliver model-based patterns for different cycling\nconfigurations. The experimental evaluation shows that our method can find a\nmodel-based pattern that induces higher cycling speed than an EMG-based\npattern. By using just 100 seconds of cycling data, our method can deliver a\nfine-tuned pattern that gives better cycling performance. Beyond FES cycling,\nthis work is a showcase, displaying the feasibility and potential of\nhuman-in-the-loop AI in real-world rehabilitation.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2303.09986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07130v3","updated":"2023-03-17T13:58:52Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) based scoring method is used to identify the extent of\nlung involvement observed on a CT scan. This paper presents a domain\nknowledge-based pipeline for extracting regions of infection in COVID-19\npatients using a combination of image-processing algorithms and a pre-trained\nUNET model. The severity of the infection is then classified into different\ncategories using an ensemble of three machine-learning models: Extreme Gradient\nBoosting, Extremely Randomized Trees, and Support Vector Machine. The proposed\nsystem was evaluated on a validation dataset in the AI-Enabled Medical Image\nAnalysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and\nachieved a macro F1 score of 64%. These results demonstrate the potential of\ncombining domain knowledge with machine learning techniques for accurate\nCOVID-19 diagnosis using CT scans. The implementation of the proposed system\nfor severity analysis is available at\n\\textit{https://github.com/aanandt/Enhancing-COVID-19-Severity-Analysis-through-Ensemble-Methods.git\n}\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09981v1","updated":"2023-03-17T13:58:06Z","published":"2023-03-17T13:58:06Z","title":"Inferring Traffic Models in Terminal Airspace from Flight Tracks and\n  Procedures","summary":"  Realistic aircraft trajectory models are useful in the design and validation\nof air traffic management (ATM) systems. Models of aircraft operated under\ninstrument flight rules (IFR) require capturing the variability inherent in how\naircraft follow standard flight procedures. The variability in aircraft\nbehavior varies among flight stages. In this paper, we propose a probabilistic\nmodel that can learn the variability from the procedural data and flight tracks\ncollected from radar surveillance data. For each segment, a Gaussian mixture\nmodel is used to learn the deviations of aircraft trajectories from their\nprocedures. Given new procedures, we can generate synthetic trajectories by\nsampling a series of deviations from the trained Gaussian distributions and\nreconstructing the aircraft trajectory using the deviations and the procedures.\nWe extend this method to capture pairwise correlations between aircraft and\nshow how a pairwise model can be used to generate traffic involving an\narbitrary number of aircraft. We demonstrate the proposed models on the arrival\ntracks and procedures of the John F. Kennedy International Airport. The\ndistributional similarity between the original and the synthetic trajectory\ndataset was evaluated using the Jensen-Shannon divergence between the empirical\ndistributions of different variables. We also provide qualitative analyses of\nthe synthetic trajectories generated from the models.\n","authors":["Soyeon Jung","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2303.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04004v2","updated":"2023-03-17T13:55:29Z","published":"2023-01-10T14:50:37Z","title":"Towards AI-controlled FES-restoration of arm movements:\n  neuromechanics-based reinforcement learning for 3-D reaching","summary":"  Reaching disabilities affect the quality of life. Functional Electrical\nStimulation (FES) can restore lost motor functions. Yet, there remain\nchallenges in controlling FES to induce desired movements. Neuromechanical\nmodels are valuable tools for developing FES control methods. However, focusing\non the upper extremity areas, several existing models are either overly\nsimplified or too computationally demanding for control purposes. Besides the\nmodel-related issues, finding a general method for governing the control rules\nfor different tasks and subjects remains an engineering challenge. Here, we\npresent our approach toward FES-based restoration of arm movements to address\nthose fundamental issues in controlling FES. Firstly, we present our\nsurface-FES-oriented neuromechanical models of human arms built using\nwell-accepted, open-source software. The models are designed to capture\nsignificant dynamics in FES controls with minimal computational cost. Our\nmodels are customisable and can be used for testing different control methods.\nSecondly, we present the application of reinforcement learning (RL) as a\ngeneral method for governing the control rules. In combination, our\ncustomisable models and RL-based control method open the possibility of\ndelivering customised FES controls for different subjects and settings with\nminimal engineering intervention. We demonstrate our approach in planar and 3D\nsettings.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2301.04004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.07677v4","updated":"2023-03-17T13:52:27Z","published":"2022-01-19T15:59:41Z","title":"Tiny, always-on and fragile: Bias propagation through design choices in\n  on-device machine learning workflows","summary":"  Billions of distributed, heterogeneous and resource constrained IoT devices\ndeploy on-device machine learning (ML) for private, fast and offline inference\non personal data. On-device ML is highly context dependent, and sensitive to\nuser, usage, hardware and environment attributes. This sensitivity and the\npropensity towards bias in ML makes it important to study bias in on-device\nsettings. Our study is one of the first investigations of bias in this emerging\ndomain, and lays important foundations for building fairer on-device ML. We\napply a software engineering lens, investigating the propagation of bias\nthrough design choices in on-device ML workflows. We first identify reliability\nbias as a source of unfairness and propose a measure to quantify it. We then\nconduct empirical experiments for a keyword spotting task to show how complex\nand interacting technical design choices amplify and propagate reliability\nbias. Our results validate that design choices made during model training, like\nthe sample rate and input feature type, and choices made to optimize models,\nlike light-weight architectures, the pruning learning rate and pruning\nsparsity, can result in disparate predictive performance across male and female\ngroups. Based on our findings we suggest low effort strategies for engineers to\nmitigate bias in on-device ML.\n","authors":["Wiebke Toussaint","Aaron Yi Ding","Fahim Kawsar","Akhil Mathur"],"pdf_url":"https://arxiv.org/pdf/2201.07677v4.pdf","comment":"To be published in ACM Transactions on Software Engineering and\n  Methodology"},{"id":"http://arxiv.org/abs/2301.04005v2","updated":"2023-03-17T13:52:21Z","published":"2023-01-10T14:51:55Z","title":"Towards AI-controlled FES-restoration of arm movements: Controlling for\n  progressive muscular fatigue with Gaussian state-space models","summary":"  Reaching disability limits an individual's ability in performing daily tasks.\nSurface Functional Electrical Stimulation (FES) offers a non-invasive solution\nto restore the lost abilities. However, inducing desired movements using FES is\nstill an open engineering problem. This problem is accentuated by the\ncomplexities of human arms' neuromechanics and the variations across\nindividuals. Reinforcement Learning (RL) emerges as a promising approach to\ngovern customised control rules for different subjects and settings. Yet, one\nremaining challenge of using RL to control FES is unobservable muscle fatigue\nthat progressively changes as an unknown function of the stimulation, breaking\nthe Markovian assumption of RL. In this work, we present a method to address\nthe unobservable muscle fatigue issue, allowing our RL controller to achieve\nhigher control performances. Our method is based on a Gaussian State-Space\nModel (GSSM) that utilizes recurrent neural networks to learn Markovian\nstate-spaces from partial observations. The GSSM is used as a filter that\nconverts the observations into the state-space representation for RL to\npreserve the Markovian assumption. Here, we start with presenting the\nmodification of the original GSSM to address an overconfident issue. We then\npresent the interaction between RL and the modified GSSM, followed by the setup\nfor FES control learning. We test our RL-GSSM system on a planar reaching\nsetting in simulation using a detailed neuromechanical model and show that the\nGSSM can help RL maintain its control performance against the fatigue.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2301.04005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v1","updated":"2023-03-17T13:48:17Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.09972v1","updated":"2023-03-17T13:44:52Z","published":"2023-03-17T13:44:52Z","title":"Neighborhood Averaging for Improving Outlier Detectors","summary":"  We hypothesize that similar objects should have similar outlier scores. To\nour knowledge, all existing outlier detectors calculate the outlier score for\neach object independently regardless of the outlier scores of the other\nobjects. Therefore, they do not guarantee that similar objects have similar\noutlier scores. To verify our proposed hypothesis, we propose an outlier score\npost-processing technique for outlier detectors, called neighborhood\naveraging(NA), which pays attention to objects and their neighbors and\nguarantees them to have more similar outlier scores than their original scores.\nGiven an object and its outlier score from any outlier detector, NA modifies\nits outlier score by combining it with its k nearest neighbors' scores. We\ndemonstrate the effectivity of NA by using the well-known k-nearest neighbors\n(k-NN). Experimental results show that NA improves all 10 tested baseline\ndetectors by 13% (from 0.70 to 0.79 AUC) on average evaluated on nine\nreal-world datasets. Moreover, even outlier detectors that are already based on\nk-NN are also improved. The experiments also show that in some applications,\nthe choice of detector is no more significant when detectors are jointly used\nwith NA, which may pose a challenge to the generally considered idea that the\ndata model is the most important factor. We open our code on www.outlierNet.com\nfor reproducibility.\n","authors":["Jiawei Yang","Susanto Rahardja","Pasi Franti"],"pdf_url":"https://arxiv.org/pdf/2303.09972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09960v1","updated":"2023-03-17T13:32:33Z","published":"2023-03-17T13:32:33Z","title":"Stochastic Submodular Maximization via Polynomial Estimators","summary":"  In this paper, we study stochastic submodular maximization problems with\ngeneral matroid constraints, that naturally arise in online learning, team\nformation, facility location, influence maximization, active learning and\nsensing objective functions. In other words, we focus on maximizing submodular\nfunctions that are defined as expectations over a class of submodular functions\nwith an unknown distribution. We show that for monotone functions of this form,\nthe stochastic continuous greedy algorithm attains an approximation ratio (in\nexpectation) arbitrarily close to $(1-1/e) \\approx 63\\%$ using a polynomial\nestimation of the gradient. We argue that using this polynomial estimator\ninstead of the prior art that uses sampling eliminates a source of randomness\nand experimentally reduces execution time.\n","authors":["Gözde Özcan","Stratis Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2303.09960v1.pdf","comment":"23 pages, accepted to 27th Pasific-Asian Conference on Knowledge\n  Discovery and Data Mining"},{"id":"http://arxiv.org/abs/2303.09946v1","updated":"2023-03-17T13:07:35Z","published":"2023-03-17T13:07:35Z","title":"An Adaptive Fuzzy Reinforcement Learning Cooperative Approach for the\n  Autonomous Control of Flock Systems","summary":"  The flock-guidance problem enjoys a challenging structure where multiple\noptimization objectives are solved simultaneously. This usually necessitates\ndifferent control approaches to tackle various objectives, such as guidance,\ncollision avoidance, and cohesion. The guidance schemes, in particular, have\nlong suffered from complex tracking-error dynamics. Furthermore, techniques\nthat are based on linear feedback strategies obtained at equilibrium conditions\neither may not hold or degrade when applied to uncertain dynamic environments.\nPre-tuned fuzzy inference architectures lack robustness under such unmodeled\nconditions. This work introduces an adaptive distributed technique for the\nautonomous control of flock systems. Its relatively flexible structure is based\non online fuzzy reinforcement learning schemes which simultaneously target a\nnumber of objectives; namely, following a leader, avoiding collision, and\nreaching a flock velocity consensus. In addition to its resilience in the face\nof dynamic disturbances, the algorithm does not require more than the agent\nposition as a feedback signal. The effectiveness of the proposed method is\nvalidated with two simulation scenarios and benchmarked against a similar\ntechnique from the literature.\n","authors":["Shuzheng Qu","Mohammed Abouheaf","Wail Gueaieb","Davide Spinello"],"pdf_url":"https://arxiv.org/pdf/2303.09946v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.03770v3","updated":"2023-03-17T12:59:22Z","published":"2023-03-07T10:04:55Z","title":"Guiding Pseudo-labels with Uncertainty Estimation for Source-free\n  Unsupervised Domain Adaptation","summary":"  Standard Unsupervised Domain Adaptation (UDA) methods assume the availability\nof both source and target data during the adaptation. In this work, we\ninvestigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific\ncase of UDA where a model is adapted to a target domain without access to\nsource data. We propose a novel approach for the SF-UDA setting based on a loss\nreweighting strategy that brings robustness against the noise that inevitably\naffects the pseudo-labels. The classification loss is reweighted based on the\nreliability of the pseudo-labels that is measured by estimating their\nuncertainty. Guided by such reweighting strategy, the pseudo-labels are\nprogressively refined by aggregating knowledge from neighbouring samples.\nFurthermore, a self-supervised contrastive framework is leveraged as a target\nspace regulariser to enhance such knowledge aggregation. A novel negative pairs\nexclusion strategy is proposed to identify and exclude negative pairs made of\nsamples sharing the same class, even in presence of some noise in the\npseudo-labels. Our method outperforms previous methods on three major\nbenchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C\nand DomainNet with a performance gain of +1.8% on both benchmarks and on PACS\nwith +12.3% in the single-source setting and +6.6% in multi-target adaptation.\nAdditional analyses demonstrate that the proposed approach is robust to the\nnoise, which results in significantly more accurate pseudo-labels compared to\nstate-of-the-art approaches.\n","authors":["Mattia Litrico","Alessio Del Bue","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2303.03770v3.pdf","comment":"To be published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.06845v2","updated":"2023-03-17T12:47:39Z","published":"2023-03-13T04:21:33Z","title":"Transformer Encoder with Multiscale Deep Learning for Pain\n  Classification Using Physiological Signals","summary":"  Pain is a serious worldwide health problem that affects a vast proportion of\nthe population. For efficient pain management and treatment, accurate\nclassification and evaluation of pain severity are necessary. However, this can\nbe challenging as pain is a subjective sensation-driven experience. Traditional\ntechniques for measuring pain intensity, e.g. self-report scales, are\nsusceptible to bias and unreliable in some instances. Consequently, there is a\nneed for more objective and automatic pain intensity assessment strategies. In\nthis paper, we develop PainAttnNet (PAN), a novel transfomer-encoder\ndeep-learning framework for classifying pain intensities with physiological\nsignals as input. The proposed approach is comprised of three feature\nextraction architectures: multiscale convolutional networks (MSCN), a\nsqueeze-and-excitation residual network (SEResNet), and a transformer encoder\nblock. On the basis of pain stimuli, MSCN extracts short- and long-window\ninformation as well as sequential features. SEResNet highlights relevant\nextracted features by mapping the interdependencies among features. The third\nmodule employs a transformer encoder consisting of three temporal convolutional\nnetworks (TCN) with three multi-head attention (MHA) layers to extract temporal\ndependencies from the features. Using the publicly available BioVid pain\ndataset, we test the proposed PainAttnNet model and demonstrate that our\noutcomes outperform state-of-the-art models. These results confirm that our\napproach can be utilized for automated classification of pain intensity using\nphysiological signals to improve pain management and treatment.\n","authors":["Zhenyuan Lu","Burcu Ozek","Sagar Kamarthi"],"pdf_url":"https://arxiv.org/pdf/2303.06845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02075v2","updated":"2023-03-17T12:30:33Z","published":"2023-03-03T16:48:38Z","title":"Adaptive Interventions for Global Health: A Case Study of Malaria","summary":"  Malaria can be prevented, diagnosed, and treated; however, every year, there\nare more than 200 million cases and 200.000 preventable deaths. Malaria remains\na pressing public health concern in low- and middle-income countries,\nespecially in sub-Saharan Africa. We describe how by means of mobile health\napplications, machine-learning-based adaptive interventions can strengthen\nmalaria surveillance and treatment adherence, increase testing, measure\nprovider skills and quality of care, improve public health by supporting\nfront-line workers and patients (e.g., by capacity building and encouraging\nbehavioral changes, like using bed nets), reduce test stockouts in pharmacies\nand clinics and informing public health for policy intervention.\n","authors":["África Periáñez","Andrew Trister","Madhav Nekkar","Ana Fernández del Río","Pedro L. Alonso"],"pdf_url":"https://arxiv.org/pdf/2303.02075v2.pdf","comment":"Accepted for ICLR 2023 Workshop on Machine Learning and Global Health"},{"id":"http://arxiv.org/abs/2303.09909v1","updated":"2023-03-17T11:59:33Z","published":"2023-03-17T11:59:33Z","title":"An evaluation framework for dimensionality reduction through sectional\n  curvature","summary":"  Unsupervised machine learning lacks ground truth by definition. This poses a\nmajor difficulty when designing metrics to evaluate the performance of such\nalgorithms. In sharp contrast with supervised learning, for which plenty of\nquality metrics have been studied in the literature, in the field of\ndimensionality reduction only a few over-simplistic metrics has been proposed.\nIn this work, we aim to introduce the first highly non-trivial dimensionality\nreduction performance metric. This metric is based on the sectional curvature\nbehaviour arising from Riemannian geometry. To test its feasibility, this\nmetric has been used to evaluate the performance of the most commonly used\ndimension reduction algorithms in the state of the art. Furthermore, to make\nthe evaluation of the algorithms robust and representative, using curvature\nproperties of planar curves, a new parameterized problem instance generator has\nbeen constructed in the form of a function generator. Experimental results are\nconsistent with what could be expected based on the design and characteristics\nof the evaluated algorithms and the features of the data instances used to feed\nthe method.\n","authors":["Raúl Lara-Cabrera","Ángel González-Prieto","Diego Pérez-López","Diego Trujillo","Fernando Ortega"],"pdf_url":"https://arxiv.org/pdf/2303.09909v1.pdf","comment":"16 pages, 4 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2303.09906v1","updated":"2023-03-17T11:49:17Z","published":"2023-03-17T11:49:17Z","title":"Discovering mesoscopic descriptions of collective movement with neural\n  stochastic modelling","summary":"  Collective motion is an ubiquitous phenomenon in nature, inspiring engineers,\nphysicists and mathematicians to develop mathematical models and bio-inspired\ndesigns. Collective motion at small to medium group sizes ($\\sim$10-1000\nindividuals, also called the `mesoscale'), can show nontrivial features due to\nstochasticity. Therefore, characterizing both the deterministic and stochastic\naspects of the dynamics is crucial in the study of mesoscale collective\nphenomena. Here, we use a physics-inspired, neural-network based approach to\ncharacterize the stochastic group dynamics of interacting individuals, through\na stochastic differential equation (SDE) that governs the collective dynamics\nof the group. We apply this technique on both synthetic and real-world\ndatasets, and identify the deterministic and stochastic aspects of the dynamics\nusing drift and diffusion fields, enabling us to make novel inferences about\nthe nature of order in these systems.\n","authors":["Utkarsh Pratiush","Arshed Nabeel","Vishwesha Guttal","Prathosh AP"],"pdf_url":"https://arxiv.org/pdf/2303.09906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06431v2","updated":"2023-03-17T11:43:18Z","published":"2022-03-12T13:14:13Z","title":"Deep learning-based conditional inpainting for restoration of\n  artifact-affected 4D CT images","summary":"  4D CT imaging is an essential component of radiotherapy of thoracic/abdominal\ntumors. 4D CT images are, however, often affected by artifacts that compromise\ntreatment planning quality. In this work, deep learning (DL)-based conditional\ninpainting is proposed to restore anatomically correct image information of\nartifact-affected areas. The restoration approach consists of a two-stage\nprocess: DL-based detection of common interpolation (INT) and double structure\n(DS) artifacts, followed by conditional inpainting applied to the artifact\nareas. In this context, conditional refers to a guidance of the inpainting\nprocess by patient-specific image data to ensure anatomically reliable results.\nThe study is based on 65 in-house 4D CT images of lung cancer patients (48 with\nonly slight artifacts, 17 with pronounced artifacts) and two publicly available\n4D CT data sets that serve as independent external test sets. Automated\nartifact detection revealed a ROC-AUC of 0.99 for INT and of 0.97 for DS\nartifacts (in-house data). The proposed inpainting method decreased the average\nroot mean squared error (RMSE) by 52%(INT) and 59% (DS) for the in-house data.\nFor the external test data sets, the RMSE improvement is similar (50% and 59 %,\nrespectively). Applied to 4D CT data with pronounced artifacts (not part of the\ntraining set), 72% of the detectable artifacts were removed. The results\nhighlight the potential of DL-based inpainting for restoration of\nartifact-affected 4D CT data. Compared to recent 4D CT inpainting and\nrestoration approaches, the proposed methodology illustrates the advantages of\nexploiting patient-specific prior image information.\n","authors":["Frederic Madesta","Thilo Sentker","Tobias Gauer","Rene Werner"],"pdf_url":"https://arxiv.org/pdf/2203.06431v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.09901v1","updated":"2023-03-17T11:33:06Z","published":"2023-03-17T11:33:06Z","title":"mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive\n  Pre-Training of Transformers for Few- and Zero-shot Framing Detection","summary":"  This paper presents the winning system for the zero-shot Spanish framing\ndetection task, which also achieves competitive places in eight additional\nlanguages. The challenge of the framing detection task lies in identifying a\nset of 14 frames when only a few or zero samples are available, i.e., a\nmultilingual multi-label few- or zero-shot setting. Our developed solution\nemploys a pre-training procedure based on multilingual Transformers using a\nlabel-aware contrastive loss function. In addition to describing the system, we\nperform an embedding space analysis and ablation study to demonstrate how our\npre-training procedure supports framing detection to advance computational\nframing analysis.\n","authors":["Markus Reiter-Haas","Alexander Ertl","Kevin Innerhofer","Elisabeth Lex"],"pdf_url":"https://arxiv.org/pdf/2303.09901v1.pdf","comment":"Manuscript submitted for publication at SemEval'23"},{"id":"http://arxiv.org/abs/2201.11316v2","updated":"2023-03-17T11:10:31Z","published":"2022-01-27T04:22:25Z","title":"Transformer Module Networks for Systematic Generalization in Visual\n  Question Answering","summary":"  Transformers achieve great performance on Visual Question Answering (VQA).\nHowever, their systematic generalization capabilities, i.e., handling novel\ncombinations of known concepts, is unclear. We reveal that Neural Module\nNetworks (NMNs), i.e., question-specific compositions of modules that tackle a\nsub-task, achieve better or similar systematic generalization performance than\nthe conventional Transformers, even though NMNs' modules are CNN-based. In\norder to address this shortcoming of Transformers with respect to NMNs, in this\npaper we investigate whether and how modularity can bring benefits to\nTransformers. Namely, we introduce Transformer Module Network (TMN), a novel\nNMN based on compositions of Transformer modules. TMNs achieve state-of-the-art\nsystematic generalization performance in three VQA datasets, improving more\nthan 30% over standard Transformers for novel compositions of sub-tasks. We\nshow that not only the module composition but also the module specialization\nfor each sub-task are the key of such performance gain.\n","authors":["Moyuru Yamada","Vanessa D'Amario","Kentaro Takemoto","Xavier Boix","Tomotake Sasaki"],"pdf_url":"https://arxiv.org/pdf/2201.11316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.04248v2","updated":"2023-03-17T11:07:16Z","published":"2021-10-08T16:58:20Z","title":"Observations on K-image Expansion of Image-Mixing Augmentation for\n  Classification","summary":"  Image-mixing augmentations (e.g., Mixup and CutMix), which typically involve\nmixing two images, have become the de-facto training techniques for image\nclassification. Despite their huge success in image classification, the number\nof images to be mixed has not been elucidated in the literature: only the naive\nK-image expansion has been shown to lead to performance degradation. This study\nderives a new K-image mixing augmentation based on the stick-breaking process\nunder Dirichlet prior distribution. We demonstrate the superiority of our\nK-image expansion augmentation over conventional two-image mixing augmentation\nmethods through extensive experiments and analyses: (1) more robust and\ngeneralized classifiers; (2) a more desirable loss landscape shape; (3) better\nadversarial robustness. Moreover, we show that our probabilistic model can\nmeasure the sample-wise uncertainty and boost the efficiency for network\narchitecture search by achieving a 7-fold reduction in the search time. Code\nwill be available at https://github.com/yjyoo3312/DCutMix-PyTorch.git.\n","authors":["Joonhyun Jeong","Sungmin Cha","Youngjoon Yoo","Sangdoo Yun","Taesup Moon","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2110.04248v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2301.01113v2","updated":"2023-03-17T10:56:58Z","published":"2023-01-03T14:16:32Z","title":"Invalidator: Automated Patch Correctness Assessment via Semantic and\n  Syntactic Reasoning","summary":"  Automated program repair (APR) faces the challenge of test overfitting, where\ngenerated patches pass validation tests but fail to generalize. Existing\nmethods for patch assessment involve generating new tests or manual inspection,\nwhich can be time-consuming or biased. In this paper, we propose a novel\ntechnique, INVALIDATOR, to automatically assess the correctness of\nAPR-generated patches via semantic and syntactic reasoning. INVALIDATOR\nleverages program invariants to reason about program semantics while also\ncapturing program syntax through language semantics learned from a large code\ncorpus using a pre-trained language model. Given a buggy program and the\ndeveloper-patched program, INVALIDATOR infers likely invariants on both\nprograms. Then, INVALIDATOR determines that an APR-generated patch overfits if:\n(1) it violates correct specifications or (2) maintains erroneous behaviors\nfrom the original buggy program. In case our approach fails to determine an\noverfitting patch based on invariants, INVALIDATOR utilizes a trained model\nfrom labeled patches to assess patch correctness based on program syntax. The\nbenefit of INVALIDATOR is threefold. First, INVALIDATOR leverages both semantic\nand syntactic reasoning to enhance its discriminative capability. Second,\nINVALIDATOR does not require new test cases to be generated, but instead only\nrelies on the current test suite and uses invariant inference to generalize\nprogram behaviors. Third, INVALIDATOR is fully automated. Experimental results\ndemonstrate that INVALIDATOR outperforms existing methods in terms of Accuracy\nand F-measure, correctly identifying 79% of overfitting patches and detecting\n23% more overfitting patches than the best baseline.\n","authors":["Thanh Le-Cong","Duc-Minh Luong","Xuan Bach D. Le","David Lo","Nhat-Hoa Tran","Bui Quang-Huy","Quyet-Thang Huynh"],"pdf_url":"https://arxiv.org/pdf/2301.01113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11578v2","updated":"2023-03-17T10:54:32Z","published":"2023-01-27T07:53:50Z","title":"Learning to Unlearn: Instance-wise Unlearning for Pre-trained\n  Classifiers","summary":"  Since the recent advent of regulations for data protection (e.g., the General\nData Protection Regulation), there has been increasing demand in deleting\ninformation learned from sensitive data in pre-trained models without\nretraining from scratch. The inherent vulnerability of neural networks towards\nadversarial attacks and unfairness also calls for a robust method to remove or\ncorrect information in an instance-wise fashion, while retaining the predictive\nperformance across remaining data. To this end, we define instance-wise\nunlearning, of which the goal is to delete information on a set of instances\nfrom a pre-trained model, by either misclassifying each instance away from its\noriginal prediction or relabeling the instance to a different label. We also\npropose two methods that reduce forgetting on the remaining data: 1) utilizing\nadversarial examples to overcome forgetting at the representation-level and 2)\nleveraging weight importance metrics to pinpoint network parameters guilty of\npropagating unwanted information. Both methods only require the pre-trained\nmodel and data instances to forget, allowing painless application to real-life\nsettings where the entire training set is unavailable. Through extensive\nexperimentation on various image classification benchmarks, we show that our\napproach effectively preserves knowledge of remaining data while unlearning\ngiven instances in both single-task and continual unlearning scenarios.\n","authors":["Sungmin Cha","Sungjun Cho","Dasol Hwang","Honglak Lee","Taesup Moon","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2301.11578v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.09877v1","updated":"2023-03-17T10:51:38Z","published":"2023-03-17T10:51:38Z","title":"On the Effects of Self-supervision and Contrastive Alignment in Deep\n  Multi-view Clustering","summary":"  Self-supervised learning is a central component in recent approaches to deep\nmulti-view clustering (MVC). However, we find large variations in the\ndevelopment of self-supervision-based methods for deep MVC, potentially slowing\nthe progress of the field. To address this, we present DeepMVC, a unified\nframework for deep MVC that includes many recent methods as instances. We\nleverage our framework to make key observations about the effect of\nself-supervision, and in particular, drawbacks of aligning representations with\ncontrastive learning. Further, we prove that contrastive alignment can\nnegatively influence cluster separability, and that this effect becomes worse\nwhen the number of views increases. Motivated by our findings, we develop\nseveral new DeepMVC instances with new forms of self-supervision. We conduct\nextensive experiments and find that (i) in line with our theoretical findings,\ncontrastive alignments decreases performance on datasets with many views; (ii)\nall methods benefit from some form of self-supervision; and (iii) our new\ninstances outperform previous methods on several datasets. Based on our\nresults, we suggest several promising directions for future research. To\nenhance the openness of the field, we provide an open-source implementation of\nDeepMVC, including recent models and our new instances. Our implementation\nincludes a consistent evaluation protocol, facilitating fair and accurate\nevaluation of methods and components.\n","authors":["Daniel J. Trosten","Sigurd Løkse","Robert Jenssen","Michael C. Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2303.09877v1.pdf","comment":"CVPR 2023. Code available at https://github.com/DanielTrosten/DeepMVC"},{"id":"http://arxiv.org/abs/2303.09874v1","updated":"2023-03-17T10:38:27Z","published":"2023-03-17T10:38:27Z","title":"Disentangling the Link Between Image Statistics and Human Perception","summary":"  In the 1950s Horace Barlow and Fred Attneave suggested a connection between\nsensory systems and how they are adapted to the environment: early vision\nevolved to maximise the information it conveys about incoming signals.\nFollowing Shannon's definition, this information was described using the\nprobability of the images taken from natural scenes. Previously, direct\naccurate predictions of image probabilities were not possible due to\ncomputational limitations. Despite the exploration of this idea being indirect,\nmainly based on oversimplified models of the image density or on system design\nmethods, these methods had success in reproducing a wide range of physiological\nand psychophysical phenomena. In this paper, we directly evaluate the\nprobability of natural images and analyse how it may determine perceptual\nsensitivity. We employ image quality metrics that correlate well with human\nopinion as a surrogate of human vision, and an advanced generative model to\ndirectly estimate the probability. Specifically, we analyse how the sensitivity\nof full-reference image quality metrics can be predicted from quantities\nderived directly from the probability distribution of natural images. First, we\ncompute the mutual information between a wide range of probability surrogates\nand the sensitivity of the metrics and find that the most influential factor is\nthe probability of the noisy image. Then we explore how these probability\nsurrogates can be combined using a simple model to predict the metric\nsensitivity, giving an upper bound for the correlation of 0.85 between the\nmodel predictions and the actual perceptual sensitivity. Finally, we explore\nhow to combine the probability surrogates using simple expressions, and obtain\ntwo functional forms (using one or two surrogates) that can be used to predict\nthe sensitivity of the human visual system given a particular pair of images.\n","authors":["Alexander Hepburn","Valero Laparra","Raul Santos-Rodriguez","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2303.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12824v4","updated":"2023-03-17T10:14:57Z","published":"2021-07-27T13:44:13Z","title":"A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge\n  Domain Adaptation on FPGAs","summary":"  High-performance deep neural network (DNN)-based systems are in high demand\nin edge environments. Due to its high computational complexity, it is\nchallenging to deploy DNNs on edge devices with strict limitations on\ncomputational resources. In this paper, we derive a compact while\nhighly-accurate DNN model, termed dsODENet, by combining recently-proposed\nparameter reduction techniques: Neural ODE (Ordinary Differential Equation) and\nDSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between\nResNet and ODE, and shares most of weight parameters among multiple layers,\nwhich greatly reduces the memory consumption. We apply dsODENet to a domain\nadaptation as a practical use case with image classification datasets. We also\npropose a resource-efficient FPGA-based design for dsODENet, where all the\nparameters and feature maps except for pre- and post-processing layers can be\nmapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and\nevaluated in terms of domain adaptation accuracy, inference speed, FPGA\nresource utilization, and speedup rate compared to a software counterpart. The\nresults demonstrate that dsODENet achieves comparable or slightly better domain\nadaptation accuracy compared to our baseline Neural ODE implementation, while\nthe total parameter size without pre- and post-processing layers is reduced by\n54.2% to 79.8%. Our FPGA implementation accelerates the inference speed by 23.8\ntimes.\n","authors":["Hiroki Kawakami","Hirohisa Watanabe","Keisuke Sugiura","Hiroki Matsutani"],"pdf_url":"https://arxiv.org/pdf/2107.12824v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16961v3","updated":"2023-03-17T10:10:08Z","published":"2022-11-30T13:11:46Z","title":"Pattern Attention Transformer with Doughnut Kernel","summary":"  We present in this paper a new architecture, the Pattern Attention\nTransformer (PAT), that is composed of the new doughnut kernel. Compared with\ntokens in the NLP field, Transformer in computer vision has the problem of\nhandling the high resolution of pixels in images. In ViT, an image is cut into\nsquare-shaped patches. As the follow-up of ViT, Swin Transformer proposes an\nadditional step of shifting to decrease the existence of fixed boundaries,\nwhich also incurs 'two connected Swin Transformer blocks' as the minimum unit\nof the model. Inheriting the patch/window idea, our doughnut kernel enhances\nthe design of patches further. It replaces the line-cut boundaries with two\ntypes of areas: sensor and updating, which is based on the comprehension of\nself-attention (named QKVA grid). The doughnut kernel also brings a new topic\nabout the shape of kernels beyond square. To verify its performance on image\nclassification, PAT is designed with Transformer blocks of regular octagon\nshape doughnut kernels. Its architecture is lighter: the minimum pattern\nattention layer is only one for each stage. Under similar complexity of\ncomputation, its performances on ImageNet 1K reach higher throughput (+10%) and\nsurpass Swin Transformer (+0.1 acc1).\n","authors":["WenYuan Sheng"],"pdf_url":"https://arxiv.org/pdf/2211.16961v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09863v1","updated":"2023-03-17T10:01:32Z","published":"2023-03-17T10:01:32Z","title":"Deep Nonparametric Estimation of Intrinsic Data Structures by Chart\n  Autoencoders: Generalization Error and Robustness","summary":"  Autoencoders have demonstrated remarkable success in learning low-dimensional\nlatent features of high-dimensional data across various applications. Assuming\nthat data are sampled near a low-dimensional manifold, we employ chart\nautoencoders, which encode data into low-dimensional latent features on a\ncollection of charts, preserving the topology and geometry of the data\nmanifold. Our paper establishes statistical guarantees on the generalization\nerror of chart autoencoders, and we demonstrate their denoising capabilities by\nconsidering $n$ noisy training samples, along with their noise-free\ncounterparts, on a $d$-dimensional manifold. By training autoencoders, we show\nthat chart autoencoders can effectively denoise the input data with normal\nnoise. We prove that, under proper network architectures, chart autoencoders\nachieve a squared generalization error in the order of $\\displaystyle\nn^{-\\frac{2}{d+2}}\\log^4 n$, which depends on the intrinsic dimension of the\nmanifold and only weakly depends on the ambient dimension and noise level. We\nfurther extend our theory on data with noise containing both normal and\ntangential components, where chart autoencoders still exhibit a denoising\neffect for the normal component. As a special case, our theory also applies to\nclassical autoencoders, as long as the data manifold has a global\nparametrization. Our results provide a solid theoretical foundation for the\neffectiveness of autoencoders, which is further validated through several\nnumerical experiments.\n","authors":["Hao Liu","Alex Havrilla","Rongjie Lai","Wenjing Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01817v2","updated":"2023-03-17T09:58:40Z","published":"2022-11-03T13:55:36Z","title":"Liability regimes in the age of AI: a use-case driven analysis of the\n  burden of proof","summary":"  New emerging technologies powered by Artificial Intelligence (AI) have the\npotential to disruptively transform our societies for the better. In\nparticular, data-driven learning approaches (i.e., Machine Learning (ML)) have\nbeen a true revolution in the advancement of multiple technologies in various\napplication domains. But at the same time there is growing concern about\ncertain intrinsic characteristics of these methodologies that carry potential\nrisks to both safety and fundamental rights. Although there are mechanisms in\nthe adoption process to minimize these risks (e.g., safety regulations), these\ndo not exclude the possibility of harm occurring, and if this happens, victims\nshould be able to seek compensation. Liability regimes will therefore play a\nkey role in ensuring basic protection for victims using or interacting with\nthese systems. However, the same characteristics that make AI systems\ninherently risky, such as lack of causality, opacity, unpredictability or their\nself and continuous learning capabilities, may lead to considerable\ndifficulties when it comes to proving causation. This paper presents three case\nstudies, as well as the methodology to reach them, that illustrate these\ndifficulties. Specifically, we address the cases of cleaning robots, delivery\ndrones and robots in education. The outcome of the proposed analysis suggests\nthe need to revise liability regimes to alleviate the burden of proof on\nvictims in cases involving AI technologies.\n","authors":["David Fernández Llorca","Vicky Charisi","Ronan Hamon","Ignacio Sánchez","Emilia Gómez"],"pdf_url":"https://arxiv.org/pdf/2211.01817v2.pdf","comment":"Paper published at the Journal of Artificial Intelligence Research"},{"id":"http://arxiv.org/abs/2206.08101v2","updated":"2023-03-17T09:39:27Z","published":"2022-06-16T11:44:11Z","title":"Towards More Objective Evaluation of Class Incremental Learning:\n  Representation Learning Perspective","summary":"  Class incremental learning (CIL) is the process of continually learning new\nobject classes from incremental data while not forgetting past learned classes.\nWhile the common method for evaluating CIL algorithms is based on average test\naccuracy for all learned classes, we argue that maximizing accuracy alone does\nnot necessarily lead to effective CIL algorithms. In this paper, we\nexperimentally analyze neural network models trained by CIL algorithms using\nvarious evaluation protocols in representation learning and propose a new\nanalysis method. Our experiments show that most state-of-the-art algorithms\nprioritize high stability and do not significantly change the learned\nrepresentation, and sometimes even learn a representation of lower quality than\na naive baseline. However, we observe that these algorithms can still achieve\nhigh test accuracy because they learn a classifier that is closer to the\noptimal classifier. We also found that the base model learned in the first task\nvaries in representation quality across different algorithms, and changes in\nthe final performance were observed when each algorithm was trained under\nsimilar representation quality of the base model. Thus, we suggest that\nrepresentation-level evaluation is an additional recipe for more objective\nevaluation and effective development of CIL algorithms.\n","authors":["Sungmin Cha","Jihwan Kwak","Dongsub Shim","Hyunwoo Kim","Moontae Lee","Honglak Lee","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2206.08101v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.09857v1","updated":"2023-03-17T09:37:07Z","published":"2023-03-17T09:37:07Z","title":"Dual-path Adaptation from Image to Video Transformers","summary":"  In this paper, we efficiently transfer the surpassing representation power of\nthe vision foundation models, such as ViT and Swin, for video understanding\nwith only a few trainable parameters. Previous adaptation methods have\nsimultaneously considered spatial and temporal modeling with a unified\nlearnable module but still suffered from fully leveraging the representative\ncapabilities of image transformers. We argue that the popular dual-path\n(two-stream) architecture in video models can mitigate this problem. We propose\na novel DualPath adaptation separated into spatial and temporal adaptation\npaths, where a lightweight bottleneck adapter is employed in each transformer\nblock. Especially for temporal dynamic modeling, we incorporate consecutive\nframes into a grid-like frameset to precisely imitate vision transformers'\ncapability that extrapolates relationships between tokens. In addition, we\nextensively investigate the multiple baselines from a unified perspective in\nvideo understanding and compare them with DualPath. Experimental results on\nfour action recognition benchmarks prove that pretrained image transformers\nwith DualPath can be effectively generalized beyond the data domain.\n","authors":["Jungin Park","Jiyoung Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2303.09857v1.pdf","comment":"CVPR 2023. Code is available at\n  https://github.com/park-jungin/DualPath"},{"id":"http://arxiv.org/abs/2109.01636v3","updated":"2023-03-17T09:32:37Z","published":"2021-09-03T17:28:04Z","title":"Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding","summary":"  With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.\n","authors":["Xin Chen","Qi Zhao","Xinyang Liu"],"pdf_url":"https://arxiv.org/pdf/2109.01636v3.pdf","comment":"Want to correct"},{"id":"http://arxiv.org/abs/2303.08455v3","updated":"2023-03-17T09:28:19Z","published":"2023-03-15T08:59:03Z","title":"On the uncertainty analysis of the data-enabled physics-informed neural\n  network for solving neutron diffusion eigenvalue problem","summary":"  In practical engineering experiments, the data obtained through detectors are\ninevitably noisy. For the already proposed data-enabled physics-informed neural\nnetwork (DEPINN) \\citep{DEPINN}, we investigate the performance of DEPINN in\ncalculating the neutron diffusion eigenvalue problem from several perspectives\nwhen the prior data contain different scales of noise. Further, in order to\nreduce the effect of noise and improve the utilization of the noisy prior data,\nwe propose innovative interval loss functions and give some rigorous\nmathematical proofs. The robustness of DEPINN is examined on two typical\nbenchmark problems through a large number of numerical results, and the\neffectiveness of the proposed interval loss function is demonstrated by\ncomparison. This paper confirms the feasibility of the improved DEPINN for\npractical engineering applications in nuclear reactor physics.\n","authors":["Yu Yang","Helin Gong","Qihong Yang","Yangtao Deng","Qiaolin He","Shiquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08455v3.pdf","comment":"The experiments in Figures 6 and 10 in the article have errors that\n  need to be corrected. Moreover, we intend to make massive changes to the\n  content of the article, and therefore need to withdraw the article"},{"id":"http://arxiv.org/abs/2212.04614v3","updated":"2023-03-17T09:26:38Z","published":"2022-12-09T00:43:49Z","title":"Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning\n  vs. Backprop","summary":"  Bio-inspired learning has been gaining popularity recently given that\nBackpropagation (BP) is not considered biologically plausible. Many algorithms\nhave been proposed in the literature which are all more biologically plausible\nthan BP. However, apart from overcoming the biological implausibility of BP, a\nstrong motivation for using Bio-inspired algorithms remains lacking. In this\nstudy, we undertake a holistic comparison of BP vs. multiple Bio-inspired\nalgorithms to answer the question of whether Bio-learning offers additional\nbenefits over BP. We test Bio-algorithms under different design choices such as\naccess to only partial training data, resource constraints in terms of the\nnumber of training epochs, sparsification of the neural network parameters and\naddition of noise to input samples. Through these experiments, we notably find\ntwo key advantages of Bio-algorithms over BP. Firstly, Bio-algorithms perform\nmuch better than BP when the entire training dataset is not supplied. Four of\nthe five Bio-algorithms tested outperform BP by upto 5% accuracy when only 20%\nof the training dataset is available. Secondly, even when the full dataset is\navailable, Bio-algorithms learn much quicker and converge to a stable accuracy\nin far lesser training epochs than BP. Hebbian learning, specifically, is able\nto learn in just 5 epochs compared to around 100 epochs required by BP. These\ninsights present practical reasons for utilising Bio-learning beyond just their\nbiological plausibility and also point towards interesting new directions for\nfuture work on Bio-learning.\n","authors":["Manas Gupta","Sarthak Ketanbhai Modi","Hang Zhang","Joon Hei Lee","Joo Hwee Lim"],"pdf_url":"https://arxiv.org/pdf/2212.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09841v1","updated":"2023-03-17T08:49:09Z","published":"2023-03-17T08:49:09Z","title":"GADFormer: An Attention-based Model for Group Anomaly Detection on\n  Trajectories","summary":"  Group Anomaly Detection (GAD) reveals anomalous behavior among groups\nconsisting of multiple member instances, which are, individually considered,\nnot necessarily anomalous. This task is of major importance across multiple\ndisciplines, in which also sequences like trajectories can be considered as a\ngroup. However, with increasing amount and heterogenity of group members,\nactual abnormal groups get harder to detect, especially in an unsupervised or\nsemi-supervised setting. Recurrent Neural Networks are well established deep\nsequence models, but recent works have shown that their performance can\ndecrease with increasing sequence lengths. Hence, we introduce with this paper\nGADFormer, a GAD specific BERT architecture, capable to perform attention-based\nGroup Anomaly Detection on trajectories in an unsupervised and semi-supervised\nsetting. We show formally and experimentally how trajectory outlier detection\ncan be realized as an attention-based Group Anomaly Detection problem.\nFurthermore, we introduce a Block Attention-anomaly Score (BAS) to improve the\ninterpretability of transformer encoder blocks for GAD. In addition to that,\nsynthetic trajectory generation allows us to optimize the training for\ndomain-specific GAD. In extensive experiments we investigate our approach\nversus GRU in their robustness for trajectory noise and novelties on synthetic\nand real world datasets.\n","authors":["Andreas Lohrer","Darpan Malik","Peer Kröger"],"pdf_url":"https://arxiv.org/pdf/2303.09841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03228v3","updated":"2023-03-17T08:47:51Z","published":"2021-06-06T20:03:50Z","title":"Distributional Reinforcement Learning with Unconstrained Monotonic\n  Neural Networks","summary":"  The distributional reinforcement learning (RL) approach advocates for\nrepresenting the complete probability distribution of the random return instead\nof only modelling its expectation. A distributional RL algorithm may be\ncharacterised by two main components, namely the representation of the\ndistribution together with its parameterisation and the probability metric\ndefining the loss. The present research work considers the unconstrained\nmonotonic neural network (UMNN) architecture, a universal approximator of\ncontinuous monotonic functions which is particularly well suited for modelling\ndifferent representations of a distribution. This property enables the\nefficient decoupling of the effect of the function approximator class from that\nof the probability metric. The research paper firstly introduces a methodology\nfor learning different representations of the random return distribution (PDF,\nCDF and QF). Secondly, a novel distributional RL algorithm named unconstrained\nmonotonic deep Q-network (UMDQN) is presented. To the authors' knowledge, it is\nthe first distributional RL method supporting the learning of three, valid and\ncontinuous representations of the random return distribution. Lastly, in light\nof this new algorithm, an empirical comparison is performed between three\nprobability quasi-metrics, namely the Kullback-Leibler divergence, Cramer\ndistance, and Wasserstein distance. The results highlight the main strengths\nand weaknesses associated with each probability metric together with an\nimportant limitation of the Wasserstein distance.\n","authors":["Thibaut Théate","Antoine Wehenkel","Adrien Bolland","Gilles Louppe","Damien Ernst"],"pdf_url":"https://arxiv.org/pdf/2106.03228v3.pdf","comment":"Research paper accepted for publication in the peer-reviewed\n  Neurocomputing journal edited by Elsevier"},{"id":"http://arxiv.org/abs/2303.09350v2","updated":"2023-03-17T08:42:26Z","published":"2023-03-16T14:31:50Z","title":"Unsupervised domain adaptation by learning using privileged information","summary":"  Successful unsupervised domain adaptation (UDA) is guaranteed only under\nstrong assumptions such as covariate shift and overlap between input domains.\nThe latter is often violated in high-dimensional applications such as image\nclassification which, despite this challenge, continues to serve as inspiration\nand benchmark for algorithm development. In this work, we show that access to\nside information about examples from the source and target domains can help\nrelax these assumptions and increase sample efficiency in learning, at the cost\nof collecting a richer variable set. We call this domain adaptation by learning\nusing privileged information (DALUPI). Tailored for this task, we propose a\nsimple two-stage learning algorithm inspired by our analysis and a practical\nend-to-end algorithm for multi-label image classification. In a suite of\nexperiments, including an application to medical image analysis, we demonstrate\nthat incorporating privileged information in learning can reduce errors in\ndomain transfer compared to classical learning.\n","authors":["Adam Breitholtz","Anton Matsson","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.09350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.08277v3","updated":"2023-03-17T08:42:23Z","published":"2021-07-17T16:44:27Z","title":"Learning Augmented Online Facility Location","summary":"  Following the research agenda initiated by Munoz & Vassilvitskii [1] and\nLykouris & Vassilvitskii [2] on learning-augmented online algorithms for\nclassical online optimization problems, in this work, we consider the Online\nFacility Location problem under this framework. In Online Facility Location\n(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)\nassigned to an open facility upon arrival, without any knowledge about future\ndemands.\n  We present an online algorithm for OFL that exploits potentially imperfect\npredictions on the locations of the optimal facilities. We prove that the\ncompetitive ratio decreases smoothly from sublogarithmic in the number of\ndemands to constant, as the error, i.e., the total distance of the predicted\nlocations to the optimal facility locations, decreases towards zero. We\ncomplement our analysis with a matching lower bound establishing that the\ndependence of the algorithm's competitive ratio on the error is optimal, up to\nconstant factors. Finally, we evaluate our algorithm on real world data and\ncompare our learning augmented approach with the current best online algorithm\nfor the problem.\n","authors":["Dimitris Fotakis","Evangelia Gergatsouli","Themis Gouleakis","Nikolas Patris"],"pdf_url":"https://arxiv.org/pdf/2107.08277v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10803v2","updated":"2023-03-17T08:38:35Z","published":"2023-02-16T12:59:08Z","title":"Eagle: Large-Scale Learning of Turbulent Fluid Dynamics with Mesh\n  Transformers","summary":"  Estimating fluid dynamics is classically done through the simulation and\nintegration of numerical models solving the Navier-Stokes equations, which is\ncomputationally complex and time-consuming even on high-end hardware. This is a\nnotoriously hard problem to solve, which has recently been addressed with\nmachine learning, in particular graph neural networks (GNN) and variants\ntrained and evaluated on datasets of static objects in static scenes with fixed\ngeometry. We attempt to go beyond existing work in complexity and introduce a\nnew model, method and benchmark. We propose EAGLE, a large-scale dataset of 1.1\nmillion 2D meshes resulting from simulations of unsteady fluid dynamics caused\nby a moving flow source interacting with nonlinear scene structure, comprised\nof 600 different scenes of three different types. To perform future forecasting\nof pressure and velocity on the challenging EAGLE dataset, we introduce a new\nmesh transformer. It leverages node clustering, graph pooling and global\nattention to learn long-range dependencies between spatially distant data\npoints without needing a large number of iterations, as existing GNN methods\ndo. We show that our transformer outperforms state-of-the-art performance on,\nboth, existing synthetic and real datasets and on EAGLE. Finally, we highlight\nthat our approach learns to attend to airflow, integrating complex information\nin a single iteration.\n","authors":["Steeven Janny","Aurélien Béneteau","Madiha Nadri","Julie Digne","Nicolas Thome","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2302.10803v2.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09823v1","updated":"2023-03-17T08:02:54Z","published":"2023-03-17T08:02:54Z","title":"Transformers and Ensemble methods: A solution for Hate Speech Detection\n  in Arabic languages","summary":"  This paper describes our participation in the shared task of hate speech\ndetection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our\nexperiments evaluate the performance of six transformer models and their\ncombination using 2 ensemble approaches. The best results on the training set,\nin a five-fold cross validation scenario, were obtained by using the ensemble\napproach based on the majority vote. The evaluation of this approach on the\ntest set resulted in an F1-score of 0.60 and an Accuracy of 0.86.\n","authors":["Angel Felipe Magnossão de Paula","Imene Bensalem","Paolo Rosso","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2303.09823v1.pdf","comment":"7 pages, 3 tables"},{"id":"http://arxiv.org/abs/2302.13356v2","updated":"2023-03-17T07:44:31Z","published":"2023-02-26T17:22:40Z","title":"Performance is not enough: a story of the Rashomon's quartet","summary":"  Predictive modelling is often reduced to finding the best model that\noptimizes a selected performance measure. But what if the second-best model\ndescribes the data equally well but in a completely different way? What about\nthe third? Is it possible that the most effective models learn completely\ndifferent relationships in the data? Inspired by Anscombe's quartet, this paper\nintroduces Rashomon's quartet, a synthetic dataset for which four models from\ndifferent classes have practically identical predictive performance. However,\ntheir visualization reveals drastically distinct ways of understanding the\ncorrelation structure in data. The introduced simple illustrative example aims\nto further facilitate visualization as a mandatory tool to compare predictive\nmodels beyond their performance. We need to develop insightful techniques for\nthe explanatory analysis of model sets.\n","authors":["Przemyslaw Biecek","Hubert Baniecki","Mateusz Krzyzinski","Dianne Cook"],"pdf_url":"https://arxiv.org/pdf/2302.13356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11375v2","updated":"2023-03-17T07:43:42Z","published":"2022-12-21T21:32:36Z","title":"Semi-supervised Bladder Tissue Classification in Multi-Domain Endoscopic\n  Images","summary":"  Objective: Accurate visual classification of bladder tissue during\nTrans-Urethral Resection of Bladder Tumor (TURBT) procedures is essential to\nimprove early cancer diagnosis and treatment. During TURBT interventions, White\nLight Imaging (WLI) and Narrow Band Imaging (NBI) techniques are used for\nlesion detection. Each imaging technique provides diverse visual information\nthat allows clinicians to identify and classify cancerous lesions. Computer\nvision methods that use both imaging techniques could improve endoscopic\ndiagnosis. We address the challenge of tissue classification when annotations\nare available only in one domain, in our case WLI, and the endoscopic images\ncorrespond to an unpaired dataset, i.e. there is no exact equivalent for every\nimage in both NBI and WLI domains. Method: We propose a semi-surprised\nGenerative Adversarial Network (GAN)-based method composed of three main\ncomponents: a teacher network trained on the labeled WLI data; a\ncycle-consistency GAN to perform unpaired image-to-image translation, and a\nmulti-input student network. To ensure the quality of the synthetic images\ngenerated by the proposed GAN we perform a detailed quantitative, and\nqualitative analysis with the help of specialists. Conclusion: The overall\naverage classification accuracy, precision, and recall obtained with the\nproposed method for tissue classification are 0.90, 0.88, and 0.89\nrespectively, while the same metrics obtained in the unlabeled domain (NBI) are\n0.92, 0.64, and 0.94 respectively. The quality of the generated images is\nreliable enough to deceive specialists. Significance: This study shows the\npotential of using semi-supervised GAN-based bladder tissue classification when\nannotations are limited in multi-domain data. The dataset is available at\nhttps://zenodo.org/record/7741476#.ZBQUK7TMJ6k\n","authors":["Jorge F. Lazo","Benoit Rosa","Michele Catellani","Matteo Fontana","Francesco A. Mistretta","Gennaro Musi","Ottavio de Cobelli","Michel de Mathelin","Elena De Momi"],"pdf_url":"https://arxiv.org/pdf/2212.11375v2.pdf","comment":"Title and abstract updated. Typos corrected"},{"id":"http://arxiv.org/abs/2210.02871v2","updated":"2023-03-17T07:07:10Z","published":"2022-09-30T02:25:12Z","title":"Self-Distillation for Further Pre-training of Transformers","summary":"  Pre-training a large transformer model on a massive amount of unlabeled data\nand fine-tuning it on labeled datasets for diverse downstream tasks has proven\nto be a successful strategy, for a variety of vision and natural language\nprocessing tasks. However, direct fine-tuning of the pre-trained model may be\nsuboptimal if there exist large discrepancies across data domains for\npre-training and fine-tuning. To tackle this issue, several previous studies\nhave proposed further pre-training strategies, where we continue to pre-train\nthe model on the target unlabeled dataset before fine-tuning. However, all of\nthem solely focus on language models and we empirically find that a Vision\nTransformer is vulnerable to overfitting as we continue to pretrain the model\non target unlabeled data. In order to tackle this limitation, we propose\nself-distillation as a regularization for a further pre-training stage.\nSpecifically, we first further pre-train the initial pre-trained model on the\ntarget unlabeled data and then consider it as a teacher for self-distillation.\nThen we take the same initial pre-trained model as a student and enforce its\nhidden representations to be close to those of the teacher while optimizing the\nstudent with a masked auto-encoding objective. We empirically validate the\nefficacy of self-distillation on a variety of benchmark datasets for image and\ntext classification tasks. Experimentally, we show that our proposed method\noutperforms all the relevant baselines. Theoretically, we analyze the proposed\nmethod with a simplified model to understand how self-distillation for further\npre-training can potentially help improve the performance of the downstream\ntasks.\n","authors":["Seanie Lee","Minki Kang","Juho Lee","Sung Ju Hwang","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2210.02871v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/1905.10575v3","updated":"2023-03-17T06:10:49Z","published":"2019-05-25T11:33:02Z","title":"Deep Image Feature Learning with Fuzzy Rules","summary":"  The methods of extracting image features are the key to many image processing\ntasks. At present, the most popular method is the deep neural network which can\nautomatically extract robust features through end-to-end training instead of\nhand-crafted feature extraction. However, the deep neural network currently\nfaces many challenges: 1) its effectiveness is heavily dependent on large\ndatasets, so the computational complexity is very high; 2) it is usually\nregarded as a black box model with poor interpretability. To meet the above\nchallenges, a more interpretable and scalable feature learning method, i.e.,\ndeep image feature learning with fuzzy rules (DIFL-FR), is proposed in the\npaper, which combines the rule-based fuzzy modeling technique and the deep\nstacked learning strategy. The method progressively learns image features\nthrough a layer-by-layer manner based on fuzzy rules, so the feature learning\nprocess can be better explained by the generated rules. More importantly, the\nlearning process of the method is only based on forward propagation without\nback propagation and iterative learning, which results in the high learning\nefficiency. In addition, the method is under the settings of unsupervised\nlearning and can be easily extended to scenes of supervised and semi-supervised\nlearning. Extensive experiments are conducted on image datasets of different\nscales. The results obviously show the effectiveness of the proposed method.\n","authors":["Xiang Ma","Liangzhe Chen","Zhaohong Deng","Peng Xu","Qisheng Yan","Kup-Sze Choi","Shitong Wang"],"pdf_url":"https://arxiv.org/pdf/1905.10575v3.pdf","comment":"Accepted by IEEE Trans. Emerging Topics in Computational Intelligence"},{"id":"http://arxiv.org/abs/2201.09391v2","updated":"2023-03-17T05:57:03Z","published":"2022-01-23T22:51:14Z","title":"Partition-Based Active Learning for Graph Neural Networks","summary":"  We study the problem of semi-supervised learning with Graph Neural Networks\n(GNNs) in an active learning setup. We propose GraphPart, a novel\npartition-based active learning approach for GNNs. GraphPart first splits the\ngraph into disjoint partitions and then selects representative nodes within\neach partition to query. The proposed method is motivated by a novel analysis\nof the classification error under realistic smoothness assumptions over the\ngraph and the node features. Extensive experiments on multiple benchmark\ndatasets demonstrate that the proposed method outperforms existing active\nlearning methods for GNNs under a wide range of annotation budget constraints.\nIn addition, the proposed method does not introduce additional hyperparameters,\nwhich is crucial for model training, especially in the active learning setting\nwhere a labeled validation set may not be available.\n","authors":["Jiaqi Ma","Ziqiao Ma","Joyce Chai","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2201.09391v2.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR). Code\n  available at: https://github.com/Mars-tin/GraphPart"},{"id":"http://arxiv.org/abs/2303.09780v1","updated":"2023-03-17T05:27:16Z","published":"2023-03-17T05:27:16Z","title":"Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox\n  Spread","summary":"  The challenge on forestalling monkeypox (Mpox) spread is the timely,\nconvenient and accurate diagnosis for earlystage infected individuals. Here, we\npropose a remote and realtime online visualization strategy, called \"Super\nMonitoring\" to construct a low cost, convenient, timely and unspecialized\ndiagnosis of early-stage Mpox. Such AI-mediated \"Super Monitoring\" (Mpox-AISM)\ninvokes a framework assembled by deep learning, data augmentation and\nself-supervised learning, as well as professionally classifies four subtypes\naccording to dataset characteristics and evolution trend of Mpox and seven\nother types of dermatopathya with high similarity, hence these features\ntogether with reasonable program interface and threshold setting ensure that\nits Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%.\nAs a result, with the help of cloud service on Internet and communication\nterminal, this strategy can be potentially utilized for the real-time detection\nof earlystage Mpox in various scenarios including entry-exit inspection in\nairport, family doctor, rural area in underdeveloped region and wild to\neffectively shorten the window period of Mpox spread.\n","authors":["Yubiao Yue","Zhenzhang Li","Xinyue Zhang","Jialong Xu","Jinbao Liu","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2303.09780v1.pdf","comment":"7pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.09778v1","updated":"2023-03-17T05:20:24Z","published":"2023-03-17T05:20:24Z","title":"SE-GSL: A General and Effective Graph Structure Learning Framework\n  through Structural Entropy Optimization","summary":"  Graph Neural Networks (GNNs) are de facto solutions to structural data\nlearning. However, it is susceptible to low-quality and unreliable structure,\nwhich has been a norm rather than an exception in real-world graphs. Existing\ngraph structure learning (GSL) frameworks still lack robustness and\ninterpretability. This paper proposes a general GSL framework, SE-GSL, through\nstructural entropy and the graph hierarchy abstracted in the encoding tree.\nParticularly, we exploit the one-dimensional structural entropy to maximize\nembedded information content when auxiliary neighbourhood attributes are fused\nto enhance the original graph. A new scheme of constructing optimal encoding\ntrees is proposed to minimize the uncertainty and noises in the graph whilst\nassuring proper community partition in hierarchical abstraction. We present a\nnovel sample-based mechanism for restoring the graph structure via node\nstructural entropy distribution. It increases the connectivity among nodes with\nlarger uncertainty in lower-level communities. SE-GSL is compatible with\nvarious GNN models and enhances the robustness towards noisy and heterophily\nstructures. Extensive experiments show significant improvements in the\neffectiveness and robustness of structure learning and node representation\nlearning.\n","authors":["Dongcheng Zou","Hao Peng","Xiang Huang","Renyu Yang","Jianxin Li","Jia Wu","Chunyang Liu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2303.09778v1.pdf","comment":"12 pages,5 figures, accepted by WWW2023"},{"id":"http://arxiv.org/abs/2303.09772v1","updated":"2023-03-17T04:40:07Z","published":"2023-03-17T04:40:07Z","title":"QUBO Decision Tree: Annealing Machine Extends Decision Tree Splitting","summary":"  This paper proposes an extension of regression trees by quadratic\nunconstrained binary optimization (QUBO). Regression trees are very popular\nprediction models that are trainable with tabular datasets, but their accuracy\nis insufficient because the decision rules are too simple. The proposed method\nextends the decision rules in decision trees to multi-dimensional boundaries.\nSuch an extension is generally unimplementable because of computational\nlimitations, however, the proposed method transforms the training process to\nQUBO, which enables an annealing machine to solve this problem.\n","authors":["Koichiro Yawata","Yoshihiro Osakabe","Takuya Okuyama","Akinori Asahara"],"pdf_url":"https://arxiv.org/pdf/2303.09772v1.pdf","comment":"2022 IEEE International Conference on Knowledge Graph (ICKG)"},{"id":"http://arxiv.org/abs/2209.08110v2","updated":"2023-03-17T04:24:10Z","published":"2022-09-16T18:00:13Z","title":"Detecting Political Biases of Named Entities and Hashtags on Twitter","summary":"  Ideological divisions in the United States have become increasingly prominent\nin daily communication. Accordingly, there has been much research on political\npolarization, including many recent efforts that take a computational\nperspective. By detecting political biases in a corpus of text, one can attempt\nto describe and discern the polarity of that text. Intuitively, the named\nentities (i.e., the nouns and the phrases that act as nouns) and hashtags in\ntext often carry information about political views. For example, people who use\nthe term \"pro-choice\" are likely to be liberal, whereas people who use the term\n\"pro-life\" are likely to be conservative. In this paper, we seek to reveal\npolitical polarities in social-media text data and to quantify these polarities\nby explicitly assigning a polarity score to entities and hashtags. Although\nthis idea is straightforward, it is difficult to perform such inference in a\ntrustworthy quantitative way. Key challenges include the small number of known\nlabels, the continuous spectrum of political views, and the preservation of\nboth a polarity score and a polarity-neutral semantic meaning in an embedding\nvector of words. To attempt to overcome these challenges, we propose the\nPolarity-aware Embedding Multi-task learning (PEM) model. This model consists\nof (1) a self-supervised context-preservation task, (2) an attention-based\ntweet-level polarity-inference task, and (3) an adversarial learning task that\npromotes independence between an embedding's polarity dimension and its\nsemantic dimensions. Our experimental results demonstrate that our PEM model\ncan successfully learn polarity-aware embeddings that perform well\nclassification tasks. We examine a variety of applications and we thereby\ndemonstrate the effectiveness of our PEM model. We also discuss important\nlimitations of our work and encourage caution when applying the it to\nreal-world scenarios.\n","authors":["Zhiping Xiao","Jeffrey Zhu","Yining Wang","Pei Zhou","Wen Hong Lam","Mason A. Porter","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2209.08110v2.pdf","comment":"Submitted to EPJ -- Data Science, under review"},{"id":"http://arxiv.org/abs/2303.09769v1","updated":"2023-03-17T04:20:47Z","published":"2023-03-17T04:20:47Z","title":"Denoising Diffusion Autoencoders are Unified Self-supervised Learners","summary":"  Inspired by recent advances in diffusion models, which are reminiscent of\ndenoising autoencoders, we investigate whether they can acquire discriminative\nrepresentations for classification via generative pre-training. This paper\nshows that the networks in diffusion models, namely denoising diffusion\nautoencoders (DDAE), are unified self-supervised learners: by pre-training on\nunconditional image generation, DDAE has already learned strongly\nlinear-separable representations at its intermediate layers without auxiliary\nencoders, thus making diffusion pre-training emerge as a general approach for\nself-supervised generative and discriminative learning. To verify this, we\nperform linear probe and fine-tuning evaluations on multi-class datasets. Our\ndiffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on\nCIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked\nautoencoders and contrastive learning for the first time. Additionally,\ntransfer learning from ImageNet confirms DDAE's suitability for latent-space\nVision Transformers, suggesting the potential for scaling DDAEs as unified\nfoundation models.\n","authors":["Weilai Xiang","Hongyu Yang","Di Huang","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09767v1","updated":"2023-03-17T04:18:03Z","published":"2023-03-17T04:18:03Z","title":"It Is All About Data: A Survey on the Effects of Data on Adversarial\n  Robustness","summary":"  Adversarial examples are inputs to machine learning models that an attacker\nhas intentionally designed to confuse the model into making a mistake. Such\nexamples pose a serious threat to the applicability of machine-learning-based\nsystems, especially in life- and safety-critical domains. To address this\nproblem, the area of adversarial robustness investigates mechanisms behind\nadversarial attacks and defenses against these attacks. This survey reviews\nliterature that focuses on the effects of data used by a model on the model's\nadversarial robustness. It systematically identifies and summarizes the\nstate-of-the-art research in this area and further discusses gaps of knowledge\nand promising future research directions.\n","authors":["Peiyu Xiong","Michael Tegegn","Jaskeerat Singh Sarin","Shubhraneel Pal","Julia Rubin"],"pdf_url":"https://arxiv.org/pdf/2303.09767v1.pdf","comment":"41 pages, 25 figures, under review"},{"id":"http://arxiv.org/abs/2303.09760v1","updated":"2023-03-17T03:47:10Z","published":"2023-03-17T03:47:10Z","title":"Diffusing the Optimal Topology: A Generative Optimization Approach","summary":"  Topology Optimization seeks to find the best design that satisfies a set of\nconstraints while maximizing system performance. Traditional iterative\noptimization methods like SIMP can be computationally expensive and get stuck\nin local minima, limiting their applicability to complex or large-scale\nproblems. Learning-based approaches have been developed to accelerate the\ntopology optimization process, but these methods can generate designs with\nfloating material and low performance when challenged with out-of-distribution\nconstraint configurations. Recently, deep generative models, such as Generative\nAdversarial Networks and Diffusion Models, conditioned on constraints and\nphysics fields have shown promise, but they require extensive pre-processing\nand surrogate models for improving performance. To address these issues, we\npropose a Generative Optimization method that integrates classic optimization\nlike SIMP as a refining mechanism for the topology generated by a deep\ngenerative model. We also remove the need for conditioning on physical fields\nusing a computationally inexpensive approximation inspired by classic ODE\nsolutions and reduce the number of steps needed to generate a feasible and\nperformant topology. Our method allows us to efficiently generate good\ntopologies and explicitly guide them to regions with high manufacturability and\nhigh performance, without the need for external auxiliary models or additional\nlabeled data. We believe that our method can lead to significant advancements\nin the design and optimization of structures in engineering applications, and\ncan be applied to a broader spectrum of performance-aware engineering design\nproblems.\n","authors":["Giorgio Giannone","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.09760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09752v1","updated":"2023-03-17T03:28:17Z","published":"2023-03-17T03:28:17Z","title":"CoLT5: Faster Long-Range Transformers with Conditional Computation","summary":"  Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n","authors":["Joshua Ainslie","Tao Lei","Michiel de Jong","Santiago Ontañón","Siddhartha Brahma","Yury Zemlyanskiy","David Uthus","Mandy Guo","James Lee-Thorp","Yi Tay","Yun-Hsuan Sung","Sumit Sanghai"],"pdf_url":"https://arxiv.org/pdf/2303.09752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09746v1","updated":"2023-03-17T02:55:08Z","published":"2023-03-17T02:55:08Z","title":"Detecting Out-of-distribution Examples via Class-conditional Impressions\n  Reappearing","summary":"  Out-of-distribution (OOD) detection aims at enhancing standard deep neural\nnetworks to distinguish anomalous inputs from original training data. Previous\nprogress has introduced various approaches where the in-distribution training\ndata and even several OOD examples are prerequisites. However, due to privacy\nand security, auxiliary data tends to be impractical in a real-world scenario.\nIn this paper, we propose a data-free method without training on natural data,\ncalled Class-Conditional Impressions Reappearing (C2IR), which utilizes image\nimpressions from the fixed model to recover class-conditional feature\nstatistics. Based on that, we introduce Integral Probability Metrics to\nestimate layer-wise class-conditional deviations and obtain layer weights by\nMeasuring Gradient-based Importance (MGI). The experiments verify the\neffectiveness of our method and indicate that C2IR outperforms other post-hoc\nmethods and reaches comparable performance to the full access (ID and OOD)\ndetection method, especially in the far-OOD dataset (SVHN).\n","authors":["Jinggang Chen","Xiaoyang Qu","Junjie Li","Jianzong Wang","Jiguang Wan","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.09746v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.04470v2","updated":"2023-03-17T02:38:59Z","published":"2022-10-10T07:47:56Z","title":"Actor-Critic or Critic-Actor? A Tale of Two Time Scales","summary":"  We revisit the standard formulation of tabular actor-critic algorithm as a\ntwo time-scale stochastic approximation with value function computed on a\nfaster time-scale and policy computed on a slower time-scale. This emulates\npolicy iteration. We begin by observing that reversal of the time scales will\nin fact emulate value iteration and is a legitimate algorithm. We provide a\nproof of convergence and compare the two empirically with and without function\napproximation (with both linear and nonlinear function approximators) and\nobserve that our proposed critic-actor algorithm performs on par with\nactor-critic in terms of both accuracy and computational effort.\n","authors":["Shalabh Bhatnagar","Vivek S. Borkar","Soumyajit Guin"],"pdf_url":"https://arxiv.org/pdf/2210.04470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09065v2","updated":"2023-03-17T02:34:18Z","published":"2023-03-16T03:45:46Z","title":"Maximum Margin Learning of t-SPNs for Cell Classification with Filtering","summary":"  An algorithm based on a deep probabilistic architecture referred to as a\ntree-structured sum-product network (t-SPN) is considered for cell\nclassification. The t-SPN is constructed such that the unnormalized probability\nis represented as conditional probabilities of a subset of most similar cell\nclasses. The constructed t-SPN architecture is learned by maximizing the\nmargin, which is the difference in the conditional probability between the true\nand the most competitive false label. To enhance the generalization ability of\nthe architecture, L2-regularization (REG) is considered along with the maximum\nmargin (MM) criterion in the learning process. To highlight cell features, this\npaper investigates the effectiveness of two generic high-pass filters: ideal\nhigh-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both\nHEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on\nthe max-margin criterion with regularization produced the highest accuracy rate\ncompared to other state-of-the-art algorithms that include convolutional neural\nnetwork (CNN) based algorithms. The ideal high-pass filter was more effective\non the HEp-2 dataset, which is based on immunofluorescence staining, while the\nLOG was more effective on the Feulgen dataset, which is based on Feulgen\nstaining.\n","authors":["Haeyong Kang","Chang D. Yoo","Yongcheon Na"],"pdf_url":"https://arxiv.org/pdf/2303.09065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07050v2","updated":"2023-03-17T02:32:06Z","published":"2022-12-14T06:04:18Z","title":"Significantly Improving Zero-Shot X-ray Pathology Classification via\n  Fine-tuning Pre-trained Image-Text Encoders","summary":"  Deep neural networks have been successfully adopted to diverse domains\nincluding pathology classification based on medical images. However,\nlarge-scale and high-quality data to train powerful neural networks are rare in\nthe medical domain as the labeling must be done by qualified experts.\nResearchers recently tackled this problem with some success by taking advantage\nof models pre-trained on large-scale general domain data. Specifically,\nresearchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it\nwith chest X-ray images and paired reports to perform zero-shot pathology\nclassification, thus completely removing the need for pathology-annotated\nimages to train a classification model. Existing studies, however, fine-tuned\nthe pre-trained model with the same contrastive learning objective, and failed\nto exploit the multi-labeled nature of medical image-report pairs. In this\npaper, we propose a new fine-tuning strategy based on sentence sampling and\npositive pair loss relaxation for improving the downstream zero-shot pathology\nclassification performance, which can be applied to any pre-trained contrastive\nimage-text encoders. Our method consistently showed dramatically improved\nzero-shot pathology classification performance on four different chest X-ray\ndatasets and 3 different pre-trained models (5.77% average AUROC increase). In\nparticular, fine-tuning CLIP with our method showed much comparable or\nmarginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1\nscore and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent\ndiseases from the CheXpert dataset.\n","authors":["Jongseong Jang","Daeun Kyung","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2212.07050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09732v1","updated":"2023-03-17T02:21:41Z","published":"2023-03-17T02:21:41Z","title":"Rethinking White-Box Watermarks on Deep Learning Models under Neural\n  Structural Obfuscation","summary":"  Copyright protection for deep neural networks (DNNs) is an urgent need for AI\ncorporations. To trace illegally distributed model copies, DNN watermarking is\nan emerging technique for embedding and verifying secret identity messages in\nthe prediction behaviors or the model internals. Sacrificing less functionality\nand involving more knowledge about the target DNN, the latter branch called\n\\textit{white-box DNN watermarking} is believed to be accurate, credible and\nsecure against most known watermark removal attacks, with emerging research\nefforts in both the academy and the industry.\n  In this paper, we present the first systematic study on how the mainstream\nwhite-box DNN watermarks are commonly vulnerable to neural structural\nobfuscation with \\textit{dummy neurons}, a group of neurons which can be added\nto a target model but leave the model behavior invariant. Devising a\ncomprehensive framework to automatically generate and inject dummy neurons with\nhigh stealthiness, our novel attack intensively modifies the architecture of\nthe target model to inhibit the success of watermark verification. With\nextensive evaluation, our work for the first time shows that nine published\nwatermarking schemes require amendments to their verification procedures.\n","authors":["Yifan Yan","Xudong Pan","Mi Zhang","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09732v1.pdf","comment":"Accepted by USENIX Security 2023. arXiv admin note: text overlap with\n  arXiv:2205.00199"},{"id":"http://arxiv.org/abs/2110.01894v2","updated":"2023-03-17T02:17:07Z","published":"2021-10-05T09:30:56Z","title":"Combining Physics and Deep Learning to learn Continuous-Time Dynamics\n  Models","summary":"  Deep learning has been widely used within learning algorithms for robotics.\nOne disadvantage of deep networks is that these networks are black-box\nrepresentations. Therefore, the learned approximations ignore the existing\nknowledge of physics or robotics. Especially for learning dynamics models,\nthese black-box models are not desirable as the underlying principles are well\nunderstood and the standard deep networks can learn dynamics that violate these\nprinciples. To learn dynamics models with deep networks that guarantee\nphysically plausible dynamics, we introduce physics-inspired deep networks that\ncombine first principles from physics with deep learning. We incorporate\nLagrangian mechanics within the model learning such that all approximated\nmodels adhere to the laws of physics and conserve energy. Deep Lagrangian\nNetworks (DeLaN) parametrize the system energy using two networks. The\nparameters are obtained by minimizing the squared residual of the\nEuler-Lagrange differential equation. Therefore, the resulting model does not\nrequire specific knowledge of the individual system, is interpretable, and can\nbe used as a forward, inverse, and energy model. Previously these properties\nwere only obtained when using system identification techniques that require\nknowledge of the kinematic structure. We apply DeLaN to learning dynamics\nmodels and apply these models to control simulated and physical rigid body\nsystems. The results show that the proposed approach obtains dynamics models\nthat can be applied to physical systems for real-time control. Compared to\nstandard deep networks, the physics-inspired models learn better models and\ncapture the underlying structure of the dynamics.\n","authors":["Michael Lutter","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2110.01894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03396v2","updated":"2023-03-17T02:15:38Z","published":"2022-12-07T01:42:47Z","title":"Learning to Select Prototypical Parts for Interpretable Sequential Data\n  Modeling","summary":"  Prototype-based interpretability methods provide intuitive explanations of\nmodel prediction by comparing samples to a reference set of memorized exemplars\nor typical representatives in terms of similarity. In the field of sequential\ndata modeling, similarity calculations of prototypes are usually based on\nencoded representation vectors. However, due to highly recursive functions,\nthere is usually a non-negligible disparity between the prototype-based\nexplanations and the original input. In this work, we propose a Self-Explaining\nSelective Model (SESM) that uses a linear combination of prototypical concepts\nto explain its own predictions. The model employs the idea of case-based\nreasoning by selecting sub-sequences of the input that mostly activate\ndifferent concepts as prototypical parts, which users can compare to\nsub-sequences selected from different example inputs to understand model\ndecisions. For better interpretability, we design multiple constraints\nincluding diversity, stability, and locality as training objectives. Extensive\nexperiments in different domains demonstrate that our method exhibits promising\ninterpretability and competitive accuracy.\n","authors":["Yifei Zhang","Neng Gao","Cunqing Ma"],"pdf_url":"https://arxiv.org/pdf/2212.03396v2.pdf","comment":"To be appeared in AAAI 2023"},{"id":"http://arxiv.org/abs/2211.00313v2","updated":"2023-03-17T01:55:07Z","published":"2022-11-01T07:41:03Z","title":"RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection","summary":"  Purpose: Self-supervised learning is rapidly advancing computer-aided\ndiagnosis in the medical field. Masked image modeling (MIM) is one of the\nself-supervised learning methods that masks a subset of input pixels and\nattempts to predict the masked pixels. Traditional MIM methods often employ a\nrandom masking strategy. In comparison to ordinary images, medical images often\nhave a small region of interest for disease detection. Consequently, we focus\non fixing the problem in this work, which is evaluated by automatic COVID-19\nidentification. Methods: In this study, we propose a novel region-guided masked\nimage modeling method (RGMIM) for COVID-19 detection in this paper. In our\nmethod, we devise a new masking strategy that employed lung mask information to\nidentify valid regions to learn more useful information for COVID-19 detection.\nThe proposed method was contrasted with five self-supervised learning\ntechniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative\nevaluation of open COVID-19 CXR datasets as well as masking ratio\nhyperparameter studies. Results: When using the entire training set, RGMIM\noutperformed other comparable methods, achieving 0.962 detection accuracy.\nSpecifically, RGMIM significantly improved COVID-19 detection in small data\nvolumes, such as 5% and 10% of the training set (846 and 1,693 images) compared\nto other methods, and achieved 0.957 detection accuracy even when only 50% of\nthe training set was used. Conclusions: RGMIM can mask more valid lung-related\nregions, facilitating the learning of discriminative representations and the\nsubsequent high-accuracy COVID-19 detection. RGMIM outperforms other\nstate-of-the-art self-supervised learning methods in experiments, particularly\nwhen limited training data is used. RGMIM also has the potential to be applied\nto other medical images.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2211.00313v2.pdf","comment":"Submitted as a journal paper at Springer IJCARS"},{"id":"http://arxiv.org/abs/2209.07040v3","updated":"2023-03-17T01:39:36Z","published":"2022-09-15T04:49:06Z","title":"Learning-Based Adaptive Control for Stochastic Linear Systems with Input\n  Constraints","summary":"  We propose a certainty-equivalence scheme for adaptive control of scalar\nlinear systems subject to additive, i.i.d. Gaussian disturbances and bounded\ncontrol input constraints, without requiring prior knowledge of the bounds of\nthe system parameters, nor the control direction. Assuming that the system is\nat-worst marginally stable, mean square boundedness of the closed-loop system\nstates is proven. Lastly, numerical examples are presented to illustrate our\nresults.\n","authors":["Seth Siriya","Jingge Zhu","Dragan Nešić","Ye Pu"],"pdf_url":"https://arxiv.org/pdf/2209.07040v3.pdf","comment":"16 pages, 2 figures, accepted at IEEE Control Systems Letters"},{"id":"http://arxiv.org/abs/2303.09716v1","updated":"2023-03-17T01:20:22Z","published":"2023-03-17T01:20:22Z","title":"A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum\n  Markov Games","summary":"  Many model-based reinforcement learning (RL) algorithms can be viewed as\nhaving two phases that are iteratively implemented: a learning phase where the\nmodel is approximately learned and a planning phase where the learned model is\nused to derive a policy. In the case of standard MDPs, the learning problem can\nbe solved using either value iteration or policy iteration. However, in the\ncase of zero-sum Markov games, there is no efficient policy iteration\nalgorithm; e.g., it has been shown in Hansen et al. (2013) that one has to\nsolve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement\nthe only known convergent version of policy iteration. Another algorithm for\nMarkov zero-sum games, called naive policy iteration, is easy to implement but\nis only provably convergent under very restrictive assumptions. Prior attempts\nto fix naive policy iteration algorithm have several limitations. Here, we show\nthat a simple variant of naive policy iteration for games converges, and\nconverges exponentially fast. The only addition we propose to naive policy\niteration is the use of lookahead in the policy improvement phase. This is\nappealing because lookahead is anyway often used in RL for games. We further\nshow that lookahead can be implemented efficiently in linear Markov games,\nwhich are the counterpart of the linear MDPs and have been the subject of much\nattention recently. We then consider multi-agent reinforcement learning which\nuses our algorithm in the planning phases, and provide sample and time\ncomplexity bounds for such an algorithm.\n","authors":["Anna Winnicki","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2303.09716v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2303.09715v1","updated":"2023-03-17T01:13:18Z","published":"2023-03-17T01:13:18Z","title":"Understanding why shooters shoot -- An AI-powered engine for basketball\n  performance profiling","summary":"  Understanding player shooting profiles is an essential part of basketball\nanalysis: knowing where certain opposing players like to shoot from can help\ncoaches neutralize offensive gameplans from their opponents; understanding\nwhere their players are most comfortable can lead them to developing more\neffective offensive strategies. An automatic tool that can provide these\nperformance profiles in a timely manner can become invaluable for coaches to\nmaximize both the effectiveness of their game plan as well as the time\ndedicated to practice and other related activities. Additionally, basketball is\ndictated by many variables, such as playstyle and game dynamics, that can\nchange the flow of the game and, by extension, player performance profiles. It\nis crucial that the performance profiles can reflect the diverse playstyles, as\nwell as the fast-changing dynamics of the game. We present a tool that can\nvisualize player performance profiles in a timely manner while taking into\naccount factors such as play-style and game dynamics. Our approach generates\ninterpretable heatmaps that allow us to identify and analyze how non-spatial\nfactors, such as game dynamics or playstyle, affect player performance\nprofiles.\n","authors":["Alejandro Rodriguez Pascual","Ishan Mehta","Muhammad Khan","Frank Rodriz","Rose Yu"],"pdf_url":"https://arxiv.org/pdf/2303.09715v1.pdf","comment":"12 pages, 6 figures, to be published at MIT Sloan Sports Analytics\n  Conference, March 4-5 2022"},{"id":"http://arxiv.org/abs/2210.16380v2","updated":"2023-03-17T01:11:23Z","published":"2022-10-28T19:39:14Z","title":"An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble\n  Modeling, Normalized Distributions of Annotations, and Entropic Measures of\n  Uncertainty","summary":"  Performing classification on noisy, crowdsourced image datasets can prove\nchallenging even for the best neural networks. Two issues which complicate the\nproblem on such datasets are class imbalance and ground-truth uncertainty in\nlabeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped,\nindividual characters from images of ancient Greek papyri -- are strongly\naffected by both issues. The application of ensemble modeling to such datasets\ncan help identify images where the ground-truth is questionable and quantify\nthe trustworthiness of those samples. As such, we apply stacked generalization\nconsisting of nearly identical ResNets with different loss functions: one\nutilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence\n(KLD). Both networks use labels drawn from the crowdsourced consensus. For the\nsecond network, the KLD is calculated with respect to the proposed Normalized\nDistribution of Annotations (NDA). For our ensemble model, we apply a k-nearest\nneighbors model to the outputs of the CXE and KLD networks. Individually, the\nResNet models have approximately 93% accuracy, while the ensemble model\nachieves an accuracy of > 95%. We also perform an analysis of the Shannon\nentropy of the various models' output distributions to measure classification\nuncertainty. Our results suggest that entropy is useful for predicting model\nmisclassifications.\n","authors":["Graham West","Matthew I. Swindall","Ben Keener","Timothy Player","Alex C. Williams","James H. Brusuelas","John F. Wallin"],"pdf_url":"https://arxiv.org/pdf/2210.16380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.03313v2","updated":"2023-03-17T01:06:57Z","published":"2022-08-05T17:59:06Z","title":"A Non-Asymptotic Framework for Approximate Message Passing in Spiked\n  Models","summary":"  Approximate message passing (AMP) emerges as an effective iterative paradigm\nfor solving high-dimensional statistical problems. However, prior AMP theory --\nwhich focused mostly on high-dimensional asymptotics -- fell short of\npredicting the AMP dynamics when the number of iterations surpasses\n$o\\big(\\frac{\\log n}{\\log\\log n}\\big)$ (with $n$ the problem dimension). To\naddress this inadequacy, this paper develops a non-asymptotic framework for\nunderstanding AMP in spiked matrix estimation. Built upon new decomposition of\nAMP updates and controllable residual terms, we lay out an analysis recipe to\ncharacterize the finite-sample behavior of AMP in the presence of an\nindependent initialization, which is further generalized to allow for spectral\ninitialization. As two concrete consequences of the proposed analysis recipe:\n(i) when solving $\\mathbb{Z}_2$ synchronization, we predict the behavior of\nspectrally initialized AMP for up to $O\\big(\\frac{n}{\\mathrm{poly}\\log n}\\big)$\niterations, showing that the algorithm succeeds without the need of a\nsubsequent refinement stage (as conjectured recently by\n\\citet{celentano2021local}); (ii) we characterize the non-asymptotic behavior\nof AMP in sparse PCA (in the spiked Wigner model) for a broad range of\nsignal-to-noise ratio.\n","authors":["Gen Li","Yuting Wei"],"pdf_url":"https://arxiv.org/pdf/2208.03313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12353v3","updated":"2023-03-17T00:52:56Z","published":"2022-10-22T05:04:54Z","title":"Leveraging Large Language Models for Multiple Choice Question Answering","summary":"  While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.\n","authors":["Joshua Robinson","Christopher Michael Rytting","David Wingate"],"pdf_url":"https://arxiv.org/pdf/2210.12353v3.pdf","comment":"Accepted for ICLR 2023"},{"id":"http://arxiv.org/abs/2205.11459v3","updated":"2023-03-17T00:49:25Z","published":"2022-05-23T16:53:22Z","title":"CELEST: Federated Learning for Globally Coordinated Threat Detection","summary":"  The cyber-threat landscape has evolved tremendously in recent years, with new\nthreat variants emerging daily, and large-scale coordinated campaigns becoming\nmore prevalent. In this study, we propose CELEST (CollaborativE LEarning for\nScalable Threat detection, a federated machine learning framework for global\nthreat detection over HTTP, which is one of the most commonly used protocols\nfor malware dissemination and communication. CELEST leverages federated\nlearning in order to collaboratively train a global model across multiple\nclients who keep their data locally, thus providing increased privacy and\nconfidentiality assurances. Through a novel active learning component\nintegrated with the federated learning technique, our system continuously\ndiscovers and learns the behavior of new, evolving, and globally-coordinated\ncyber threats. We show that CELEST is able to expose attacks that are largely\ninvisible to individual organizations. For instance, in one challenging attack\nscenario with data exfiltration malware, the global model achieves a three-fold\nincrease in Precision-Recall AUC compared to the local model. We also design a\npoisoning detection and mitigation method, DTrust, specifically designed for\nfederated learning in the collaborative threat detection domain. DTrust\nsuccessfully detects poisoning clients using the feedback from participating\nclients to investigate and remove them from the training process. We deploy\nCELEST on two university networks and show that it is able to detect the\nmalicious HTTP communication with high precision and low false positive rates.\nFurthermore, during its deployment, CELEST detected a set of previously unknown\n42 malicious URLs and 20 malicious domains in one day, which were confirmed to\nbe malicious by VirusTotal.\n","authors":["Talha Ongun","Simona Boboila","Alina Oprea","Tina Eliassi-Rad","Jason Hiser","Jack Davidson"],"pdf_url":"https://arxiv.org/pdf/2205.11459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06280v2","updated":"2023-03-17T00:43:03Z","published":"2023-03-11T02:10:21Z","title":"Investigating Stateful Defenses Against Black-Box Adversarial Examples","summary":"  Defending machine-learning (ML) models against white-box adversarial attacks\nhas proven to be extremely difficult. Instead, recent work has proposed\nstateful defenses in an attempt to defend against a more restricted black-box\nattacker. These defenses operate by tracking a history of incoming model\nqueries, and rejecting those that are suspiciously similar. The current\nstate-of-the-art stateful defense Blacklight was proposed at USENIX Security\n'22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and\nImageNet datasets. In this paper, we observe that an attacker can significantly\nreduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to\n6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box\nattack. Motivated by this surprising observation, since existing attacks were\nevaluated by the Blacklight authors, we provide a systematization of stateful\ndefenses to understand why existing stateful defense models fail. Finally, we\npropose a stronger evaluation strategy for stateful defenses comprised of\nadaptive score and hard-label based black-box attacks. We use these attacks to\nsuccessfully reduce even reconfigured versions of Blacklight to as low as 0%\nrobust accuracy.\n","authors":["Ryan Feng","Ashish Hooda","Neal Mangaokar","Kassem Fawaz","Somesh Jha","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2303.06280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13476v5","updated":"2023-03-17T00:41:23Z","published":"2022-09-27T15:50:31Z","title":"Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with\n  Extremely Limited Labels","summary":"  Recent studies on contrastive learning have achieved remarkable performance\nsolely by leveraging few labels in the context of medical image segmentation.\nExisting methods mainly focus on instance discrimination and invariant mapping.\nHowever, they face three common pitfalls: (1) tailness: medical image data\nusually follows an implicit long-tail class distribution. Blindly leveraging\nall pixels in training hence can lead to the data imbalance issues, and cause\ndeteriorated performance; (2) consistency: it remains unclear whether a\nsegmentation model has learned meaningful and yet consistent anatomical\nfeatures due to the intra-class variations between different anatomical\nfeatures; and (3) diversity: the intra-slice correlations within the entire\ndataset have received significantly less attention. This motivates us to seek a\nprincipled approach for strategically making use of the dataset itself to\ndiscover similar yet distinct samples from different anatomical views. In this\npaper, we introduce a novel semi-supervised 2D medical image segmentation\nframework termed Mine yOur owN Anatomy (MONA), and make three contributions.\nFirst, prior work argues that every pixel equally matters to the model\ntraining; we observe empirically that this alone is unlikely to define\nmeaningful anatomical features, mainly due to lacking the supervision signal.\nWe show two simple solutions towards learning invariances - through the use of\nstronger data augmentations and nearest neighbors. Second, we construct a set\nof objectives that encourage the model to be capable of decomposing medical\nimages into a collection of anatomical features in an unsupervised manner.\nLastly, we both empirically and theoretically, demonstrate the efficacy of our\nMONA on three benchmark datasets with different labeled settings, achieving new\nstate-of-the-art under different labeled semi-supervised settings\n","authors":["Chenyu You","Weicheng Dai","Fenglin Liu","Yifei Min","Haoran Su","Xiaoran Zhang","Xiaoxiao Li","David A. Clifton","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2209.13476v5.pdf","comment":"In this version: Add theoretical analysis and correct some typos"},{"id":"http://arxiv.org/abs/2303.09705v1","updated":"2023-03-17T00:27:55Z","published":"2023-03-17T00:27:55Z","title":"Batch Updating of a Posterior Tree Distribution over a Meta-Tree","summary":"  Previously, we proposed a probabilistic data generation model represented by\nan unobservable tree and a sequential updating method to calculate a posterior\ndistribution over a set of trees. The set is called a meta-tree. In this paper,\nwe propose a more efficient batch updating method.\n","authors":["Yuta Nakahara","Toshiyasu Matsushima"],"pdf_url":"https://arxiv.org/pdf/2303.09705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09703v1","updated":"2023-03-17T00:24:28Z","published":"2023-03-17T00:24:28Z","title":"A Bi-LSTM Autoencoder Framework for Anomaly Detection -- A Case Study of\n  a Wind Power Dataset","summary":"  Anomalies refer to data points or events that deviate from normal and\nhomogeneous events, which can include fraudulent activities, network\ninfiltrations, equipment malfunctions, process changes, or other significant\nbut infrequent events. Prompt detection of such events can prevent potential\nlosses in terms of finances, information, and human resources. With the\nadvancement of computational capabilities and the availability of large\ndatasets, anomaly detection has become a major area of research. Among these,\nanomaly detection in time series has gained more attention recently due to the\nadded complexity imposed by the time dimension. This study presents a novel\nframework for time series anomaly detection using a combination of\nBidirectional Long Short Term Memory (Bi-LSTM) architecture and Autoencoder.\nThe Bi-LSTM network, which comprises two unidirectional LSTM networks, can\nanalyze the time series data from both directions and thus effectively discover\nthe long-term dependencies hidden in the sequential data. Meanwhile, the\nAutoencoder mechanism helps to establish the optimal threshold beyond which an\nevent can be classified as an anomaly. To demonstrate the effectiveness of the\nproposed framework, it is applied to a real-world multivariate time series\ndataset collected from a wind farm. The Bi-LSTM Autoencoder model achieved a\nclassification accuracy of 96.79% and outperformed more commonly used LSTM\nAutoencoder models.\n","authors":["Ahmed Shoyeb Raihan","Imtiaz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.09703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09700v1","updated":"2023-03-17T00:09:19Z","published":"2023-03-17T00:09:19Z","title":"Delayed and Indirect Impacts of Link Recommendations","summary":"  The impacts of link recommendations on social networks are challenging to\nevaluate, and so far they have been studied in limited settings. Observational\nstudies are restricted in the kinds of causal questions they can answer and\nnaive A/B tests often lead to biased evaluations due to unaccounted network\ninterference. Furthermore, evaluations in simulation settings are often limited\nto static network models that do not take into account the potential feedback\nloops between link recommendation and organic network evolution. To this end,\nwe study the impacts of recommendations on social networks in dynamic settings.\nAdopting a simulation-based approach, we consider an explicit dynamic formation\nmodel -- an extension of the celebrated Jackson-Rogers model -- and investigate\nhow link recommendations affect network evolution over time. Empirically, we\nfind that link recommendations have surprising delayed and indirect effects on\nthe structural properties of networks. Specifically, we find that link\nrecommendations can exhibit considerably different impacts in the immediate\nterm and in the long term. For instance, we observe that friend-of-friend\nrecommendations can have an immediate effect in decreasing degree inequality,\nbut in the long term, they can make the degree distribution substantially more\nunequal. Moreover, we show that the effects of recommendations can persist in\nnetworks, in part due to their indirect impacts on natural dynamics even after\nrecommendations are turned off. We show that, in counterfactual simulations,\nremoving the indirect effects of link recommendations can make the network\ntrend faster toward what it would have been under natural growth dynamics.\n","authors":["Han Zhang","Shangen Lu","Yixin Wang","Mihaela Curmei"],"pdf_url":"https://arxiv.org/pdf/2303.09700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08213v2","updated":"2023-03-17T00:00:54Z","published":"2022-03-15T19:26:29Z","title":"HUMUS-Net: Hybrid unrolled multi-scale network architecture for\n  accelerated MRI reconstruction","summary":"  In accelerated MRI reconstruction, the anatomy of a patient is recovered from\na set of under-sampled and noisy measurements. Deep learning approaches have\nbeen proven to be successful in solving this ill-posed inverse problem and are\ncapable of producing very high quality reconstructions. However, current\narchitectures heavily rely on convolutions, that are content-independent and\nhave difficulties modeling long-range dependencies in images. Recently,\nTransformers, the workhorse of contemporary natural language processing, have\nemerged as powerful building blocks for a multitude of vision tasks. These\nmodels split input images into non-overlapping patches, embed the patches into\nlower-dimensional tokens and utilize a self-attention mechanism that does not\nsuffer from the aforementioned weaknesses of convolutional architectures.\nHowever, Transformers incur extremely high compute and memory cost when 1) the\ninput image resolution is high and 2) when the image needs to be split into a\nlarge number of patches to preserve fine detail information, both of which are\ntypical in low-level vision problems such as MRI reconstruction, having a\ncompounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid\narchitecture that combines the beneficial implicit bias and efficiency of\nconvolutions with the power of Transformer blocks in an unrolled and\nmulti-scale network. HUMUS-Net extracts high-resolution features via\nconvolutional blocks and refines low-resolution features via a novel\nTransformer-based multi-scale feature extractor. Features from both levels are\nthen synthesized into a high-resolution output reconstruction. Our network\nestablishes new state of the art on the largest publicly available MRI dataset,\nthe fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two\nother popular MRI datasets and perform fine-grained ablation studies to\nvalidate our design.\n","authors":["Zalan Fabian","Berk Tinaz","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2203.08213v2.pdf","comment":"18 pages, 11 figures, NeurIPS 2022"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.10056v1","updated":"2023-03-17T15:37:07Z","published":"2023-03-17T15:37:07Z","title":"GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation","summary":"  Text-to-image (T2I) models based on diffusion processes have achieved\nremarkable success in controllable image generation using user-provided\ncaptions. However, the tight coupling between the current text encoder and\nimage decoder in T2I models makes it challenging to replace or upgrade. Such\nchanges often require massive fine-tuning or even training from scratch with\nthe prohibitive expense. To address this problem, we propose GlueGen, which\napplies a newly proposed GlueNet model to align features from single-modal or\nmulti-modal encoders with the latent space of an existing T2I model. The\napproach introduces a new training objective that leverages parallel corpora to\nalign the representation spaces of different encoders. Empirical results show\nthat GlueNet can be trained efficiently and enables various capabilities beyond\nprevious state-of-the-art models: 1) multilingual language models such as\nXLM-Roberta can be aligned with existing T2I models, allowing for the\ngeneration of high-quality images from captions beyond English; 2) GlueNet can\nalign multi-modal encoders such as AudioCLIP with the Stable Diffusion model,\nenabling sound-to-image generation; 3) it can also upgrade the current text\nencoder of the latent diffusion model for challenging case generation. By the\nalignment of various feature representations, the GlueNet allows for flexible\nand efficient integration of new functionality into existing T2I models and\nsheds light on X-to-image (X2I) generation.\n","authors":["Can Qin","Ning Yu","Chen Xing","Shu Zhang","Zeyuan Chen","Stefano Ermon","Yun Fu","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.10056v1.pdf","comment":"26 pages, 23 figures"},{"id":"http://arxiv.org/abs/2303.09858v1","updated":"2023-03-17T09:37:41Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zha","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09818v1","updated":"2023-03-17T07:54:47Z","published":"2023-03-17T07:54:47Z","title":"A real-time blind quality-of-experience assessment metric for HTTP\n  adaptive streaming","summary":"  In today's Internet, HTTP Adaptive Streaming (HAS) is the mainstream standard\nfor video streaming, which switches the bitrate of the video content based on\nan Adaptive BitRate (ABR) algorithm. An effective Quality of Experience (QoE)\nassessment metric can provide crucial feedback to an ABR algorithm. However,\npredicting such real-time QoE on the client side is challenging. The QoE\nprediction requires high consistency with the Human Visual System (HVS), low\nlatency, and blind assessment, which are difficult to realize together. To\naddress this challenge, we analyzed various characteristics of HAS systems and\npropose a non-uniform sampling metric to reduce time complexity. Furthermore,\nwe design an effective QoE metric that integrates resolution and rebuffering\ntime as the Quality of Service (QoS), as well as spatiotemporal output from a\ndeep neural network and specific switching events as content information. These\nreward and penalty features are regressed into quality scores with a Support\nVector Regression (SVR) model. Experimental results show that the accuracy of\nour metric outperforms the mainstream blind QoE metrics by 0.3, and its\ncomputing time is only 60\\% of the video playback, indicating that the proposed\nmetric is capable of providing real-time guidance to ABR algorithms and\nimproving the overall performance of HAS.\n","authors":["Chunyi Li","May Lim","Abdelhak Bentaleb","Roger Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2303.09818v1.pdf","comment":"6 pages,4 figures"},{"id":"http://arxiv.org/abs/2303.09695v1","updated":"2023-03-17T00:03:38Z","published":"2023-03-17T00:03:38Z","title":"PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point\n  Clouds","summary":"  Garment pattern design aims to convert a 3D garment to the corresponding 2D\npanels and their sewing structure. Existing methods rely either on template\nfitting with heuristics and prior assumptions, or on model learning with\ncomplicated shape parameterization. Importantly, both approaches do not allow\nfor personalization of the output garment, which today has increasing demands.\nTo fill this demand, we introduce PersonalTailor: a personalized 2D pattern\ndesign method, where the user can input specific constraints or demands (in\nlanguage or sketch) for personal 2D panel fabrication from 3D point clouds.\nPersonalTailor first learns a multi-modal panel embeddings based on\nunsupervised cross-modal association and attentive fusion. It then predicts a\nbinary panel masks individually using a transformer encoder-decoder framework.\nExtensive experiments show that our PersonalTailor excels on both personalized\nand standard pattern fabrication tasks.\n","authors":["Anran Qi","Sauradip Nag","Xiatian Zhu","Ariel Shamir"],"pdf_url":"https://arxiv.org/pdf/2303.09695v1.pdf","comment":"Technical Report"}]},"2023-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.11331v1","updated":"2023-03-20T17:59:59Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v1.pdf","comment":"To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.11315v1","updated":"2023-03-20T17:54:58Z","published":"2023-03-20T17:54:58Z","title":"Context-faithful Prompting for Large Language Models","summary":"  Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts.\n","authors":["Wenxuan Zhou","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11315v1.pdf","comment":"Code and data will be released at\n  https://github.com/wzhouad/context-faithful-llm"},{"id":"http://arxiv.org/abs/2303.11192v1","updated":"2023-03-20T15:22:11Z","published":"2023-03-20T15:22:11Z","title":"Multimodal Shannon Game with Images","summary":"  The Shannon game has long been used as a thought experiment in linguistics\nand NLP, asking participants to guess the next letter in a sentence based on\nits preceding context. We extend the game by introducing an optional extra\nmodality in the form of image information. To investigate the impact of\nmultimodal information in this game, we use human participants and a language\nmodel (LM, GPT-2). We show that the addition of image information improves both\nself-reported confidence and accuracy for both humans and LM. Certain word\nclasses, such as nouns and determiners, benefit more from the additional\nmodality information. The priming effect in both humans and the LM becomes more\napparent as the context size (extra modality information + sentence context)\nincreases. These findings highlight the potential of multimodal information in\nimproving language understanding and modeling.\n","authors":["Vilém Zouhar","Sunit Bhattacharya","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2303.11192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11184v1","updated":"2023-03-20T15:10:45Z","published":"2023-03-20T15:10:45Z","title":"Conversation Modeling to Predict Derailment","summary":"  Conversations among online users sometimes derail, i.e., break down into\npersonal attacks. Such derailment has a negative impact on the healthy growth\nof cyberspace communities. The ability to predict whether ongoing conversations\nare likely to derail could provide valuable real-time insight to interlocutors\nand moderators. Prior approaches predict conversation derailment\nretrospectively without the ability to forestall the derailment proactively.\nSome works attempt to make dynamic prediction as the conversation develops, but\nfail to incorporate multisource information, such as conversation structure and\ndistance to derailment.\n  We propose a hierarchical transformer-based framework that combines\nutterance-level and conversation-level information to capture fine-grained\ncontextual semantics. We propose a domain-adaptive pretraining objective to\nintegrate conversational structure information and a multitask learning scheme\nto leverage the distance from each utterance to derailment. An evaluation of\nour framework on two conversation derailment datasets yields improvement over\nF1 score for the prediction of derailment. These results demonstrate the\neffectiveness of incorporating multisource information.\n","authors":["Jiaqing Yuan","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2303.11184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11141v1","updated":"2023-03-20T14:19:58Z","published":"2023-03-20T14:19:58Z","title":"DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction\n  Dataset","summary":"  Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.\n","authors":["Hongbo Wang","Weimin Xiong","Yifan Song","Dawei Zhu","Yu Xia","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2303.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11131v1","updated":"2023-03-20T14:07:13Z","published":"2023-03-20T14:07:13Z","title":"Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture\n  and Single-Source Speech","summary":"  Self-supervised learning leverages unlabeled data effectively, improving\nlabel efficiency and generalization to domains without labeled data. While\nrecent work has studied generalization to more acoustic/linguistic domains,\nlanguages, and modalities, these investigations are limited to single-source\nspeech with one primary speaker in the recording. This paper presents Cocktail\nHuBERT, a self-supervised learning framework that generalizes to mixture speech\nusing a masked pseudo source separation objective. This objective encourages\nthe model to identify the number of sources, separate and understand the\ncontext, and infer the content of masked regions represented as discovered\nunits. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER\non multi-speaker ASR, 31% lower DER on diarization, and is competitive on\nsingle- and multi-speaker tasks from SUPERB.\n","authors":["Maryam Fazel-Zarandi","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.11131v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11117v1","updated":"2023-03-20T13:58:35Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling\n  for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. However, previous approaches to modeling\nglobal and local context dependencies lost the diversity of dependency\ninformation and do not take the context dependency into account at the\nclassification level. In this paper, we propose a novel approach to dependency\nmodeling driven by Emotional Inertia and Contagion (EmotionIC) for\nconversational emotion recognition at the feature extraction and classification\nlevels. At the feature extraction level, our designed Identity Masked\nMulti-head Attention (IM-MHA) captures the identity-based long-distant context\nin the dialogue to contain the diverse influence of different participants and\nconstruct the global emotional atmosphere, while the devised Dialogue-based\nGate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of\ndyadic dialogue is applied to refine the contextual features with inter- and\nintra-speaker dependencies. At the classification level, by introducing skip\nconnections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF\n(SkipCRF) to capture the high-order dependencies within and between speakers,\nand to emulate the emotional flow of distant participants. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Liu Yingjian","Li Jiang","Wang Xiaoping","Zeng Zhigang"],"pdf_url":"https://arxiv.org/pdf/2303.11117v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2110.04845v4","updated":"2023-03-20T13:34:47Z","published":"2021-10-10T16:23:54Z","title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset\n  and Empirical Study","summary":"  The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n  Our dataset, data statement, and annotation questionnaire can be found at:\nhttps://doi.org/10.5281/zenodo.7599667\n","authors":["Mohamed Abdalla","Krishnapriya Vishnubhotla","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2110.04845v4.pdf","comment":"Accepted to EACL 2023; Our dataset, data statement, and annotation\n  questionnaire can be found at: https://doi.org/10.5281/zenodo.7599667"},{"id":"http://arxiv.org/abs/2303.11082v1","updated":"2023-03-20T13:14:59Z","published":"2023-03-20T13:14:59Z","title":"Evaluating Language Models for Knowledge Base Completion","summary":"  Structured knowledge bases (KBs) are a foundation of many intelligent\napplications, yet are notoriously incomplete. Language models (LMs) have\nrecently been proposed for unsupervised knowledge base completion (KBC), yet,\ndespite encouraging initial results, questions regarding their suitability\nremain open. Existing evaluations often fall short because they only evaluate\non popular subjects, or sample already existing facts from KBs. In this work,\nwe introduce a novel, more challenging benchmark dataset, and a methodology\ntailored for a realistic assessment of the KBC potential of LMs. For automated\nassessment, we curate a dataset called WD-KNOWN, which provides an unbiased\nrandom sample of Wikidata, containing over 3.9 million facts. In a second step,\nwe perform a human evaluation on predictions that are not yet in the KB, as\nonly this provides real insights into the added value over existing KBs. Our\nkey finding is that biases in dataset conception of previous benchmarks lead to\na systematic overestimate of LM performance for KBC. However, our results also\nreveal strong areas of LMs. We could, for example, perform a significant\ncompletion of Wikidata on the relations nativeLanguage, by a factor of ~21\n(from 260k to 5.8M) at 82% precision, usedLanguage, by a factor of ~2.1 (from\n2.1M to 6.6M) at 82% precision, and citizenOf by a factor of ~0.3 (from 4.2M to\n5.3M) at 90% precision. Moreover, we find that LMs possess surprisingly strong\ngeneralization capabilities: even on relations where most facts were not\ndirectly observed in LM training, prediction quality can be high.\n","authors":["Blerta Veseli","Sneha Singhania","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2303.11082v1.pdf","comment":"Data and code available at https://github.com/bveseli/LMsForKBC"},{"id":"http://arxiv.org/abs/2302.13007v3","updated":"2023-03-20T11:39:47Z","published":"2023-02-25T06:58:16Z","title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation","summary":"  Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.\n","authors":["Haixing Dai","Zhengliang Liu","Wenxiong Liao","Xiaoke Huang","Yihan Cao","Zihao Wu","Lin Zhao","Shaochen Xu","Wei Liu","Ninghao Liu","Sheng Li","Dajiang Zhu","Hongmin Cai","Lichao Sun","Quanzheng Li","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2302.13007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11032v1","updated":"2023-03-20T11:34:37Z","published":"2023-03-20T11:34:37Z","title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4","summary":"  The digitization of healthcare has facilitated the sharing and re-using of\nmedical data but has also raised concerns about confidentiality and privacy.\nHIPAA (Health Insurance Portability and Accountability Act) mandates removing\nre-identifying information before the dissemination of medical records. Thus,\neffective and efficient solutions for de-identifying medical data, especially\nthose in free-text forms, are highly needed. While various computer-assisted\nde-identification methods, including both rule-based and learning-based, have\nbeen developed and used in prior practice, such solutions still lack\ngeneralizability or need to be fine-tuned according to different scenarios,\nsignificantly imposing restrictions in wider use. The advancement of large\nlanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential in\nprocessing text data in the medical domain with zero-shot in-context learning,\nespecially in the task of privacy protection, as these models can identify\nconfidential information by their powerful named entity recognition (NER)\ncapability. In this work, we developed a novel GPT4-enabled de-identification\nframework (\"DeID-GPT\") to automatically identify and remove the identifying\ninformation. Compared to existing commonly used medical text data\nde-identification methods, our developed DeID-GPT showed the highest accuracy\nand remarkable reliability in masking private information from the unstructured\nmedical text while preserving the original structure and meaning of the text.\nThis study is one of the earliest to utilize ChatGPT and GPT-4 for medical text\ndata processing and de-identification, which provides insights for further\nresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 in\nhealthcare. Codes and benchmarking data information are available at\nhttps://github.com/yhydhx/ChatGPT-API.\n","authors":["Zhengliang Liu","Xiaowei Yu","Lu Zhang","Zihao Wu","Chao Cao","Haixing Dai","Lin Zhao","Wei Liu","Dinggang Shen","Quanzheng Li","Tianming Liu","Dajiang Zhu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2303.11032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01685v2","updated":"2023-03-20T10:11:41Z","published":"2022-06-03T17:01:46Z","title":"Toward a realistic model of speech processing in the brain with\n  self-supervised learning","summary":"  Several deep neural networks have recently been shown to generate activations\nsimilar to those of the brain in response to the same input. These algorithms,\nhowever, remain largely implausible: they require (1) extraordinarily large\namounts of data, (2) unobtainable supervised labels, (3) textual rather than\nraw sensory input, and / or (4) implausibly large memory (e.g. thousands of\ncontextual words). These elements highlight the need to identify algorithms\nthat, under these limitations, would suffice to account for both behavioral and\nbrain responses. Focusing on the issue of speech processing, we here\nhypothesize that self-supervised algorithms trained on the raw waveform\nconstitute a promising candidate. Specifically, we compare a recent\nself-supervised architecture, Wav2Vec 2.0, to the brain activity of 412\nEnglish, French, and Mandarin individuals recorded with functional Magnetic\nResonance Imaging (fMRI), while they listened to ~1h of audio books. Our\nresults are four-fold. First, we show that this algorithm learns brain-like\nrepresentations with as little as 600 hours of unlabelled speech -- a quantity\ncomparable to what infants can be exposed to during language acquisition.\nSecond, its functional hierarchy aligns with the cortical hierarchy of speech\nprocessing. Third, different training regimes reveal a functional\nspecialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,\nspeech-specific and language-specific representations similar to those of the\nprefrontal and temporal cortices. Fourth, we confirm the similarity of this\nspecialization with the behavior of 386 additional participants. These\nelements, resulting from the largest neuroimaging benchmark to date, show how\nself-supervised learning can account for a rich organization of speech\nprocessing in the brain, and thus delineate a path to identify the laws of\nlanguage acquisition which shape the human brain.\n","authors":["Juliette Millet","Charlotte Caucheteux","Pierre Orhan","Yves Boubenec","Alexandre Gramfort","Ewan Dunbar","Christophe Pallier","Jean-Remi King"],"pdf_url":"https://arxiv.org/pdf/2206.01685v2.pdf","comment":"Accepted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.10974v1","updated":"2023-03-20T09:52:52Z","published":"2023-03-20T09:52:52Z","title":"Translate your gibberish: black-box adversarial attack on machine\n  translation systems","summary":"  Neural networks are deployed widely in natural language processing tasks on\nthe industrial scale, and perhaps the most often they are used as compounds of\nautomatic machine translation systems. In this work, we present a simple\napproach to fool state-of-the-art machine translation tools in the task of\ntranslation from Russian to English and vice versa. Using a novel black-box\ngradient-free tensor-based optimizer, we show that many online translation\ntools, such as Google, DeepL, and Yandex, may both produce wrong or offensive\ntranslations for nonsensical adversarial input queries and refuse to translate\nseemingly benign input phrases. This vulnerability may interfere with\nunderstanding a new language and simply worsen the user's experience while\nusing machine translation systems, and, hence, additional improvements of these\ntools are required to establish better translation.\n","authors":["Andrei Chertkov","Olga Tsymboi","Mikhail Pautov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2303.10974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10966v1","updated":"2023-03-20T09:41:28Z","published":"2023-03-20T09:41:28Z","title":"Towards Reliable Neural Machine Translation with Consistency-Aware\n  Meta-Learning","summary":"  Neural machine translation (NMT) has achieved remarkable success in producing\nhigh-quality translations. However, current NMT systems suffer from a lack of\nreliability, as their outputs that are often affected by lexical or syntactic\nchanges in inputs, resulting in large variations in quality. This limitation\nhinders the practicality and trustworthiness of NMT. A contributing factor to\nthis problem is that NMT models trained with the one-to-one paradigm struggle\nto handle the source diversity phenomenon, where inputs with the same meaning\ncan be expressed differently. In this work, we treat this problem as a bilevel\noptimization problem and present a consistency-aware meta-learning (CAML)\nframework derived from the model-agnostic meta-learning (MAML) algorithm to\naddress it. Specifically, the NMT model with CAML (named CoNMT) first learns a\nconsistent meta representation of semantically equivalent sentences in the\nouter loop. Subsequently, a mapping from the meta representation to the output\nsentence is learned in the inner loop, allowing the NMT model to translate\nsemantically equivalent sentences to the same target sentence. We conduct\nexperiments on the NIST Chinese to English task, three WMT translation tasks,\nand the TED M2O task. The results demonstrate that CoNMT effectively improves\noverall translation quality and reliably handles diverse inputs.\n","authors":["Rongxiang Weng","Qiang Wang","Wensen Cheng","Changfeng Zhu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10949v1","updated":"2023-03-20T09:13:27Z","published":"2023-03-20T09:13:27Z","title":"Code-Switching Text Generation and Injection in Mandarin-English ASR","summary":"  Code-switching speech refers to a means of expression by mixing two or more\nlanguages within a single utterance. Automatic Speech Recognition (ASR) with\nEnd-to-End (E2E) modeling for such speech can be a challenging task due to the\nlack of data. In this study, we investigate text generation and injection for\nimproving the performance of an industry commonly-used streaming model,\nTransformer-Transducer (T-T), in Mandarin-English code-switching speech\nrecognition. We first propose a strategy to generate code-switching text data\nand then investigate injecting generated text into T-T model explicitly by\nText-To-Speech (TTS) conversion or implicitly by tying speech and text latent\nspaces. Experimental results on the T-T model trained with a dataset containing\n1,800 hours of real Mandarin-English code-switched speech show that our\napproaches to inject generated code-switching text significantly boost the\nperformance of T-T models, i.e., 16% relative Token-based Error Rate (TER)\nreduction averaged on three evaluation sets, and the approach of tying speech\nand text latent spaces is superior to that of TTS conversion on the evaluation\nset which contains more homogeneous data with the training set.\n","authors":["Haibin Yu","Yuxuan Hu","Yao Qian","Ma Jin","Linquan Liu","Shujie Liu","Yu Shi","Yanmin Qian","Edward Lin","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.10949v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07205v2","updated":"2023-03-20T08:59:41Z","published":"2023-02-04T04:49:17Z","title":"The Science of Detecting LLM-Generated Texts","summary":"  The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.\n","authors":["Ruixiang Tang","Yu-Neng Chuang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10942v1","updated":"2023-03-20T08:54:40Z","published":"2023-03-20T08:54:40Z","title":"On-the-fly Text Retrieval for End-to-End ASR Adaptation","summary":"  End-to-end speech recognition models are improved by incorporating external\ntext sources, typically by fusion with an external language model. Such\nlanguage models have to be retrained whenever the corpus of interest changes.\nFurthermore, since they store the entire corpus in their parameters, rare words\ncan be challenging to recall. In this work, we propose augmenting a\ntransducer-based ASR model with a retrieval language model, which directly\nretrieves from an external text corpus plausible completions for a partial ASR\nhypothesis. These completions are then integrated into subsequent predictions\nby an adapter, which is trained once, so that the corpus of interest can be\nswitched without incurring the computational overhead of retraining. Our\nexperiments show that the proposed model significantly improves the performance\nof a transducer baseline on a pair of question-answering datasets. Further, it\noutperforms shallow fusion on recognition of named entities by about 7\nrelative; when the two are combined, the relative improvement increases to 13%.\n","authors":["Bolaji Yusuf","Aditya Gourav","Ankur Gandhe","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2303.10942v1.pdf","comment":"Accepted to ICASSP 2023; Appendix added to include ablations that\n  could not fit into the conference 4-page limit"},{"id":"http://arxiv.org/abs/2301.06323v2","updated":"2023-03-20T08:37:45Z","published":"2023-01-16T09:27:45Z","title":"An Error-Guided Correction Model for Chinese Spelling Error Correction","summary":"  Although existing neural network approaches have achieved great success on\nChinese spelling correction, there is still room to improve. The model is\nrequired to avoid over-correction and to distinguish a correct token from its\nphonological and visually similar ones. In this paper, we propose an\nerror-guided correction model (EGCM) to improve Chinese spelling correction. By\nborrowing the powerful ability of BERT, we propose a novel zero-shot error\ndetection method to do a preliminary detection, which guides our model to\nattend more on the probably wrong tokens in encoding and to avoid modifying the\ncorrect tokens in generating. Furthermore, we introduce a new loss function to\nintegrate the error confusion set, which enables our model to distinguish\neasily misused tokens. Moreover, our model supports highly parallel decoding to\nmeet real application requirements. Experiments are conducted on widely used\nbenchmarks. Our model achieves superior performance against state-of-the-art\napproaches by a remarkable margin, on both the correction quality and\ncomputation speed.\n","authors":["Rui Sun","Xiuyu Wu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2301.06323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06924v4","updated":"2023-03-20T08:33:18Z","published":"2022-06-14T15:43:44Z","title":"The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity","summary":"  A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n","authors":["Lluís Alemany-Puig","Juan Luis Esteban","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2206.06924v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05405v4","updated":"2023-03-20T08:29:29Z","published":"2022-11-10T08:19:44Z","title":"VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning","summary":"  Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n","authors":["Nghia Hieu Nguyen","Duong T. D. Vo","Minh-Quan Ha"],"pdf_url":"https://arxiv.org/pdf/2211.05405v4.pdf","comment":"Accepted for publishing at the VNU Journal of Science: Computer\n  Science and Communication Engineering"},{"id":"http://arxiv.org/abs/2303.10912v1","updated":"2023-03-20T07:09:26Z","published":"2023-03-20T07:09:26Z","title":"Exploring Representation Learning for Small-Footprint Keyword Spotting","summary":"  In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.\n","authors":["Fan Cui","Liyong Guo","Quandong Wang","Peng Gao","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10897v1","updated":"2023-03-20T06:34:22Z","published":"2023-03-20T06:34:22Z","title":"Relate auditory speech to EEG by shallow-deep attention-based network","summary":"  Electroencephalography (EEG) plays a vital role in detecting how brain\nresponses to different stimulus. In this paper, we propose a novel Shallow-Deep\nAttention-based Network (SDANet) to classify the correct auditory stimulus\nevoking the EEG signal. It adopts the Attention-based Correlation Module (ACM)\nto discover the connection between auditory speech and EEG from global aspect,\nand the Shallow-Deep Similarity Classification Module (SDSCM) to decide the\nclassification result via the embeddings learned from the shallow and deep\nlayers. Moreover, various training strategies and data augmentation are used to\nboost the model robustness. Experiments are conducted on the dataset provided\nby Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023).\nResults show that the proposed model has a significant gain over the baseline\non the match-mismatch track.\n","authors":["Fan Cui","Liyong Guo","Lang He","Jiyao Liu","ErCheng Pei","Yujun Wang","Dongmei Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.10897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10893v1","updated":"2023-03-20T06:20:03Z","published":"2023-03-20T06:20:03Z","title":"Character, Word, or Both? Revisiting the Segmentation Granularity for\n  Chinese Pre-trained Language Models","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.\n","authors":["Xinnian Liang","Zefan Zhou","Hui Huang","Shuangzhi Wu","Tong Xiao","Muyun Yang","Zhoujun Li","Chao Bian"],"pdf_url":"https://arxiv.org/pdf/2303.10893v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.10888v1","updated":"2023-03-20T06:13:03Z","published":"2023-03-20T06:13:03Z","title":"Self-Improving-Leaderboard(SIL): A Call for Real-World Centric Natural\n  Language Processing Leaderboards","summary":"  Leaderboard systems allow researchers to objectively evaluate Natural\nLanguage Processing (NLP) models and are typically used to identify models that\nexhibit superior performance on a given task in a predetermined setting.\nHowever, we argue that evaluation on a given test dataset is just one of many\nperformance indications of the model. In this paper, we claim leaderboard\ncompetitions should also aim to identify models that exhibit the best\nperformance in a real-world setting. We highlight three issues with current\nleaderboard systems: (1) the use of a single, static test set, (2) discrepancy\nbetween testing and real-world application (3) the tendency for\nleaderboard-centric competition to be biased towards the test set. As a\nsolution, we propose a new paradigm of leaderboard systems that addresses these\nissues of current leaderboard system. Through this study, we hope to induce a\nparadigm shift towards more real -world-centric leaderboard competitions.\n","authors":["Chanjun Park","Hyeonseok Moon","Seolhwa Lee","Jaehyung Seo","Sugyeong Eo","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2303.10888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10870v1","updated":"2023-03-20T05:11:22Z","published":"2023-03-20T05:11:22Z","title":"Multi-task Transformer with Relation-attention and Type-attention for\n  Named Entity Recognition","summary":"  Named entity recognition (NER) is an important research problem in natural\nlanguage processing. There are three types of NER tasks, including flat, nested\nand discontinuous entity recognition. Most previous sequential labeling models\nare task-specific, while recent years have witnessed the rising of generative\nmodels due to the advantage of unifying all NER tasks into the seq2seq model\nframework. Although achieving promising performance, our pilot studies\ndemonstrate that existing generative models are ineffective at detecting entity\nboundaries and estimating entity types. This paper proposes a multi-task\nTransformer, which incorporates an entity boundary detection task into the\nnamed entity recognition task. More concretely, we achieve entity boundary\ndetection by classifying the relations between tokens within the sentence. To\nimprove the accuracy of entity-type mapping during decoding, we adopt an\nexternal knowledge base to calculate the prior entity-type distributions and\nthen incorporate the information into the model via the self and\ncross-attention mechanisms. We perform experiments on an extensive set of NER\nbenchmarks, including two flat, three nested, and three discontinuous NER\ndatasets. Experimental results show that our approach considerably improves the\ngenerative NER model's performance.\n","authors":["Ying Mo","Hongyin Tang","Jiahao Liu","Qifan Wang","Zenglin Xu","Jingang Wang","Wei Wu","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2303.10870v1.pdf","comment":"5 pages,accepted ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.10868v1","updated":"2023-03-20T05:07:41Z","published":"2023-03-20T05:07:41Z","title":"Retrieving Multimodal Information for Augmented Generation: A Survey","summary":"  In this survey, we review methods that retrieve multimodal knowledge to\nassist and augment generative models. This group of works focuses on retrieving\ngrounding contexts from external sources, including images, codes, tables,\ngraphs, and audio. As multimodal learning and generative AI have become more\nand more impactful, such retrieval augmentation offers a promising solution to\nimportant concerns such as factuality, reasoning, interpretability, and\nrobustness. We provide an in-depth review of retrieval-augmented generation in\ndifferent modalities and discuss potential future directions. As this is an\nemerging field, we continue to add new papers and methods.\n","authors":["Ruochen Zhao","Hailin Chen","Weishi Wang","Fangkai Jiao","Xuan Long Do","Chengwei Qin","Bosheng Ding","Xiaobao Guo","Minzhi Li","Xingxuan Li","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2303.10868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.10845v1","updated":"2023-03-20T03:39:27Z","published":"2023-03-20T03:39:27Z","title":"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse\n  Heterogeneous Computing","summary":"  The scaling of large language models has greatly improved natural language\nunderstanding, generation, and reasoning. In this work, we develop a system\nthat trained a trillion-parameter language model on a cluster of Ascend 910 AI\nprocessors and MindSpore framework, and present the language model with 1.085T\nparameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha},\nwe extend the dense Transformer model to sparse one with Random Routed Experts\n(RRE), and efficiently train the model over 329B tokens by using Expert\nComputation and Storage Separation(ECSS). This resulted in a 6.3x increase in\ntraining throughput through heterogeneous computing. Our experimental findings\nshow that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot\nlearning of various Chinese NLP downstream tasks. Moreover, it demonstrates\nstrong abilities when fine-tuned in application data of open-domain dialogue,\nquestion answering, machine translation and code generation.\n","authors":["Xiaozhe Ren","Pingyi Zhou","Xinfan Meng","Xinjing Huang","Yadao Wang","Weichao Wang","Pengfei Li","Xiaoda Zhang","Alexander Podolskiy","Grigory Arshinov","Andrey Bout","Irina Piontkovskaya","Jiansheng Wei","Xin Jiang","Teng Su","Qun Liu","Jun Yao"],"pdf_url":"https://arxiv.org/pdf/2303.10845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15042v3","updated":"2023-03-20T01:33:23Z","published":"2022-10-26T21:18:31Z","title":"Privately Fine-Tuning Large Language Models with Differential Privacy","summary":"  Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n","authors":["Rouzbeh Behnia","Mohamamdreza Ebrahimi","Jason Pacheco","Balaji Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2210.15042v3.pdf","comment":"Publised at IEEE ICDM Workshop on Machine Learning for Cybersecurity\n  (MLC) 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.11331v1","updated":"2023-03-20T17:59:59Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v1.pdf","comment":"To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.11330v1","updated":"2023-03-20T17:59:58Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v1.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.11329v1","updated":"2023-03-20T17:59:55Z","published":"2023-03-20T17:59:55Z","title":"Sound Localization from Motion: Jointly Learning Sound Direction and\n  Camera Rotation","summary":"  The images and sounds that we perceive undergo subtle but geometrically\nconsistent changes as we rotate our heads. In this paper, we use these cues to\nsolve a problem we call Sound Localization from Motion (SLfM): jointly\nestimating camera rotation and localizing sound sources. We learn to solve\nthese tasks solely through self-supervision. A visual model predicts camera\nrotation from a pair of images, while an audio model predicts the direction of\nsound sources from binaural sounds. We train these models to generate\npredictions that agree with one another. At test time, the models can be\ndeployed independently. To obtain a feature representation that is well-suited\nto solving this challenging problem, we also propose a method for learning an\naudio-visual representation through cross-view binauralization: estimating\nbinaural sound from one view, given images and sound from another. Our model\ncan successfully estimate accurate rotations on both real and synthetic scenes,\nand localize sound sources with accuracy competitive with state-of-the-art\nself-supervised approaches. Project site: https://ificl.github.io/SLfM/\n","authors":["Ziyang Chen","Shengyi Qian","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2303.11329v1.pdf","comment":"Project site: https://ificl.github.io/SLfM/"},{"id":"http://arxiv.org/abs/2303.11328v1","updated":"2023-03-20T17:59:50Z","published":"2023-03-20T17:59:50Z","title":"Zero-1-to-3: Zero-shot One Image to 3D Object","summary":"  We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an\nobject given just a single RGB image. To perform novel view synthesis in this\nunder-constrained setting, we capitalize on the geometric priors that\nlarge-scale diffusion models learn about natural images. Our conditional\ndiffusion model uses a synthetic dataset to learn controls of the relative\ncamera viewpoint, which allow new images to be generated of the same object\nunder a specified camera transformation. Even though it is trained on a\nsynthetic dataset, our model retains a strong zero-shot generalization ability\nto out-of-distribution datasets as well as in-the-wild images, including\nimpressionist paintings. Our viewpoint-conditioned diffusion approach can\nfurther be used for the task of 3D reconstruction from a single image.\nQualitative and quantitative experiments show that our method significantly\noutperforms state-of-the-art single-view 3D reconstruction and novel view\nsynthesis models by leveraging Internet-scale pre-training.\n","authors":["Ruoshi Liu","Rundi Wu","Basile Van Hoorick","Pavel Tokmakov","Sergey Zakharov","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2303.11328v1.pdf","comment":"Website: https://zero123.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2303.11327v1","updated":"2023-03-20T17:59:49Z","published":"2023-03-20T17:59:49Z","title":"3D Concept Learning and Reasoning from Multi-View Images","summary":"  Humans are able to accurately reason in 3D by gathering multi-view\nobservations of the surrounding world. Inspired by this insight, we introduce a\nnew large-scale benchmark for 3D multi-view visual question answering\n(3DMV-VQA). This dataset is collected by an embodied agent actively moving and\ncapturing RGB images in an environment using the Habitat simulator. In total,\nit consists of approximately 5k scenes, 600k images, paired with 50k questions.\nWe evaluate various state-of-the-art models for visual reasoning on our\nbenchmark and find that they all perform poorly. We suggest that a principled\napproach for 3D reasoning from multi-view images should be to infer a compact\n3D representation of the world from the multi-view images, which is further\ngrounded on open-vocabulary semantic concepts, and then to execute reasoning on\nthese 3D representations. As the first step towards this approach, we propose a\nnovel 3D concept learning and reasoning (3D-CLR) framework that seamlessly\ncombines these components via neural fields, 2D pre-trained vision-language\nmodels, and neural reasoning operators. Experimental results suggest that our\nframework outperforms baseline models by a large margin, but the challenge\nremains largely unsolved. We further perform an in-depth analysis of the\nchallenges and highlight potential future directions.\n","authors":["Yining Hong","Chunru Lin","Yilun Du","Zhenfang Chen","Joshua B. Tenenbaum","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.11327v1.pdf","comment":"CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/"},{"id":"http://arxiv.org/abs/2303.11325v1","updated":"2023-03-20T17:59:03Z","published":"2023-03-20T17:59:03Z","title":"Towards Better 3D Knowledge Transfer via Masked Image Modeling for\n  Multi-view 3D Understanding","summary":"  Multi-view camera-based 3D detection is a challenging problem in computer\nvision. Recent works leverage a pretrained LiDAR detection model to transfer\nknowledge to a camera-based student network. However, we argue that there is a\nmajor domain gap between the LiDAR BEV features and the camera-based BEV\nfeatures, as they have different characteristics and are derived from different\nsources. In this paper, we propose Geometry Enhanced Masked Image Modeling\n(GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune\nparadigm for improving the multi-view camera-based 3D detection. GeoMIM is a\nmulti-camera vision transformer with Cross-View Attention (CVA) blocks that\nuses LiDAR BEV features encoded by the pretrained BEV model as learning\ntargets. During pretraining, GeoMIM's decoder has a semantic branch completing\ndense perspective-view features and the other geometry branch reconstructing\ndense perspective-view depth maps. The depth branch is designed to be\ncamera-aware by inputting the camera's parameters for better transfer\ncapability. Extensive results demonstrate that GeoMIM outperforms existing\nmethods on nuScenes benchmark, achieving state-of-the-art performance for\ncamera-based 3D object detection and 3D segmentation.\n","authors":["Jihao Liu","Tai Wang","Boxiao Liu","Qihang Zhang","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.11325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11324v1","updated":"2023-03-20T17:58:48Z","published":"2023-03-20T17:58:48Z","title":"Open-vocabulary Panoptic Segmentation with Embedding Modulation","summary":"  Open-vocabulary image segmentation is attracting increasing attention due to\nits critical applications in the real world. Traditional closed-vocabulary\nsegmentation methods are not able to characterize novel objects, whereas\nseveral recent open-vocabulary attempts obtain unsatisfactory results, i.e.,\nnotable performance reduction on the closed vocabulary and massive demand for\nextra data. To this end, we propose OPSNet, an omnipotent and data-efficient\nframework for Open-vocabulary Panoptic Segmentation. Specifically, the\nexquisitely designed Embedding Modulation module, together with several\nmeticulous components, enables adequate embedding enhancement and information\nexchange between the segmentation model and the visual-linguistic well-aligned\nCLIP encoder, resulting in superior segmentation performance under both open-\nand closed-vocabulary settings with much fewer need of additional data.\nExtensive experimental evaluations are conducted across multiple datasets\n(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various\ncircumstances, where the proposed OPSNet achieves state-of-the-art results,\nwhich demonstrates the effectiveness and generality of the proposed approach.\nThe code and trained models will be made publicly available.\n","authors":["Xi Chen","Shuang Li","Ser-Nam Lim","Antonio Torralba","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11320v1","updated":"2023-03-20T17:57:03Z","published":"2023-03-20T17:57:03Z","title":"ScribbleSeg: Scribble-based Interactive Image Segmentation","summary":"  Interactive segmentation enables users to extract masks by providing simple\nannotations to indicate the target, such as boxes, clicks, or scribbles. Among\nthese interaction formats, scribbles are the most flexible as they can be of\narbitrary shapes and sizes. This enables scribbles to provide more indications\nof the target object. However, previous works mainly focus on click-based\nconfiguration, and the scribble-based setting is rarely explored. In this work,\nwe attempt to formulate a standard protocol for scribble-based interactive\nsegmentation. Basically, we design diversified strategies to simulate scribbles\nfor training, propose a deterministic scribble generator for evaluation, and\nconstruct a challenging benchmark. Besides, we build a strong framework\nScribbleSeg, consisting of a Prototype Adaption Module(PAM) and a Corrective\nRefine Module (CRM), for the task. Extensive experiments show that ScribbleSeg\nperforms notably better than previous click-based methods. We hope this could\nserve as a more powerful and general solution for interactive segmentation. Our\ncode will be made available.\n","authors":["Xi Chen","Yau Shing Jonathan Cheung","Ser-Nam Lim","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11316v1","updated":"2023-03-20T17:55:37Z","published":"2023-03-20T17:55:37Z","title":"Generative Semantic Segmentation","summary":"  We present Generative Semantic Segmentation (GSS), a generative learning\napproach for semantic segmentation. Uniquely, we cast semantic segmentation as\nan image-conditioned mask generation problem. This is achieved by replacing the\nconventional per-pixel discriminative learning with a latent prior learning\nprocess. Specifically, we model the variational posterior distribution of\nlatent variables given the segmentation mask. To that end, the segmentation\nmask is expressed with a special type of image (dubbed as maskige). This\nposterior distribution allows to generate segmentation masks unconditionally.\nTo achieve semantic segmentation on a given image, we further introduce a\nconditioning network. It is optimized by minimizing the divergence between the\nposterior distribution of maskige (i.e., segmentation masks) and the latent\nprior distribution of input training images. Extensive experiments on standard\nbenchmarks show that our GSS can perform competitively to prior art\nalternatives in the standard semantic segmentation setting, whilst achieving a\nnew state of the art in the more challenging cross-domain setting.\n","authors":["Jiaqi Chen","Jiachen Lu","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11316v1.pdf","comment":"To appear at CVPR2023, code at http://github.com/fudan-zvg/GSS"},{"id":"http://arxiv.org/abs/2303.11313v1","updated":"2023-03-20T17:52:24Z","published":"2023-03-20T17:52:24Z","title":"CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D\n  Recognition","summary":"  Vision-Language models like CLIP have been widely adopted for various tasks\ndue to their impressive zero-shot capabilities. However, CLIP is not suitable\nfor extracting 3D geometric features as it was trained on only images and text\nby natural language supervision. We work on addressing this limitation and\npropose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder is\nlearned to exhibit zero-shot capabilities. CG3D is trained using triplets of\npointclouds, corresponding rendered 2D images, and texts using natural language\nsupervision. To align the features in a multimodal embedding space, we utilize\ncontrastive loss on 3D features obtained from the 3D encoder, as well as visual\nand text features extracted from CLIP. We note that the natural images used to\ntrain CLIP and the rendered 2D images in CG3D have a distribution shift.\nAttempting to train the visual and text encoder to account for this shift\nresults in catastrophic forgetting and a notable decrease in performance. To\nsolve this, we employ prompt tuning and introduce trainable parameters in the\ninput space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.\nWe extensively test our pre-trained CG3D framework and demonstrate its\nimpressive capabilities in zero-shot, open scene understanding, and retrieval\ntasks. Further, it also serves as strong starting weights for fine-tuning in\ndownstream 3D recognition tasks.\n","authors":["Deepti Hegde","Jeya Maria Jose Valanarasu","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.11313v1.pdf","comment":"Website: https://jeya-maria-jose.github.io/cg3d-web/"},{"id":"http://arxiv.org/abs/2207.01614v2","updated":"2023-03-20T17:51:09Z","published":"2022-07-04T17:56:14Z","title":"Beyond mAP: Towards better evaluation of instance segmentation","summary":"  Correctness of instance segmentation constitutes counting the number of\nobjects, correctly localizing all predictions and classifying each localized\nprediction. Average Precision is the de-facto metric used to measure all these\nconstituents of segmentation. However, this metric does not penalize duplicate\npredictions in the high-recall range, and cannot distinguish instances that are\nlocalized correctly but categorized incorrectly. This weakness has\ninadvertently led to network designs that achieve significant gains in AP but\nalso introduce a large number of false positives. We therefore cannot rely on\nAP to choose a model that provides an optimal tradeoff between false positives\nand high recall. To resolve this dilemma, we review alternative metrics in the\nliterature and propose two new measures to explicitly measure the amount of\nboth spatial and categorical duplicate predictions. We also propose a Semantic\nSorting and NMS module to remove these duplicates based on a pixel occupancy\nmatching scheme. Experiments show that modern segmentation networks have\nsignificant gains in AP, but also contain a considerable amount of duplicates.\nOur Semantic Sorting and NMS can be added as a plug-and-play module to mitigate\nhedged predictions and preserve AP.\n","authors":["Rohit Jena","Lukas Zhornyak","Nehal Doiphode","Pratik Chaudhari","Vivek Buch","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2207.01614v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11307v1","updated":"2023-03-20T17:45:12Z","published":"2023-03-20T17:45:12Z","title":"DIME-Net: Neural Network-Based Dynamic Intrinsic Parameter Rectification\n  for Cameras with Optical Image Stabilization System","summary":"  Optical Image Stabilization (OIS) system in mobile devices reduces image\nblurring by steering lens to compensate for hand jitters. However, OIS changes\nintrinsic camera parameters (i.e. $\\mathrm{K}$ matrix) dynamically which\nhinders accurate camera pose estimation or 3D reconstruction. Here we propose a\nnovel neural network-based approach that estimates $\\mathrm{K}$ matrix in\nreal-time so that pose estimation or scene reconstruction can be run at camera\nnative resolution for the highest accuracy on mobile devices. Our network\ndesign takes gratified projection model discrepancy feature and 3D point\npositions as inputs and employs a Multi-Layer Perceptron (MLP) to approximate\n$f_{\\mathrm{K}}$ manifold. We also design a unique training scheme for this\nnetwork by introducing a Back propagated PnP (BPnP) layer so that reprojection\nerror can be adopted as the loss function. The training process utilizes\nprecise calibration patterns for capturing accurate $f_{\\mathrm{K}}$ manifold\nbut the trained network can be used anywhere. We name the proposed Dynamic\nIntrinsic Manifold Estimation network as DIME-Net and have it implemented and\ntested on three different mobile devices. In all cases, DIME-Net can reduce\nreprojection error by at least $64\\%$ indicating that our design is successful.\n","authors":["Shu-Hao Yeh","Shuangyu Xie","Di Wang","Wei Yan","Dezhen Song"],"pdf_url":"https://arxiv.org/pdf/2303.11307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11306v1","updated":"2023-03-20T17:45:08Z","published":"2023-03-20T17:45:08Z","title":"Localizing Object-level Shape Variations with Text-to-Image Diffusion\n  Models","summary":"  Text-to-image models give rise to workflows which often begin with an\nexploration step, where users sift through a large collection of generated\nimages. The global nature of the text-to-image generation process prevents\nusers from narrowing their exploration to a particular object in the image. In\nthis paper, we present a technique to generate a collection of images that\ndepicts variations in the shape of a specific object, enabling an object-level\nshape exploration process. Creating plausible variations is challenging as it\nrequires control over the shape of the generated object while respecting its\nsemantics. A particular challenge when generating object variations is\naccurately localizing the manipulation applied over the object's shape. We\nintroduce a prompt-mixing technique that switches between prompts along the\ndenoising process to attain a variety of shape choices. To localize the\nimage-space operation, we present two techniques that use the self-attention\nlayers in conjunction with the cross-attention layers. Moreover, we show that\nthese localization techniques are general and effective beyond the scope of\ngenerating object variations. Extensive results and comparisons demonstrate the\neffectiveness of our method in generating object variations, and the competence\nof our localization techniques.\n","authors":["Or Patashnik","Daniel Garibi","Idan Azuri","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.11306v1.pdf","comment":"Project page at https://orpatashnik.github.io/local-prompt-mixing/"},{"id":"http://arxiv.org/abs/2303.11305v1","updated":"2023-03-20T17:45:02Z","published":"2023-03-20T17:45:02Z","title":"SVDiff: Compact Parameter Space for Diffusion Fine-Tuning","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size (1.7MB for StableDiffusion) compared to\nexisting methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it\nmore practical for real-world applications.\n","authors":["Ligong Han","Yinxiao Li","Han Zhang","Peyman Milanfar","Dimitris Metaxas","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11305v1.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2303.11302v1","updated":"2023-03-20T17:41:11Z","published":"2023-03-20T17:41:11Z","title":"Learning Audio-Visual Source Localization via False Negative Aware\n  Contrastive Learning","summary":"  Self-supervised audio-visual source localization aims to locate sound-source\nobjects in video frames without extra annotations. Recent methods often\napproach this goal with the help of contrastive learning, which assumes only\nthe audio and visual contents from the same video are positive samples for each\nother. However, this assumption would suffer from false negative samples in\nreal-world training. For example, for an audio sample, treating the frames from\nthe same audio class as negative samples may mislead the model and therefore\nharm the learned representations e.g., the audio of a siren wailing may\nreasonably correspond to the ambulances in multiple images). Based on this\nobservation, we propose a new learning strategy named False Negative Aware\nContrastive (FNAC) to mitigate the problem of misleading the training with such\nfalse negative samples. Specifically, we utilize the intra-modal similarities\nto identify potentially similar samples and construct corresponding adjacency\nmatrices to guide contrastive learning. Further, we propose to strengthen the\nrole of true negative samples by explicitly leveraging the visual features of\nsound sources to facilitate the differentiation of authentic sounding source\nregions. FNAC achieves state-of-the-art performances on Flickr-SoundNet,\nVGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in\nmitigating the false negative issue. The code is available at\n\\url{https://github.com/weixuansun/FNAC-AVL}.\n","authors":["Weixuan Sun","Jiayi Zhang","Jianyuan Wang","Zheyuan Liu","Yiran Zhong","Tianpeng Feng","Yandong Guo","Yanhao Zhang","Nick Barnes"],"pdf_url":"https://arxiv.org/pdf/2303.11302v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11301v1","updated":"2023-03-20T17:40:44Z","published":"2023-03-20T17:40:44Z","title":"VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking","summary":"  3D object detectors usually rely on hand-crafted proxies, e.g., anchors or\ncenters, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel\nfeatures need to be densified and processed by dense prediction heads, which\ninevitably costs extra computation. In this paper, we instead propose VoxelNext\nfor fully sparse 3D object detection. Our core insight is to predict objects\ndirectly based on sparse voxel features, without relying on hand-crafted\nproxies. Our strong sparse convolutional network VoxelNeXt detects and tracks\n3D objects through voxel features entirely. It is an elegant and efficient\nframework, with no need for sparse-to-dense conversion or NMS post-processing.\nOur method achieves a better speed-accuracy trade-off than other mainframe\ndetectors on the nuScenes dataset. For the first time, we show that a fully\nsparse voxel-based representation works decently for LIDAR 3D object detection\nand tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2\nbenchmarks validate the effectiveness of our approach. Without bells and\nwhistles, our model outperforms all existing LIDAR methods on the nuScenes\ntracking test benchmark.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.11301v1.pdf","comment":"In CVPR 2023, Code and models are available at\n  https://github.com/dvlab-research/VoxelNeXt"},{"id":"http://arxiv.org/abs/2303.11298v1","updated":"2023-03-20T17:38:24Z","published":"2023-03-20T17:38:24Z","title":"Reliability in Semantic Segmentation: Are We on the Right Track?","summary":"  Motivated by the increasing popularity of transformers in computer vision, in\nrecent times there has been a rapid development of novel architectures. While\nin-domain performance follows a constant, upward trend, properties like\nrobustness or uncertainty estimation are less explored -leaving doubts about\nadvances in model reliability. Studies along these axes exist, but they are\nmainly limited to classification models. In contrast, we carry out a study on\nsemantic segmentation, a relevant task for many real-world applications where\nmodel reliability is paramount. We analyze a broad variety of models, spanning\nfrom older ResNet-based architectures to novel transformers and assess their\nreliability based on four metrics: robustness, calibration, misclassification\ndetection and out-of-distribution (OOD) detection. We find that while recent\nmodels are significantly more robust, they are not overall more reliable in\nterms of uncertainty estimation. We further explore methods that can come to\nthe rescue and show that improving calibration can also help with other\nuncertainty metrics such as misclassification or OOD detection. This is the\nfirst study on modern segmentation models focused on both robustness and\nuncertainty estimation and we hope it will help practitioners and researchers\ninterested in this fundamental vision task. Code available at\nhttps://github.com/naver/relis.\n","authors":["Pau de Jorge","Riccardo Volpi","Philip Torr","Gregory Rogez"],"pdf_url":"https://arxiv.org/pdf/2303.11298v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11296v1","updated":"2023-03-20T17:34:05Z","published":"2023-03-20T17:34:05Z","title":"Attribute-preserving Face Dataset Anonymization via Latent Code\n  Optimization","summary":"  This work addresses the problem of anonymizing the identity of faces in a\ndataset of images, such that the privacy of those depicted is not violated,\nwhile at the same time the dataset is useful for downstream task such as for\ntraining machine learning models. To the best of our knowledge, we are the\nfirst to explicitly address this issue and deal with two major drawbacks of the\nexisting state-of-the-art approaches, namely that they (i) require the costly\ntraining of additional, purpose-trained neural networks, and/or (ii) fail to\nretain the facial attributes of the original images in the anonymized\ncounterparts, the preservation of which is of paramount importance for their\nuse in downstream tasks. We accordingly present a task-agnostic anonymization\nprocedure that directly optimizes the images' latent representation in the\nlatent space of a pre-trained GAN. By optimizing the latent codes directly, we\nensure both that the identity is of a desired distance away from the original\n(with an identity obfuscation loss), whilst preserving the facial attributes\n(using a novel feature-matching loss in FaRL's deep feature space). We\ndemonstrate through a series of both qualitative and quantitative experiments\nthat our method is capable of anonymizing the identity of the images whilst --\ncrucially -- better-preserving the facial attributes. We make the code and the\npre-trained models publicly available at: https://github.com/chi0tzp/FALCO.\n","authors":["Simone Barattin","Christos Tzelepis","Ioannis Patras","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.11296v1.pdf","comment":"Accepted for publication in CVPR 2023"},{"id":"http://arxiv.org/abs/2211.13221v2","updated":"2023-03-20T17:29:45Z","published":"2022-11-23T18:58:39Z","title":"Latent Video Diffusion Models for High-Fidelity Long Video Generation","summary":"  AI-generated content has attracted lots of attention recently, but\nphoto-realistic video synthesis is still challenging. Although many attempts\nusing GANs and autoregressive models have been made in this area, the visual\nquality and length of generated videos are far from satisfactory. Diffusion\nmodels have shown remarkable results recently but require significant\ncomputational resources. To address this, we introduce lightweight video\ndiffusion models by leveraging a low-dimensional 3D latent space, significantly\noutperforming previous pixel-space video diffusion models under a limited\ncomputational budget. In addition, we propose hierarchical diffusion in the\nlatent space such that longer videos with more than one thousand frames can be\nproduced. To further overcome the performance degradation issue for long video\ngeneration, we propose conditional latent perturbation and unconditional\nguidance that effectively mitigate the accumulated errors during the extension\nof video length. Extensive experiments on small domain datasets of different\ncategories suggest that our framework generates more realistic and longer\nvideos than previous strong baselines. We additionally provide an extension to\nlarge-scale text-to-video generation to demonstrate the superiority of our\nwork. Our code and models will be made publicly available.\n","authors":["Yingqing He","Tianyu Yang","Yong Zhang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2211.13221v2.pdf","comment":"Project Page: https://yingqinghe.github.io/LVDM/ Github:\n  https://github.com/YingqingHe/LVDM"},{"id":"http://arxiv.org/abs/2303.11274v1","updated":"2023-03-20T17:08:48Z","published":"2023-03-20T17:08:48Z","title":"Cascading Hierarchical Networks with Multi-task Balanced Loss for\n  Fine-grained hashing","summary":"  With the explosive growth in the number of fine-grained images in the\nInternet era, it has become a challenging problem to perform fast and efficient\nretrieval from large-scale fine-grained images. Among the many retrieval\nmethods, hashing methods are widely used due to their high efficiency and small\nstorage space occupation. Fine-grained hashing is more challenging than\ntraditional hashing problems due to the difficulties such as low inter-class\nvariances and high intra-class variances caused by the characteristics of\nfine-grained images. To improve the retrieval accuracy of fine-grained hashing,\nwe propose a cascaded network to learn compact and highly semantic hash codes,\nand introduce an attention-guided data augmentation method. We refer to this\nnetwork as a cascaded hierarchical data augmentation network. We also propose a\nnovel approach to coordinately balance the loss of multi-task learning. We do\nextensive experiments on some common fine-grained visual classification\ndatasets. The experimental results demonstrate that our proposed method\noutperforms several state-of-art hashing methods and can effectively improve\nthe accuracy of fine-grained retrieval. The source code is publicly available:\nhttps://github.com/kaiba007/FG-CNET.\n","authors":["Xianxian Zeng","Yanjun Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.11274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08319v2","updated":"2023-03-20T16:54:34Z","published":"2023-03-15T02:14:56Z","title":"FAQ: Feature Aggregated Queries for Transformer-based Video Object\n  Detectors","summary":"  Video object detection needs to solve feature degradation situations that\nrarely happen in the image domain. One solution is to use the temporal\ninformation and fuse the features from the neighboring frames. With\nTransformerbased object detectors getting a better performance on the image\ndomain tasks, recent works began to extend those methods to video object\ndetection. However, those existing Transformer-based video object detectors\nstill follow the same pipeline as those used for classical object detectors,\nlike enhancing the object feature representations by aggregation. In this work,\nwe take a different perspective on video object detection. In detail, we\nimprove the qualities of queries for the Transformer-based models by\naggregation. To achieve this goal, we first propose a vanilla query aggregation\nmodule that weighted averages the queries according to the features of the\nneighboring frames. Then, we extend the vanilla module to a more practical\nversion, which generates and aggregates queries according to the features of\nthe input frames. Extensive experimental results validate the effectiveness of\nour proposed methods: On the challenging ImageNet VID benchmark, when\nintegrated with our proposed modules, the current state-of-the-art\nTransformer-based object detectors can be improved by more than 2.4% on mAP and\n4.2% on AP50.\n","authors":["Yiming Cui","Linjie Yang"],"pdf_url":"https://arxiv.org/pdf/2303.08319v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11267v1","updated":"2023-03-20T16:50:29Z","published":"2023-03-20T16:50:29Z","title":"Rethinking the backbone architecture for tiny object detection","summary":"  Tiny object detection has become an active area of research because images\nwith tiny targets are common in several important real-world scenarios.\nHowever, existing tiny object detection methods use standard deep neural\nnetworks as their backbone architecture. We argue that such backbones are\ninappropriate for detecting tiny objects as they are designed for the\nclassification of larger objects, and do not have the spatial resolution to\nidentify small targets. Specifically, such backbones use max-pooling or a large\nstride at early stages in the architecture. This produces lower resolution\nfeature-maps that can be efficiently processed by subsequent layers. However,\nsuch low-resolution feature-maps do not contain information that can reliably\ndiscriminate tiny objects. To solve this problem we design 'bottom-heavy'\nversions of backbones that allocate more resources to processing\nhigher-resolution features without introducing any additional computational\nburden overall. We also investigate if pre-training these backbones on images\nof appropriate size, using CIFAR100 and ImageNet32, can further improve\nperformance on tiny object detection. Results on TinyPerson and WiderFace show\nthat detectors with our proposed backbones achieve better results than the\ncurrent state-of-the-art methods.\n","authors":["Jinlai Ning","Haoyan Guan","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.11267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11253v1","updated":"2023-03-20T16:40:37Z","published":"2023-03-20T16:40:37Z","title":"Zero-Shot Noise2Noise: Efficient Image Denoising without any Data","summary":"  Recently, self-supervised neural networks have shown excellent image\ndenoising performance. However, current dataset free methods are either\ncomputationally expensive, require a noise model, or have inadequate image\nquality. In this work we show that a simple 2-layer network, without any\ntraining data or knowledge of the noise distribution, can enable high-quality\nimage denoising at low computational cost. Our approach is motivated by\nNoise2Noise and Neighbor2Neighbor and works well for denoising pixel-wise\nindependent noise. Our experiments on artificial, real-world camera, and\nmicroscope noise show that our method termed ZS-N2N (Zero Shot Noise2Noise)\noften outperforms existing dataset-free methods at a reduced cost, making it\nsuitable for use cases with scarce data availability and limited compute\nresources. A demo of our implementation including our code and hyperparameters\ncan be found in the following colab notebook:\nhttps://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b\n","authors":["Youssef Mansour","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2303.11253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06051v2","updated":"2023-03-20T16:36:27Z","published":"2023-01-15T09:31:58Z","title":"DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets","summary":"  Designing an efficient yet deployment-friendly 3D backbone to handle sparse\npoint clouds is a fundamental problem in 3D perception. Compared with the\ncustomized sparse convolution, the attention mechanism in Transformers is more\nappropriate for flexibly modeling long-range relationships and is easier to be\ndeployed in real-world applications. However, due to the sparse characteristics\nof point clouds, it is non-trivial to apply a standard transformer on sparse\npoints. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a\nsingle-stride window-based voxel Transformer backbone for outdoor 3D\nperception. In order to efficiently process sparse points in parallel, we\npropose Dynamic Sparse Window Attention, which partitions a series of local\nregions in each window according to its sparsity and then computes the features\nof all regions in a fully parallel manner. To allow the cross-set connection,\nwe design a rotated set partitioning strategy that alternates between two\npartitioning configurations in consecutive self-attention layers. To support\neffective downsampling and better encode geometric information, we also propose\nan attention-style 3D pooling module on sparse points, which is powerful and\ndeployment-friendly without utilizing any customized CUDA operations. Our model\nachieves state-of-the-art performance with a broad range of 3D perception\ntasks. More importantly, DSVT can be easily deployed by TensorRT with real-time\ninference speed (27Hz). Code will be available at\n\\url{https://github.com/Haiyang-W/DSVT}.\n","authors":["Haiyang Wang","Chen Shi","Shaoshuai Shi","Meng Lei","Sen Wang","Di He","Bernt Schiele","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2301.06051v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.11251v1","updated":"2023-03-20T16:35:38Z","published":"2023-03-20T16:35:38Z","title":"Towards End-to-End Generative Modeling of Long Videos with\n  Memory-Efficient Bidirectional Transformers","summary":"  Autoregressive transformers have shown remarkable success in video\ngeneration. However, the transformers are prohibited from directly learning the\nlong-term dependency in videos due to the quadratic complexity of\nself-attention, and inherently suffering from slow inference time and error\npropagation due to the autoregressive process. In this paper, we propose\nMemory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of\nlong-term dependency in videos and fast inference. Based on recent advances in\nbidirectional transformers, our method learns to decode the entire\nspatio-temporal volume of a video in parallel from partially observed patches.\nThe proposed transformer achieves a linear time complexity in both encoding and\ndecoding, by projecting observable context tokens into a fixed number of latent\ntokens and conditioning them to decode the masked tokens through the\ncross-attention. Empowered by linear complexity and bidirectional modeling, our\nmethod demonstrates significant improvement over the autoregressive\nTransformers for generating moderately long videos in both quality and speed.\n","authors":["Jaehoon Yoo","Semin Kim","Doyup Lee","Chiheon Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11243v1","updated":"2023-03-20T16:28:15Z","published":"2023-03-20T16:28:15Z","title":"Augment and Criticize: Exploring Informative Samples for Semi-Supervised\n  Monocular 3D Object Detection","summary":"  In this paper, we improve the challenging monocular 3D object detection\nproblem with a general semi-supervised framework. Specifically, having observed\nthat the bottleneck of this task lies in lacking reliable and informative\nsamples to train the detector, we introduce a novel, simple, yet effective\n`Augment and Criticize' framework that explores abundant informative samples\nfrom unlabeled data for learning more robust detection models. In the `Augment'\nstage, we present the Augmentation-based Prediction aGgregation (APG), which\naggregates detections from various automatically learned augmented views to\nimprove the robustness of pseudo label generation. Since not all pseudo labels\nfrom APG are beneficially informative, the subsequent `Criticize' phase is\npresented. In particular, we introduce the Critical Retraining Strategy (CRS)\nthat, unlike simply filtering pseudo labels using a fixed threshold (e.g.,\nclassification score) as in 2D semi-supervised tasks, leverages a learnable\nnetwork to evaluate the contribution of unlabeled images at different training\ntimestamps. This way, the noisy samples prohibitive to model evolution could be\neffectively suppressed. To validate our framework, we apply it to MonoDLE and\nMonoFlex. The two new detectors, dubbed 3DSeMo_DLE and 3DSeMo_FLEX, achieve\nstate-of-the-art results with remarkable improvements for over 3.5% AP_3D/BEV\n(Easy) on KITTI, showing its effectiveness and generality. Code and models will\nbe released.\n","authors":["Zhenyu Li","Zhipeng Zhang","Heng Fan","Yuan He","Ke Wang","Xianming Liu","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.11243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11242v1","updated":"2023-03-20T16:27:36Z","published":"2023-03-20T16:27:36Z","title":"Make Landscape Flatter in Differentially Private Federated Learning","summary":"  To defend the inference attacks and mitigate the sensitive information\nleakages in Federated Learning (FL), client-level Differentially Private FL\n(DPFL) is the de-facto standard for privacy protection by clipping local\nupdates and adding random noise. However, existing DPFL methods tend to make a\nsharper loss landscape and have poorer weight perturbation robustness,\nresulting in severe performance degradation. To alleviate these issues, we\npropose a novel DPFL algorithm named DP-FedSAM, which leverages gradient\nperturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM\nintegrates Sharpness Aware Minimization (SAM) optimizer to generate local\nflatness models with better stability and weight perturbation robustness, which\nresults in the small norm of local updates and robustness to DP noise, thereby\nimproving the performance. From the theoretical perspective, we analyze in\ndetail how DP-FedSAM mitigates the performance degradation induced by DP.\nMeanwhile, we give rigorous privacy guarantees with R\\'enyi DP and present the\nsensitivity analysis of local updates. At last, we empirically confirm that our\nalgorithm achieves state-of-the-art (SOTA) performance compared with existing\nSOTA baselines in DPFL.\n","authors":["Yifan Shi","Yingqi Liu","Kang Wei","Li Shen","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11242v1.pdf","comment":"CVPR2023, 18 pages"},{"id":"http://arxiv.org/abs/2303.11239v1","updated":"2023-03-20T16:24:06Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Koethe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v1.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2303.10103v2","updated":"2023-03-20T16:21:34Z","published":"2023-03-17T16:26:20Z","title":"Image comparison and scaling via nonlinear elasticity","summary":"  A nonlinear elasticity model for comparing images is formulated and analyzed,\nin which optimal transformations between images are sought as minimizers of an\nintegral functional. The existence of minimizers in a suitable class of\nhomeomorphisms between image domains is established under natural hypotheses.\nWe investigate whether for linearly related images the minimization algorithm\ndelivers the linear transformation as the unique minimizer.\n","authors":["John M. Ball","Christopher L. Horner"],"pdf_url":"https://arxiv.org/pdf/2303.10103v2.pdf","comment":"SSVM2023 Proceedings to appear. New references added plus related\n  minor changes"},{"id":"http://arxiv.org/abs/2303.11235v1","updated":"2023-03-20T16:19:23Z","published":"2023-03-20T16:19:23Z","title":"FullFormer: Generating Shapes Inside Shapes","summary":"  Implicit generative models have been widely employed to model 3D data and\nhave recently proven to be successful in encoding and generating high-quality\n3D shapes. This work builds upon these models and alleviates current\nlimitations by presenting the first implicit generative model that facilitates\nthe generation of complex 3D shapes with rich internal geometric details. To\nachieve this, our model uses unsigned distance fields to represent nested 3D\nsurfaces allowing learning from non-watertight mesh data. We propose a\ntransformer-based autoregressive model for 3D shape generation that leverages\ncontext-rich tokens from vector quantized shape embeddings. The generated\ntokens are decoded into an unsigned distance field which is rendered into a\nnovel 3D shape exhibiting a rich internal structure. We demonstrate that our\nmodel achieves state-of-the-art point cloud generation results on popular\nclasses of 'Cars', 'Planes', and 'Chairs' of the ShapeNet dataset.\nAdditionally, we curate a dataset that exclusively comprises shapes with\nrealistic internal details from the `Cars' class of ShapeNet and demonstrate\nour method's efficacy in generating these shapes with internal geometry.\n","authors":["Tejaswini Medi","Jawad Tayyub","Muhammad Sarmad","Frank Lindseth","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.11235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.02273v2","updated":"2023-03-20T16:18:27Z","published":"2021-10-05T18:26:22Z","title":"Bilevel Imaging Learning Problems as Mathematical Programs with\n  Complementarity Constraints: Reformulation and Theory","summary":"  We investigate a family of bilevel imaging learning problems where the\nlower-level instance corresponds to a convex variational model involving first-\nand second-order nonsmooth sparsity-based regularizers. By using geometric\nproperties of the primal-dual reformulation of the lower-level problem and\nintroducing suitable auxiliar variables, we are able to reformulate the\noriginal bilevel problems as Mathematical Programs with Complementarity\nConstraints (MPCC). For the latter, we prove tight constraint qualification\nconditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and\nStrong (S-) stationarity conditions. The stationarity systems for the MPCC turn\nalso into stationarity conditions for the original formulation. Second-order\nsufficient optimality conditions are derived as well, together with a local\nuniqueness result for stationary points. The proposed reformulation may be\nextended to problems in function spaces, leading to MPCC's with constraints on\nthe gradient of the state. The MPCC reformulation also leads to the efficient\nuse of available large-scale nonlinear programming solvers, as shown in a\ncompanion paper, where different imaging applications are studied.\n","authors":["Juan Carlos De los Reyes"],"pdf_url":"https://arxiv.org/pdf/2110.02273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11228v1","updated":"2023-03-20T16:09:25Z","published":"2023-03-20T16:09:25Z","title":"Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for\n  Robotic Grasping","summary":"  Object segmentation for robotic grasping under dynamic conditions often faces\nchallenges such as occlusion, low light conditions, motion blur and object size\nvariance. To address these challenges, we propose a Deep Learning network that\nfuses two types of visual signals, event-based data and RGB frame data. The\nproposed Bimodal SegNet network has two distinct encoders, one for each signal\ninput and a spatial pyramidal pooling with atrous convolutions. Encoders\ncapture rich contextual information by pooling the concatenated features at\ndifferent resolutions while the decoder obtains sharp object boundaries. The\nevaluation of the proposed method undertakes five unique image degradation\nchallenges including occlusion, blur, brightness, trajectory and scale variance\non the Event-based Segmentation (ESD) Dataset. The evaluation results show a\n6-10\\% segmentation accuracy improvement over state-of-the-art methods in terms\nof mean intersection over the union and pixel accuracy. The model code is\navailable at https://github.com/sanket0707/Bimodal-SegNet.git\n","authors":["Sanket Kachole","Xiaoqian Huang","Fariborz Baghaei Naeini","Rajkumar Muthusamy","Dimitrios Makris","Yahya Zweiri"],"pdf_url":"https://arxiv.org/pdf/2303.11228v1.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2303.11225v1","updated":"2023-03-20T16:07:02Z","published":"2023-03-20T16:07:02Z","title":"HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and\n  Dynamic Details","summary":"  3D Morphable Models (3DMMs) demonstrate great potential for reconstructing\nfaithful and animatable 3D facial surfaces from a single image. The facial\nsurface is influenced by the coarse shape, as well as the static detail (e,g.,\nperson-specific appearance) and dynamic detail (e.g., expression-driven\nwrinkles). Previous work struggles to decouple the static and dynamic details\nthrough image-level supervision, leading to reconstructions that are not\nrealistic. In this paper, we aim at high-fidelity 3D face reconstruction and\npropose HiFace to explicitly model the static and dynamic details.\nSpecifically, the static detail is modeled as the linear combination of a\ndisplacement basis, while the dynamic detail is modeled as the linear\ninterpolation of two displacement maps with polarized expressions. We exploit\nseveral loss functions to jointly learn the coarse shape and fine details with\nboth synthetic and real-world datasets, which enable HiFace to reconstruct\nhigh-fidelity 3D shapes with animatable details. Extensive quantitative and\nqualitative experiments demonstrate that HiFace presents state-of-the-art\nreconstruction quality and faithfully recovers both the static and dynamic\ndetails. Our project page can be found at https://project-hiface.github.io\n","authors":["Zenghao Chai","Tianke Zhang","Tianyu He","Xu Tan","Tadas Baltrusaitis","HsiangTao Wu","Runnan Li","Sheng Zhao","Chun Yuan","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.11225v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11224v1","updated":"2023-03-20T16:00:20Z","published":"2023-03-20T16:00:20Z","title":"Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis","summary":"  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n","authors":["Tobias Weber","Michael Ingrisch","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2303.11224v1.pdf","comment":"accepted at PAKDD 2023"},{"id":"http://arxiv.org/abs/2210.09276v3","updated":"2023-03-20T15:58:50Z","published":"2022-10-17T17:27:32Z","title":"Imagic: Text-Based Real Image Editing with Diffusion Models","summary":"  Text-conditioned image editing has recently attracted considerable interest.\nHowever, most methods are currently either limited to specific editing types\n(e.g., object overlay, style transfer), or apply to synthetically generated\nimages, or require multiple input images of a common object. In this paper we\ndemonstrate, for the very first time, the ability to apply complex (e.g.,\nnon-rigid) text-guided semantic edits to a single real image. For example, we\ncan change the posture and composition of one or multiple objects inside an\nimage, while preserving its original characteristics. Our method can make a\nstanding dog sit down or jump, cause a bird to spread its wings, etc. -- each\nwithin its single high-resolution natural image provided by the user. Contrary\nto previous work, our proposed method requires only a single input image and a\ntarget text (the desired edit). It operates on real images, and does not\nrequire any additional inputs (such as image masks or additional views of the\nobject). Our method, which we call \"Imagic\", leverages a pre-trained\ntext-to-image diffusion model for this task. It produces a text embedding that\naligns with both the input image and the target text, while fine-tuning the\ndiffusion model to capture the image-specific appearance. We demonstrate the\nquality and versatility of our method on numerous inputs from various domains,\nshowcasing a plethora of high quality complex semantic image edits, all within\na single unified framework.\n","authors":["Bahjat Kawar","Shiran Zada","Oran Lang","Omer Tov","Huiwen Chang","Tali Dekel","Inbar Mosseri","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2210.09276v3.pdf","comment":"Project page: https://imagic-editing.github.io/"},{"id":"http://arxiv.org/abs/2303.11219v1","updated":"2023-03-20T15:50:00Z","published":"2023-03-20T15:50:00Z","title":"NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion\n  Aware Refraction-Tracing","summary":"  We present a novel method, called NeTO, for capturing 3D geometry of solid\ntransparent objects from 2D images via volume rendering. Reconstructing\ntransparent objects is a very challenging task, which is ill-suited for\ngeneral-purpose reconstruction techniques due to the specular light transport\nphenomena. Although existing refraction-tracing based methods, designed\nspecially for this task, achieve impressive results, they still suffer from\nunstable optimization and loss of fine details, since the explicit surface\nrepresentation they adopted is difficult to be optimized, and the\nself-occlusion problem is ignored for refraction-tracing. In this paper, we\npropose to leverage implicit Signed Distance Function (SDF) as surface\nrepresentation, and optimize the SDF field via volume rendering with a\nself-occlusion aware refractive ray tracing. The implicit representation\nenables our method to be capable of reconstructing high-quality reconstruction\neven with a limited set of images, and the self-occlusion aware strategy makes\nit possible for our method to accurately reconstruct the self-occluded regions.\nExperiments show that our method achieves faithful reconstruction results and\noutperforms prior works by a large margin. Visit our project page at\n\\url{https://www.xxlong.site/NeTO/}\n","authors":["Zongcheng Li","Xiaoxiao Long","Yusen Wang","Tuo Cao","Wenping Wang","Fei Luo","Chunxia Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.11219v1.pdf","comment":"www.xxlong.site/NeTO/"},{"id":"http://arxiv.org/abs/2303.11217v1","updated":"2023-03-20T15:49:18Z","published":"2023-03-20T15:49:18Z","title":"Inverse problem regularization with hierarchical variational\n  autoencoders","summary":"  In this paper, we propose to regularize ill-posed inverse problems using a\ndeep hierarchical variational autoencoder (HVAE) as an image prior. The\nproposed method synthesizes the advantages of i) denoiser-based Plug \\& Play\napproaches and ii) generative model based approaches to inverse problems.\nFirst, we exploit VAE properties to design an efficient algorithm that benefits\nfrom convergence guarantees of Plug-and-Play (PnP) methods. Second, our\napproach is not restricted to specialized datasets and the proposed PnP-HVAE\nmodel is able to solve image restoration problems on natural images of any\nsize. Our experiments show that the proposed PnP-HVAE method is competitive\nwith both SOTA denoiser-based PnP approaches, and other SOTA restoration\nmethods based on generative models.\n","authors":["Jean Prost","Antoine Houdard","Andrés Almansa","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2303.11217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11215v1","updated":"2023-03-20T15:47:05Z","published":"2023-03-20T15:47:05Z","title":"Learning to Generate 3D Representations of Building Roofs Using\n  Single-View Aerial Imagery","summary":"  We present a novel pipeline for learning the conditional distribution of a\nbuilding roof mesh given pixels from an aerial image, under the assumption that\nroof geometry follows a set of regular patterns. Unlike alternative methods\nthat require multiple images of the same object, our approach enables\nestimating 3D roof meshes using only a single image for predictions. The\napproach employs the PolyGen, a deep generative transformer architecture for 3D\nmeshes. We apply this model in a new domain and investigate the sensitivity of\nthe image resolution. We propose a novel metric to evaluate the performance of\nthe inferred meshes, and our results show that the model is robust even at\nlower resolutions, while qualitatively producing realistic representations for\nout-of-distribution samples.\n","authors":["Maxim Khomiakov","Alejandro Valverde Mahou","Alba Reinders Sánchez","Jes Frellsen","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2303.11215v1.pdf","comment":"Copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11214v1","updated":"2023-03-20T15:46:49Z","published":"2023-03-20T15:46:49Z","title":"Accurate Detection of Mediastinal Lesions with nnDetection","summary":"  The accurate detection of mediastinal lesions is one of the rarely explored\nmedical object detection problems. In this work, we applied a modified version\nof the self-configuring method nnDetection to the Mediastinal Lesion Analysis\n(MELA) Challenge 2022. By incorporating automatically generated pseudo masks,\ntraining high capacity models with large patch sizes in a multi GPU setup and\nan adapted augmentation scheme to reduce localization errors caused by\nrotations, our method achieved an excellent FROC score of 0.9922 at IoU 0.10\nand 0.9880 at IoU 0.3 in our cross-validation experiments. The submitted\nensemble ranked third in the competition with a FROC score of 0.9897 on the\nMELA challenge leaderboard.\n","authors":["Michael Baumgartner","Peter M. Full","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.11214v1.pdf","comment":"Published in \"Lesion Segmentation in Surgical and Diagnostic\n  Applications\""},{"id":"http://arxiv.org/abs/2303.11203v1","updated":"2023-03-20T15:36:10Z","published":"2023-03-20T15:36:10Z","title":"Less is More: Reducing Task and Model Complexity for 3D Point Cloud\n  Semantic Segmentation","summary":"  Whilst the availability of 3D LiDAR point cloud data has significantly grown\nin recent years, annotation remains expensive and time-consuming, leading to a\ndemand for semi-supervised semantic segmentation methods with application\ndomains such as autonomous driving. Existing work very often employs relatively\nlarge segmentation backbone networks to improve segmentation accuracy, at the\nexpense of computational costs. In addition, many use uniform sampling to\nreduce ground truth data requirements for learning needed, often resulting in\nsub-optimal performance. To address these issues, we propose a new pipeline\nthat employs a smaller architecture, requiring fewer ground-truth annotations\nto achieve superior segmentation accuracy compared to contemporary approaches.\nThis is facilitated via a novel Sparse Depthwise Separable Convolution module\nthat significantly reduces the network parameter count while retaining overall\ntask performance. To effectively sub-sample our training data, we propose a new\nSpatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages\nknowledge of sensor motion within the environment to extract a more diverse\nsubset of training data frame samples. To leverage the use of limited annotated\ndata samples, we further propose a soft pseudo-label method informed by LiDAR\nreflectivity. Our method outperforms contemporary semi-supervised work in terms\nof mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and\nScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in model\nparameters and 641x fewer multiply-add operations whilst also demonstrating\nsignificant performance improvement on limited training data (i.e., Less is\nMore).\n","authors":["Li Li","Hubert P. H. Shum","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2303.11203v1.pdf","comment":"Accepted by CVPR 2023; Code at https://github.com/l1997i/lim3d; 11\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2302.00291v2","updated":"2023-03-20T15:28:36Z","published":"2023-02-01T07:45:54Z","title":"Development of Real-time Rendering Technology for High-Precision Models\n  in Autonomous Driving","summary":"  Our autonomous driving simulation lab produces a high-precision 3D model\nsimulating the parking lot. However, the current model still has poor rendering\nquality in some aspects. In this work, we develop a system to improve the\nrendering of the model and evaluate the quality of the rendered model.\n","authors":["Zhang Wencheng","Wang Chengyi"],"pdf_url":"https://arxiv.org/pdf/2302.00291v2.pdf","comment":"3 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.01498v3","updated":"2023-03-20T15:25:09Z","published":"2023-03-02T18:58:15Z","title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit\n  Detection & Emotional Reaction Intensity Estimation Challenges","summary":"  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part\nof the respective ABAW Workshop which will be held in conjunction with IEEE\nComputer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW\nCompetition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR\n2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at\nautomatically analyzing affect. For this year's Competition, we feature two\ncorpora: i) an extended version of the Aff-Wild2 database and ii) the\nHume-Reaction dataset. The former database is an audiovisual one of around 600\nvideos of around 3M frames and is annotated with respect to:a) two continuous\naffect dimensions -valence (how positive/negative a person is) and arousal (how\nactive/passive a person is)-; b) basic expressions (e.g. happiness, sadness,\nneutral state); and c) atomic facial muscle actions (i.e., action units). The\nlatter dataset is an audiovisual one in which reactions of individuals to\nemotional stimuli have been annotated with respect to seven emotional\nexpression intensities. Thus the 5th ABAW Competition encompasses four\nChallenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression\nClassification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction\nIntensity Estimation. In this paper, we present these Challenges, along with\ntheir corpora, we outline the evaluation metrics, we present the baseline\nsystems and illustrate their obtained performance.\n","authors":["Dimitrios Kollias","Panagiotis Tzirakis","Alice Baird","Alan Cowen","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2303.01498v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.10659"},{"id":"http://arxiv.org/abs/2212.12053v3","updated":"2023-03-20T15:22:59Z","published":"2022-12-22T22:05:16Z","title":"On Calibrating Semantic Segmentation Models: Analyses and An Algorithm","summary":"  We study the problem of semantic segmentation calibration. Lots of solutions\nhave been proposed to approach model miscalibration of confidence in image\nclassification. However, to date, confidence calibration research on semantic\nsegmentation is still limited. We provide a systematic study on the calibration\nof semantic segmentation models and propose a simple yet effective approach.\nFirst, we find that model capacity, crop size, multi-scale testing, and\nprediction correctness have impact on calibration. Among them, prediction\ncorrectness, especially misprediction, is more important to miscalibration due\nto over-confidence. Next, we propose a simple, unifying, and effective\napproach, namely selective scaling, by separating correct/incorrect prediction\nfor scaling and more focusing on misprediction logit smoothing. Then, we study\npopular existing calibration methods and compare them with selective scaling on\nsemantic segmentation calibration. We conduct extensive experiments with a\nvariety of benchmarks on both in-domain and domain-shift calibration, and show\nthat selective scaling consistently outperforms other methods.\n","authors":["Dongdong Wang","Boqing Gong","Liqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.12053v3.pdf","comment":"Accepted to CVPR2023 (8 pages, 4 figures)"},{"id":"http://arxiv.org/abs/2303.11183v1","updated":"2023-03-20T15:10:41Z","published":"2023-03-20T15:10:41Z","title":"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning","summary":"  The goal of data-free meta-learning is to learn useful prior knowledge from a\ncollection of pre-trained models without accessing their training data.\nHowever, existing works only solve the problem in parameter space, which (i)\nignore the fruitful data knowledge contained in the pre-trained models; (ii)\ncan not scale to large-scale pre-trained models; (iii) can only meta-learn\npre-trained models with the same network architecture. To address those issues,\nwe propose a unified framework, dubbed PURER, which contains: (1) ePisode\ncUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion\ncalibRation following inner loop (ICFIL) during meta testing. During meta\ntraining, we propose ECI to perform pseudo episode training for learning to\nadapt fast to new unseen tasks. Specifically, we progressively synthesize a\nsequence of pseudo episodes by distilling the training data from each\npre-trained model. The ECI adaptively increases the difficulty level of pseudo\nepisodes according to the real-time feedback of the meta model. We formulate\nthe optimization process of meta training with ECI as an adversarial form in an\nend-to-end manner. During meta testing, we further propose a simple\nplug-and-play supplement-ICFIL-only used during meta testing to narrow the gap\nbetween meta training and meta testing task distribution. Extensive experiments\nin various real-world scenarios show the superior performance of ours.\n","authors":["Zixuan Hu","Li Shen","Zhenyi Wang","Tongliang Liu","Chun Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11180v1","updated":"2023-03-20T15:05:24Z","published":"2023-03-20T15:05:24Z","title":"Self-Correctable and Adaptable Inference for Generalizable Human Pose\n  Estimation","summary":"  A central challenge in human pose estimation, as well as in many other\nmachine learning and prediction tasks, is the generalization problem. The\nlearned network does not have the capability to characterize the prediction\nerror, generate feedback information from the test sample, and correct the\nprediction error on the fly for each individual test sample, which results in\ndegraded performance in generalization. In this work, we introduce a\nself-correctable and adaptable inference (SCAI) method to address the\ngeneralization challenge of network prediction and use human pose estimation as\nan example to demonstrate its effectiveness and performance. We learn a\ncorrection network to correct the prediction result conditioned by a fitness\nfeedback error. This feedback error is generated by a learned fitness feedback\nnetwork which maps the prediction result to the original input domain and\ncompares it against the original input. Interestingly, we find that this\nself-referential feedback error is highly correlated with the actual prediction\nerror. This strong correlation suggests that we can use this error as feedback\nto guide the correction process. It can be also used as a loss function to\nquickly adapt and optimize the correction network during the inference process.\nOur extensive experimental results on human pose estimation demonstrate that\nthe proposed SCAI method is able to significantly improve the generalization\ncapability and performance of human pose estimation.\n","authors":["Zhehan Kan","Shuoshuo Chen","Ce Zhang","Yushun Tang","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2303.11180v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11177v1","updated":"2023-03-20T15:00:52Z","published":"2023-03-20T15:00:52Z","title":"Integration of Radiomics and Tumor Biomarkers in Interpretable Machine\n  Learning Models","summary":"  Despite the unprecedented performance of deep neural networks (DNNs) in\ncomputer vision, their practical application in the diagnosis and prognosis of\ncancer using medical imaging has been limited. One of the critical challenges\nfor integrating diagnostic DNNs into radiological and oncological applications\nis their lack of interpretability, preventing clinicians from understanding the\nmodel predictions. Therefore, we study and propose the integration of\nexpert-derived radiomics and DNN-predicted biomarkers in interpretable\nclassifiers which we call ConRad, for computerized tomography (CT) scans of\nlung cancer. Importantly, the tumor biomarkers are predicted from a concept\nbottleneck model (CBM) such that once trained, our ConRad models do not require\nlabor-intensive and time-consuming biomarkers. In our evaluation and practical\napplication, the only input to ConRad is a segmented CT scan. The proposed\nmodel is compared to convolutional neural networks (CNNs) which act as a black\nbox classifier. We further investigated and evaluated all combinations of\nradiomics, predicted biomarkers and CNN features in five different classifiers.\nWe found the ConRad models using non-linear SVM and the logistic regression\nwith the Lasso outperform others in five-fold cross-validation, although we\nhighlight that interpretability of ConRad is its primary advantage. The Lasso\nis used for feature selection, which substantially reduces the number of\nnon-zero weights while increasing the accuracy. Overall, the proposed ConRad\nmodel combines CBM-derived biomarkers and radiomics features in an\ninterpretable ML model which perform excellently for the lung nodule malignancy\nclassification.\n","authors":["Lennart Brocki","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10941v2","updated":"2023-03-20T14:57:52Z","published":"2023-01-26T05:14:12Z","title":"GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency","summary":"  We present a novel framework to regularize Neural Radiance Field (NeRF) in a\nfew-shot setting with a geometry-aware consistency regularization. The proposed\napproach leverages a rendered depth map at unobserved viewpoint to warp sparse\ninput images to the unobserved viewpoint and impose them as pseudo ground\ntruths to facilitate learning of NeRF. By encouraging such geometry-aware\nconsistency at a feature-level instead of using pixel-level reconstruction\nloss, we regularize the NeRF at semantic and structural levels while allowing\nfor modeling view dependent radiance to account for color variations across\nviewpoints. We also propose an effective method to filter out erroneous warped\nsolutions, along with training strategies to stabilize training during\noptimization. We show that our model achieves competitive results compared to\nstate-of-the-art few-shot NeRF models. Project page is available at\nhttps://ku-cvlab.github.io/GeCoNeRF/.\n","authors":["Minseop Kwak","Jiuhn Song","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2301.10941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.06600v3","updated":"2023-03-20T14:53:07Z","published":"2021-08-14T18:16:12Z","title":"A Self-Distillation Embedded Supervised Affinity Attention Model for\n  Few-Shot Segmentation","summary":"  Few-shot segmentation focuses on the generalization of models to segment\nunseen object with limited annotated samples. However, existing approaches\nstill face two main challenges. First, huge feature distinction between support\nand query images causes knowledge transferring barrier, which harms the\nsegmentation performance. Second, limited support prototypes cannot adequately\nrepresent features of support objects, hard to guide high-quality query\nsegmentation. To deal with the above two issues, we propose self-distillation\nembedded supervised affinity attention model to improve the performance of\nfew-shot segmentation task. Specifically, the self-distillation guided\nprototype module uses self-distillation to align the features of support and\nquery. The supervised affinity attention module generates high-quality query\nattention map to provide sufficient object information. Extensive experiments\nprove that our model significantly improves the performance compared to\nexisting methods. Comprehensive ablation experiments and visualization studies\nalso show the significant effect of our method on few-shot segmentation task.\nOn COCO-20i dataset, we achieve new state-of-the-art results. Training code and\npretrained models are available at https://github.com/cv516Buaa/SD-AANet.\n","authors":["Qi Zhao","Binghao Liu","Shuchang Lyu","Huojin Chen"],"pdf_url":"https://arxiv.org/pdf/2108.06600v3.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.11165v1","updated":"2023-03-20T14:50:27Z","published":"2023-03-20T14:50:27Z","title":"Computationally Budgeted Continual Learning: What Does Matter?","summary":"  Continual Learning (CL) aims to sequentially train models on streams of\nincoming data that vary in distribution by preserving previous knowledge while\nadapting to new data. Current CL literature focuses on restricted access to\npreviously seen data, while imposing no constraints on the computational budget\nfor training. This is unreasonable for applications in-the-wild, where systems\nare primarily constrained by computational and time budgets, not storage. We\nrevisit this problem with a large-scale benchmark and analyze the performance\nof traditional CL approaches in a compute-constrained setting, where effective\nmemory samples used in training can be implicitly restricted as a consequence\nof limited computation. We conduct experiments evaluating various CL sampling\nstrategies, distillation losses, and partial fine-tuning on two large-scale\ndatasets, namely ImageNet2K and Continual Google Landmarks V2 in data\nincremental, class incremental, and time incremental settings. Through\nextensive experiments amounting to a total of over 1500 GPU-hours, we find\nthat, under compute-constrained setting, traditional CL approaches, with no\nexception, fail to outperform a simple minimal baseline that samples uniformly\nfrom memory. Our conclusions are consistent in a different number of stream\ntime steps, e.g., 20 to 200, and under several computational budgets. This\nsuggests that most existing CL methods are particularly too computationally\nexpensive for realistic budgeted deployment. Code for this project is available\nat: https://github.com/drimpossible/BudgetCL.\n","authors":["Ameya Prabhu","Hasan Abed Al Kader Hammoud","Puneet Dokania","Philip H. S. Torr","Ser-Nam Lim","Bernard Ghanem","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2303.11165v1.pdf","comment":"Appearing in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11162v1","updated":"2023-03-20T14:49:03Z","published":"2023-03-20T14:49:03Z","title":"Picture that Sketch: Photorealistic Image Generation from Abstract\n  Sketches","summary":"  Given an abstract, deformed, ordinary sketch from untrained amateurs like you\nand me, this paper turns it into a photorealistic image - just like those shown\nin Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in\nthat we do not dictate an edgemap-like sketch to start with, but aim to work\nwith abstract free-hand human sketches. In doing so, we essentially democratise\nthe sketch-to-photo pipeline, \"picturing\" a sketch regardless of how good you\nsketch. Our contribution at the outset is a decoupled encoder-decoder training\nparadigm, where the decoder is a StyleGAN trained on photos only. This\nimportantly ensures that generated results are always photorealistic. The rest\nis then all centred around how best to deal with the abstraction gap between\nsketch and photo. For that, we propose an autoregressive sketch mapper trained\non sketch-photo pairs that maps a sketch to the StyleGAN latent space. We\nfurther introduce specific designs to tackle the abstract nature of human\nsketches, including a fine-grained discriminative loss on the back of a trained\nsketch-photo retrieval model, and a partial-aware sketch augmentation strategy.\nFinally, we showcase a few downstream tasks our generation model enables,\namongst them is showing how fine-grained sketch-based image retrieval, a\nwell-studied problem in the sketch community, can be reduced to an image\n(generated) to image retrieval task, surpassing state-of-the-arts. We put\nforward generated results in the supplementary for everyone to scrutinise.\n","authors":["Subhadeep Koley","Ayan Kumar Bhunia","Aneeshan Sain","Pinaki Nath Chowdhury","Tao Xiang","Yi-Zhe song"],"pdf_url":"https://arxiv.org/pdf/2303.11162v1.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://subhadeepkoley.github.io/PictureThatSketch"},{"id":"http://arxiv.org/abs/2303.09917v2","updated":"2023-03-20T14:39:32Z","published":"2023-03-16T13:43:02Z","title":"Vision Transformer for Action Units Detection","summary":"  Facial Action Units detection (FAUs) represents a fine-grained classification\nproblem that involves identifying different units on the human face, as defined\nby the Facial Action Coding System. In this paper, we present a simple yet\nefficient Vision Transformer-based approach for addressing the task of Action\nUnits (AU) detection in the context of Affective Behavior Analysis in-the-wild\n(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to\ncapture the temporal facial change in the video. Besides, to reduce massive\nsize of the Vision Transformers model, we replace the ViViT feature extraction\nlayers with the CNN backbone (Regnet). Our model outperform the baseline model\nof ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,\nthe achieved results are comparable to those of the top three teams in the\nprevious ABAW 2022 challenge.\n","authors":["Tu Vu","Van Thong Huynh","Soo Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2303.09917v2.pdf","comment":"Will be updated"},{"id":"http://arxiv.org/abs/2211.13203v3","updated":"2023-03-20T14:32:01Z","published":"2022-11-23T18:44:25Z","title":"Inversion-Based Style Transfer with Diffusion Models","summary":"  The artistic style within a painting is the means of expression, which\nincludes not only the painting material, colors, and brushstrokes, but also the\nhigh-level attributes including semantic elements, object shapes, etc. Previous\narbitrary example-guided artistic image generation methods often fail to\ncontrol shape changes or convey elements. The pre-trained text-to-image\nsynthesis diffusion probabilistic models have achieved remarkable quality, but\nit often requires extensive textual descriptions to accurately portray\nattributes of a particular painting. We believe that the uniqueness of an\nartwork lies precisely in the fact that it cannot be adequately explained with\nnormal language. Our key idea is to learn artistic style directly from a single\npainting and then guide the synthesis without providing complex textual\ndescriptions. Specifically, we assume style as a learnable textual description\nof a painting. We propose an inversion-based style transfer method (InST),\nwhich can efficiently and accurately learn the key information of an image,\nthus capturing and transferring the artistic style of a painting. We\ndemonstrate the quality and efficiency of our method on numerous paintings of\nvarious artists and styles. Code and models are available at\nhttps://github.com/zyxElsa/InST.\n","authors":["Yuxin Zhang","Nisha Huang","Fan Tang","Haibin Huang","Chongyang Ma","Weiming Dong","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2211.13203v3.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11137v1","updated":"2023-03-20T14:15:23Z","published":"2023-03-20T14:15:23Z","title":"AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion\n  Models","summary":"  It is a time-consuming and tedious work for manually colorizing anime line\ndrawing images, which is an essential stage in cartoon animation creation\npipeline. Reference-based line drawing colorization is a challenging task that\nrelies on the precise cross-domain long-range dependency modelling between the\nline drawing and reference image. Existing learning methods still utilize\ngenerative adversarial networks (GANs) as one key module of their model\narchitecture. In this paper, we propose a novel method called AnimeDiffusion\nusing diffusion models that performs anime face line drawing colorization\nautomatically. To the best of our knowledge, this is the first diffusion model\ntailored for anime content creation. In order to solve the huge training\nconsumption problem of diffusion models, we design a hybrid training strategy,\nfirst pre-training a diffusion model with classifier-free guidance and then\nfine-tuning it with image reconstruction guidance. We find that with a few\niterations of fine-tuning, the model shows wonderful colorization performance,\nas illustrated in Fig. 1. For training AnimeDiffusion, we conduct an anime face\nline drawing colorization benchmark dataset, which contains 31696 training data\nand 579 testing data. We hope this dataset can fill the gap of no available\nhigh resolution anime face dataset for colorization method evaluation. Through\nmultiple quantitative metrics evaluated on our dataset and a user study, we\ndemonstrate AnimeDiffusion outperforms state-of-the-art GANs-based models for\nanime face line drawing colorization. We also collaborate with professional\nartists to test and apply our AnimeDiffusion for their creation work. We\nrelease our code on https://github.com/xq-meng/AnimeDiffusion.\n","authors":["Yu Cao","Xiangqiao Meng","P. Y. Mok","Xueting Liu","Tong-Yee Lee","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2303.11137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11135v1","updated":"2023-03-20T14:12:55Z","published":"2023-03-20T14:12:55Z","title":"TWINS: A Fine-Tuning Framework for Improved Transferability of\n  Adversarial Robustness and Generalization","summary":"  Recent years have seen the ever-increasing importance of pre-trained models\nand their downstream training in deep learning research and applications. At\nthe same time, the defense for adversarial examples has been mainly\ninvestigated in the context of training from random initialization on simple\nclassification tasks. To better exploit the potential of pre-trained models in\nadversarial robustness, this paper focuses on the fine-tuning of an\nadversarially pre-trained model in various classification tasks. Existing\nresearch has shown that since the robust pre-trained model has already learned\na robust feature extractor, the crucial question is how to maintain the\nrobustness in the pre-trained model when learning the downstream task. We study\nthe model-based and data-based approaches for this goal and find that the two\ncommon approaches cannot achieve the objective of improving both generalization\nand adversarial robustness. Thus, we propose a novel statistics-based approach,\nTwo-WIng NormliSation (TWINS) fine-tuning framework, which consists of two\nneural networks where one of them keeps the population means and variances of\npre-training data in the batch normalization layers. Besides the robust\ninformation transfer, TWINS increases the effective learning rate without\nhurting the training stability since the relationship between a weight norm and\nits gradient norm in standard batch normalization layer is broken, resulting in\na faster escape from the sub-optimal initialization and alleviating the robust\noverfitting. Finally, TWINS is shown to be effective on a wide range of image\nclassification datasets in terms of both generalization and robustness. Our\ncode is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n","authors":["Ziquan Liu","Yi Xu","Xiangyang Ji","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2303.11135v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11130v1","updated":"2023-03-20T14:06:32Z","published":"2023-03-20T14:06:32Z","title":"Deep learning automated quantification of lung disease in pulmonary\n  hypertension on CT pulmonary angiography: A preliminary clinical study with\n  external validation","summary":"  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)\nis essential for appropriate patient management. This study aims to develop an\nartificial intelligence (AI) deep learning model for lung texture\nclassification in CT Pulmonary Angiography (CTPA), and evaluate its correlation\nwith clinical assessment methods.\n  Materials and Methods: In this retrospective study with external validation,\n122 patients with pre-capillary PH were used to train (n=83), validate (n=17)\nand test (n=10 internal test, n=12 external test) a patch based DenseNet-121\nclassification model. \"Normal\", \"Ground glass\", \"Ground glass with\nreticulation\", \"Honeycombing\", and \"Emphysema\" were classified as per the\nFleishner Society glossary of terms. Ground truth classes were segmented by two\nradiologists with patches extracted from the labelled regions. Proportion of\nlung volume for each texture was calculated by classifying patches throughout\nthe entire lung volume to generate a coarse texture classification mapping\nthroughout the lung parenchyma. AI output was assessed against diffusing\ncapacity of carbon monoxide (DLCO) and specialist radiologist reported disease\nseverity.\n  Results: Micro-average AUCs for the validation, internal test, and external\ntest were 0.92, 0.95, and 0.94, respectively. The model had consistent\nperformance across parenchymal textures, demonstrated strong correlation with\ndiffusing capacity of carbon monoxide (DLCO), and showed good correspondence\nwith disease severity reported by specialist radiologists.\n  Conclusion: The classification model demonstrates excellent performance on\nexternal validation. The clinical utility of its output has been demonstrated.\nThis objective, repeatable measure of disease severity can aid in patient\nmanagement in adjunct to radiological reporting.\n","authors":["Michael J. Sharkey","Krit Dwivedi","Samer Alabed","Andrew J. Swift"],"pdf_url":"https://arxiv.org/pdf/2303.11130v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.11127v1","updated":"2023-03-20T14:04:50Z","published":"2023-03-20T14:04:50Z","title":"MT-SNN: Enhance Spiking Neural Network with Multiple Thresholds","summary":"  Spiking neural networks (SNNs), as a biology-inspired method mimicking the\nspiking nature of brain neurons, is a promising energy-efficient alternative to\nthe traditional artificial neural networks (ANNs). The energy saving of SNNs is\nmainly from multiplication free property brought by binarized intermediate\nactivations. In this paper, we proposed a Multiple Threshold (MT) approach to\nalleviate the precision loss brought by the binarized activations, such that\nSNNs can reach higher accuracy at fewer steps. We evaluate the approach on\nCIFAR10, CIFAR100 and DVS-CIFAR10, and demonstrate that MT can promote SNNs\nextensively, especially at early steps. For example, With MT,\nParametric-Leaky-Integrate-Fire(PLIF) based VGG net can even outperform the ANN\ncounterpart with 1 step.\n","authors":["Xiaoting Wang","Yanxiang Zhang","Yongzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11127v1.pdf","comment":"10 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.11126v1","updated":"2023-03-20T14:04:40Z","published":"2023-03-20T14:04:40Z","title":"Robustifying Token Attention for Vision Transformers","summary":"  Despite the success of vision transformers (ViTs), they still suffer from\nsignificant drops in accuracy in the presence of common corruptions, such as\nnoise or blur. Interestingly, we observe that the attention mechanism of ViTs\ntends to rely on few important tokens, a phenomenon we call token overfocusing.\nMore critically, these tokens are not robust to corruptions, often leading to\nhighly diverging attention patterns. In this paper, we intend to alleviate this\noverfocusing issue and make attention more stable through two general\ntechniques: First, our Token-aware Average Pooling (TAP) module encourages the\nlocal neighborhood of each token to take part in the attention mechanism.\nSpecifically, TAP learns average pooling schemes for each token such that the\ninformation of potentially important tokens in the neighborhood can adaptively\nbe taken into account. Second, we force the output tokens to aggregate\ninformation from a diverse set of input tokens rather than focusing on just a\nfew by using our Attention Diversification Loss (ADL). We achieve this by\npenalizing high cosine similarity between the attention vectors of different\ntokens. In experiments, we apply our methods to a wide range of transformer\narchitectures and improve robustness significantly. For example, we improve\ncorruption robustness on ImageNet-C by 2.4% while simultaneously improving\naccuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when\nfinetuning on semantic segmentation tasks, we improve robustness on\nCityScapes-C by 2.4% and ACDC by 3.1%.\n","authors":["Yong Guo","David Stutz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11126v1.pdf","comment":"7 figures and 5 tables in the main paper"},{"id":"http://arxiv.org/abs/2303.11120v1","updated":"2023-03-20T14:01:01Z","published":"2023-03-20T14:01:01Z","title":"Positional Diffusion: Ordering Unordered Sets with Diffusion\n  Probabilistic Models","summary":"  Positional reasoning is the process of ordering unsorted parts contained in a\nset into a consistent structure. We present Positional Diffusion, a\nplug-and-play graph formulation with Diffusion Probabilistic Models to address\npositional reasoning. We use the forward process to map elements' positions in\na set to random positions in a continuous space. Positional Diffusion learns to\nreverse the noising process and recover the original positions through an\nAttention-based Graph Neural Network. We conduct extensive experiments with\nbenchmark datasets including two puzzle datasets, three sentence ordering\ndatasets, and one visual storytelling dataset, demonstrating that our method\noutperforms long-lasting research on puzzle solving with up to +18% compared to\nthe second-best deep learning method, and performs on par against the\nstate-of-the-art methods on sentence ordering and visual storytelling. Our work\nhighlights the suitability of diffusion models for ordering problems and\nproposes a novel formulation and method for solving various ordering tasks.\nProject website at https://iit-pavis.github.io/Positional_Diffusion/\n","authors":["Francesco Giuliari","Gianluca Scarpellini","Stuart James","Yiming Wang","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2303.11120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11114v1","updated":"2023-03-20T13:55:35Z","published":"2023-03-20T13:55:35Z","title":"SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel\n  Storage","summary":"  We need billion-scale images to achieve more generalizable and\nground-breaking vision models, as well as massive dataset storage to ship the\nimages (e.g., the LAION-4B dataset needs 240TB storage space). However, it has\nbecome challenging to deal with unlimited dataset storage with limited storage\ninfrastructure. A number of storage-efficient training methods have been\nproposed to tackle the problem, but they are rarely scalable or suffer from\nsevere damage to performance. In this paper, we propose a storage-efficient\ntraining strategy for vision classifiers for large-scale datasets (e.g.,\nImageNet) that only uses 1024 tokens per instance without using the raw level\npixels; our token storage only needs <1% of the original JPEG-compressed raw\npixels. We also propose token augmentations and a Stem-adaptor module to make\nour approach able to use the same architecture as pixel-based approaches with\nonly minimal modifications on the stem layer and the carefully tuned\noptimization settings. Our experimental results on ImageNet-1k show that our\nmethod significantly outperforms other storage-efficient training methods with\na large gap. We further show the effectiveness of our method in other practical\nscenarios, storage-efficient pre-training, and continual learning. Code is\navailable at https://github.com/naver-ai/seit\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Wonjae Kim","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11114v1.pdf","comment":"First two authors contributed equally; 15 pages, 1.1MB"},{"id":"http://arxiv.org/abs/2303.11108v1","updated":"2023-03-20T13:45:58Z","published":"2023-03-20T13:45:58Z","title":"I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue","summary":"  Although there have been considerable research efforts on controllable facial\nimage editing, the desirable interactive setting where the users can interact\nwith the system to adjust their requirements dynamically hasn't been well\nexplored. This paper focuses on facial image editing via dialogue and\nintroduces a new benchmark dataset, Multi-turn Interactive Image Editing\n(I2Edit), for evaluating image editing quality and interaction ability in\nreal-world interactive facial editing scenarios. The dataset is constructed\nupon the CelebA-HQ dataset with images annotated with a multi-turn dialogue\nthat corresponds to the user editing requirements. I2Edit is challenging, as it\nneeds to 1) track the dynamically updated user requirements and edit the images\naccordingly, as well as 2) generate the appropriate natural language response\nto communicate with the user. To address these challenges, we propose a\nframework consisting of a dialogue module and an image editing module. The\nformer is for user edit requirements tracking and generating the corresponding\nindicative responses, while the latter edits the images conditioned on the\ntracked user edit requirements. In contrast to previous works that simply treat\nmulti-turn interaction as a sequence of single-turn interactions, we extract\nthe user edit requirements from the whole dialogue history instead of the\ncurrent single turn. The extracted global user edit requirements enable us to\ndirectly edit the input raw image to avoid error accumulation and attribute\nforgetting issues. Extensive quantitative and qualitative experiments on the\nI2Edit dataset demonstrate the advantage of our proposed framework over the\nprevious single-turn methods. We believe our new dataset could serve as a\nvaluable resource to push forward the exploration of real-world, complex\ninteractive image editing. Code and data will be made public.\n","authors":["Xing Cui","Zekun Li","Peipei Li","Yibo Hu","Hailin Shi","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11101v1","updated":"2023-03-20T13:38:29Z","published":"2023-03-20T13:38:29Z","title":"Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning","summary":"  Deep learning in general domains has constantly been extended to\ndomain-specific tasks requiring the recognition of fine-grained\ncharacteristics. However, real-world applications for fine-grained tasks suffer\nfrom two challenges: a high reliance on expert knowledge for annotation and\nnecessity of a versatile model for various downstream tasks in a specific\ndomain (e.g., prediction of categories, bounding boxes, or pixel-wise\nannotations). Fortunately, the recent self-supervised learning (SSL) is a\npromising approach to pretrain a model without annotations, serving as an\neffective initialization for any downstream tasks. Since SSL does not rely on\nthe presence of annotation, in general, it utilizes the large-scale unlabeled\ndataset, referred to as an open-set. In this sense, we introduce a novel\nOpen-Set Self-Supervised Learning problem under the assumption that a\nlarge-scale unlabeled open-set is available, as well as the fine-grained target\ndataset, during a pretraining phase. In our problem setup, it is crucial to\nconsider the distribution mismatch between the open-set and target dataset.\nHence, we propose SimCore algorithm to sample a coreset, the subset of an\nopen-set that has a minimum distance to the target dataset in the latent space.\nWe demonstrate that SimCore significantly improves representation learning\nperformance through extensive experimental settings, including eleven\nfine-grained datasets and seven open-sets in various downstream tasks.\n","authors":["Sungnyun Kim","Sangmin Bae","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11101v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11100v1","updated":"2023-03-20T13:34:28Z","published":"2023-03-20T13:34:28Z","title":"A Multi-Task Deep Learning Approach for Sensor-based Human Activity\n  Recognition and Segmentation","summary":"  Sensor-based human activity segmentation and recognition are two important\nand challenging problems in many real-world applications and they have drawn\nincreasing attention from the deep learning community in recent years. Most of\nthe existing deep learning works were designed based on pre-segmented sensor\nstreams and they have treated activity segmentation and recognition as two\nseparate tasks. In practice, performing data stream segmentation is very\nchallenging. We believe that both activity segmentation and recognition may\nconvey unique information which can complement each other to improve the\nperformance of the two tasks. In this paper, we firstly proposes a new\nmultitask deep neural network to solve the two tasks simultaneously. The\nproposed neural network adopts selective convolution and features multiscale\nwindows to segment activities of long or short time durations. First, multiple\nwindows of different scales are generated to center on each unit of the feature\nsequence. Then, the model is trained to predict, for each window, the activity\nclass and the offset to the true activity boundaries. Finally, overlapping\nwindows are filtered out by non-maximum suppression, and adjacent windows of\nthe same activity are concatenated to complete the segmentation task. Extensive\nexperiments were conducted on eight popular benchmarking datasets, and the\nresults show that our proposed method outperforms the state-of-the-art methods\nboth for activity recognition and segmentation.\n","authors":["Furong Duan","Tao Zhu","Jinqiang Wang","Liming Chen","Huansheng Ning","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2303.11100v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.11098v1","updated":"2023-03-20T13:33:31Z","published":"2023-03-20T13:33:31Z","title":"A closer look at the training dynamics of knowledge distillation","summary":"  In this paper we revisit the efficacy of knowledge distillation as a function\nmatching and metric learning problem. In doing so we verify three important\ndesign decisions, namely the normalisation, soft maximum function, and\nprojection layers as key ingredients. We theoretically show that the projector\nimplicitly encodes information on past examples, enabling relational gradients\nfor the student. We then show that the normalisation of representations is\ntightly coupled with the training dynamics of this projector, which can have a\nlarge impact on the students performance. Finally, we show that a simple soft\nmaximum function can be used to address any significant capacity gap problems.\nExperimental results on various benchmark datasets demonstrate that using these\ninsights can lead to superior or comparable performance to state-of-the-art\nknowledge distillation techniques, despite being much more computationally\nefficient. In particular, we obtain these results across image classification\n(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult\ndistillation objectives, such as training data efficient transformers, whereby\nwe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.\n","authors":["Roy Miles","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2303.11098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07564v2","updated":"2023-03-20T13:28:36Z","published":"2023-03-14T01:10:59Z","title":"Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow","summary":"  Optical flow has achieved great success under clean scenes, but suffers from\nrestricted performance under foggy scenes. To bridge the clean-to-foggy domain\ngap, the existing methods typically adopt the domain adaptation to transfer the\nmotion knowledge from clean to synthetic foggy domain. However, these methods\nunexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous\nwhen applied to real-world scenes. To handle the practical optical flow under\nreal foggy scenes, in this work, we propose a novel unsupervised cumulative\ndomain adaptation optical flow (UCDA-Flow) framework: depth-association motion\nadaptation and correlation-alignment motion adaptation. Specifically, we\ndiscover that depth is a key ingredient to influence the optical flow: the\ndeeper depth, the inferior optical flow, which motivates us to design a\ndepth-association motion adaptation module to bridge the clean-to-foggy domain\ngap. Moreover, we figure out that the cost volume correlation shares similar\ndistribution of the synthetic and real foggy images, which enlightens us to\ndevise a correlation-alignment motion adaptation module to distill motion\nknowledge of the synthetic foggy domain to the real foggy domain. Note that\nsynthetic fog is designed as the intermediate domain. Under this unified\nframework, the proposed cumulative adaptation progressively transfers knowledge\nfrom clean scenes to real foggy scenes. Extensive experiments have been\nperformed to verify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Wending Yan","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2303.07564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11090v1","updated":"2023-03-20T13:22:56Z","published":"2023-03-20T13:22:56Z","title":"Scene Graph Based Fusion Network For Image-Text Retrieval","summary":"  A critical challenge to image-text retrieval is how to learn accurate\ncorrespondences between images and texts. Most existing methods mainly focus on\ncoarse-grained correspondences based on co-occurrences of semantic objects,\nwhile failing to distinguish the fine-grained local correspondences. In this\npaper, we propose a novel Scene Graph based Fusion Network (dubbed SGFN), which\nenhances the images'/texts' features through intra- and cross-modal fusion for\nimage-text retrieval. To be specific, we design an intra-modal hierarchical\nattention fusion to incorporate semantic contexts, such as objects, attributes,\nand relationships, into images'/texts' feature vectors via scene graphs, and a\ncross-modal attention fusion to combine the contextual semantics and local\nfusion via contextual vectors. Extensive experiments on public datasets\nFlickr30K and MSCOCO show that our SGFN performs better than quite a few SOTA\nimage-text retrieval methods.\n","authors":["Guoliang Wang","Yanlei Shang","Yong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11089v1","updated":"2023-03-20T13:22:04Z","published":"2023-03-20T13:22:04Z","title":"EmoTalk: Speech-driven emotional disentanglement for 3D face animation","summary":"  Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk\n","authors":["Ziqiao Peng","Haoyu Wu","Zhenbo Song","Hao Xu","Xiangyu Zhu","Hongyan Liu","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11086v1","updated":"2023-03-20T13:20:14Z","published":"2023-03-20T13:20:14Z","title":"Pluralistic Aging Diffusion Autoencoder","summary":"  Face aging is an ill-posed problem because multiple plausible aging patterns\nmay correspond to a given input. Most existing methods often produce one\ndeterministic estimation. This paper proposes a novel CLIP-driven Pluralistic\nAging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.\nFirst, we employ diffusion models to generate diverse low-level aging details\nvia a sequential denoising reverse process. Second, we present Probabilistic\nAging Embedding (PAE) to capture diverse high-level aging patterns, which\nrepresents age information as probabilistic distributions in the common CLIP\nlatent space. A text-guided KL-divergence loss is designed to guide this\nlearning. Our method can achieve pluralistic face aging conditioned on\nopen-world aging texts and arbitrary unseen face images. Qualitative and\nquantitative experiments demonstrate that our method can generate more diverse\nand high-quality plausible aging results.\n","authors":["Peipei Li","Rui Wang","Huaibo Huang","Ran He","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11086v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.09858v2","updated":"2023-03-20T13:00:49Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zhao","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11073v1","updated":"2023-03-20T12:59:32Z","published":"2023-03-20T12:59:32Z","title":"Discovering Interpretable Directions in the Semantic Latent Space of\n  Diffusion Models","summary":"  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to\nGenerative Adversarial Networks (GANs). However, despite their widespread use\nin image synthesis and editing applications, their latent space is still not as\nwell understood. Recently, a semantic latent space for DDMs, coined\n`$h$-space', was shown to facilitate semantic image editing in a way\nreminiscent of GANs. The $h$-space is comprised of the bottleneck activations\nin the DDM's denoiser across all timesteps of the diffusion process. In this\npaper, we explore the properties of h-space and propose several novel methods\nfor finding meaningful semantic directions within it. We start by studying\nunsupervised methods for revealing interpretable semantic directions in\npretrained DDMs. Specifically, we show that global latent directions emerge as\nthe principal components in the latent space. Additionally, we provide a novel\nmethod for discovering image-specific semantic directions by spectral analysis\nof the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the\nanalysis by finding directions in a supervised fashion in unconditional DDMs.\nWe demonstrate how such directions can be found by relying on either a labeled\ndata set of real images or by annotating generated samples with a\ndomain-specific attribute classifier. We further show how to semantically\ndisentangle the found direction by simple linear projection. Our approaches are\napplicable without requiring any architectural modifications, text-based\nguidance, CLIP-based optimization, or model fine-tuning.\n","authors":["René Haas","Inbar Huberman-Spiegelglas","Rotem Mulayoff","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2303.11073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11436v3","updated":"2023-03-20T12:48:37Z","published":"2022-11-21T13:23:52Z","title":"N-Gram in Swin Transformers for Efficient Lightweight Image\n  Super-Resolution","summary":"  While some studies have proven that Swin Transformer (Swin) with window\nself-attention (WSA) is suitable for single image super-resolution (SR), the\nplain WSA ignores the broad regions when reconstructing high-resolution images\ndue to a limited receptive field. In addition, many deep learning SR methods\nsuffer from intensive computations. To address these problems, we introduce the\nN-Gram context to the low-level vision with Transformers for the first time. We\ndefine N-Gram as neighboring local windows in Swin, which differs from text\nanalysis that views N-Gram as consecutive characters or words. N-Grams interact\nwith each other by sliding-WSA, expanding the regions seen to restore degraded\npixels. Using the N-Gram context, we propose NGswin, an efficient SR network\nwith SCDP bottleneck taking multi-scale outputs of the hierarchical encoder.\nExperimental results show that NGswin achieves competitive performance while\nmaintaining an efficient structure when compared with previous leading methods.\nMoreover, we also improve other Swin-based SR methods with the N-Gram context,\nthereby building an enhanced model: SwinIR-NG. Our improved SwinIR-NG\noutperforms the current best lightweight SR approaches and establishes\nstate-of-the-art results. Codes are available at\nhttps://github.com/rami0205/NGramSwin.\n","authors":["Haram Choi","Jeongmin Lee","Jihoon Yang"],"pdf_url":"https://arxiv.org/pdf/2211.11436v3.pdf","comment":"CVPR 2023 camera-ready. Codes are available at\n  https://github.com/rami0205/NGramSwin"},{"id":"http://arxiv.org/abs/2303.11066v1","updated":"2023-03-20T12:44:11Z","published":"2023-03-20T12:44:11Z","title":"Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data","summary":"  Semi-supervised learning (SSL) has attracted enormous attention due to its\nvast potential of mitigating the dependence on large labeled datasets. The\nlatest methods (e.g., FixMatch) use a combination of consistency regularization\nand pseudo-labeling to achieve remarkable successes. However, these methods all\nsuffer from the waste of complicated examples since all pseudo-labels have to\nbe selected by a high threshold to filter out noisy ones. Hence, the examples\nwith ambiguous predictions will not contribute to the training phase. For\nbetter leveraging all unlabeled examples, we propose two novel techniques:\nEntropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML\nincorporates the prediction distribution of non-target classes into the\noptimization objective to avoid competition with target class, and thus\ngenerating more high-confidence predictions for selecting pseudo-label. ANL\nintroduces the additional negative pseudo-label for all unlabeled data to\nleverage low-confidence examples. It adaptively allocates this label by\ndynamically evaluating the top-k performance of the model. EML and ANL do not\nintroduce any additional parameter and hyperparameter. We integrate these\ntechniques with FixMatch, and develop a simple yet powerful framework called\nFullMatch. Extensive experiments on several common SSL benchmarks\n(CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate that FullMatch exceeds\nFixMatch by a large margin. Integrated with FlexMatch (an advanced\nFixMatch-based framework), we achieve state-of-the-art performance. Source code\nis at https://github.com/megvii-research/FullMatch.\n","authors":["Yuhao Chen","Xin Tan","Borui Zhao","Zhaowei Chen","Renjie Song","Jiajun Liang","Xuequan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.11066v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2112.12925v2","updated":"2023-03-20T12:30:36Z","published":"2021-12-24T03:25:40Z","title":"Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel\n  Perspective","summary":"  We revisit Semantic Scene Completion (SSC), a useful task to predict the\nsemantic and occupancy representation of 3D scenes, in this paper. A number of\nmethods for this task are always based on voxelized scene representations for\nkeeping local scene structure. However, due to the existence of visible empty\nvoxels, these methods always suffer from heavy computation redundancy when the\nnetwork goes deeper, and thus limit the completion quality. To address this\ndilemma, we propose our novel point-voxel aggregation network for this task.\nFirstly, we transfer the voxelized scenes to point clouds by removing these\nvisible empty voxels and adopt a deep point stream to capture semantic\ninformation from the scene efficiently. Meanwhile, a light-weight voxel stream\ncontaining only two 3D convolution layers preserves local structures of the\nvoxelized scenes. Furthermore, we design an anisotropic voxel aggregation\noperator to fuse the structure details from the voxel stream into the point\nstream, and a semantic-aware propagation module to enhance the up-sampling\nprocess in the point stream by semantic labels. We demonstrate that our model\nsurpasses state-of-the-arts on two benchmarks by a large margin, with only\ndepth images as the input.\n","authors":["Xiaokang Chen","Jiaxiang Tang","Jingbo Wang","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2112.12925v2.pdf","comment":"Accepted to AAAI 2022"},{"id":"http://arxiv.org/abs/2303.06869v3","updated":"2023-03-20T12:24:58Z","published":"2023-03-13T05:37:40Z","title":"Adaptive Data-Free Quantization","summary":"  Data-free quantization (DFQ) recovers the performance of quantized network\n(Q) without the original data, but generates the fake sample via a generator\n(G) by learning from full-precision network (P), which, however, is totally\nindependent of Q, overlooking the adaptability of the knowledge from generated\nsamples, i.e., informative or not to the learning process of Q, resulting into\nthe overflow of generalization error. Building on this, several critical\nquestions -- how to measure the sample adaptability to Q under varied bit-width\nscenarios? whether the largest adaptability is the best? how to generate the\nsamples with adaptive adaptability to improve Q's generalization? To answer the\nabove questions, in this paper, we propose an Adaptive Data-Free Quantization\n(AdaDFQ) method, which revisits DFQ from a zero-sum game perspective upon the\nsample adaptability between two players -- a generator and a quantized network.\nFollowing this viewpoint, we further define the disagreement and agreement\nsamples to form two boundaries, where the margin is optimized to adaptively\nregulate the adaptability of generated samples to Q, so as to address the\nover-and-under fitting issues. Our AdaDFQ reveals: 1) the largest adaptability\nis NOT the best for sample generation to benefit Q's generalization; 2) the\nknowledge of the generated sample should not be informative to Q only, but also\nrelated to the category and distribution information of the training data for\nP. The theoretical and empirical analysis validate the advantages of AdaDFQ\nover the state-of-the-arts. Our code is available at\nhttps://github.com/hfutqian/AdaDFQ.\n","authors":["Biao Qian","Yang Wang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06869v3.pdf","comment":"9 pages, 6 figures, Refined camera ready version for CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09158v2","updated":"2023-03-20T12:17:53Z","published":"2023-03-16T08:47:36Z","title":"Facial Affect Recognition based on Transformer Encoder and Audiovisual\n  Fusion for the ABAW5 Challenge","summary":"  In this paper, we present our solutions for the 5th Workshop and Competition\non Affective Behavior Analysis in-the-wild (ABAW), which includes four\nsub-challenges of Valence-Arousal (VA) Estimation, Expression (Expr)\nClassification, Action Unit (AU) Detection and Emotional Reaction Intensity\n(ERI) Estimation. The 5th ABAW competition focuses on facial affect recognition\nutilizing different modalities and datasets. In our work, we extract powerful\naudio and visual features using a large number of sota models. These features\nare fused by Transformer Encoder and TEMMA. Besides, to avoid the possible\nimpact of large dimensional differences between various features, we design an\nAffine Module to align different features to the same dimension. Extensive\nexperiments demonstrate that the superiority of the proposed method. For the VA\nEstimation sub-challenge, our method obtains the mean Concordance Correlation\nCoefficient (CCC) of 0.6066. For the Expression Classification sub-challenge,\nthe average F1 Score is 0.4055. For the AU Detection sub-challenge, the average\nF1 Score is 0.5296. For the Emotional Reaction Intensity Estimation\nsub-challenge, the average pearson's correlations coefficient on the validation\nset is 0.3968. All of the results of four sub-challenges outperform the\nbaseline with a large margin.\n","authors":["Ziyang Zhang","Liuwei An","Zishun Cui","Ao xu","Tengteng Dong","Yueqi Jiang","Jingyi Shi","Xin Liu","Xiao Sun","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11057v1","updated":"2023-03-20T12:14:13Z","published":"2023-03-20T12:14:13Z","title":"Learning Foresightful Dense Visual Affordance for Deformable Object\n  Manipulation","summary":"  Understanding and manipulating deformable objects (e.g., ropes and fabrics)\nis an essential yet challenging task with broad applications. Difficulties come\nfrom complex states and dynamics, diverse configurations and high-dimensional\naction space of deformable objects. Besides, the manipulation tasks usually\nrequire multiple steps to accomplish, and greedy policies may easily lead to\nlocal optimal states. Existing studies usually tackle this problem using\nreinforcement learning or imitating expert demonstrations, with limitations in\nmodeling complex states or requiring hand-crafted expert policies. In this\npaper, we study deformable object manipulation using dense visual affordance,\nwith generalization towards diverse states, and propose a novel kind of\nforesightful dense affordance, which avoids local optima by estimating states'\nvalues for long-term manipulation. We propose a framework for learning this\nrepresentation, with novel designs such as multi-stage stable learning and\nefficient self-supervised data collection without experts. Experiments\ndemonstrate the superiority of our proposed foresightful dense affordance.\nProject page: https://hyperplane-lab.github.io/DeformableAffordance\n","authors":["Ruihai Wu","Chuanruo Ning","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2303.11057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11055v1","updated":"2023-03-20T12:08:58Z","published":"2023-03-20T12:08:58Z","title":"Parameter-Free Channel Attention for Image Classification and\n  Super-Resolution","summary":"  The channel attention mechanism is a useful technique widely employed in deep\nconvolutional neural networks to boost the performance for image processing\ntasks, eg, image classification and image super-resolution. It is usually\ndesigned as a parameterized sub-network and embedded into the convolutional\nlayers of the network to learn more powerful feature representations. However,\ncurrent channel attention induces more parameters and therefore leads to higher\ncomputational costs. To deal with this issue, in this work, we propose a\nParameter-Free Channel Attention (PFCA) module to boost the performance of\npopular image classification and image super-resolution networks, but\ncompletely sweep out the parameter growth of channel attention. Experiments on\nCIFAR-100, ImageNet, and DIV2K validate that our PFCA module improves the\nperformance of ResNet on image classification and improves the performance of\nMSRResNet on image super-resolution tasks, respectively, while bringing little\ngrowth of parameters and FLOPs.\n","authors":["Yuxuan Shi","Lingxiao Yang","Wangpeng An","Xiantong Zhen","Liuqing Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11052v1","updated":"2023-03-20T12:06:14Z","published":"2023-03-20T12:06:14Z","title":"ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real\n  Novel View Synthesis via Contrastive Learning","summary":"  Although many recent works have investigated generalizable NeRF-based novel\nview synthesis for unseen scenes, they seldom consider the synthetic-to-real\ngeneralization, which is desired in many practical applications. In this work,\nwe first investigate the effects of synthetic data in synthetic-to-real novel\nview synthesis and surprisingly observe that models trained with synthetic data\ntend to produce sharper but less accurate volume densities. For pixels where\nthe volume densities are correct, fine-grained details will be obtained.\nOtherwise, severe artifacts will be produced. To maintain the advantages of\nusing synthetic data while avoiding its negative effects, we propose to\nintroduce geometry-aware contrastive learning to learn multi-view consistent\nfeatures with geometric constraints. Meanwhile, we adopt cross-view attention\nto further enhance the geometry perception of features by querying features\nacross input views. Experiments demonstrate that under the synthetic-to-real\nsetting, our method can render images with higher quality and better\nfine-grained details, outperforming existing generalizable novel view synthesis\nmethods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our\nmethod also achieves state-of-the-art results.\n","authors":["Hao Yang","Lanqing Hong","Aoxue Li","Tianyang Hu","Zhenguo Li","Gim Hee Lee","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11048v1","updated":"2023-03-20T11:59:23Z","published":"2023-03-20T11:59:23Z","title":"Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation","summary":"  In this paper, we propose the semantic graph Transformer (SGT) for the 3D\nscene graph generation. The task aims to parse a cloud point-based scene into a\nsemantic structural graph, with the core challenge of modeling the complex\nglobal structure. Existing methods based on graph convolutional networks (GCNs)\nsuffer from the over-smoothing dilemma and could only propagate information\nfrom limited neighboring nodes. In contrast, our SGT uses Transformer layers as\nthe base building block to allow global information passing, with two types of\nproposed Transformer layers tailored for the 3D scene graph generation task.\nSpecifically, we introduce the graph embedding layer to best utilize the global\ninformation in graph edges while maintaining comparable computation costs.\nAdditionally, we propose the semantic injection layer to leverage categorical\ntext labels and visual object knowledge. We benchmark our SGT on the\nestablished 3DSSG benchmark and achieve a 35.9% absolute improvement in\nrelationship prediction's R@50 and an 80.40% boost on the subset with complex\nscenes over the state-of-the-art. Our analyses further show SGT's superiority\nin the long-tailed and zero-shot scenarios. We will release the code and model.\n","authors":["Changsheng Lv","Mengshi Qi","Xia Li","Zhengyuan Yang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11041v1","updated":"2023-03-20T11:47:02Z","published":"2023-03-20T11:47:02Z","title":"From Sparse to Precise: A Practical Editing Approach for Intracardiac\n  Echocardiography Segmentation","summary":"  Accurate and safe catheter ablation procedures for patients with atrial\nfibrillation require precise segmentation of cardiac structures in Intracardiac\nEchocardiography (ICE) imaging. Prior studies have suggested methods that\nemploy 3D geometry information from the ICE transducer to create a sparse ICE\nvolume by placing 2D frames in a 3D grid, enabling training of 3D segmentation\nmodels. However, the resulting 3D masks from these models can be inaccurate and\nmay lead to serious clinical complications due to the sparse sampling in ICE\ndata, frames misalignment, and cardiac motion. To address this issue, we\npropose an interactive editing framework that allows users to edit segmentation\noutput by drawing scribbles on a 2D frame. The user interaction is mapped to\nthe 3D grid and utilized to execute an editing step that modifies the\nsegmentation in the vicinity of the interaction while preserving the previous\nsegmentation away from the interaction. Furthermore, our framework accommodates\nmultiple edits to the segmentation output in a sequential manner without\ncompromising previous edits. This paper presents a novel loss function and a\nnovel evaluation metric specifically designed for editing. Results from\ncross-validation and testing indicate that our proposed loss function\noutperforms standard losses and training strategies in terms of segmentation\nquality and following user input. Additionally, we show quantitatively and\nqualitatively that subsequent edits do not compromise previous edits when using\nour method, as opposed to standard segmentation losses. Overall, our approach\nenhances the accuracy of the segmentation while avoiding undesired changes away\nfrom user interactions and without compromising the quality of previously\nedited regions, leading to better patient outcomes.\n","authors":["Ahmed H. Shahin","Yan Zhuang","Noha El-Zehiry"],"pdf_url":"https://arxiv.org/pdf/2303.11041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11040v1","updated":"2023-03-20T11:45:54Z","published":"2023-03-20T11:45:54Z","title":"Benchmarking Robustness of 3D Object Detection to Common Corruptions in\n  Autonomous Driving","summary":"  3D object detection is an important task in autonomous driving to perceive\nthe surroundings. Despite the excellent performance, the existing 3D detectors\nlack the robustness to real-world corruptions caused by adverse weathers,\nsensor noises, etc., provoking concerns about the safety and reliability of\nautonomous driving systems. To comprehensively and rigorously benchmark the\ncorruption robustness of 3D detectors, in this paper we design 27 types of\ncommon corruptions for both LiDAR and camera inputs considering real-world\ndriving scenarios. By synthesizing these corruptions on public datasets, we\nestablish three corruption robustness benchmarks -- KITTI-C, nuScenes-C, and\nWaymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object\ndetection models to evaluate their corruption robustness. Based on the\nevaluation results, we draw several important findings, including: 1)\nmotion-level corruptions are the most threatening ones that lead to significant\nperformance drop of all models; 2) LiDAR-camera fusion models demonstrate\nbetter robustness; 3) camera-only models are extremely vulnerable to image\ncorruptions, showing the indispensability of LiDAR point clouds. We release the\nbenchmarks and codes at https://github.com/kkkcx/3D_Corruptions_AD. We hope\nthat our benchmarks and findings can provide insights for future research on\ndeveloping robust 3D object detection models.\n","authors":["Yinpeng Dong","Caixin Kang","Jinlai Zhang","Zijian Zhu","Yikai Wang","Xiao Yang","Hang Su","Xingxing Wei","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11040v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11034v1","updated":"2023-03-20T11:36:09Z","published":"2023-03-20T11:36:09Z","title":"Internal Structure Attention Network for Fingerprint Presentation Attack\n  Detection from Optical Coherence Tomography","summary":"  As a non-invasive optical imaging technique, optical coherence tomography\n(OCT) has proven promising for automatic fingerprint recognition system (AFRS)\napplications. Diverse approaches have been proposed for OCT-based fingerprint\npresentation attack detection (PAD). However, considering the complexity and\nvariety of PA samples, it is extremely challenging to increase the\ngeneralization ability with the limited PA dataset. To solve the challenge,\nthis paper presents a novel supervised learning-based PAD method, denoted as\nISAPAD, which applies prior knowledge to guide network training and enhance the\ngeneralization ability. The proposed dual-branch architecture can not only\nlearns global features from the OCT image, but also concentrate on layered\nstructure feature which comes from the internal structure attention module\n(ISAM). The simple yet effective ISAM enables the proposed network to obtain\nlayered segmentation features belonging only to Bonafide from noisy OCT volume\ndata directly. Combined with effective training strategies and PAD score\ngeneration rules, ISAPAD obtains optimal PAD performance in limited training\ndata. Domain generalization experiments and visualization analysis validate the\neffectiveness of the proposed method for OCT PAD.\n","authors":["Haohao Sun","Yilong Zhang","Peng Chen","Haixia Wang","Ronghua Liang"],"pdf_url":"https://arxiv.org/pdf/2303.11034v1.pdf","comment":"12 pages, 14 figures"},{"id":"http://arxiv.org/abs/2211.11674v2","updated":"2023-03-20T11:33:18Z","published":"2022-11-21T17:42:42Z","title":"Shape, Pose, and Appearance from a Single Image via Bootstrapped\n  Radiance Field Inversion","summary":"  Neural Radiance Fields (NeRF) coupled with GANs represent a promising\ndirection in the area of 3D reconstruction from a single view, owing to their\nability to efficiently model arbitrary topologies. Recent work in this area,\nhowever, has mostly focused on synthetic datasets where exact ground-truth\nposes are known, and has overlooked pose estimation, which is important for\ncertain downstream applications such as augmented reality (AR) and robotics. We\nintroduce a principled end-to-end reconstruction framework for natural images,\nwhere accurate ground-truth poses are not available. Our approach recovers an\nSDF-parameterized 3D shape, pose, and appearance from a single image of an\nobject, without exploiting multiple views during training. More specifically,\nwe leverage an unconditional 3D-aware generator, to which we apply a hybrid\ninversion scheme where a model produces a first guess of the solution which is\nthen refined via optimization. Our framework can de-render an image in as few\nas 10 steps, enabling its use in practical scenarios. We demonstrate\nstate-of-the-art results on a variety of real and synthetic benchmarks.\n","authors":["Dario Pavllo","David Joseph Tan","Marie-Julie Rakotosaona","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2211.11674v2.pdf","comment":"CVPR 2023. Code and models are available at\n  https://github.com/google-research/nerf-from-image"},{"id":"http://arxiv.org/abs/2211.10307v2","updated":"2023-03-20T11:30:49Z","published":"2022-11-18T15:46:24Z","title":"SeaTurtleID: A novel long-span dataset highlighting the importance of\n  timestamps in wildlife re-identification","summary":"  This paper introduces SeaTurtleID, the first public large-scale, long-span\ndataset with sea turtle photographs captured in the wild. The dataset is\nsuitable for benchmarking re-identification methods and evaluating several\nother computer vision tasks. The dataset consists of 7774 high-resolution\nphotographs of 400 unique individuals collected within 12 years in 1081\nencounters. Each photograph is accompanied by rich metadata, e.g., identity\nlabel, head segmentation mask, and encounter timestamp. The 12-year span of the\ndataset makes it the longest-spanned public wild animal dataset with\ntimestamps. By exploiting this unique property, we show that timestamps are\nnecessary for an unbiased evaluation of animal re-identification methods\nbecause they allow time-aware splits of the dataset into reference and query\nsets. We show that time-unaware (random) splits can lead to performance\noverestimation of more than 100% compared to the time-aware splits for both\nfeature- and CNN-based re-identification methods. We also argue that time-aware\nsplits correspond to more realistic re-identification pipelines than the\ntime-unaware ones. We recommend that animal re-identification methods should\nonly be tested on datasets with timestamps using time-aware splits, and we\nencourage dataset curators to include such information in the associated\nmetadata.\n","authors":["Kostas Papafitsoros","Lukáš Adam","Vojtěch Čermák","Lukáš Picek"],"pdf_url":"https://arxiv.org/pdf/2211.10307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09807v2","updated":"2023-03-20T10:57:45Z","published":"2023-03-17T07:26:16Z","title":"TKN: Transformer-based Keypoint Prediction Network For Real-time Video\n  Prediction","summary":"  Video prediction is a complex time-series forecasting task with great\npotential in many use cases. However, conventional methods overemphasize\naccuracy while ignoring the slow prediction speed caused by complicated model\nstructures that learn too much redundant information with excessive GPU memory\nconsumption. Furthermore, conventional methods mostly predict frames\nsequentially (frame-by-frame) and thus are hard to accelerate. Consequently,\nvaluable use cases such as real-time danger prediction and warning cannot\nachieve fast enough inference speed to be applicable in reality. Therefore, we\npropose a transformer-based keypoint prediction neural network (TKN), an\nunsupervised learning method that boost the prediction process via constrained\ninformation extraction and parallel prediction scheme. TKN is the first\nreal-time video prediction solution to our best knowledge, while significantly\nreducing computation costs and maintaining other performance. Extensive\nexperiments on KTH and Human3.6 datasets demonstrate that TKN predicts 11 times\nfaster than existing methods while reducing memory consumption by 17.4% and\nachieving state-of-the-art prediction performance on average.\n","authors":["Haoran Li","Pengyuan Zhou","Yihang Lin","Yanbin Hao","Haiyong Xie","Yong Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11019v1","updated":"2023-03-20T10:57:28Z","published":"2023-03-20T10:57:28Z","title":"A Dual-branch Self-supervised Representation Learning Framework for\n  Tumour Segmentation in Whole Slide Images","summary":"  Supervised deep learning methods have achieved considerable success in\nmedical image analysis, owing to the availability of large-scale and\nwell-annotated datasets. However, creating such datasets for whole slide images\n(WSIs) in histopathology is a challenging task due to their gigapixel size. In\nrecent years, self-supervised learning (SSL) has emerged as an alternative\nsolution to reduce the annotation overheads in WSIs, as it does not require\nlabels for training. These SSL approaches, however, are not designed for\nhandling multi-resolution WSIs, which limits their performance in learning\ndiscriminative image features. In this paper, we propose a Dual-branch SSL\nFramework for WSI tumour segmentation (DSF-WSI) that can effectively learn\nimage features from multi-resolution WSIs. Our DSF-WSI connected two branches\nand jointly learnt low and high resolution WSIs in a self-supervised manner.\nMoreover, we introduced a novel Context-Target Fusion Module (CTFM) and a\nmasked jigsaw pretext task to align the learnt multi-resolution features.\nFurthermore, we designed a Dense SimSiam Learning (DSL) strategy to maximise\nthe similarity of different views of WSIs, enabling the learnt representations\nto be more efficient and discriminative. We evaluated our method using two\npublic datasets on breast and liver cancer segmentation tasks. The experiment\nresults demonstrated that our DSF-WSI can effectively extract robust and\nefficient representations, which we validated through subsequent fine-tuning\nand semi-supervised settings. Our proposed method achieved better accuracy than\nother state-of-the-art approaches. Code is available at\nhttps://github.com/Dylan-H-Wang/dsf-wsi.\n","authors":["Hao Wang","Euijoon Ahn","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08131v3","updated":"2023-03-20T10:52:40Z","published":"2023-03-14T17:58:34Z","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","summary":"  We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, OpenSeeD beats the state-of-the-art\nmethod for open-vocabulary instance and panoptic segmentation across 5\ndatasets, and outperforms previous work for open-vocabulary detection on LVIS\nand ODinW under similar settings. When transferred to specific tasks, our model\nachieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance\nsegmentation on ADE20K and Cityscapes.\n  Finally, we note that OpenSeeD is the first to explore the potential of joint\ntraining on segmentation and detection, and hope it can be received as a strong\nbaseline for developing a single model for both tasks in open world.\n","authors":["Hao Zhang","Feng Li","Xueyan Zou","Shilong Liu","Chunyuan Li","Jianfeng Gao","Jianwei Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08131v3.pdf","comment":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.11011v1","updated":"2023-03-20T10:44:32Z","published":"2023-03-20T10:44:32Z","title":"Learning Optical Flow from Event Camera with Rendered Dataset","summary":"  We study the problem of estimating optical flow from event cameras. One\nimportant issue is how to build a high-quality event-flow dataset with accurate\nevent values and flow labels. Previous datasets are created by either capturing\nreal scenes by event cameras or synthesizing from images with pasted foreground\nobjects. The former case can produce real event values but with calculated flow\nlabels, which are sparse and inaccurate. The later case can generate dense flow\nlabels but the interpolated events are prone to errors. In this work, we\npropose to render a physically correct event-flow dataset using computer\ngraphics models. In particular, we first create indoor and outdoor 3D scenes by\nBlender with rich scene content variations. Second, diverse camera motions are\nincluded for the virtual capturing, producing images and accurate flow labels.\nThird, we render high-framerate videos between images for accurate events. The\nrendered dataset can adjust the density of events, based on which we further\nintroduce an adaptive density module (ADM). Experiments show that our proposed\ndataset can facilitate event-flow learning, whereas previous approaches when\ntrained on our dataset can improve their performances constantly by a\nrelatively large margin. In addition, event-flow pipelines when equipped with\nour ADM can further improve performances.\n","authors":["Xinglong Luo","Kunming Luo","Ao Luo","Zhengning Wang","Ping Tan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11003v1","updated":"2023-03-20T10:31:35Z","published":"2023-03-20T10:31:35Z","title":"Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization","summary":"  We propose a self-supervised method for learning motion-focused video\nrepresentations. Existing approaches minimize distances between temporally\naugmented videos, which maintain high spatial similarity. We instead propose to\nlearn similarities between videos with identical local motion dynamics but an\notherwise different appearance. We do so by adding synthetic motion\ntrajectories to videos which we refer to as tubelets. By simulating different\ntubelet motions and applying transformations, such as scaling and rotation, we\nintroduce motion patterns beyond what is present in the pretraining data. This\nallows us to learn a video representation that is remarkably data-efficient:\nour approach maintains performance when using only 25% of the pretraining\nvideos. Experiments on 10 diverse downstream settings demonstrate our\ncompetitive performance and generalizability to new domains and fine-grained\nactions.\n","authors":["Fida Mohammad Thoker","Hazel Doughty","Cees Snoek"],"pdf_url":"https://arxiv.org/pdf/2303.11003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10999v1","updated":"2023-03-20T10:29:35Z","published":"2023-03-20T10:29:35Z","title":"Induced Feature Selection by Structured Pruning","summary":"  The advent of sparsity inducing techniques in neural networks has been of a\ngreat help in the last few years. Indeed, those methods allowed to find lighter\nand faster networks, able to perform more efficiently in resource-constrained\nenvironment such as mobile devices or highly requested servers. Such a sparsity\nis generally imposed on the weights of neural networks, reducing the footprint\nof the architecture. In this work, we go one step further by imposing sparsity\njointly on the weights and on the input data. This can be achieved following a\nthree-step process: 1) impose a certain structured sparsity on the weights of\nthe network; 2) track back input features corresponding to zeroed blocks of\nweight; 3) remove useless weights and input features and retrain the network.\nPerforming pruning both on the network and on input data not only allows for\nextreme reduction in terms of parameters and operations but can also serve as\nan interpretation process. Indeed, with the help of data pruning, we now have\ninformation about which input feature is useful for the network to keep its\nperformance. Experiments conducted on a variety of architectures and datasets:\nMLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),\nvalidated on CIFAR10/100 and CALTECH101 respectively, show that it is possible\nto achieve additional gains in terms of total parameters and in FLOPs by\nperforming pruning on input data, while also increasing accuracy.\n","authors":["Nathan Hubens","Victor Delvigne","Matei Mancas","Bernard Gosselin","Marius Preda","Titus Zaharia"],"pdf_url":"https://arxiv.org/pdf/2303.10999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10991v1","updated":"2023-03-20T10:19:50Z","published":"2023-03-20T10:19:50Z","title":"Versatile Depth Estimator Based on Common Relative Depth Estimation and\n  Camera-Specific Relative-to-Metric Depth Conversion","summary":"  A typical monocular depth estimator is trained for a single camera, so its\nperformance drops severely on images taken with different cameras. To address\nthis issue, we propose a versatile depth estimator (VDE), composed of a common\nrelative depth estimator (CRDE) and multiple relative-to-metric converters\n(R2MCs). The CRDE extracts relative depth information, and each R2MC converts\nthe relative information to predict metric depths for a specific camera. The\nproposed VDE can cope with diverse scenes, including both indoor and outdoor\nscenes, with only a 1.12\\% parameter increase per camera. Experimental results\ndemonstrate that VDE supports multiple cameras effectively and efficiently and\nalso achieves state-of-the-art performance in the conventional single-camera\nscenario.\n","authors":["Jinyoung Jun","Jae-Han Lee","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2303.10991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02669v3","updated":"2023-03-20T10:19:37Z","published":"2022-02-06T01:20:50Z","title":"SRPCN: Structure Retrieval based Point Completion Network","summary":"  Given partial objects and some complete ones as references, point cloud\ncompletion aims to recover authentic shapes. However, existing methods pay\nlittle attention to general shapes, which leads to the poor authenticity of\ncompletion results. Besides, the missing patterns are diverse in reality, but\nexisting methods can only handle fixed ones, which means a poor generalization\nability. Considering that a partial point cloud is a subset of the\ncorresponding complete one, we regard them as different samples of the same\ndistribution and propose Structure Retrieval based Point Completion Network\n(SRPCN). It first uses k-means clustering to extract structure points and\ndisperses them into distributions, and then KL Divergence is used as a metric\nto find the complete structure point cloud that best matches the input in a\ndatabase. Finally, a PCN-like decoder network is adopted to generate the final\nresults based on the retrieved structure point clouds. As structure plays an\nimportant role in describing the general shape of an object and the proposed\nstructure retrieval method is robust to missing patterns, experiments show that\nour method can generate more authentic results and has a stronger\ngeneralization ability.\n","authors":["Kaiyi Zhang","Ximing Yang","Yuan Wu","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2202.02669v3.pdf","comment":"I think the proposed method has some defects"},{"id":"http://arxiv.org/abs/2301.04525v2","updated":"2023-03-20T10:18:28Z","published":"2023-01-11T15:44:42Z","title":"Clustering disease trajectories in contrastive feature space for\n  biomarker discovery in age-related macular degeneration","summary":"  Age-related macular degeneration (AMD) is the leading cause of blindness in\nthe elderly. Current grading systems based on imaging biomarkers only coarsely\ngroup disease stages into broad categories and are unable to predict future\ndisease progression. It is widely believed that this is due to their focus on a\nsingle point in time, disregarding the dynamic nature of the disease. In this\nwork, we present the first method to automatically discover biomarkers that\ncapture temporal dynamics of disease progression. Our method represents patient\ntime series as trajectories in a latent feature space built with contrastive\nlearning. Then, individual trajectories are partitioned into atomic\nsub-sequences that encode transitions between disease states. These are\nclustered using a newly introduced distance metric. In quantitative experiments\nwe found our method yields temporal biomarkers that are predictive of\nconversion to late AMD. Furthermore, these clusters were highly interpretable\nto ophthalmologists who confirmed that many of the clusters represent dynamics\nthat have previously been linked to the progression of AMD, even though they\nare currently not included in any clinical grading system.\n","authors":["Robbie Holland","Oliver Leingang","Christopher Holmes","Philipp Anders","Rebecca Kaye","Sophie Riedl","Johannes C. Paetzold","Ivan Ezhov","Hrvoje Bogunović","Ursula Schmidt-Erfurth","Lars Fritsche","Hendrik P. N. Scholl","Sobha Sivaprasad","Andrew J. Lotery","Daniel Rueckert","Martin J. Menten"],"pdf_url":"https://arxiv.org/pdf/2301.04525v2.pdf","comment":"Submitted to MICCAI2023"},{"id":"http://arxiv.org/abs/2303.10976v1","updated":"2023-03-20T09:56:35Z","published":"2023-03-20T09:56:35Z","title":"Attention Disturbance and Dual-Path Constraint Network for Occluded\n  Person Re-Identification","summary":"  Occluded person re-identification (Re-ID) aims to address the potential\nocclusion problem when matching occluded or holistic pedestrians from different\ncamera views. Many methods use the background as artificial occlusion and rely\non attention networks to exclude noisy interference. However, the significant\ndiscrepancy between simple background occlusion and realistic occlusion can\nnegatively impact the generalization of the network.To address this issue, we\npropose a novel transformer-based Attention Disturbance and Dual-Path\nConstraint Network (ADP) to enhance the generalization of attention networks.\nFirstly, to imitate real-world obstacles, we introduce an Attention Disturbance\nMask (ADM) module that generates an offensive noise, which can distract\nattention like a realistic occluder, as a more complex form of\nocclusion.Secondly, to fully exploit these complex occluded images, we develop\na Dual-Path Constraint Module (DPC) that can obtain preferable supervision\ninformation from holistic images through dual-path interaction. With our\nproposed method, the network can effectively circumvent a wide variety of\nocclusions using the basic ViT baseline. Comprehensive experimental evaluations\nconducted on person re-ID benchmarks demonstrate the superiority of ADP over\nstate-of-the-art methods.\n","authors":["Jiaer Xia","Lei Tan","Pingyang Dai","Mingbo Zhao","Yongjian Wu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.10976v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10975v1","updated":"2023-03-20T09:56:17Z","published":"2023-03-20T09:56:17Z","title":"VIMI: Vehicle-Infrastructure Multi-view Intermediate Fusion for\n  Camera-based 3D Object Detection","summary":"  In autonomous driving, Vehicle-Infrastructure Cooperative 3D Object Detection\n(VIC3D) makes use of multi-view cameras from both vehicles and traffic\ninfrastructure, providing a global vantage point with rich semantic context of\nroad conditions beyond a single vehicle viewpoint. Two major challenges prevail\nin VIC3D: 1) inherent calibration noise when fusing multi-view images, caused\nby time asynchrony across cameras; 2) information loss when projecting 2D\nfeatures into 3D space. To address these issues, We propose a novel 3D object\ndetection framework, Vehicles-Infrastructure Multi-view Intermediate fusion\n(VIMI). First, to fully exploit the holistic perspectives from both vehicles\nand infrastructure, we propose a Multi-scale Cross Attention (MCA) module that\nfuses infrastructure and vehicle features on selective multi-scales to correct\nthe calibration noise introduced by camera asynchrony. Then, we design a\nCamera-aware Channel Masking (CCM) module that uses camera parameters as priors\nto augment the fused features. We further introduce a Feature Compression (FC)\nmodule with channel and spatial compression blocks to reduce the size of\ntransmitted features for enhanced efficiency. Experiments show that VIMI\nachieves 15.61% overall AP_3D and 21.44% AP_BEV on the new VIC3D dataset,\nDAIR-V2X-C, significantly outperforming state-of-the-art early fusion and late\nfusion methods with comparable transmission cost.\n","authors":["Zhe Wang","Siqi Fan","Xiaoliang Huo","Tongda Xu","Yan Wang","Jingjing Liu","Yilun Chen","Ya-Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10975v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.10972v1","updated":"2023-03-20T09:50:07Z","published":"2023-03-20T09:50:07Z","title":"Semantic segmentation of surgical hyperspectral images under geometric\n  domain shifts","summary":"  Robust semantic segmentation of intraoperative image data could pave the way\nfor automatic surgical scene understanding and autonomous robotic surgery.\nGeometric domain shifts, however, although common in real-world open surgeries\ndue to variations in surgical procedures or situs occlusions, remain a topic\nlargely unaddressed in the field. To address this gap in the literature, we (1)\npresent the first analysis of state-of-the-art (SOA) semantic segmentation\nnetworks in the presence of geometric out-of-distribution (OOD) data, and (2)\naddress generalizability with a dedicated augmentation technique termed \"Organ\nTransplantation\" that we adapted from the general computer vision community.\nAccording to a comprehensive validation on six different OOD data sets\ncomprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs\nsemantically annotated with 19 classes, we demonstrate a large performance drop\nof SOA organ segmentation networks applied to geometric OOD data. Surprisingly,\nthis holds true not only for conventional RGB data (drop of Dice similarity\ncoefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the\nlatter's rich information content per pixel. Using our augmentation scheme\nimproves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders\nperformance on par with in-distribution performance on real OOD test data. The\nsimplicity and effectiveness of our augmentation scheme makes it a valuable\nnetwork-independent tool for addressing geometric domain shifts in semantic\nscene segmentation of intraoperative data. Our code and pre-trained models will\nbe made publicly available.\n","authors":["Jan Sellner","Silvia Seidlitz","Alexander Studier-Fischer","Alessandro Motta","Berkin Özdemir","Beat Peter Müller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.10972v1.pdf","comment":"The first two authors (Jan Sellner and Silvia Seidlitz) contributed\n  equally to this paper"},{"id":"http://arxiv.org/abs/2303.10971v1","updated":"2023-03-20T09:47:02Z","published":"2023-03-20T09:47:02Z","title":"Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching","summary":"  The matching of 3D shapes has been extensively studied for shapes represented\nas surface meshes, as well as for shapes represented as point clouds. While\npoint clouds are a common representation of raw real-world 3D data (e.g. from\nlaser scanners), meshes encode rich and expressive topological information, but\ntheir creation typically requires some form of (often manual) curation. In\nturn, methods that purely rely on point clouds are unable to meet the matching\nquality of mesh-based methods that utilise the additional topological\nstructure. In this work we close this gap by introducing a self-supervised\nmultimodal learning strategy that combines mesh-based functional map\nregularisation with a contrastive loss that couples mesh and point cloud data.\nOur shape matching approach allows to obtain intramodal correspondences for\ntriangle meshes, complete point clouds, and partially observed point clouds, as\nwell as correspondences across these data modalities. We demonstrate that our\nmethod achieves state-of-the-art results on several challenging benchmark\ndatasets even in comparison to recent supervised methods, and that our method\nreaches previously unseen cross-dataset generalisation ability.\n","authors":["Dongliang Cao","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2303.10971v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.11432v3","updated":"2023-03-20T09:44:58Z","published":"2022-11-21T13:19:08Z","title":"MATE: Masked Autoencoders are Online 3D Test-Time Learners","summary":"  Our MATE is the first Test-Time-Training (TTT) method designed for 3D data,\nwhich makes deep networks trained for point cloud classification robust to\ndistribution shifts occurring in test data. Like existing TTT methods from the\n2D image domain, MATE also leverages test data for adaptation. Its test-time\nobjective is that of a Masked Autoencoder: a large portion of each test point\ncloud is removed before it is fed to the network, tasked with reconstructing\nthe full point cloud. Once the network is updated, it is used to classify the\npoint cloud. We test MATE on several 3D object classification datasets and show\nthat it significantly improves robustness of deep networks to several types of\ncorruptions commonly occurring in 3D point clouds. We show that MATE is very\nefficient in terms of the fraction of points it needs for the adaptation. It\ncan effectively adapt given as few as 5% of tokens of each test sample, making\nit extremely lightweight. Our experiments show that MATE also achieves\ncompetitive performance by adapting sparsely on the test data, which further\nreduces its computational overhead, making it ideal for real-time applications.\n","authors":["M. Jehanzeb Mirza","Inkyu Shin","Wei Lin","Andreas Schriebl","Kunyang Sun","Jaesung Choe","Horst Possegger","Mateusz Kozinski","In So Kweon","Kun-Jin Yoon","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2211.11432v3.pdf","comment":"Code is available at this repository:\n  https://github.com/jmiemirza/MATE"},{"id":"http://arxiv.org/abs/2303.10967v1","updated":"2023-03-20T09:41:50Z","published":"2023-03-20T09:41:50Z","title":"Real-time Semantic Scene Completion Via Feature Aggregation and\n  Conditioned Prediction","summary":"  Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric\noccupancy and semantic category of a 3D scene. In this paper, we propose a\nreal-time semantic scene completion method with a feature aggregation strategy\nand conditioned prediction module. Feature aggregation fuses feature with\ndifferent receptive fields and gathers context to improve scene completion\nperformance. And the conditioned prediction module adopts a two-step prediction\nscheme that takes volumetric occupancy as a condition to enhance semantic\ncompletion prediction. We conduct experiments on three recognized benchmarks\nNYU, NYUCAD, and SUNCG. Our method achieves competitive performance at a speed\nof 110 FPS on one GTX 1080 Ti GPU.\n","authors":["Xiaokang Chen","Yajie Xing","Gang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.10967v1.pdf","comment":"Accepted by ICIP"},{"id":"http://arxiv.org/abs/2212.04245v2","updated":"2023-03-20T09:41:47Z","published":"2022-12-07T12:44:41Z","title":"Domain generalization of 3D semantic segmentation in autonomous driving","summary":"  Using deep learning, 3D autonomous driving semantic segmentation has become a\nwell-studied subject, with methods that can reach very high performance.\nNonetheless, because of the limited size of the training datasets, these models\ncannot see every type of object and scene found in real-world applications. The\nability to be reliable in these various unknown environments is called domain\ngeneralization.\n  Despite its importance, domain generalization is relatively unexplored in the\ncase of 3D autonomous driving semantic segmentation. To fill this gap, this\npaper presents the first benchmark for this application by testing\nstate-of-the-art methods and discussing the difficulty of tackling Laser\nImaging Detection and Ranging (LiDAR) domain shifts.\n  We also propose the first method designed to address this domain\ngeneralization, which we call 3DLabelProp. This method relies on leveraging the\ngeometry and sequentiality of the LiDAR data to enhance its generalization\nperformances by working on partially accumulated point clouds. It reaches a\nmean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on\nPandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it\nthe state-of-the-art method for generalization (+5% and +33% better,\nrespectively, than the second best method).\n  The code for this method will be available on GitHub.\n","authors":["Jules Sanchez","Jean-Emmanuel Deschaud","Francois Goulette"],"pdf_url":"https://arxiv.org/pdf/2212.04245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10962v1","updated":"2023-03-20T09:38:09Z","published":"2023-03-20T09:38:09Z","title":"Neural Implicit Vision-Language Feature Fields","summary":"  Recently, groundbreaking results have been presented on open-vocabulary\nsemantic image segmentation. Such methods segment each pixel in an image into\narbitrary categories provided at run-time in the form of text prompts, as\nopposed to a fixed set of classes defined at training time. In this work, we\npresent a zero-shot volumetric open-vocabulary semantic scene segmentation\nmethod. Our method builds on the insight that we can fuse image features from a\nvision-language model into a neural implicit representation. We show that the\nresulting feature field can be segmented into different classes by assigning\npoints to natural language text prompts. The implicit volumetric representation\nenables us to segment the scene both in 3D and 2D by rendering feature maps\nfrom any given viewpoint of the scene. We show that our method works on noisy\nreal-world data and can run in real-time on live sensor data dynamically\nadjusting to text prompts. We also present quantitative comparisons on the\nScanNet dataset.\n","authors":["Kenneth Blomqvist","Francesco Milano","Jen Jen Chung","Lionel Ott","Roland Siegwart"],"pdf_url":"https://arxiv.org/pdf/2303.10962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10961v1","updated":"2023-03-20T09:37:41Z","published":"2023-03-20T09:37:41Z","title":"LFACon: Introducing Anglewise Attention to No-Reference Quality\n  Assessment in Light Field Space","summary":"  Light field imaging can capture both the intensity information and the\ndirection information of light rays. It naturally enables a\nsix-degrees-of-freedom viewing experience and deep user engagement in virtual\nreality. Compared to 2D image assessment, light field image quality assessment\n(LFIQA) needs to consider not only the image quality in the spatial domain but\nalso the quality consistency in the angular domain. However, there is a lack of\nmetrics to effectively reflect the angular consistency and thus the angular\nquality of a light field image (LFI). Furthermore, the existing LFIQA metrics\nsuffer from high computational costs due to the excessive data volume of LFIs.\nIn this paper, we propose a novel concept of \"anglewise attention\" by\nintroducing a multihead self-attention mechanism to the angular domain of an\nLFI. This mechanism better reflects the LFI quality. In particular, we propose\nthree new attention kernels, including anglewise self-attention, anglewise grid\nattention, and anglewise central attention. These attention kernels can realize\nangular self-attention, extract multiangled features globally or selectively,\nand reduce the computational cost of feature extraction. By effectively\nincorporating the proposed kernels, we further propose our light field\nattentional convolutional neural network (LFACon) as an LFIQA metric. Our\nexperimental results show that the proposed LFACon metric significantly\noutperforms the state-of-the-art LFIQA metrics. For the majority of distortion\ntypes, LFACon attains the best performance with lower complexity and less\ncomputational time.\n","authors":["Qiang Qu","Xiaoming Chen","Yuk Ying Chung","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2303.10961v1.pdf","comment":"Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)"},{"id":"http://arxiv.org/abs/2303.10960v1","updated":"2023-03-20T09:33:47Z","published":"2023-03-20T09:33:47Z","title":"Improved Benthic Classification using Resolution Scaling and SymmNet\n  Unsupervised Domain Adaptation","summary":"  Autonomous Underwater Vehicles (AUVs) conduct regular visual surveys of\nmarine environments to characterise and monitor the composition and diversity\nof the benthos. The use of machine learning classifiers for this task is\nlimited by the low numbers of annotations available and the many fine-grained\nclasses involved. In addition to these challenges, there are domain shifts\nbetween image sets acquired during different AUV surveys due to changes in\ncamera systems, imaging altitude, illumination and water column properties\nleading to a drop in classification performance for images from a different\nsurvey where some or all these elements may have changed. This paper proposes a\nframework to improve the performance of a benthic morphospecies classifier when\nused to classify images from a different survey compared to the training data.\nWe adapt the SymmNet state-of-the-art Unsupervised Domain Adaptation method\nwith an efficient bilinear pooling layer and image scaling to normalise spatial\nresolution, and show improved classification accuracy. We test our approach on\ntwo datasets with images from AUV surveys with different imaging payloads and\nlocations. The results show that generic domain adaptation can be enhanced to\nproduce a significant increase in accuracy for images from an AUV survey that\ndiffers from the training images.\n","authors":["Heather Doig","Oscar Pizarro","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2303.10960v1.pdf","comment":"7 pages, 6 figures. Accepted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2023, London UK"},{"id":"http://arxiv.org/abs/2303.10959v1","updated":"2023-03-20T09:33:05Z","published":"2023-03-20T09:33:05Z","title":"Long-Term Indoor Localization with Metric-Semantic Mapping using a Floor\n  Plan Prior","summary":"  Object-based maps are relevant for scene understanding since they integrate\ngeometric and semantic information of the environment, allowing autonomous\nrobots to robustly localize and interact with on objects. In this paper, we\naddress the task of constructing a metric-semantic map for the purpose of\nlong-term object-based localization. We exploit 3D object detections from\nmonocular RGB frames for both, the object-based map construction, and for\nglobally localizing in the constructed map. To tailor the approach to a target\nenvironment, we propose an efficient way of generating 3D annotations to\nfinetune the 3D object detection model. We evaluate our map construction in an\noffice building, and test our long-term localization approach on challenging\nsequences recorded in the same environment over nine months. The experiments\nsuggest that our approach is suitable for constructing metric-semantic maps,\nand that our localization approach is robust to long-term changes. Both, the\nmapping algorithm and the localization pipeline can run online on an onboard\ncomputer. We will release an open-source C++/ROS implementation of our\napproach.\n","authors":["Nicky Zimmerman","Matteo Sodano","Elias Marks","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2303.10959v1.pdf","comment":"7 pages, submitted to IROS 2023"},{"id":"http://arxiv.org/abs/2211.00288v2","updated":"2023-03-20T09:20:03Z","published":"2022-11-01T05:48:18Z","title":"Self-supervised Character-to-Character Distillation for Text Recognition","summary":"  When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released\nsoon.\n","authors":["Tongkun Guan","Wei Shen","Xue Yang","Qi Feng","Zekun Jiang"],"pdf_url":"https://arxiv.org/pdf/2211.00288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10951v1","updated":"2023-03-20T09:18:52Z","published":"2023-03-20T09:18:52Z","title":"Tracker Meets Night: A Transformer Enhancer for UAV Tracking","summary":"  Most previous progress in object tracking is realized in daytime scenes with\nfavorable illumination. State-of-the-arts can hardly carry on their superiority\nat night so far, thereby considerably blocking the broadening of visual\ntracking-related unmanned aerial vehicle (UAV) applications. To realize\nreliable UAV tracking at night, a spatial-channel Transformer-based low-light\nenhancer (namely SCT), which is trained in a novel task-inspired manner, is\nproposed and plugged prior to tracking approaches. To achieve semantic-level\nlow-light enhancement targeting the high-level task, the novel spatial-channel\nattention module is proposed to model global information while preserving local\ncontext. In the enhancement process, SCT denoises and illuminates nighttime\nimages simultaneously through a robust non-linear curve projection. Moreover,\nto provide a comprehensive evaluation, we construct a challenging nighttime\ntracking benchmark, namely DarkTrack2021, which contains 110 challenging\nsequences with over 100 K frames in total. Evaluations on both the public\nUAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show\nthat the task-inspired design enables SCT with significant performance gains\nfor nighttime UAV tracking compared with other top-ranked low-light enhancers.\nReal-world tests on a typical UAV platform further verify the practicability of\nthe proposed approach. The DarkTrack2021 benchmark and the code of the proposed\napproach are publicly available at https://github.com/vision4robotics/SCT.\n","authors":["Junjie Ye","Changhong Fu","Ziang Cao","Shan An","Guangze Zheng","Bowen Li"],"pdf_url":"https://arxiv.org/pdf/2303.10951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05871v2","updated":"2023-03-20T09:12:21Z","published":"2023-01-14T09:43:23Z","title":"Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth\n  Estimation in Dynamic Scenes","summary":"  Self-supervised methods have showed promising results on depth estimation\ntask. However, previous methods estimate the target depth map and camera\nego-motion simultaneously, underusing multi-frame correlation information and\nignoring the motion of dynamic objects. In this paper, we propose a novel\nDyna-Depthformer framework, which predicts scene depth and 3D motion field\njointly and aggregates multi-frame information with transformer. Our\ncontributions are two-fold. First, we leverage multi-view correlation through a\nseries of self- and cross-attention layers in order to obtain enhanced depth\nfeature representation. Specifically, we use the perspective transformation to\nacquire the initial reference point, and use deformable attention to reduce the\ncomputational cost. Second, we propose a warping-based Motion Network to\nestimate the motion field of dynamic objects without using semantic prior. To\nimprove the motion field predictions, we propose an iterative optimization\nstrategy, together with a sparsity-regularized loss. The entire pipeline\nachieves end-to-end self-supervised training by constructing a minimum\nreprojection loss. Extensive experiments on the KITTI and Cityscapes benchmarks\ndemonstrate the effectiveness of our method and show that our method\noutperforms state-of-the-art algorithms.\n","authors":["Songchun Zhang","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2301.05871v2.pdf","comment":"ICRA 2023"},{"id":"http://arxiv.org/abs/2211.12860v3","updated":"2023-03-20T09:11:57Z","published":"2022-11-22T16:19:52Z","title":"DETRs with Collaborative Hybrid Assignments Training","summary":"  In this paper, we provide the observation that too few queries assigned as\npositive samples in DETR with one-to-one set matching leads to sparse\nsupervisions on the encoder's output which considerably hurt the discriminative\nfeature learning of the encoder and vice visa for attention learning in the\ndecoder. To alleviate this, we present a novel collaborative hybrid assignments\ntraining scheme, namely Co-DETR, to learn more efficient and effective\nDETR-based detectors from versatile label assignment manners. This new training\nscheme can easily enhance the encoder's learning ability in end-to-end\ndetectors by training the multiple parallel auxiliary heads supervised by\none-to-many label assignments such as ATSS, FCOS, and Faster RCNN. In addition,\nwe conduct extra customized positive queries by extracting the positive\ncoordinates from these auxiliary heads to improve the training efficiency of\npositive samples in the decoder. In inference, these auxiliary heads are\ndiscarded and thus our method introduces no additional parameters and\ncomputational cost to the original detector while requiring no hand-crafted\nnon-maximum suppression (NMS). We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach on DETR variants, including DAB-DETR,\nDeformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic\nDeformable-DETR by 5.8% in 12-epoch training and 3.2% in 36-epoch training. The\nstate-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from\n58.5% to 59.5%. Surprisingly, incorporated with the large-scale backbone\nMixMIM-g with 1-Billion parameters, we achieve the 64.5% mAP on MS COCO\ntest-dev, achieving superior performance with much fewer extra data sizes.\nCodes will be available at https://github.com/Sense-X/Co-DETR.\n","authors":["Zhuofan Zong","Guanglu Song","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.12860v3.pdf","comment":"Tech report. Codes will be available at\n  https://github.com/Sense-X/Co-DETR"},{"id":"http://arxiv.org/abs/2303.09941v2","updated":"2023-03-20T09:07:49Z","published":"2023-03-17T12:55:22Z","title":"Leaping Into Memories: Space-Time Deep Feature Synthesis","summary":"  The success of deep learning models has led to their adaptation and adoption\nby prominent video understanding methods. The majority of these approaches\nencode features in a joint space-time modality for which the inner workings and\nlearned representations are difficult to visually interpret. We propose LEArned\nPreconscious Synthesis (LEAPS), an architecture-agnostic method for\nsynthesizing videos from the internal spatiotemporal representations of models.\nUsing a stimulus video and a target class, we prime a fixed space-time model\nand iteratively optimize a video initialized with random noise. We incorporate\nadditional regularizers to improve the feature diversity of the synthesized\nvideos as well as the cross-frame temporal coherence of motions. We\nquantitatively and qualitatively evaluate the applicability of LEAPS by\ninverting a range of spatiotemporal convolutional and attention-based\narchitectures trained on Kinetics-400, which to the best of our knowledge has\nnot been previously accomplished.\n","authors":["Alexandros Stergiou","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2303.09941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13901v2","updated":"2023-03-20T09:07:21Z","published":"2022-11-25T05:20:04Z","title":"Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent\n  Portrait Synthesis from Monocular Image","summary":"  A key challenge for novel view synthesis of monocular portrait images is 3D\nconsistency under continuous pose variations. Most existing methods rely on 2D\ngenerative models which often leads to obvious 3D inconsistency artifacts. We\npresent a 3D-consistent novel view synthesis approach for monocular portrait\nimages based on a recent proposed 3D-aware GAN, namely Generative Radiance\nManifolds (GRAM), which has shown strong 3D consistency at multiview image\ngeneration of virtual subjects via the radiance manifolds representation.\nHowever, simply learning an encoder to map a real image into the latent space\nof GRAM can only reconstruct coarse radiance manifolds without faithful fine\ndetails, while improving the reconstruction fidelity via instance-specific\noptimization is time-consuming. We introduce a novel detail manifolds\nreconstructor to learn 3D-consistent fine details on the radiance manifolds\nfrom monocular images, and combine them with the coarse radiance manifolds for\nhigh-fidelity reconstruction. The 3D priors derived from the coarse radiance\nmanifolds are used to regulate the learned details to ensure reasonable\nsynthesized results at novel views. Trained on in-the-wild 2D images, our\nmethod achieves high-fidelity and 3D-consistent portrait synthesis largely\noutperforming the prior art.\n","authors":["Yu Deng","Baoyuan Wang","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2211.13901v2.pdf","comment":"CVPR 2023 camera-ready version. Project page:\n  https://yudeng.github.io/GRAMInverter/"},{"id":"http://arxiv.org/abs/2303.10945v1","updated":"2023-03-20T09:01:23Z","published":"2023-03-20T09:01:23Z","title":"Open-World Pose Transfer via Sequential Test-Time Adaption","summary":"  Pose transfer aims to transfer a given person into a specified posture, has\nrecently attracted considerable attention. A typical pose transfer framework\nusually employs representative datasets to train a discriminative model, which\nis often violated by out-of-distribution (OOD) instances. Recently, test-time\nadaption (TTA) offers a feasible solution for OOD data by using a pre-trained\nmodel that learns essential features with self-supervision. However, those\nmethods implicitly make an assumption that all test distributions have a\nunified signal that can be learned directly. In open-world conditions, the pose\ntransfer task raises various independent signals: OOD appearance and skeleton,\nwhich need to be extracted and distributed in speciality. To address this\npoint, we develop a SEquential Test-time Adaption (SETA). In the test-time\nphrase, SETA extracts and distributes external appearance texture by augmenting\nOOD data for self-supervised training. To make non-Euclidean similarity among\ndifferent postures explicit, SETA uses the image representations derived from a\nperson re-identification (Re-ID) model for similarity computation. By\naddressing implicit posture representation in the test-time sequentially, SETA\ngreatly improves the generalization performance of current pose transfer\nmodels. In our experiment, we first show that pose transfer can be applied to\nopen-world applications, including Tiktok reenactment and celebrity motion\nsynthesis.\n","authors":["Junyang Chen","Xiaoyu Xian","Zhijing Yang","Tianshui Chen","Yongyi Lu","Yukai Shi","Jinshan Pan","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10945v1.pdf","comment":"We call for a solid pose transfer model that can handle open-world\n  instances beyond a specific dataset"},{"id":"http://arxiv.org/abs/2201.03597v2","updated":"2023-03-20T08:58:19Z","published":"2022-01-10T19:04:28Z","title":"Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image\n  Representations","summary":"  In tissue characterization and cancer diagnostics, multimodal imaging has\nemerged as a powerful technique. Thanks to computational advances, large\ndatasets can be exploited to discover patterns in pathologies and improve\ndiagnosis. However, this requires efficient and scalable image retrieval\nmethods. Cross-modality image retrieval is particularly challenging, since\nimages of similar (or even the same) content captured by different modalities\nmight share few common structures. We propose a new application-independent\ncontent-based image retrieval (CBIR) system for reverse (sub-)image search\nacross modalities, which combines deep learning to generate representations\n(embedding the different modalities in a common space) with classical feature\nextraction and bag-of-words models for efficient and reliable retrieval. We\nillustrate its advantages through a replacement study, exploring a number of\nfeature extractors and learned representations, as well as through comparison\nto recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval\non a (publicly available) dataset of brightfield and second harmonic generation\nmicroscopy images, the results show that our approach is superior to all tested\nalternatives. We discuss the shortcomings of the compared methods and observe\nthe importance of equivariance and invariance properties of the learned\nrepresentations and feature extractors in the CBIR pipeline. Code is available\nat: \\url{https://github.com/MIDA-group/CrossModal_ImgRetrieval}.\n","authors":["Eva Breznik","Elisabeth Wetzer","Joakim Lindblad","Nataša Sladoje"],"pdf_url":"https://arxiv.org/pdf/2201.03597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10944v1","updated":"2023-03-20T08:57:45Z","published":"2023-03-20T08:57:45Z","title":"Location-Free Scene Graph Generation","summary":"  Scene Graph Generation (SGG) is a challenging visual understanding task. It\ncombines the detection of entities and relationships between them in a scene.\nBoth previous works and existing evaluation metrics rely on bounding box\nlabels, even though many downstream scene graph applications do not need\nlocation information. The need for localization labels significantly increases\nthe annotation cost and hampers the creation of more and larger scene graph\ndatasets. We suggest breaking the dependency of scene graphs on bounding box\nlabels by proposing location-free scene graph generation (LF-SGG). This new\ntask aims at predicting instances of entities, as well as their relationships,\nwithout spatial localization. To objectively evaluate the task, the predicted\nand ground truth scene graphs need to be compared. We solve this NP-hard\nproblem through an efficient algorithm using branching. Additionally, we design\nthe first LF-SGG method, Pix2SG, using autoregressive sequence modeling. Our\nproposed method is evaluated on Visual Genome and 4D-OR. Although using\nsignificantly fewer labels during training, we achieve 74.12\\% of the\nlocation-supervised SOTA performance on Visual Genome and even outperform the\nbest method on 4D-OR.\n","authors":["Ege Özsoy","Felix Holm","Tobias Czempiel","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2303.10944v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.10941v1","updated":"2023-03-20T08:52:34Z","published":"2023-03-20T08:52:34Z","title":"HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting","summary":"  We present a simple yet effective method for skeleton-free motion\nretargeting. Previous methods transfer motion between high-resolution meshes,\nfailing to preserve the inherent local-part motions in the mesh. Addressing\nthis issue, our proposed method learns the correspondence in a coarse-to-fine\nfashion by integrating the retargeting process with a mesh-coarsening pipeline.\nFirst, we propose a mesh-coarsening module that coarsens the mesh\nrepresentations for better motion transfer. This module improves the ability to\nhandle small-part motion and preserves the local motion interdependence between\nneighboring mesh vertices. Furthermore, we leverage a hierarchical refinement\nprocedure to complement missing mesh details by gradually improving the\nlow-resolution mesh output with a higher-resolution one. We evaluate our method\non several well-known 3D character datasets, and it yields an average\nimprovement of 25% on point-wise mesh euclidean distance (PMD) against the\nstart-of-art method. Moreover, our qualitative results show that our method is\nsignificantly helpful in preserving the moving consistency of different body\nparts on the target character due to disentangling body-part structures and\nmesh details in a hierarchical way.\n","authors":["Haoyu Wang","Shaoli Huang","Fang Zhao","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2303.10941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10049v2","updated":"2023-03-20T08:52:12Z","published":"2023-03-17T15:23:15Z","title":"Uncertainty-informed Mutual Learning for Joint Medical Image\n  Classification and Segmentation","summary":"  Classification and segmentation are crucial in medical image analysis as they\nenable accurate diagnosis and disease monitoring. However, current methods\noften prioritize the mutual learning features and shared model parameters,\nwhile neglecting the reliability of features and performances. In this paper,\nwe propose a novel Uncertainty-informed Mutual Learning (UML) framework for\nreliable and interpretable medical image analysis. Our UML introduces\nreliability to joint classification and segmentation tasks, leveraging mutual\nlearning with uncertainty to improve performance. To achieve this, we first use\nevidential deep learning to provide image-level and pixel-wise confidences.\nThen, an Uncertainty Navigator Decoder is constructed for better using mutual\nfeatures and generating segmentation results. Besides, an Uncertainty\nInstructor is proposed to screen reliable masks for classification. Overall,\nUML could produce confidence estimation in features and performance for each\nlink (classification and segmentation). The experiments on the public datasets\ndemonstrate that our UML outperforms existing methods in terms of both accuracy\nand robustness. Our UML has the potential to explore the development of more\nreliable and explainable medical image analysis models. We will release the\ncodes for reproduction after acceptance.\n","authors":["Kai Ren","Ke Zou","Xianjie Liu","Yidi Chen","Xuedong Yuan","Xiaojing Shen","Meng Wang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.10049v2.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2211.05405v4","updated":"2023-03-20T08:29:29Z","published":"2022-11-10T08:19:44Z","title":"VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning","summary":"  Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n","authors":["Nghia Hieu Nguyen","Duong T. D. Vo","Minh-Quan Ha"],"pdf_url":"https://arxiv.org/pdf/2211.05405v4.pdf","comment":"Accepted for publishing at the VNU Journal of Science: Computer\n  Science and Communication Engineering"},{"id":"http://arxiv.org/abs/2301.09489v3","updated":"2023-03-20T08:29:04Z","published":"2023-01-23T15:32:27Z","title":"Contracting Skeletal Kinematic Embeddings for Anomaly Detection","summary":"  Detecting the anomaly of human behavior is paramount to timely recognizing\nendangering situations, such as street fights or elderly falls. However,\nanomaly detection is complex, since anomalous events are rare and because it is\nan open set recognition task, i.e., what is anomalous at inference has not been\nobserved at training. We propose COSKAD, a novel model which encodes skeletal\nhuman motion by an efficient graph convolutional network and learns to COntract\nSKeletal kinematic embeddings onto a latent hypersphere of minimum volume for\nAnomaly Detection. We propose and analyze three latent space designs for\nCOSKAD: the commonly-adopted Euclidean, and the new spherical-radial and\nhyperbolic volumes. All three variants outperform the state-of-the-art,\nincluding video-based techniques, on the ShangaiTechCampus, the Avenue, and on\nthe most recent UBnormal dataset, for which we contribute novel skeleton\nannotations and the selection of human-related videos. The source code and\ndataset will be released upon acceptance.\n","authors":["Alessandro Flaborea","Guido D'Amely","Stefano D'Arrigo","Marco Aurelio Sterpa","Alessio Sampieri","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2301.09489v3.pdf","comment":"Submitted to Pattern Recognition Journal"},{"id":"http://arxiv.org/abs/2303.10937v1","updated":"2023-03-20T08:26:29Z","published":"2023-03-20T08:26:29Z","title":"Boosting Weakly Supervised Object Detection using Fusion and Priors from\n  Hallucinated Depth","summary":"  Despite recent attention and exploration of depth for various tasks, it is\nstill an unexplored modality for weakly-supervised object detection (WSOD). We\npropose an amplifier method for enhancing the performance of WSOD by\nintegrating depth information. Our approach can be applied to any WSOD method\nbased on multiple-instance learning, without necessitating additional\nannotations or inducing large computational expenses. Our proposed method\nemploys a monocular depth estimation technique to obtain hallucinated depth\ninformation, which is then incorporated into a Siamese WSOD network using\ncontrastive loss and fusion. By analyzing the relationship between language\ncontext and depth, we calculate depth priors to identify the bounding box\nproposals that may contain an object of interest. These depth priors are then\nutilized to update the list of pseudo ground-truth boxes, or adjust the\nconfidence of per-box predictions. Our proposed method is evaluated on six\ndatasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and\nComic2k) by implementing it on top of two state-of-the-art WSOD methods, and we\ndemonstrate a substantial enhancement in performance.\n","authors":["Cagri Gungor","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2303.10937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09232v2","updated":"2023-03-20T08:25:38Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wang","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09288v3","updated":"2023-03-20T08:24:20Z","published":"2022-11-17T01:43:53Z","title":"Longitudinal thermal imaging for scalable non-residential HVAC and\n  occupant behaviour characterization","summary":"  This work presents a study on the characterization of the air-conditioning\n(AC) usage pattern of non-residential buildings from thermal images collected\nfrom an urban-scale infrared (IR) observatory. To achieve this first, an image\nprocessing scheme, for cleaning and extraction of the temperature time series\nfrom the thermal images is implemented. To test the accuracy of the thermal\nmeasurements using IR camera, the extracted temperature is compared against the\nground truth surface temperature measurements. It is observed that the\ndetrended thermal measurements match well with the ground truth surface\ntemperature measurements. Subsequently, the operational pattern of the\nwater-cooled systems and window AC units are extracted from the analysis of the\nthermal signature. It is observed that for the water-cooled system, the\ndifference between the rate of change of the window and wall can be used to\nextract the operational pattern. While, in the case of the window AC units,\nwavelet transform of the AC unit temperature is used to extract the frequency\nand time domain information of the AC unit operation. The results of the\nanalysis are compared against the indoor temperature sensors installed in the\noffice spaces of the building. It is realized that the accuracy in the\nprediction of the operational pattern is highest between 8 pm to 10 am, and it\nreduces during the day because of solar radiation and high daytime temperature.\nSubsequently, a characterization study is conducted for eight window/split AC\nunits from the thermal image collected during the nighttime. This forms one of\nthe first studies on the operational behavior of HVAC systems for\nnon-residential buildings using the longitudinal thermal imaging technique. The\noutput from this study can be used to better understand the operational and\noccupant behavior, without requiring to deploy a large array of sensors in the\nbuilding space.\n","authors":["Vasantha Ramani","Miguel Martin","Pandarasamy Arjunan","Adrian Chong","Kameshwar Poolla","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2211.09288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10936v1","updated":"2023-03-20T08:20:04Z","published":"2023-03-20T08:20:04Z","title":"Learning to Explore Informative Trajectories and Samples for Embodied\n  Perception","summary":"  We are witnessing significant progress on perception models, specifically\nthose trained on large-scale internet images. However, efficiently generalizing\nthese perception models to unseen embodied tasks is insufficiently studied,\nwhich will help various relevant applications (e.g., home robots). Unlike\nstatic perception methods trained on pre-collected images, the embodied agent\ncan move around in the environment and obtain images of objects from any\nviewpoints. Therefore, efficiently learning the exploration policy and\ncollection method to gather informative training samples is the key to this\ntask. To do this, we first build a 3D semantic distribution map to train the\nexploration policy self-supervised by introducing the semantic distribution\ndisagreement and the semantic distribution uncertainty rewards. Note that the\nmap is generated from multi-view observations and can weaken the impact of\nmisidentification from an unfamiliar viewpoint. Our agent is then encouraged to\nexplore the objects with different semantic distributions across viewpoints, or\nuncertain semantic distributions. With the explored informative trajectories,\nwe propose to select hard samples on trajectories based on the semantic\ndistribution uncertainty to reduce unnecessary observations that can be\ncorrectly identified. Experiments show that the perception model fine-tuned\nwith our method outperforms the baselines trained with other exploration\npolicies. Further, we demonstrate the robustness of our method in real-robot\nexperiments.\n","authors":["Ya Jing","Tao Kong"],"pdf_url":"https://arxiv.org/pdf/2303.10936v1.pdf","comment":"To be published in IEEE International Conference on Robotics and\n  Automation (ICRA), 2023"},{"id":"http://arxiv.org/abs/2207.11209v3","updated":"2023-03-20T08:05:08Z","published":"2022-07-22T17:19:00Z","title":"Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise\n  Binarization","summary":"  Instance segmentation on point clouds is crucially important for 3D scene\nunderstanding. Most SOTAs adopt distance clustering, which is typically\neffective but does not perform well in segmenting adjacent objects with the\nsame semantic label (especially when they share neighboring points). Due to the\nuneven distribution of offset points, these existing methods can hardly cluster\nall instance points. To this end, we design a novel divide-and-conquer strategy\nnamed PBNet that binarizes each point and clusters them separately to segment\ninstances. Our binary clustering divides offset instance points into two\ncategories: high and low density points (HPs vs. LPs). Adjacent objects can be\nclearly separated by removing LPs, and then be completed and refined by\nassigning LPs via a neighbor voting method. To suppress potential\nover-segmentation, we propose to construct local scenes with the weight mask\nfor each instance. As a plug-in, the proposed binary clustering can replace the\ntraditional distance clustering and lead to consistent performance gains on\nmany mainstream baselines. A series of experiments on ScanNetV2 and S3DIS\ndatasets indicate the superiority of our model. In particular, PBNet ranks\nfirst on the ScanNetV2 official benchmark challenge, achieving the highest mAP.\n","authors":["Weiguang Zhao","Yuyao Yan","Chaolong Yang","Jianan Ye","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2207.11209v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03376v3","updated":"2023-03-20T07:52:57Z","published":"2022-02-07T17:47:46Z","title":"Message Passing Neural PDE Solvers","summary":"  The numerical solution of partial differential equations (PDEs) is difficult,\nhaving led to a century of research so far. Recently, there have been pushes to\nbuild neural--numerical hybrid solvers, which piggy-backs the modern trend\ntowards fully end-to-end learned systems. Most works so far can only generalize\nover a subset of properties to which a generic solver would be faced,\nincluding: resolution, topology, geometry, boundary conditions, domain\ndiscretization regularity, dimensionality, etc. In this work, we build a\nsolver, satisfying these properties, where all the components are based on\nneural message passing, replacing all heuristically designed components in the\ncomputation graph with backprop-optimized neural function approximators. We\nshow that neural message passing solvers representationally contain some\nclassical methods, such as finite differences, finite volumes, and WENO\nschemes. In order to encourage stability in training autoregressive models, we\nput forward a method that is based on the principle of zero-stability, posing\nstability as a domain adaptation problem. We validate our method on various\nfluid-like flow problems, demonstrating fast, stable, and accurate performance\nacross different domain topologies, equation parameters, discretizations, etc.,\nin 1D and 2D.\n","authors":["Johannes Brandstetter","Daniel Worrall","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2202.03376v3.pdf","comment":"Published at ICLR 2022 (Spotlight paper), Github:\n  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers"},{"id":"http://arxiv.org/abs/2207.01331v2","updated":"2023-03-20T07:50:13Z","published":"2022-07-04T11:24:10Z","title":"Improving Nighttime Driving-Scene Segmentation via Dual Image-adaptive\n  Learnable Filters","summary":"  Semantic segmentation on driving-scene images is vital for autonomous\ndriving. Although encouraging performance has been achieved on daytime images,\nthe performance on nighttime images are less satisfactory due to the\ninsufficient exposure and the lack of labeled data. To address these issues, we\npresent an add-on module called dual image-adaptive learnable filters\n(DIAL-Filters) to improve the semantic segmentation in nighttime driving\nconditions, aiming at exploiting the intrinsic features of driving-scene images\nunder different illuminations. DIAL-Filters consist of two parts, including an\nimage-adaptive processing module (IAPM) and a learnable guided filter (LGF).\nWith DIAL-Filters, we design both unsupervised and supervised frameworks for\nnighttime driving-scene segmentation, which can be trained in an end-to-end\nmanner. Specifically, the IAPM module consists of a small convolutional neural\nnetwork with a set of differentiable image filters, where each image can be\nadaptively enhanced for better segmentation with respect to the different\nilluminations. The LGF is employed to enhance the output of segmentation\nnetwork to get the final segmentation result. The DIAL-Filters are light-weight\nand efficient and they can be readily applied for both daytime and nighttime\nimages. Our experiments show that DAIL-Filters can significantly improve the\nsupervised segmentation performance on ACDC_Night and NightCity datasets, while\nit demonstrates the state-of-the-art performance on unsupervised nighttime\nsemantic segmentation on Dark Zurich and Nighttime Driving testbeds.\n","authors":["Wenyu Liu","Wentong Li","Jianke Zhu","Miaomiao Cui","Xuansong Xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.01331v2.pdf","comment":"Accepted by IEEE TCSVT(2023)"},{"id":"http://arxiv.org/abs/2303.10923v1","updated":"2023-03-20T07:47:36Z","published":"2023-03-20T07:47:36Z","title":"Dynamic Object Removal for Effective Slam","summary":"  This research paper focuses on the problem of dynamic objects and their\nimpact on effective motion planning and localization. The paper proposes a\ntwo-step process to address this challenge, which involves finding the dynamic\nobjects in the scene using a Flow-based method and then using a deep Video\ninpainting algorithm to remove them. The study aims to test the validity of\nthis approach by comparing it with baseline results using two state-of-the-art\nSLAM algorithms, ORB-SLAM2 and LSD, and understanding the impact of dynamic\nobjects and the corresponding trade-offs. The proposed approach does not\nrequire any significant modifications to the baseline SLAM algorithms, and\ntherefore, the computational effort required remains unchanged. The paper\npresents a detailed analysis of the results obtained and concludes that the\nproposed method is effective in removing dynamic objects from the scene,\nleading to improved SLAM performance.\n","authors":["Phani Krishna Uppala","Abhishek Bamotra","Raj Kolamuri"],"pdf_url":"https://arxiv.org/pdf/2303.10923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13386v2","updated":"2023-03-20T07:33:09Z","published":"2022-04-28T10:01:36Z","title":"Self-supervised Contrastive Learning for Audio-Visual Action Recognition","summary":"  The underlying correlation between audio and visual modalities can be\nutilized to learn supervised information for unlabeled videos. In this paper,\nwe propose an end-to-end self-supervised framework named Audio-Visual\nContrastive Learning (AVCL), to learn discriminative audio-visual\nrepresentations for action recognition. Specifically, we design an attention\nbased multi-modal fusion module (AMFM) to fuse audio and visual modalities. To\nalign heterogeneous audio-visual modalities, we construct a novel\nco-correlation guided representation alignment module (CGRA). To learn\nsupervised information from unlabeled videos, we propose a novel\nself-supervised contrastive learning module (SelfCL). Furthermore, we build a\nnew audio-visual action recognition dataset named Kinetics-Sounds100.\nExperimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets\ndemonstrate the superiority of our AVCL over the state-of-the-art methods on\nlarge-scale action recognition benchmark.\n","authors":["Yang Liu","Ying Tan","Haoyuan Lan"],"pdf_url":"https://arxiv.org/pdf/2204.13386v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.10916v1","updated":"2023-03-20T07:16:58Z","published":"2023-03-20T07:16:58Z","title":"Learning Behavior Recognition in Smart Classroom with Multiple Students\n  Based on YOLOv5","summary":"  Deep learning-based computer vision technology has grown stronger in recent\nyears, and cross-fertilization using computer vision technology has been a\npopular direction in recent years. The use of computer vision technology to\nidentify students' learning behavior in the classroom can reduce the workload\nof traditional teachers in supervising students in the classroom, and ensure\ngreater accuracy and comprehensiveness. However, existing student learning\nbehavior detection systems are unable to track and detect multiple targets\nprecisely, and the accuracy of learning behavior recognition is not high enough\nto meet the existing needs for the accurate recognition of student behavior in\nthe classroom. To solve this problem, we propose a YOLOv5s network structure\nbased on you only look once (YOLO) algorithm to recognize and analyze students'\nclassroom behavior in this paper. Firstly, the input images taken in the smart\nclassroom are pre-processed. Then, the pre-processed image is fed into the\ndesigned YOLOv5 networks to extract deep features through convolutional layers,\nand the Squeeze-and-Excitation (SE) attention detection mechanism is applied to\nreduce the weight of background information in the recognition process.\nFinally, the extracted features are classified by the Feature Pyramid Networks\n(FPN) and Path Aggregation Network (PAN) structures. Multiple groups of\nexperiments were performed to compare with traditional learning behavior\nrecognition methods to validate the effectiveness of the proposed method. When\ncompared with YOLOv4, the proposed method is able to improve the mAP\nperformance by 11%.\n","authors":["Zhifeng Wang","Jialong Yao","Chunyan Zeng","Wanxuan Wu","Hongmin Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10916v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.09554v2","updated":"2023-03-20T07:02:38Z","published":"2023-03-16T17:59:22Z","title":"PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D\n  Supervision","summary":"  Impressive progress in generative models and implicit representations gave\nrise to methods that can generate 3D shapes of high quality. However, being\nable to locally control and edit shapes is another essential property that can\nunlock several content creation applications. Local control can be achieved\nwith part-aware models, but existing methods require 3D supervision and cannot\nproduce textures. In this work, we devise PartNeRF, a novel part-aware\ngenerative model for editable 3D shape synthesis that does not require any\nexplicit 3D supervision. Our model generates objects as a set of locally\ndefined NeRFs, augmented with an affine transformation. This enables several\nediting operations such as applying transformations on parts, mixing parts from\ndifferent objects etc. To ensure distinct, manipulable parts we enforce a hard\nassignment of rays to parts that makes sure that the color of each ray is only\ndetermined by a single NeRF. As a result, altering one part does not affect the\nappearance of the others. Evaluations on various ShapeNet categories\ndemonstrate the ability of our model to generate editable 3D objects of\nimproved fidelity, compared to previous part-based generative approaches that\nrequire 3D supervision or models relying on NeRFs.\n","authors":["Konstantinos Tertikas","Paschalidou Despoina","Boxiao Pan","Jeong Joon Park","Mikaela Angelina Uy","Ioannis Emiris","Yannis Avrithis","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.09554v2.pdf","comment":"To appear in CVPR 2023, Project Page:\n  https://ktertikas.github.io/part_nerf"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2303.05499v4","updated":"2023-03-20T06:57:39Z","published":"2023-03-09T18:52:16Z","title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set\n  Object Detection","summary":"  In this paper, we present an open-set object detector, called Grounding DINO,\nby marrying Transformer-based detector DINO with grounded pre-training, which\ncan detect arbitrary objects with human inputs such as category names or\nreferring expressions. The key solution of open-set object detection is\nintroducing language to a closed-set detector for open-set concept\ngeneralization. To effectively fuse language and vision modalities, we\nconceptually divide a closed-set detector into three phases and propose a tight\nfusion solution, which includes a feature enhancer, a language-guided query\nselection, and a cross-modality decoder for cross-modality fusion. While\nprevious works mainly evaluate open-set object detection on novel categories,\nwe propose to also perform evaluations on referring expression comprehension\nfor objects specified with attributes. Grounding DINO performs remarkably well\non all three settings, including benchmarks on COCO, LVIS, ODinW, and\nRefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection\nzero-shot transfer benchmark, i.e., without any training data from COCO. It\nsets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code\nwill be available at \\url{https://github.com/IDEA-Research/GroundingDINO}.\n","authors":["Shilong Liu","Zhaoyang Zeng","Tianhe Ren","Feng Li","Hao Zhang","Jie Yang","Chunyuan Li","Jianwei Yang","Hang Su","Jun Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05499v4.pdf","comment":"Code will be available at\n  https://github.com/IDEA-Research/GroundingDINO"},{"id":"http://arxiv.org/abs/2303.10904v1","updated":"2023-03-20T06:47:59Z","published":"2023-03-20T06:47:59Z","title":"Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based\n  Action Recognition","summary":"  The self-supervised pretraining paradigm has achieved great success in\nskeleton-based action recognition. However, these methods treat the motion and\nstatic parts equally, and lack an adaptive design for different parts, which\nhas a negative impact on the accuracy of action recognition. To realize the\nadaptive action modeling of both parts, we propose an Actionlet-Dependent\nContrastive Learning method (ActCLR). The actionlet, defined as the\ndiscriminative subset of the human skeleton, effectively decomposes motion\nregions for better action modeling. In detail, by contrasting with the static\nanchor without motion, we extract the motion region of the skeleton data, which\nserves as the actionlet, in an unsupervised manner. Then, centering on\nactionlet, a motion-adaptive data transformation method is built. Different\ndata transformations are applied to actionlet and non-actionlet regions to\nintroduce more diversity while maintaining their own characteristics.\nMeanwhile, we propose a semantic-aware feature pooling method to build feature\nrepresentations among motion and static regions in a distinguished manner.\nExtensive experiments on NTU RGB+D and PKUMMD show that the proposed method\nachieves remarkable action recognition performance. More visualization and\nquantitative experiments demonstrate the effectiveness of our method. Our\nproject website is available at https://langlandslin.github.io/projects/ActCLR/\n","authors":["Lilang Lin","Jiahang Zhang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10904v1.pdf","comment":"Accepted to CVPR2023. The project page is at\n  https://langlandslin.github.io/projects/ActCLR/"},{"id":"http://arxiv.org/abs/2303.10902v1","updated":"2023-03-20T06:44:49Z","published":"2023-03-20T06:44:49Z","title":"Feature Alignment and Uniformity for Test Time Adaptation","summary":"  Test time adaptation (TTA) aims to adapt deep neural networks when receiving\nout of distribution test domain samples. In this setting, the model can only\naccess online unlabeled test samples and pre-trained models on the training\ndomains. We first address TTA as a feature revision problem due to the domain\ngap between source domains and target domains. After that, we follow the two\nmeasurements alignment and uniformity to discuss the test time feature\nrevision. For test time feature uniformity, we propose a test time\nself-distillation strategy to guarantee the consistency of uniformity between\nrepresentations of the current batch and all the previous batches. For test\ntime feature alignment, we propose a memorized spatial local clustering\nstrategy to align the representations among the neighborhood samples for the\nupcoming batch. To deal with the common noisy label problem, we propound the\nentropy and consistency filters to select and drop the possible noisy labels.\nTo prove the scalability and efficacy of our method, we conduct experiments on\nfour domain generalization benchmarks and four medical image segmentation tasks\nwith various backbones. Experiment results show that our method not only\nimproves baseline stably but also outperforms existing state-of-the-art test\ntime adaptation methods.\n","authors":["Shuai Wang","Daoan Zhang","Zipei Yan","Jianguo Zhang","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2303.10902v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.10898v1","updated":"2023-03-20T06:35:46Z","published":"2023-03-20T06:35:46Z","title":"A Tiny Machine Learning Model for Point Cloud Object Classification","summary":"  The design of a tiny machine learning model, which can be deployed in mobile\nand edge devices, for point cloud object classification is investigated in this\nwork. To achieve this objective, we replace the multi-scale representation of a\npoint cloud object with a single-scale representation for complexity reduction,\nand exploit rich 3D geometric information of a point cloud object for\nperformance improvement. The proposed solution is named Green-PointHop due to\nits low computational complexity. We evaluate the performance of Green-PointHop\non ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of\n64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a\nModelNet40 object of 1024 down-sampled points. Its classification performance\ngaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and\nScanObjectNN, respectively. On the other hand, the model size and inference\ncomplexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.\n","authors":["Min Zhang","Jintang Xue","Pranav Kadam","Hardik Prajapati","Shan Liu","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.10898v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10896v1","updated":"2023-03-20T06:32:55Z","published":"2023-03-20T06:32:55Z","title":"Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D\n  Images","summary":"  The function of constructing the hierarchy of objects is important to the\nvisual process of the human brain. Previous studies have successfully adopted\ncapsule networks to decompose the digits and faces into parts in an\nunsupervised manner to investigate the similar perception mechanism of neural\nnetworks. However, their descriptions are restricted to the 2D space, limiting\ntheir capacities to imitate the intrinsic 3D perception ability of humans. In\nthis paper, we propose an Inverse Graphics Capsule Network (IGC-Net) to learn\nthe hierarchical 3D face representations from large-scale unlabeled images. The\ncore of IGC-Net is a new type of capsule, named graphics capsule, which\nrepresents 3D primitives with interpretable parameters in computer graphics\n(CG), including depth, albedo, and 3D pose. Specifically, IGC-Net first\ndecomposes the objects into a set of semantic-consistent part-level\ndescriptions and then assembles them into object-level descriptions to build\nthe hierarchy. The learned graphics capsules reveal how the neural networks,\noriented at visual perception, understand faces as a hierarchy of 3D models.\nBesides, the discovered parts can be deployed to the unsupervised face\nsegmentation task to evaluate the semantic consistency of our method. Moreover,\nthe part-level descriptions with explicit physical meanings provide insight\ninto the face analysis that originally runs in a black box, such as the\nimportance of shape and texture for face recognition. Experiments on CelebA,\nBP4D, and Multi-PIE demonstrate the characteristics of our IGC-Net.\n","authors":["Chang Yu","Xiangyu Zhu","Xiaomei Zhang","Zhaoxiang Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2303.10896v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.10895v1","updated":"2023-03-20T06:32:48Z","published":"2023-03-20T06:32:48Z","title":"Leapfrog Diffusion Model for Stochastic Trajectory Prediction","summary":"  To model the indeterminacy of human behaviors, stochastic trajectory\nprediction requires a sophisticated multi-modal distribution of future\ntrajectories. Emerging diffusion models have revealed their tremendous\nrepresentation capacities in numerous generation tasks, showing potential for\nstochastic trajectory prediction. However, expensive time consumption prevents\ndiffusion models from real-time prediction, since a large number of denoising\nsteps are required to assure sufficient representation ability. To resolve the\ndilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based\ntrajectory prediction model, which provides real-time, precise, and diverse\npredictions. The core of the proposed LED is to leverage a trainable leapfrog\ninitializer to directly learn an expressive multi-modal distribution of future\ntrajectories, which skips a large number of denoising steps, significantly\naccelerating inference speed. Moreover, the leapfrog initializer is trained to\nappropriately allocate correlated samples to provide a diversity of predicted\nfuture trajectories, significantly improving prediction performances. Extensive\nexperiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show\nthat LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE\nimprovement on NFL. The proposed LED also speeds up the inference\n19.3/30.8/24.3/25.1 times compared to the standard diffusion model on\nNBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at\nhttps://github.com/MediaBrain-SJTU/LED.\n","authors":["Weibo Mao","Chenxin Xu","Qi Zhu","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10895v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.10894v1","updated":"2023-03-20T06:26:49Z","published":"2023-03-20T06:26:49Z","title":"M$^{2}$SNet: Multi-scale in Multi-scale Subtraction Network for Medical\n  Image Segmentation","summary":"  Accurate medical image segmentation is critical for early medical diagnosis.\nMost existing methods are based on U-shape structure and use element-wise\naddition or concatenation to fuse different level features progressively in\ndecoder. However, both the two operations easily generate plenty of redundant\ninformation, which will weaken the complementarity between different level\nfeatures, resulting in inaccurate localization and blurred edges of lesions. To\naddress this challenge, we propose a general multi-scale in multi-scale\nsubtraction network (M$^{2}$SNet) to finish diverse segmentation from medical\nimage. Specifically, we first design a basic subtraction unit (SU) to produce\nthe difference features between adjacent levels in encoder. Next, we expand the\nsingle-scale SU to the intra-layer multi-scale SU, which can provide the\ndecoder with both pixel-level and structure-level difference information. Then,\nwe pyramidally equip the multi-scale SUs at different levels with varying\nreceptive fields, thereby achieving the inter-layer multi-scale feature\naggregation and obtaining rich multi-scale difference information. In addition,\nwe build a training-free network ``LossNet'' to comprehensively supervise the\ntask-aware features from bottom layer to top layer, which drives our\nmulti-scale subtraction network to capture the detailed and structural cues\nsimultaneously. Without bells and whistles, our method performs favorably\nagainst most state-of-the-art methods under different evaluation metrics on\neleven datasets of four different medical image segmentation tasks of diverse\nimage modalities, including color colonoscopy imaging, ultrasound imaging,\ncomputed tomography (CT), and optical coherence tomography (OCT). The source\ncode can be available at \\url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}.\n","authors":["Xiaoqi Zhao","Hongpeng Jia","Youwei Pang","Long Lv","Feng Tian","Lihe Zhang","Weibing Sun","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10894v1.pdf","comment":"Submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2111.04019v2","updated":"2023-03-20T06:26:10Z","published":"2021-11-07T07:29:24Z","title":"Multi-Fake Evolutionary Generative Adversarial Networks for Imbalance\n  Hyperspectral Image Classification","summary":"  This paper presents a novel multi-fake evolutionary generative adversarial\nnetwork(MFEGAN) for handling imbalance hyperspectral image classification. It\nis an end-to-end approach in which different generative objective losses are\nconsidered in the generator network to improve the classification performance\nof the discriminator network. Thus, the same discriminator network has been\nused as a standard classifier by embedding the classifier network on top of the\ndiscriminating function. The effectiveness of the proposed method has been\nvalidated through two hyperspectral spatial-spectral data sets. The same\ngenerative and discriminator architectures have been utilized with two\ndifferent GAN objectives for a fair performance comparison with the proposed\nmethod. It is observed from the experimental validations that the proposed\nmethod outperforms the state-of-the-art methods with better classification\nperformance.\n","authors":["Tanmoy Dam","Nidhi Swami","Sreenatha G. Anavatti","Hussein A. Abbass"],"pdf_url":"https://arxiv.org/pdf/2111.04019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10891v1","updated":"2023-03-20T06:16:22Z","published":"2023-03-20T06:16:22Z","title":"Offline-Online Class-incremental Continual Learning via Dual-prototype\n  Self-augment and Refinement","summary":"  This paper investigates a new, practical, but challenging problem named\nOffline-Online Class-incremental Continual Learning (O$^2$CL), which aims to\npreserve the discernibility of pre-trained (i.e., offline) base classes without\nbuffering data examples, and efficiently learn novel classes continuously in a\nsingle-pass (i.e., online) data stream. The challenges of this task are mainly\ntwo-fold: 1) Both base and novel classes suffer from severe catastrophic\nforgetting as no previous samples are available for replay. 2) As the online\ndata can only be observed once, there is no way to fully re-train the whole\nmodel, e.g., re-calibrate the decision boundaries via prototype alignment or\nfeature distillation. In this paper, we propose a novel Dual-prototype\nSelf-augment and Refinement method (DSR) for O$^2$CL problem, which consists of\ntwo strategies: 1) Dual class prototypes: Inner and hyper-dimensional\nprototypes are exploited to utilize the pre-trained information and obtain\nrobust quasi-orthogonal representations rather than example buffers for both\nprivacy preservation and memory reduction. 2) Self-augment and refinement:\nInstead of updating the whole network, we jointly optimize the extra projection\nmodule with the self-augment inner prototypes from base and novel classes,\ngradually refining the hyper-dimensional prototypes to obtain accurate decision\nboundaries for learned classes. Extensive experiments demonstrate the\neffectiveness and superiority of the proposed DSR in O$^2$CL.\n","authors":["Fushuo Huo","Wenchao Xu","Jingcai Guo","Haozhao Wang","Yunfeng Fan","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2303.10891v1.pdf","comment":"Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Yunfeng Fan,\n  Song Guo"},{"id":"http://arxiv.org/abs/2303.03757v2","updated":"2023-03-20T06:05:27Z","published":"2023-03-07T09:33:49Z","title":"Deep Learning for Inertial Positioning: A Survey","summary":"  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT\ndevices, playing a crucial role in enabling ubiquitous and reliable\nlocalization. Inertial sensor-based positioning is essential in various\napplications, including personal navigation, location-based security, and\nhuman-device interaction. However, low-cost MEMS inertial sensors' measurements\nare inevitably corrupted by various error sources, leading to unbounded drifts\nwhen integrated doubly in traditional inertial navigation algorithms,\nsubjecting inertial positioning to the problem of error drifts. In recent\nyears, with the rapid increase in sensor data and computational power, deep\nlearning techniques have been developed, sparking significant research into\naddressing the problem of inertial positioning. Relevant literature in this\nfield spans across mobile computing, robotics, and machine learning. In this\narticle, we provide a comprehensive review of deep learning-based inertial\npositioning and its applications in tracking pedestrians, drones, vehicles, and\nrobots. We connect efforts from different fields and discuss how deep learning\ncan be applied to address issues such as sensor calibration, positioning error\ndrift reduction, and multi-sensor fusion. This article aims to attract readers\nfrom various backgrounds, including researchers and practitioners interested in\nthe potential of deep learning-based techniques to solve inertial positioning\nproblems. Our review demonstrates the exciting possibilities that deep learning\nbrings to the table and provides a roadmap for future research in this field.\n","authors":["Changhao Chen","Xianfei Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10883v1","updated":"2023-03-20T06:01:53Z","published":"2023-03-20T06:01:53Z","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","summary":"  We consider the generic problem of detecting low-level structures in images,\nwhich includes segmenting the manipulated parts, identifying out-of-focus\npixels, separating shadow regions, and detecting concealed objects. Whereas\neach such topic has been typically addressed with a domain-specific solution,\nwe show that a unified approach performs well across all of them. We take\ninspiration from the widely-used pre-training and then prompt tuning protocols\nin NLP and propose a new visual prompting model, named Explicit Visual\nPrompting (EVP). Different from the previous visual prompting which is\ntypically a dataset-level implicit embedding, our key insight is to enforce the\ntunable parameters focusing on the explicit visual content from each individual\nimage, i.e., the features from frozen patch embeddings and the input's\nhigh-frequency components. The proposed EVP significantly outperforms other\nparameter-efficient tuning protocols under the same amount of tunable\nparameters (5.7% extra trainable parameters of each task). EVP also achieves\nstate-of-the-art performances on diverse low-level structure segmentation tasks\ncompared to task-specific solutions. Our code is available at:\nhttps://github.com/NiFangBaAGe/Explict-Visual-Prompt.\n","authors":["Weihuang Liu","Xi Shen","Chi-Man Pun","Xiaodong Cun"],"pdf_url":"https://arxiv.org/pdf/2303.10883v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10882v1","updated":"2023-03-20T05:49:14Z","published":"2023-03-20T05:49:14Z","title":"Efficient Map Sparsification Based on 2D and 3D Discretized Grids","summary":"  Localization in a pre-built map is a basic technique for robot autonomous\nnavigation. Existing mapping and localization methods commonly work well in\nsmall-scale environments. As a map grows larger, however, more memory is\nrequired and localization becomes inefficient. To solve these problems, map\nsparsification becomes a practical necessity to acquire a subset of the\noriginal map for localization. Previous map sparsification methods add a\nquadratic term in mixed-integer programming to enforce a uniform distribution\nof selected landmarks, which requires high memory capacity and heavy\ncomputation. In this paper, we formulate map sparsification in an efficient\nlinear form and select uniformly distributed landmarks based on 2D discretized\ngrids. Furthermore, to reduce the influence of different spatial distributions\nbetween the mapping and query sequences, which is not considered in previous\nmethods, we also introduce a space constraint term based on 3D discretized\ngrids. The exhaustive experiments in different datasets demonstrate the\nsuperiority of the proposed methods in both efficiency and localization\nperformance. The relevant codes will be released at\nhttps://github.com/fishmarch/SLAM_Map_Compression.\n","authors":["Xiaoyu Zhang","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10882v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09101v2","updated":"2023-03-20T05:47:14Z","published":"2023-03-16T06:14:18Z","title":"Contrastive Semi-supervised Learning for Underwater Image Restoration\n  via Reliable Bank","summary":"  Despite the remarkable achievement of recent underwater image restoration\ntechniques, the lack of labeled data has become a major hurdle for further\nprogress. In this work, we propose a mean-teacher based Semi-supervised\nUnderwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled\ndata into network training. However, the naive mean-teacher method suffers from\ntwo main problems: (1) The consistency loss used in training might become\nineffective when the teacher's prediction is wrong. (2) Using L1 distance may\ncause the network to overfit wrong labels, resulting in confirmation bias. To\naddress the above problems, we first introduce a reliable bank to store the\n\"best-ever\" outputs as pseudo ground truth. To assess the quality of outputs,\nwe conduct an empirical analysis based on the monotonicity property to select\nthe most trustworthy NR-IQA method. Besides, in view of the confirmation bias\nproblem, we incorporate contrastive regularization to prevent the overfitting\non wrong labels. Experimental results on both full-reference and non-reference\nunderwater benchmarks demonstrate that our algorithm has obvious improvement\nover SOTA methods quantitatively and qualitatively. Code has been released at\nhttps://github.com/Huang-ShiRui/Semi-UIR.\n","authors":["Shirui Huang","Keyan Wang","Huan Liu","Jun Chen","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2303.09101v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.10876v1","updated":"2023-03-20T05:23:46Z","published":"2023-03-20T05:23:46Z","title":"EqMotion: Equivariant Multi-agent Motion Prediction with Invariant\n  Interaction Reasoning","summary":"  Learning to predict agent motions with relationship reasoning is important\nfor many applications. In motion prediction tasks, maintaining motion\nequivariance under Euclidean geometric transformations and invariance of agent\ninteraction is a critical and fundamental principle. However, such equivariance\nand invariance properties are overlooked by most existing methods. To fill this\ngap, we propose EqMotion, an efficient equivariant motion prediction model with\ninvariant interaction reasoning. To achieve motion equivariance, we propose an\nequivariant geometric feature learning module to learn a Euclidean\ntransformable feature through dedicated designs of equivariant operations. To\nreason agent's interactions, we propose an invariant interaction reasoning\nmodule to achieve a more stable interaction modeling. To further promote more\ncomprehensive motion features, we propose an invariant pattern feature learning\nmodule to learn an invariant pattern feature, which cooperates with the\nequivariant geometric feature to enhance network expressiveness. We conduct\nexperiments for the proposed model on four distinct scenarios: particle\ndynamics, molecule dynamics, human skeleton motion prediction and pedestrian\ntrajectory prediction. Experimental results show that our method is not only\ngenerally applicable, but also achieves state-of-the-art prediction\nperformances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is\navailable at https://github.com/MediaBrain-SJTU/EqMotion.\n","authors":["Chenxin Xu","Robby T. Tan","Yuhong Tan","Siheng Chen","Yu Guang Wang","Xinchao Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10876v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.15046v3","updated":"2023-03-20T05:18:49Z","published":"2022-11-28T04:08:55Z","title":"PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial\n  Networks for Radar-Based Precipitation Nowcasting","summary":"  The precipitation nowcasting methods have been elaborated over the centuries\nbecause rain has a crucial impact on human life. Not only quantitative\nprecipitation forecast (QPF) models and convolutional long short-term memory\n(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2\nare emerging. In this paper, we propose a paired complementary temporal\ncycle-consistent adversarial networks (PCT-CycleGAN) for radar-based\nprecipitation nowcasting, inspired by cycle-consistent adversarial networks\n(CycleGAN), which shows strong performance in image-to-image translation.\nPCT-CycleGAN generates temporal causality using two generator networks with\nforward and backward temporal dynamics in paired complementary cycles. Each\ngenerator network learns a huge number of one-to-one mappings about\ntime-dependent radar-based precipitation data to approximate a mapping function\nrepresenting the temporal dynamics in each direction. To create robust temporal\ncausality between paired complementary cycles, novel connection loss is\nproposed. The generator network learning forward temporal dynamics in\nPCT-CycleGAN generates radar-based precipitation data 10 minutes from the\ncurrent time. Also, it provides a reliable prediction of up to 2 hours with\niterative forecasting. The superiority of PCT-CycleGAN is demonstrated through\nqualitative and quantitative comparisons with several previous methods.\n","authors":["Jaeho Choi","Yura Kim","Kwang-Ho Kim","Sung-Hwa Jung","Ikhyun Cho"],"pdf_url":"https://arxiv.org/pdf/2211.15046v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10863v1","updated":"2023-03-20T04:54:26Z","published":"2023-03-20T04:54:26Z","title":"Decomposed Prototype Learning for Few-Shot Scene Graph Generation","summary":"  Today's scene graph generation (SGG) models typically require abundant manual\nannotations to learn new predicate types. Thus, it is difficult to apply them\nto real-world applications with a long-tailed distribution of predicates. In\nthis paper, we focus on a new promising task of SGG: few-shot SGG (FSSGG).\nFSSGG encourages models to be able to quickly transfer previous knowledge and\nrecognize novel predicates well with only a few examples. Although many\nadvanced approaches have achieved great success on few-shot learning (FSL)\ntasks, straightforwardly extending them into FSSGG is not applicable due to two\nintrinsic characteristics of predicate concepts: 1) Each predicate category\ncommonly has multiple semantic meanings under different contexts. 2) The visual\nappearance of relation triplets with the same predicate differs greatly under\ndifferent subject-object pairs. Both issues make it hard to model conventional\nlatent representations for predicate categories with state-of-the-art FSL\nmethods. To this end, we propose a novel Decomposed Prototype Learning (DPL).\nSpecifically, we first construct a decomposable prototype space to capture\nintrinsic visual patterns of subjects and objects for predicates, and enhance\ntheir feature representations with these decomposed prototypes. Then, we devise\nan intelligent metric learner to assign adaptive weights to each support sample\nby considering the relevance of their subject-object pairs. We further re-split\nthe VG dataset and compare DPL with various FSL methods to benchmark this task.\nExtensive results show that DPL achieves excellent performance in both base and\nnovel categories.\n","authors":["Xingchen Li","Long Chen","Guikun Chen","Yinfu Feng","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.10863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2210.15929v2","updated":"2023-03-20T04:36:45Z","published":"2022-10-28T06:20:55Z","title":"Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation\n  with Wordless Training","summary":"  Text-to-motion generation is an emerging and challenging problem, which aims\nto synthesize motion with the same semantics as the input text. However, due to\nthe lack of diverse labeled training data, most approaches either limit to\nspecific types of text annotations or require online optimizations to cater to\nthe texts during inference at the cost of efficiency and stability. In this\npaper, we investigate offline open-vocabulary text-to-motion generation in a\nzero-shot learning manner that neither requires paired training data nor extra\nonline optimization to adapt for unseen texts. Inspired by the prompt learning\nin NLP, we pretrain a motion generator that learns to reconstruct the full\nmotion from the masked motion. During inference, instead of changing the motion\ngenerator, our method reformulates the input text into a masked motion as the\nprompt for the motion generator to ``reconstruct'' the motion. In constructing\nthe prompt, the unmasked poses of the prompt are synthesized by a text-to-pose\ngenerator. To supervise the optimization of the text-to-pose generator, we\npropose the first text-pose alignment model for measuring the alignment between\ntexts and 3D poses. And to prevent the pose generator from overfitting to\nlimited training texts, we further propose a novel wordless training mechanism\nthat optimizes the text-to-pose generator without any training texts. The\ncomprehensive experimental results show that our method obtains a significant\nimprovement against the baseline methods. The code is available.\n","authors":["Junfan Lin","Jianlong Chang","Lingbo Liu","Guanbin Li","Liang Lin","Qi Tian","Chang-wen Chen"],"pdf_url":"https://arxiv.org/pdf/2210.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10856v1","updated":"2023-03-20T04:30:18Z","published":"2023-03-20T04:30:18Z","title":"Revisiting Realistic Test-Time Training: Sequential Inference and\n  Adaptation by Anchored Clustering Regularized Self-Training","summary":"  Deploying models on target domain data subject to distribution shift requires\nadaptation. Test-time training (TTT) emerges as a solution to this adaptation\nunder a realistic scenario where access to full source domain data is not\navailable, and instant inference on the target domain is required. Despite many\nefforts into TTT, there is a confusion over the experimental settings, thus\nleading to unfair comparisons. In this work, we first revisit TTT assumptions\nand categorize TTT protocols by two key factors. Among the multiple protocols,\nwe adopt a realistic sequential test-time training (sTTT) protocol, under which\nwe develop a test-time anchored clustering (TTAC) approach to enable stronger\ntest-time feature learning. TTAC discovers clusters in both source and target\ndomains and matches the target clusters to the source ones to improve\nadaptation. When source domain information is strictly absent (i.e.\nsource-free) we further develop an efficient method to infer source domain\ndistributions for anchored clustering. Finally, self-training~(ST) has\ndemonstrated great success in learning from unlabeled data and we empirically\nfigure out that applying ST alone to TTT is prone to confirmation bias.\nTherefore, a more effective TTT approach is introduced by regularizing\nself-training with anchored clustering, and the improved model is referred to\nas TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently\noutperforms the state-of-the-art methods on five TTT datasets, including\ncorrupted target domain, selected hard samples, synthetic-to-real adaptation\nand adversarially attacked target domain. We hope this work will provide a fair\nbenchmarking of TTT methods, and future research should be compared within\nrespective protocols.\n","authors":["Yongyi Su","Xun Xu","Tianrui Li","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10856v1.pdf","comment":"Test-time training, Self-training. arXiv admin note: substantial text\n  overlap with arXiv:2206.02721"},{"id":"http://arxiv.org/abs/2201.09548v2","updated":"2023-03-20T04:28:10Z","published":"2022-01-24T09:44:11Z","title":"Consistent 3D Hand Reconstruction in Video via self-supervised Learning","summary":"  We present a method for reconstructing accurate and consistent 3D hands from\na monocular video. We observe that detected 2D hand keypoints and the image\ntexture provide important cues about the geometry and texture of the 3D hand,\nwhich can reduce or even eliminate the requirement on 3D hand annotation. Thus\nwe propose ${\\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model,\nthat can jointly estimate pose, shape, texture, and the camera viewpoint from a\nsingle RGB input through the supervision of easily accessible 2D detected\nkeypoints. We leverage the continuous hand motion information contained in the\nunlabeled video data and propose ${\\rm {S}^{2}HAND(V)}$, which uses a set of\nweights shared ${\\rm {S}^{2}HAND}$ to process each frame and exploits\nadditional motion, texture, and shape consistency constrains to promote more\naccurate hand poses and more consistent shapes and textures. Experiments on\nbenchmark datasets demonstrate that our self-supervised approach produces\ncomparable hand reconstruction performance compared with the recent\nfull-supervised methods in single-frame as input setup, and notably improves\nthe reconstruction accuracy and consistency when using video training data.\n","authors":["Zhigang Tu","Zhisheng Huang","Yujin Chen","Di Kang","Linchao Bao","Bisheng Yang","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2201.09548v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2103.11703"},{"id":"http://arxiv.org/abs/2303.10849v1","updated":"2023-03-20T03:58:03Z","published":"2023-03-20T03:58:03Z","title":"Facial Affective Analysis based on MAE and Multi-modal Information for\n  5th ABAW Competition","summary":"  Human affective behavior analysis focuses on analyzing human expressions or\nother behaviors, which helps improve the understanding of human psychology.\nCVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) makes\ngreat efforts to provide the diversity data for the recognition of the commonly\nused emotion representations, including Action Units~(AU), basic expression\ncategories and Valence-Arousal~(VA). In this paper, we introduce our submission\nto the CVPR 2023: ABAW5 for AU detection, expression classification, VA\nestimation and emotional reaction intensity (ERI) estimation. First of all, we\nintroduce the vision information from an MAE model, which has been pre-trained\non a large-scale face image dataset in a self-supervised manner. Then the MAE\nencoder part is finetuned on the ABAW challenges on the single frame of\nAff-wild2 dataset. We also exploit the multi-modal and temporal information\nfrom the videos and design a transformer-based framework to fusion the\nmulti-modal features. Moreover, we construct a novel two-branch collaboration\ntraining strategy to further enhance the model generalization by randomly\ninterpolating the logits space. The extensive quantitative experiments, as well\nas ablation studies on the Aff-Wild2 dataset and Hume-Reaction dataset prove\nthe effectiveness of our proposed method.\n","authors":["Wei Zhang","Bowen Ma","Feng Qiu","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2303.10849v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.09598v2","updated":"2023-03-20T16:56:39Z","published":"2022-12-19T16:34:19Z","title":"Query-as-context Pre-training for Dense Passage Retrieval","summary":"  Recently, methods have been developed to improve the performance of dense\npassage retrieval by using context-supervised pre-training. These methods\nsimply consider two passages from the same document to be relevant, without\ntaking into account the possibility of weakly correlated pairs. Thus, this\npaper proposes query-as-context pre-training, a simple yet effective\npre-training technique to alleviate the issue. Query-as-context pre-training\nassumes that the query derived from a passage is more likely to be relevant to\nthat passage and forms a passage-query pair. These passage-query pairs are then\nused in contrastive or generative context-supervised pre-training. The\npre-trained models are evaluated on large-scale passage retrieval benchmarks\nand out-of-domain zero-shot benchmarks. Experimental results show that\nquery-as-context pre-training brings considerable gains and meanwhile speeds up\ntraining, demonstrating its effectiveness and efficiency. Our code will be\navailable at https://github.com/caskcsg/ir/tree/main/cotmae-qc .\n","authors":["Xing Wu","Guangyuan Ma","Wanhui Qian","Zijia Lin","Fuzheng Zhang","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2212.09598v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.11009v1","updated":"2023-03-20T10:39:25Z","published":"2023-03-20T10:39:25Z","title":"Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce\n  Search","summary":"  Retrieving relevant items that match users' queries from billion-scale corpus\nforms the core of industrial e-commerce search systems, in which\nembedding-based retrieval (EBR) methods are prevailing. These methods adopt a\ntwo-tower framework to learn embedding vectors for query and item separately\nand thus leverage efficient approximate nearest neighbor (ANN) search to\nretrieve relevant items. However, existing EBR methods usually ignore\ninconsistent user behaviors in industrial multi-stage search systems, resulting\nin insufficient retrieval efficiency with a low commercial return. To tackle\nthis challenge, we propose to improve EBR methods by learning Multi-level\nMulti-Grained Semantic Embeddings(MMSE). We propose the multi-stage information\nmining to exploit the ordered, clicked, unclicked and random sampled items in\npractical user behavior data, and then capture query-item similarity via a\npost-fusion strategy. We then propose multi-grained learning objectives that\nintegrate the retrieval loss with global comparison ability and the ranking\nloss with local comparison ability to generate semantic embeddings. Both\nexperiments on a real-world billion-scale dataset and online A/B tests verify\nthe effectiveness of MMSE in achieving significant performance improvements on\nmetrics such as offline recall and online conversion rate (CVR).\n","authors":["Binbin Wang","Mingming Li","Zhixiong Zeng","Jingwei Zhuo","Songlin Wang","Sulong Xu","Bo Long","Weipeng Yan"],"pdf_url":"https://arxiv.org/pdf/2303.11009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10871v1","updated":"2023-03-20T05:12:43Z","published":"2023-03-20T05:12:43Z","title":"NASA Science Mission Directorate Knowledge Graph Discovery","summary":"  The size of the National Aeronautics and Space Administration (NASA) Science\nMission Directorate (SMD) is growing exponentially, allowing researchers to\nmake discoveries. However, making discoveries is challenging and time-consuming\ndue to the size of the data catalogs, and as many concepts and data are\nindirectly connected. This paper proposes a pipeline to generate knowledge\ngraphs (KGs) representing different NASA SMD domains. These KGs can be used as\nthe basis for dataset search engines, saving researchers time and supporting\nthem in finding new connections. We collected textual data and used several\nmodern natural language processing (NLP) methods to create the nodes and the\nedges of the KGs. We explore the cross-domain connections, discuss our\nchallenges, and provide future directions to inspire researchers working on\nsimilar challenges.\n","authors":["Roelien C. Timmer","Fech Scen Khoo","Megan Mark","Marcella Scoczynski Ribeiro Martins","Anamaria Berea","Gregory Renard","Kaylin Bugbee"],"pdf_url":"https://arxiv.org/pdf/2303.10871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10816v1","updated":"2023-03-20T01:20:02Z","published":"2023-03-20T01:20:02Z","title":"IMF: Interactive Multimodal Fusion Model for Link Prediction","summary":"  Link prediction aims to identify potential missing triples in knowledge\ngraphs. To get better results, some recent studies have introduced multimodal\ninformation to link prediction. However, these methods utilize multimodal\ninformation separately and neglect the complicated interaction between\ndifferent modalities. In this paper, we aim at better modeling the\ninter-modality information and thus introduce a novel Interactive Multimodal\nFusion (IMF) model to integrate knowledge from different modalities. To this\nend, we propose a two-stage multimodal fusion framework to preserve\nmodality-specific knowledge as well as take advantage of the complementarity\nbetween different modalities. Instead of directly projecting different\nmodalities into a unified space, our multimodal fusion module limits the\nrepresentations of different modalities independent while leverages bilinear\npooling for fusion and incorporates contrastive learning as additional\nconstraints. Furthermore, the decision fusion module delivers the learned\nweighted average over the predictions of all modalities to better incorporate\nthe complementarity of different modalities. Our approach has been demonstrated\nto be effective through empirical evaluations on several real-world datasets.\nThe implementation code is available online at\nhttps://github.com/HestiaSky/IMF-Pytorch.\n","authors":["Xinhang Li","Xiangyu Zhao","Jiaxing Xu","Yong Zhang","Chunxiao Xing"],"pdf_url":"https://arxiv.org/pdf/2303.10816v1.pdf","comment":"9 pages, 7 figures, 4 tables, WWW'2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.11330v1","updated":"2023-03-20T17:59:58Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v1.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.11327v1","updated":"2023-03-20T17:59:49Z","published":"2023-03-20T17:59:49Z","title":"3D Concept Learning and Reasoning from Multi-View Images","summary":"  Humans are able to accurately reason in 3D by gathering multi-view\nobservations of the surrounding world. Inspired by this insight, we introduce a\nnew large-scale benchmark for 3D multi-view visual question answering\n(3DMV-VQA). This dataset is collected by an embodied agent actively moving and\ncapturing RGB images in an environment using the Habitat simulator. In total,\nit consists of approximately 5k scenes, 600k images, paired with 50k questions.\nWe evaluate various state-of-the-art models for visual reasoning on our\nbenchmark and find that they all perform poorly. We suggest that a principled\napproach for 3D reasoning from multi-view images should be to infer a compact\n3D representation of the world from the multi-view images, which is further\ngrounded on open-vocabulary semantic concepts, and then to execute reasoning on\nthese 3D representations. As the first step towards this approach, we propose a\nnovel 3D concept learning and reasoning (3D-CLR) framework that seamlessly\ncombines these components via neural fields, 2D pre-trained vision-language\nmodels, and neural reasoning operators. Experimental results suggest that our\nframework outperforms baseline models by a large margin, but the challenge\nremains largely unsolved. We further perform an in-depth analysis of the\nchallenges and highlight potential future directions.\n","authors":["Yining Hong","Chunru Lin","Yilun Du","Zhenfang Chen","Joshua B. Tenenbaum","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.11327v1.pdf","comment":"CVPR 2023. Project page: https://vis-www.cs.umass.edu/3d-clr/"},{"id":"http://arxiv.org/abs/2303.11323v1","updated":"2023-03-20T17:57:15Z","published":"2023-03-20T17:57:15Z","title":"Tangent Bundle Convolutional Learning: from Manifolds to Cellular\n  Sheaves and Back","summary":"  In this work we introduce a convolution operation over the tangent bundle of\nRiemann manifolds in terms of exponentials of the Connection Laplacian\noperator. We define tangent bundle filters and tangent bundle neural networks\n(TNNs) based on this convolution operation, which are novel continuous\narchitectures operating on tangent bundle signals, i.e. vector fields over the\nmanifolds. Tangent bundle filters admit a spectral representation that\ngeneralizes the ones of scalar manifold filters, graph filters and standard\nconvolutional filters in continuous time. We then introduce a discretization\nprocedure, both in the space and time domains, to make TNNs implementable,\nshowing that their discrete counterpart is a novel principled variant of the\nvery recently introduced sheaf neural networks. We formally prove that this\ndiscretized architecture converges to the underlying continuous TNN. Finally,\nwe numerically evaluate the effectiveness of the proposed architecture on\nvarious learning tasks, both on synthetic and real data.\n","authors":["Claudio Battiloro","Zhiyang Wang","Hans Riess","Paolo Di Lorenzo","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2303.11323v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2210.15058"},{"id":"http://arxiv.org/abs/2207.01614v2","updated":"2023-03-20T17:51:09Z","published":"2022-07-04T17:56:14Z","title":"Beyond mAP: Towards better evaluation of instance segmentation","summary":"  Correctness of instance segmentation constitutes counting the number of\nobjects, correctly localizing all predictions and classifying each localized\nprediction. Average Precision is the de-facto metric used to measure all these\nconstituents of segmentation. However, this metric does not penalize duplicate\npredictions in the high-recall range, and cannot distinguish instances that are\nlocalized correctly but categorized incorrectly. This weakness has\ninadvertently led to network designs that achieve significant gains in AP but\nalso introduce a large number of false positives. We therefore cannot rely on\nAP to choose a model that provides an optimal tradeoff between false positives\nand high recall. To resolve this dilemma, we review alternative metrics in the\nliterature and propose two new measures to explicitly measure the amount of\nboth spatial and categorical duplicate predictions. We also propose a Semantic\nSorting and NMS module to remove these duplicates based on a pixel occupancy\nmatching scheme. Experiments show that modern segmentation networks have\nsignificant gains in AP, but also contain a considerable amount of duplicates.\nOur Semantic Sorting and NMS can be added as a plug-and-play module to mitigate\nhedged predictions and preserve AP.\n","authors":["Rohit Jena","Lukas Zhornyak","Nehal Doiphode","Pratik Chaudhari","Vivek Buch","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2207.01614v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11306v1","updated":"2023-03-20T17:45:08Z","published":"2023-03-20T17:45:08Z","title":"Localizing Object-level Shape Variations with Text-to-Image Diffusion\n  Models","summary":"  Text-to-image models give rise to workflows which often begin with an\nexploration step, where users sift through a large collection of generated\nimages. The global nature of the text-to-image generation process prevents\nusers from narrowing their exploration to a particular object in the image. In\nthis paper, we present a technique to generate a collection of images that\ndepicts variations in the shape of a specific object, enabling an object-level\nshape exploration process. Creating plausible variations is challenging as it\nrequires control over the shape of the generated object while respecting its\nsemantics. A particular challenge when generating object variations is\naccurately localizing the manipulation applied over the object's shape. We\nintroduce a prompt-mixing technique that switches between prompts along the\ndenoising process to attain a variety of shape choices. To localize the\nimage-space operation, we present two techniques that use the self-attention\nlayers in conjunction with the cross-attention layers. Moreover, we show that\nthese localization techniques are general and effective beyond the scope of\ngenerating object variations. Extensive results and comparisons demonstrate the\neffectiveness of our method in generating object variations, and the competence\nof our localization techniques.\n","authors":["Or Patashnik","Daniel Garibi","Idan Azuri","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.11306v1.pdf","comment":"Project page at https://orpatashnik.github.io/local-prompt-mixing/"},{"id":"http://arxiv.org/abs/2201.04207v4","updated":"2023-03-20T17:32:54Z","published":"2022-01-11T21:31:18Z","title":"Fighting Money Laundering with Statistics and Machine Learning: An\n  Introduction and Review","summary":"  Money laundering is a profound global problem. Nonetheless, there is little\nscientific literature on statistical and machine learning methods for\nanti-money laundering. In this paper, we focus on anti-money laundering in\nbanks and provide an introduction and review of the literature. We propose a\nunifying terminology with two central elements: (i) client risk profiling and\n(ii) suspicious behavior flagging. We find that client risk profiling is\ncharacterized by diagnostics, i.e., efforts to find and explain risk factors.\nOn the other hand, suspicious behavior flagging is characterized by\nnon-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the need for more public\ndata sets. This may potentially be addressed by synthetic data generation.\nOther possible research directions include semi-supervised and deep learning,\ninterpretability, and fairness of the results.\n","authors":["Rasmus Jensen","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2201.04207v4.pdf","comment":"Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,\n  doi:10.1109/ACCESS.2023.3239549"},{"id":"http://arxiv.org/abs/2208.09099v3","updated":"2023-03-20T17:24:38Z","published":"2022-08-19T00:18:19Z","title":"Scalable Multi-Agent Lab Framework for Lab Optimization","summary":"  Autonomous materials research systems allow scientists to fail smarter, learn\nfaster, and spend less resources in their studies. As these systems grow in\nnumber, capability, and complexity, a new challenge arises - how will they work\ntogether across large facilities? We explore one solution to this question - a\nmulti-agent laboratory control frame-work. We demonstrate this framework with\nan autonomous material science lab in mind - where information from diverse\nresearch campaigns can be combined to ad-dress the scientific question at hand.\nThis framework can 1) account for realistic resource limits such as equipment\nuse, 2) allow for machine learning agents with diverse learning capabilities\nand goals capable of running re-search campaigns, and 3) facilitate multi-agent\ncollaborations and teams. The framework is dubbed the MULTI-agent auTonomous\nfAcilities - a Scalable frameworK aka MULTITASK. MULTITASK makes possible\nfacility-wide simulations, including agent-instrument and agent-agent\ninteractions. Through MULTITASK's modularity, real-world facilities can come\non-line in phases, with simulated instruments gradually replaced by real-world\ninstruments. We hope MULTITASK opens new areas of study in large-scale\nautonomous and semi-autonomous research campaigns and facilities.\n","authors":["A. Gilad Kusne","Austin McDannald"],"pdf_url":"https://arxiv.org/pdf/2208.09099v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11288v1","updated":"2023-03-20T17:23:15Z","published":"2023-03-20T17:23:15Z","title":"Rethinking SO(3)-equivariance with Bilinear Tensor Networks","summary":"  Many datasets in scientific and engineering applications are comprised of\nobjects which have specific geometric structure. A common example is data which\ninhabits a representation of the group SO$(3)$ of 3D rotations: scalars,\nvectors, tensors, \\textit{etc}. One way for a neural network to exploit prior\nknowledge of this structure is to enforce SO$(3)$-equivariance throughout its\nlayers, and several such architectures have been proposed. While general\nmethods for handling arbitrary SO$(3)$ representations exist, they\ncomputationally intensive and complicated to implement. We show that by\njudicious symmetry breaking, we can efficiently increase the expressiveness of\na network operating only on vector and order-2 tensor representations of\nSO$(2)$. We demonstrate the method on an important problem from High Energy\nPhysics known as \\textit{b-tagging}, where particle jets originating from\nb-meson decays must be discriminated from an overwhelming QCD background. In\nthis task, we find that augmenting a standard architecture with our method\nresults in a \\ensuremath{2.3\\times} improvement in rejection score.\n","authors":["Chase Shimmin","Zhelun Li","Ema Smith"],"pdf_url":"https://arxiv.org/pdf/2303.11288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11241v4","updated":"2023-03-20T17:16:33Z","published":"2022-06-22T17:42:24Z","title":"Concentration inequalities and optimal number of layers for stochastic\n  deep neural networks","summary":"  We state concentration inequalities for the output of the hidden layers of a\nstochastic deep neural network (SDNN), as well as for the output of the whole\nSDNN. These results allow us to introduce an expected classifier (EC), and to\ngive probabilistic upper bound for the classification error of the EC. We also\nstate the optimal number of layers for the SDNN via an optimal stopping\nprocedure. We apply our analysis to a stochastic version of a feedforward\nneural network with ReLU activation function.\n","authors":["Michele Caprio","Sayan Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2206.11241v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11278v1","updated":"2023-03-20T17:13:50Z","published":"2023-03-20T17:13:50Z","title":"Constructing Bayesian Pseudo-Coresets using Contrastive Divergence","summary":"  Bayesian Pseudo-Coreset (BPC) and Dataset Condensation are two parallel\nstreams of work that construct a synthetic set such that, a model trained\nindependently on this synthetic set, yields the same performance as training on\nthe original training set. While dataset condensation methods use non-bayesian,\nheuristic ways to construct such a synthetic set, BPC methods take a bayesian\napproach and formulate the problem as divergence minimization between\nposteriors associated with original data and synthetic data. However, BPC\nmethods generally rely on distributional assumptions on these posteriors which\nmakes them less flexible and hinders their performance. In this work, we\npropose to solve these issues by modeling the posterior associated with\nsynthetic data by an energy-based distribution. We derive a\ncontrastive-divergence-like loss function to learn the synthetic set and show a\nsimple and efficient way to estimate this loss. Further, we perform rigorous\nexperiments pertaining to the proposed method. Our experiments on multiple\ndatasets show that the proposed method not only outperforms previous BPC\nmethods but also gives performance comparable to dataset condensation\ncounterparts.\n","authors":["Piyush Tiwary","Kumar Shubham","Vivek Kashyap","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2303.11278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11277v1","updated":"2023-03-20T17:12:42Z","published":"2023-03-20T17:12:42Z","title":"Model Stitching: Looking For Functional Similarity Between\n  Representations","summary":"  Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare\ndifferent neural network representations, because it allows us to measure to\nwhat degree they may be interchanged. We expand on a previous work from Bansal,\nNakkiran & Barak which used model stitching to compare representations of the\nsame shapes learned by differently seeded and/or trained neural networks of the\nsame architecture. Our contribution enables us to compare the representations\nlearned by layers with different shapes from neural networks with different\narchitectures. We subsequently reveal unexpected behavior of model stitching.\nNamely, we find that stitching, based on convolutions, for small ResNets, can\nreach high accuracy if those layers come later in the first (sender) network\nthan in the second (receiver), even if those layers are far apart.\n","authors":["Adriano Hernandez","Rumen Dangovski","Peter Y. Lu","Marin Soljacic"],"pdf_url":"https://arxiv.org/pdf/2303.11277v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2209.09419v4","updated":"2023-03-20T17:06:12Z","published":"2022-09-20T02:31:42Z","title":"Multi-armed Bandit Learning on a Graph","summary":"  The multi-armed bandit(MAB) problem is a simple yet powerful framework that\nhas been extensively studied in the context of decision-making under\nuncertainty. In many real-world applications, such as robotic applications,\nselecting an arm corresponds to a physical action that constrains the choices\nof the next available arms (actions). Motivated by this, we study an extension\nof MAB called the graph bandit, where an agent travels over a graph to maximize\nthe reward collected from different nodes. The graph defines the agent's\nfreedom in selecting the next available nodes at each step. We assume the graph\nstructure is fully available, but the reward distributions are unknown. Built\nupon an offline graph-based planning algorithm and the principle of optimism,\nwe design a learning algorithm, G-UCB, that balances long-term\nexploration-exploitation using the principle of optimism. We show that our\nproposed algorithm achieves $O(\\sqrt{|S|T\\log(T)}+D|S|\\log T)$ learning regret,\nwhere $|S|$ is the number of nodes and $D$ is the diameter of the graph, which\nmatches the theoretical lower bound $\\Omega(\\sqrt{|S|T})$ up to logarithmic\nfactors. To our knowledge, this result is among the first tight regret bounds\nin non-episodic, un-discounted learning problems with known deterministic\ntransitions. Numerical experiments confirm that our algorithm outperforms\nseveral benchmarks.\n","authors":["Tianpeng Zhang","Kasper Johansson","Na Li"],"pdf_url":"https://arxiv.org/pdf/2209.09419v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11272v1","updated":"2023-03-20T17:04:59Z","published":"2023-03-20T17:04:59Z","title":"Agent-based Simulation for Online Mental Health Matching","summary":"  Online mental health communities (OMHCs) are an effective and accessible\nchannel to give and receive social support for individuals with mental and\nemotional issues. However, a key challenge on these platforms is finding\nsuitable partners to interact with given that mechanisms to match users are\ncurrently underdeveloped. In this paper, we collaborate with one of the world's\nlargest OMHC to develop an agent-based simulation framework and explore the\ntrade-offs in different matching algorithms. The simulation framework allows us\nto compare current mechanisms and new algorithmic matching policies on the\nplatform, and observe their differing effects on a variety of outcome metrics.\nOur findings include that usage of the deferred-acceptance algorithm can\nsignificantly better the experiences of support-seekers in one-on-one chats\nwhile maintaining low waiting time. We note key design considerations that\nagent-based modeling reveals in the OMHC context, including the potential\nbenefits of algorithmic matching on marginalized communities.\n","authors":["Yuhan Liu","Anna Fang","Glen Moriarty","Robert Kraut","Haiyi Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11265v1","updated":"2023-03-20T16:49:40Z","published":"2023-03-20T16:49:40Z","title":"Convergence Guarantees of Overparametrized Wide Deep Inverse Prior","summary":"  Neural networks have become a prominent approach to solve inverse problems in\nrecent years. Amongst the different existing methods, the Deep Image/Inverse\nPriors (DIPs) technique is an unsupervised approach that optimizes a highly\noverparametrized neural network to transform a random input into an object\nwhose image under the forward model matches the observation. However, the level\nof overparametrization necessary for such methods remains an open problem. In\nthis work, we aim to investigate this question for a two-layers neural network\nwith a smooth activation function. We provide overparametrization bounds under\nwhich such network trained via continuous-time gradient descent will converge\nexponentially fast with high probability which allows to derive recovery\nprediction bounds. This work is thus a first step towards a theoretical\nunderstanding of overparametrized DIP networks, and more broadly it\nparticipates to the theoretical understanding of neural networks in inverse\nproblem settings.\n","authors":["Nathan Buskulic","Yvain Quéau","Jalal Fadili"],"pdf_url":"https://arxiv.org/pdf/2303.11265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11257v1","updated":"2023-03-20T16:42:25Z","published":"2023-03-20T16:42:25Z","title":"Unit Scaling: Out-of-the-Box Low-Precision Training","summary":"  We present unit scaling, a paradigm for designing deep learning models that\nsimplifies the use of low-precision number formats. Training in FP16 or the\nrecently proposed FP8 formats offers substantial efficiency gains, but can lack\nsufficient range for out-of-the-box training. Unit scaling addresses this by\nintroducing a principled approach to model numerics: seeking unit variance of\nall weights, activations and gradients at initialisation. Unlike alternative\nmethods, this approach neither requires multiple training runs to find a\nsuitable scale nor has significant computational overhead. We demonstrate the\nefficacy of unit scaling across a range of models and optimisers. We further\nshow that existing models can be adapted to be unit-scaled, training BERT-Large\nin FP16 and then FP8 with no degradation in accuracy.\n","authors":["Charlie Blake","Douglas Orr","Carlo Luschi"],"pdf_url":"https://arxiv.org/pdf/2303.11257v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.11249v1","updated":"2023-03-20T16:34:39Z","published":"2023-03-20T16:34:39Z","title":"What Makes Data Suitable for a Locally Connected Neural Network? A\n  Necessary and Sufficient Condition Based on Quantum Entanglement","summary":"  The question of what makes a data distribution suitable for deep learning is\na fundamental open problem. Focusing on locally connected neural networks (a\nprevalent family of architectures that includes convolutional and recurrent\nneural networks as well as local self-attention models), we address this\nproblem by adopting theoretical tools from quantum physics. Our main\ntheoretical result states that a certain locally connected neural network is\ncapable of accurate prediction over a data distribution if and only if the data\ndistribution admits low quantum entanglement under certain canonical partitions\nof features. As a practical application of this result, we derive a\npreprocessing method for enhancing the suitability of a data distribution to\nlocally connected neural networks. Experiments with widespread models over\nvarious datasets demonstrate our findings. We hope that our use of quantum\nentanglement will encourage further adoption of tools from physics for formally\nreasoning about the relation between deep learning and real-world data.\n","authors":["Yotam Alexander","Nimrod De La Vega","Noam Razin","Nadav Cohen"],"pdf_url":"https://arxiv.org/pdf/2303.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11247v1","updated":"2023-03-20T16:33:17Z","published":"2023-03-20T16:33:17Z","title":"Memorization Capacity of Neural Networks with Conditional Computation","summary":"  Many empirical studies have demonstrated the performance benefits of\nconditional computation in neural networks, including reduced inference time\nand power consumption. We study the fundamental limits of neural conditional\ncomputation from the perspective of memorization capacity. For Rectified Linear\nUnit (ReLU) networks without conditional computation, it is known that\nmemorizing a collection of $n$ input-output relationships can be accomplished\nvia a neural network with $O(\\sqrt{n})$ neurons. Calculating the output of this\nneural network can be accomplished using $O(\\sqrt{n})$ elementary arithmetic\noperations of additions, multiplications and comparisons for each input. Using\na conditional ReLU network, we show that the same task can be accomplished\nusing only $O(\\log n)$ operations per input. This represents an almost\nexponential improvement as compared to networks without conditional\ncomputation. We also show that the $\\Theta(\\log n)$ rate is the best possible.\nOur achievability result utilizes a general methodology to synthesize a\nconditional network out of an unconditional network in a\ncomputationally-efficient manner, bridging the gap between unconditional and\nconditional architectures.\n","authors":["Erdem Koyuncu"],"pdf_url":"https://arxiv.org/pdf/2303.11247v1.pdf","comment":"To be presented at International Conference on Learning\n  Representations (ICLR), 2023"},{"id":"http://arxiv.org/abs/2303.11242v1","updated":"2023-03-20T16:27:36Z","published":"2023-03-20T16:27:36Z","title":"Make Landscape Flatter in Differentially Private Federated Learning","summary":"  To defend the inference attacks and mitigate the sensitive information\nleakages in Federated Learning (FL), client-level Differentially Private FL\n(DPFL) is the de-facto standard for privacy protection by clipping local\nupdates and adding random noise. However, existing DPFL methods tend to make a\nsharper loss landscape and have poorer weight perturbation robustness,\nresulting in severe performance degradation. To alleviate these issues, we\npropose a novel DPFL algorithm named DP-FedSAM, which leverages gradient\nperturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM\nintegrates Sharpness Aware Minimization (SAM) optimizer to generate local\nflatness models with better stability and weight perturbation robustness, which\nresults in the small norm of local updates and robustness to DP noise, thereby\nimproving the performance. From the theoretical perspective, we analyze in\ndetail how DP-FedSAM mitigates the performance degradation induced by DP.\nMeanwhile, we give rigorous privacy guarantees with R\\'enyi DP and present the\nsensitivity analysis of local updates. At last, we empirically confirm that our\nalgorithm achieves state-of-the-art (SOTA) performance compared with existing\nSOTA baselines in DPFL.\n","authors":["Yifan Shi","Yingqi Liu","Kang Wei","Li Shen","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11242v1.pdf","comment":"CVPR2023, 18 pages"},{"id":"http://arxiv.org/abs/2303.11239v1","updated":"2023-03-20T16:24:06Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Koethe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v1.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2110.02273v2","updated":"2023-03-20T16:18:27Z","published":"2021-10-05T18:26:22Z","title":"Bilevel Imaging Learning Problems as Mathematical Programs with\n  Complementarity Constraints: Reformulation and Theory","summary":"  We investigate a family of bilevel imaging learning problems where the\nlower-level instance corresponds to a convex variational model involving first-\nand second-order nonsmooth sparsity-based regularizers. By using geometric\nproperties of the primal-dual reformulation of the lower-level problem and\nintroducing suitable auxiliar variables, we are able to reformulate the\noriginal bilevel problems as Mathematical Programs with Complementarity\nConstraints (MPCC). For the latter, we prove tight constraint qualification\nconditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and\nStrong (S-) stationarity conditions. The stationarity systems for the MPCC turn\nalso into stationarity conditions for the original formulation. Second-order\nsufficient optimality conditions are derived as well, together with a local\nuniqueness result for stationary points. The proposed reformulation may be\nextended to problems in function spaces, leading to MPCC's with constraints on\nthe gradient of the state. The MPCC reformulation also leads to the efficient\nuse of available large-scale nonlinear programming solvers, as shown in a\ncompanion paper, where different imaging applications are studied.\n","authors":["Juan Carlos De los Reyes"],"pdf_url":"https://arxiv.org/pdf/2110.02273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02642v2","updated":"2023-03-20T16:08:30Z","published":"2022-11-01T14:12:58Z","title":"A Meta-GNN approach to personalized seizure detection and classification","summary":"  In this paper, we propose a personalized seizure detection and classification\nframework that quickly adapts to a specific patient from limited seizure\nsamples. We achieve this by combining two novel paradigms that have recently\nseen much success in a wide variety of real-world applications: graph neural\nnetworks (GNN), and meta-learning. We train a Meta-GNN based classifier that\nlearns a global model from a set of training patients such that this global\nmodel can eventually be adapted to a new unseen patient using very limited\nsamples. We apply our approach on the TUSZ-dataset, one of the largest and\npublicly available benchmark datasets for epilepsy. We show that our method\noutperforms the baselines by reaching 82.7% on accuracy and 82.08% on F1 score\nafter only 20 iterations on new unseen patients.\n","authors":["Abdellah Rahmani","Arun Venkitaraman","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2211.02642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11224v1","updated":"2023-03-20T16:00:20Z","published":"2023-03-20T16:00:20Z","title":"Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis","summary":"  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n","authors":["Tobias Weber","Michael Ingrisch","Bernd Bischl","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2303.11224v1.pdf","comment":"accepted at PAKDD 2023"},{"id":"http://arxiv.org/abs/2302.07419v3","updated":"2023-03-20T16:00:11Z","published":"2023-02-15T01:09:03Z","title":"Spatially heterogeneous learning by a deep student machine","summary":"  Despite the spectacular successes, deep neural networks (DNN) with a huge\nnumber of adjustable parameters remain largely black boxes. To shed light on\nthe hidden layers of DNN, we study supervised learning by a DNN of width $N$\nand depth $L$ consisting of perceptrons with $c$ inputs by a statistical\nmechanics approach called the teacher-student setting. We consider an ensemble\nof student machines that exactly reproduce $M$ sets of $N$ dimensional\ninput/output relations provided by a teacher machine. We analyze the ensemble\ntheoretically using a replica method (H. Yoshino (2020)) and numerically\nperforming greedy Monte Carlo simulations. The replica theory which works on\nhigh dimensional data $N \\gg 1$ becomes exact in 'dense limit' $N \\gg c \\gg 1$\nand $M \\gg 1$ with fixed $\\alpha=M/c$. Both the theory and the simulation\nsuggest learning by the DNN is quite heterogeneous in the network space:\nconfigurations of the machines are more correlated within the layers closer to\nthe input/output boundaries while the central region remains much less\ncorrelated due to over-parametrization. Deep enough systems relax faster thanks\nto the less correlated central region. Remarkably both the theory and\nsimulation suggest generalization-ability of the student machines does not\nvanish even in the deep limit $L \\gg 1$ where the system becomes strongly\nover-parametrized. We also consider the impact of effective dimension $D(\\leq\nN)$ of data by incorporating the hidden manifold model (S. Goldt et al (2020))\ninto our model. The replica theory implies that the loop corrections to the\ndense limit, which reflect correlations between different nodes in the network,\nbecome enhanced by either decreasing the width $\\ N$ or decreasing the\neffective dimension $D$ of the data. Simulation suggests both leads to\nsignificant improvements in generalization-ability.\n","authors":["Hajime Yoshino"],"pdf_url":"https://arxiv.org/pdf/2302.07419v3.pdf","comment":"34 page, 18 figures (revised version with normalized squared\n  overlaps)"},{"id":"http://arxiv.org/abs/2303.11217v1","updated":"2023-03-20T15:49:18Z","published":"2023-03-20T15:49:18Z","title":"Inverse problem regularization with hierarchical variational\n  autoencoders","summary":"  In this paper, we propose to regularize ill-posed inverse problems using a\ndeep hierarchical variational autoencoder (HVAE) as an image prior. The\nproposed method synthesizes the advantages of i) denoiser-based Plug \\& Play\napproaches and ii) generative model based approaches to inverse problems.\nFirst, we exploit VAE properties to design an efficient algorithm that benefits\nfrom convergence guarantees of Plug-and-Play (PnP) methods. Second, our\napproach is not restricted to specialized datasets and the proposed PnP-HVAE\nmodel is able to solve image restoration problems on natural images of any\nsize. Our experiments show that the proposed PnP-HVAE method is competitive\nwith both SOTA denoiser-based PnP approaches, and other SOTA restoration\nmethods based on generative models.\n","authors":["Jean Prost","Antoine Houdard","Andrés Almansa","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2303.11217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.00413v4","updated":"2023-03-20T15:48:12Z","published":"2021-08-01T09:42:29Z","title":"Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic\n  Transport Coefficients","summary":"  Finding extended hydrodynamics equations valid from the dense gas region to\nthe rarefied gas region remains a great challenge. The key to success is to\nobtain accurate constitutive relations for stress and heat flux. Data-driven\nmodels offer a new phenomenological approach to learning constitutive relations\nfrom data. Such models enable complex constitutive relations that extend\nNewton's law of viscosity and Fourier's law of heat conduction by regression on\nhigher derivatives. However, the choices of derivatives in these models are\nad-hoc without a clear physical explanation. We investigated data-driven models\ntheoretically on a linear system. We argue that these models are equivalent to\nnon-linear length scale scaling laws of transport coefficients. The equivalence\nto scaling laws justified the physical plausibility and revealed the limitation\nof data-driven models. Our argument also points out that modeling the scaling\nlaw could avoid practical difficulties in data-driven models like derivative\nestimation and variable selection on noisy data. We further proposed a\nconstitutive relation model based on scaling law and tested it on the\ncalculation of Rayleigh scattering spectra. The result shows our data-driven\nmodel has a clear advantage over the Chapman-Enskog expansion and moment\nmethods.\n","authors":["Candi Zheng","Yang Wang","Shiyi Chen"],"pdf_url":"https://arxiv.org/pdf/2108.00413v4.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11215v1","updated":"2023-03-20T15:47:05Z","published":"2023-03-20T15:47:05Z","title":"Learning to Generate 3D Representations of Building Roofs Using\n  Single-View Aerial Imagery","summary":"  We present a novel pipeline for learning the conditional distribution of a\nbuilding roof mesh given pixels from an aerial image, under the assumption that\nroof geometry follows a set of regular patterns. Unlike alternative methods\nthat require multiple images of the same object, our approach enables\nestimating 3D roof meshes using only a single image for predictions. The\napproach employs the PolyGen, a deep generative transformer architecture for 3D\nmeshes. We apply this model in a new domain and investigate the sensitivity of\nthe image resolution. We propose a novel metric to evaluate the performance of\nthe inferred meshes, and our results show that the model is robust even at\nlower resolutions, while qualitatively producing realistic representations for\nout-of-distribution samples.\n","authors":["Maxim Khomiakov","Alejandro Valverde Mahou","Alba Reinders Sánchez","Jes Frellsen","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2303.11215v1.pdf","comment":"Copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11207v1","updated":"2023-03-20T15:40:28Z","published":"2023-03-20T15:40:28Z","title":"Investigating Topological Order using Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs), originally developed for natural language\nprocessing, hold great promise for accurately describing strongly correlated\nquantum many-body systems. Here, we employ 2D RNNs to investigate two\nprototypical quantum many-body Hamiltonians exhibiting topological order.\nSpecifically, we demonstrate that RNN wave functions can effectively capture\nthe topological order of the toric code and a Bose-Hubbard spin liquid on the\nkagome lattice by estimating their topological entanglement entropies. We also\nfind that RNNs favor coherent superpositions of minimally-entangled states over\nminimally-entangled states themselves. Overall, our findings demonstrate that\nRNN wave functions constitute a powerful tool to study phases of matter beyond\nLandau's symmetry-breaking paradigm.\n","authors":["Mohamed Hibat-Allah","Roger G. Melko","Juan Carrasquilla"],"pdf_url":"https://arxiv.org/pdf/2303.11207v1.pdf","comment":"14 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2302.10681v2","updated":"2023-03-20T15:40:26Z","published":"2023-02-21T14:03:22Z","title":"FrankenSplit: Saliency Guided Neural Feature Compression with Shallow\n  Variational Bottleneck Injection","summary":"  The rise of mobile AI accelerators allows latency-sensitive applications to\nexecute lightweight Deep Neural Networks (DNNs) on the client side. However,\ncritical applications require powerful models that edge devices cannot host and\nmust therefore offload requests, where the high-dimensional data will compete\nfor limited bandwidth. This work proposes shifting away from focusing on\nexecuting shallow layers of partitioned DNNs. Instead, it advocates\nconcentrating the local resources on variational compression optimized for\nmachine interpretability. We introduce a novel framework for resource-conscious\ncompression models and extensively evaluate our method in an environment\nreflecting the asymmetric resource distribution between edge devices and\nservers. Our method achieves 60\\% lower bitrate than a state-of-the-art SC\nmethod without decreasing accuracy and is up to 16x faster than offloading with\nexisting codec standards.\n","authors":["Alireza Furutanpey","Philipp Raith","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2302.10681v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.01498v3","updated":"2023-03-20T15:25:09Z","published":"2023-03-02T18:58:15Z","title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit\n  Detection & Emotional Reaction Intensity Estimation Challenges","summary":"  The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part\nof the respective ABAW Workshop which will be held in conjunction with IEEE\nComputer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW\nCompetition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR\n2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at\nautomatically analyzing affect. For this year's Competition, we feature two\ncorpora: i) an extended version of the Aff-Wild2 database and ii) the\nHume-Reaction dataset. The former database is an audiovisual one of around 600\nvideos of around 3M frames and is annotated with respect to:a) two continuous\naffect dimensions -valence (how positive/negative a person is) and arousal (how\nactive/passive a person is)-; b) basic expressions (e.g. happiness, sadness,\nneutral state); and c) atomic facial muscle actions (i.e., action units). The\nlatter dataset is an audiovisual one in which reactions of individuals to\nemotional stimuli have been annotated with respect to seven emotional\nexpression intensities. Thus the 5th ABAW Competition encompasses four\nChallenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression\nClassification, iii) uni-task Action Unit Detection, and iv) Emotional Reaction\nIntensity Estimation. In this paper, we present these Challenges, along with\ntheir corpora, we outline the evaluation metrics, we present the baseline\nsystems and illustrate their obtained performance.\n","authors":["Dimitrios Kollias","Panagiotis Tzirakis","Alice Baird","Alan Cowen","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2303.01498v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.10659"},{"id":"http://arxiv.org/abs/2105.11977v3","updated":"2023-03-20T15:24:01Z","published":"2021-05-25T14:28:58Z","title":"Towards Teachable Autotelic Agents","summary":"  Autonomous discovery and direct instruction are two distinct sources of\nlearning in children but education sciences demonstrate that mixed approaches\nsuch as assisted discovery or guided play result in improved skill acquisition.\nIn the field of Artificial Intelligence, these extremes respectively map to\nautonomous agents learning from their own signals and interactive learning\nagents fully taught by their teachers. In between should stand teachable\nautotelic agents (TAA): agents that learn from both internal and teaching\nsignals to benefit from the higher efficiency of assisted discovery. Designing\nsuch agents will enable real-world non-expert users to orient the learning\ntrajectories of agents towards their expectations. More fundamentally, this may\nalso be a key step to build agents with human-level intelligence. This paper\npresents a roadmap towards the design of teachable autonomous agents. Building\non developmental psychology and education sciences, we start by identifying key\nfeatures enabling assisted discovery processes in child-tutor interactions.\nThis leads to the production of a checklist of features that future TAA will\nneed to demonstrate. The checklist allows us to precisely pinpoint the various\nlimitations of current reinforcement learning agents and to identify the\npromising first steps towards TAA. It also shows the way forward by\nhighlighting key research directions towards the design or autonomous agents\nthat can be taught by ordinary people via natural pedagogy.\n","authors":["Olivier Sigaud","Ahmed Akakzia","Hugo Caselles-Dupré","Cédric Colas","Pierre-Yves Oudeyer","Mohamed Chetouani"],"pdf_url":"https://arxiv.org/pdf/2105.11977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12053v3","updated":"2023-03-20T15:22:59Z","published":"2022-12-22T22:05:16Z","title":"On Calibrating Semantic Segmentation Models: Analyses and An Algorithm","summary":"  We study the problem of semantic segmentation calibration. Lots of solutions\nhave been proposed to approach model miscalibration of confidence in image\nclassification. However, to date, confidence calibration research on semantic\nsegmentation is still limited. We provide a systematic study on the calibration\nof semantic segmentation models and propose a simple yet effective approach.\nFirst, we find that model capacity, crop size, multi-scale testing, and\nprediction correctness have impact on calibration. Among them, prediction\ncorrectness, especially misprediction, is more important to miscalibration due\nto over-confidence. Next, we propose a simple, unifying, and effective\napproach, namely selective scaling, by separating correct/incorrect prediction\nfor scaling and more focusing on misprediction logit smoothing. Then, we study\npopular existing calibration methods and compare them with selective scaling on\nsemantic segmentation calibration. We conduct extensive experiments with a\nvariety of benchmarks on both in-domain and domain-shift calibration, and show\nthat selective scaling consistently outperforms other methods.\n","authors":["Dongdong Wang","Boqing Gong","Liqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.12053v3.pdf","comment":"Accepted to CVPR2023 (8 pages, 4 figures)"},{"id":"http://arxiv.org/abs/2303.11191v1","updated":"2023-03-20T15:22:10Z","published":"2023-03-20T15:22:10Z","title":"A Survey of Demonstration Learning","summary":"  With the fast improvement of machine learning, reinforcement learning (RL)\nhas been used to automate human tasks in different areas. However, training\nsuch agents is difficult and restricted to expert users. Moreover, it is mostly\nlimited to simulation environments due to the high cost and safety concerns of\ninteractions in the real world. Demonstration Learning is a paradigm in which\nan agent learns to perform a task by imitating the behavior of an expert shown\nin demonstrations. It is a relatively recent area in machine learning, but it\nis gaining significant traction due to having tremendous potential for learning\ncomplex behaviors from demonstrations. Learning from demonstration accelerates\nthe learning process by improving sample efficiency, while also reducing the\neffort of the programmer. Due to learning without interacting with the\nenvironment, demonstration learning would allow the automation of a wide range\nof real world applications such as robotics and healthcare. This paper provides\na survey of demonstration learning, where we formally introduce the\ndemonstration problem along with its main challenges and provide a\ncomprehensive overview of the process of learning from demonstrations from the\ncreation of the demonstration data set, to learning methods from\ndemonstrations, and optimization by combining demonstration learning with\ndifferent machine learning methods. We also review the existing benchmarks and\nidentify their strengths and limitations. Additionally, we discuss the\nadvantages and disadvantages of the paradigm as well as its main applications.\nLastly, we discuss our perspective on open problems and research directions for\nthis rapidly growing field.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2303.11191v1.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11187v1","updated":"2023-03-20T15:17:31Z","published":"2023-03-20T15:17:31Z","title":"A Unified Framework of Policy Learning for Contextual Bandit with\n  Confounding Bias and Missing Observations","summary":"  We study the offline contextual bandit problem, where we aim to acquire an\noptimal policy using observational data. However, this data usually contains\ntwo deficiencies: (i) some variables that confound actions are not observed,\nand (ii) missing observations exist in the collected data. Unobserved\nconfounders lead to a confounding bias and missing observations cause bias and\ninefficiency problems. To overcome these challenges and learn the optimal\npolicy from the observed dataset, we present a new algorithm called\nCausal-Adjusted Pessimistic (CAP) policy learning, which forms the reward\nfunction as the solution of an integral equation system, builds a confidence\nset, and greedily takes action with pessimism. With mild assumptions on the\ndata, we develop an upper bound to the suboptimality of CAP for the offline\ncontextual bandit problem.\n","authors":["Siyu Chen","Yitan Wang","Zhaoran Wang","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11187v1.pdf","comment":"76 page, 5 figures"},{"id":"http://arxiv.org/abs/2303.11183v1","updated":"2023-03-20T15:10:41Z","published":"2023-03-20T15:10:41Z","title":"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning","summary":"  The goal of data-free meta-learning is to learn useful prior knowledge from a\ncollection of pre-trained models without accessing their training data.\nHowever, existing works only solve the problem in parameter space, which (i)\nignore the fruitful data knowledge contained in the pre-trained models; (ii)\ncan not scale to large-scale pre-trained models; (iii) can only meta-learn\npre-trained models with the same network architecture. To address those issues,\nwe propose a unified framework, dubbed PURER, which contains: (1) ePisode\ncUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion\ncalibRation following inner loop (ICFIL) during meta testing. During meta\ntraining, we propose ECI to perform pseudo episode training for learning to\nadapt fast to new unseen tasks. Specifically, we progressively synthesize a\nsequence of pseudo episodes by distilling the training data from each\npre-trained model. The ECI adaptively increases the difficulty level of pseudo\nepisodes according to the real-time feedback of the meta model. We formulate\nthe optimization process of meta training with ECI as an adversarial form in an\nend-to-end manner. During meta testing, we further propose a simple\nplug-and-play supplement-ICFIL-only used during meta testing to narrow the gap\nbetween meta training and meta testing task distribution. Extensive experiments\nin various real-world scenarios show the superior performance of ours.\n","authors":["Zixuan Hu","Li Shen","Zhenyi Wang","Tongliang Liu","Chun Yuan","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11771v2","updated":"2023-03-20T15:05:48Z","published":"2022-12-22T15:06:24Z","title":"Few-shot human motion prediction for heterogeneous sensors","summary":"  Human motion prediction is a complex task as it involves forecasting\nvariables over time on a graph of connected sensors. This is especially true in\nthe case of few-shot learning, where we strive to forecast motion sequences for\npreviously unseen actions based on only a few examples. Despite this, almost\nall related approaches for few-shot motion prediction do not incorporate the\nunderlying graph, while it is a common component in classical motion\nprediction. Furthermore, state-of-the-art methods for few-shot motion\nprediction are restricted to motion tasks with a fixed output space meaning\nthese tasks are all limited to the same sensor graph. In this work, we propose\nto extend recent works on few-shot time-series forecasting with heterogeneous\nattributes with graph neural networks to introduce the first few-shot motion\napproach that explicitly incorporates the spatial graph while also generalizing\nacross motion tasks with heterogeneous sensors. In our experiments on motion\ntasks with heterogeneous sensors, we demonstrate significant performance\nimprovements with lifts from 10.4% up to 39.3% compared to best\nstate-of-the-art models. Moreover, we show that our model can perform on par\nwith the best approach so far when evaluating on tasks with a fixed output\nspace while maintaining two magnitudes fewer parameters.\n","authors":["Rafael Rego Drumond","Lukas Brinkmeyer","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2212.11771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11177v1","updated":"2023-03-20T15:00:52Z","published":"2023-03-20T15:00:52Z","title":"Integration of Radiomics and Tumor Biomarkers in Interpretable Machine\n  Learning Models","summary":"  Despite the unprecedented performance of deep neural networks (DNNs) in\ncomputer vision, their practical application in the diagnosis and prognosis of\ncancer using medical imaging has been limited. One of the critical challenges\nfor integrating diagnostic DNNs into radiological and oncological applications\nis their lack of interpretability, preventing clinicians from understanding the\nmodel predictions. Therefore, we study and propose the integration of\nexpert-derived radiomics and DNN-predicted biomarkers in interpretable\nclassifiers which we call ConRad, for computerized tomography (CT) scans of\nlung cancer. Importantly, the tumor biomarkers are predicted from a concept\nbottleneck model (CBM) such that once trained, our ConRad models do not require\nlabor-intensive and time-consuming biomarkers. In our evaluation and practical\napplication, the only input to ConRad is a segmented CT scan. The proposed\nmodel is compared to convolutional neural networks (CNNs) which act as a black\nbox classifier. We further investigated and evaluated all combinations of\nradiomics, predicted biomarkers and CNN features in five different classifiers.\nWe found the ConRad models using non-linear SVM and the logistic regression\nwith the Lasso outperform others in five-fold cross-validation, although we\nhighlight that interpretability of ConRad is its primary advantage. The Lasso\nis used for feature selection, which substantially reduces the number of\nnon-zero weights while increasing the accuracy. Overall, the proposed ConRad\nmodel combines CBM-derived biomarkers and radiomics features in an\ninterpretable ML model which perform excellently for the lung nodule malignancy\nclassification.\n","authors":["Lennart Brocki","Neo Christopher Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03104v3","updated":"2023-03-20T14:51:38Z","published":"2022-02-07T12:33:14Z","title":"SimGRACE: A Simple Framework for Graph Contrastive Learning without Data\n  Augmentation","summary":"  Graph contrastive learning (GCL) has emerged as a dominant technique for\ngraph representation learning which maximizes the mutual information between\npaired graph augmentations that share the same semantics. Unfortunately, it is\ndifficult to preserve semantics well during augmentations in view of the\ndiverse nature of graph data. Currently, data augmentations in GCL that are\ndesigned to preserve semantics broadly fall into three unsatisfactory ways.\nFirst, the augmentations can be manually picked per dataset by\ntrial-and-errors. Second, the augmentations can be selected via cumbersome\nsearch. Third, the augmentations can be obtained by introducing expensive\ndomain-specific knowledge as guidance. All of these limit the efficiency and\nmore general applicability of existing GCL methods. To circumvent these crucial\nissues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph\n\\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity,\nwhich does not require data augmentations. Specifically, we take original graph\nas input and GNN model with its perturbed version as two encoders to obtain two\ncorrelated views for contrast. SimGRACE is inspired by the observation that\ngraph data can preserve their semantics well during encoder perturbations while\nnot requiring manual trial-and-errors, cumbersome search or expensive domain\nknowledge for augmentations selection. Also, we explain why SimGRACE can\nsucceed. Furthermore, we devise adversarial training scheme, dubbed\n\\textbf{AT-SimGRACE}, to enhance the robustness of graph contrastive learning\nand theoretically explain the reasons. Albeit simple, we show that SimGRACE can\nyield competitive or better performance compared with state-of-the-art methods\nin terms of generalizability, transferability and robustness, while enjoying\nunprecedented degree of flexibility and efficiency.\n","authors":["Jun Xia","Lirong Wu","Jintao Chen","Bozhen Hu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2202.03104v3.pdf","comment":"Accepted by The Web Conference 2022 (WWW 2022)"},{"id":"http://arxiv.org/abs/2303.11166v1","updated":"2023-03-20T14:51:10Z","published":"2023-03-20T14:51:10Z","title":"Imitating Graph-Based Planning with Goal-Conditioned Policies","summary":"  Recently, graph-based planning algorithms have gained much attention to solve\ngoal-conditioned reinforcement learning (RL) tasks: they provide a sequence of\nsubgoals to reach the target-goal, and the agents learn to execute\nsubgoal-conditioned policies. However, the sample-efficiency of such RL schemes\nstill remains a challenge, particularly for long-horizon tasks. To address this\nissue, we present a simple yet effective self-imitation scheme which distills a\nsubgoal-conditioned policy into the target-goal-conditioned policy. Our\nintuition here is that to reach a target-goal, an agent should pass through a\nsubgoal, so target-goal- and subgoal- conditioned policies should be similar to\neach other. We also propose a novel scheme of stochastically skipping executed\nsubgoals in a planned path, which further improves performance. Unlike prior\nmethods that only utilize graph-based planning in an execution phase, our\nmethod transfers knowledge from a planner along with a graph into policy\nlearning. We empirically show that our method can significantly boost the\nsample-efficiency of the existing goal-conditioned RL methods under various\nlong-horizon control tasks.\n","authors":["Junsu Kim","Younggyo Seo","Sungsoo Ahn","Kyunghwan Son","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2303.11166v1.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2303.11165v1","updated":"2023-03-20T14:50:27Z","published":"2023-03-20T14:50:27Z","title":"Computationally Budgeted Continual Learning: What Does Matter?","summary":"  Continual Learning (CL) aims to sequentially train models on streams of\nincoming data that vary in distribution by preserving previous knowledge while\nadapting to new data. Current CL literature focuses on restricted access to\npreviously seen data, while imposing no constraints on the computational budget\nfor training. This is unreasonable for applications in-the-wild, where systems\nare primarily constrained by computational and time budgets, not storage. We\nrevisit this problem with a large-scale benchmark and analyze the performance\nof traditional CL approaches in a compute-constrained setting, where effective\nmemory samples used in training can be implicitly restricted as a consequence\nof limited computation. We conduct experiments evaluating various CL sampling\nstrategies, distillation losses, and partial fine-tuning on two large-scale\ndatasets, namely ImageNet2K and Continual Google Landmarks V2 in data\nincremental, class incremental, and time incremental settings. Through\nextensive experiments amounting to a total of over 1500 GPU-hours, we find\nthat, under compute-constrained setting, traditional CL approaches, with no\nexception, fail to outperform a simple minimal baseline that samples uniformly\nfrom memory. Our conclusions are consistent in a different number of stream\ntime steps, e.g., 20 to 200, and under several computational budgets. This\nsuggests that most existing CL methods are particularly too computationally\nexpensive for realistic budgeted deployment. Code for this project is available\nat: https://github.com/drimpossible/BudgetCL.\n","authors":["Ameya Prabhu","Hasan Abed Al Kader Hammoud","Puneet Dokania","Philip H. S. Torr","Ser-Nam Lim","Bernard Ghanem","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2303.11165v1.pdf","comment":"Appearing in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11143v1","updated":"2023-03-20T14:22:04Z","published":"2023-03-20T14:22:04Z","title":"Adversarial Attacks against Binary Similarity Systems","summary":"  In recent years, binary analysis gained traction as a fundamental approach to\ninspect software and guarantee its security. Due to the exponential increase of\ndevices running software, much research is now moving towards new autonomous\nsolutions based on deep learning models, as they have been showing\nstate-of-the-art performances in solving binary analysis problems. One of the\nhot topics in this context is binary similarity, which consists in determining\nif two functions in assembly code are compiled from the same source code.\nHowever, it is unclear how deep learning models for binary similarity behave in\nan adversarial context. In this paper, we study the resilience of binary\nsimilarity models against adversarial examples, showing that they are\nsusceptible to both targeted and untargeted attacks (w.r.t. similarity goals)\nperformed by black-box and white-box attackers. In more detail, we extensively\ntest three current state-of-the-art solutions for binary similarity against two\nblack-box greedy attacks, including a new technique that we call Spatial\nGreedy, and one white-box attack in which we repurpose a gradient-guided\nstrategy used in attacks to image classifiers.\n","authors":["Gianluca Capozzi","Daniele Cono D'Elia","Giuseppe Antonio Di Luna","Leonardo Querzoni"],"pdf_url":"https://arxiv.org/pdf/2303.11143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16997v5","updated":"2023-03-20T14:18:13Z","published":"2022-10-31T00:53:17Z","title":"Convergence Rates of Stochastic Zeroth-order Gradient Descent for Ł\n  ojasiewicz Functions","summary":"  We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD)\nalgorithms for Lojasiewicz functions. The SZGD algorithm iterates as\n\\begin{align*}\n  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t \\widehat{\\nabla} f (\\mathbf{x}_t),\n\\qquad t = 0,1,2,3,\\cdots , \\end{align*} where $f$ is the objective function\nthat satisfies the \\L ojasiewicz inequality with \\L ojasiewicz exponent\n$\\theta$, $\\eta_t$ is the step size (learning rate), and $ \\widehat{\\nabla} f\n(\\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order\ninformation only.\n  Our results show that $ \\{ f (\\mathbf{x}_t) - f (\\mathbf{x}_\\infty) \\}_{t \\in\n\\mathbb{N} } $ can converge faster than $ \\{ \\| \\mathbf{x}_t -\n\\mathbf{x}_\\infty \\| \\}_{t \\in \\mathbb{N} }$, regardless of whether the\nobjective $f$ is smooth or nonsmooth.\n","authors":["Tianyu Wang","Yasong Feng"],"pdf_url":"https://arxiv.org/pdf/2210.16997v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11138v1","updated":"2023-03-20T14:15:52Z","published":"2023-03-20T14:15:52Z","title":"Fault Detection via Occupation Kernel Principal Component Analysis","summary":"  The reliable operation of automatic systems is heavily dependent on the\nability to detect faults in the underlying dynamical system. While traditional\nmodel-based methods have been widely used for fault detection, data-driven\napproaches have garnered increasing attention due to their ease of deployment\nand minimal need for expert knowledge. In this paper, we present a novel\nprincipal component analysis (PCA) method that uses occupation kernels.\nOccupation kernels result in feature maps that are tailored to the measured\ndata, have inherent noise-robustness due to the use of integration, and can\nutilize irregularly sampled system trajectories of variable lengths for PCA.\nThe occupation kernel PCA method is used to develop a reconstruction error\napproach to fault detection and its efficacy is validated using numerical\nsimulations.\n","authors":["Zachary Morrison","Benjamin P. Russo","Yingzhao Lian","Rushikesh Kamalapurkar"],"pdf_url":"https://arxiv.org/pdf/2303.11138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11135v1","updated":"2023-03-20T14:12:55Z","published":"2023-03-20T14:12:55Z","title":"TWINS: A Fine-Tuning Framework for Improved Transferability of\n  Adversarial Robustness and Generalization","summary":"  Recent years have seen the ever-increasing importance of pre-trained models\nand their downstream training in deep learning research and applications. At\nthe same time, the defense for adversarial examples has been mainly\ninvestigated in the context of training from random initialization on simple\nclassification tasks. To better exploit the potential of pre-trained models in\nadversarial robustness, this paper focuses on the fine-tuning of an\nadversarially pre-trained model in various classification tasks. Existing\nresearch has shown that since the robust pre-trained model has already learned\na robust feature extractor, the crucial question is how to maintain the\nrobustness in the pre-trained model when learning the downstream task. We study\nthe model-based and data-based approaches for this goal and find that the two\ncommon approaches cannot achieve the objective of improving both generalization\nand adversarial robustness. Thus, we propose a novel statistics-based approach,\nTwo-WIng NormliSation (TWINS) fine-tuning framework, which consists of two\nneural networks where one of them keeps the population means and variances of\npre-training data in the batch normalization layers. Besides the robust\ninformation transfer, TWINS increases the effective learning rate without\nhurting the training stability since the relationship between a weight norm and\nits gradient norm in standard batch normalization layer is broken, resulting in\na faster escape from the sub-optimal initialization and alleviating the robust\noverfitting. Finally, TWINS is shown to be effective on a wide range of image\nclassification datasets in terms of both generalization and robustness. Our\ncode is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n","authors":["Ziquan Liu","Yi Xu","Xiangyang Ji","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2303.11135v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.11131v1","updated":"2023-03-20T14:07:13Z","published":"2023-03-20T14:07:13Z","title":"Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture\n  and Single-Source Speech","summary":"  Self-supervised learning leverages unlabeled data effectively, improving\nlabel efficiency and generalization to domains without labeled data. While\nrecent work has studied generalization to more acoustic/linguistic domains,\nlanguages, and modalities, these investigations are limited to single-source\nspeech with one primary speaker in the recording. This paper presents Cocktail\nHuBERT, a self-supervised learning framework that generalizes to mixture speech\nusing a masked pseudo source separation objective. This objective encourages\nthe model to identify the number of sources, separate and understand the\ncontext, and infer the content of masked regions represented as discovered\nunits. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER\non multi-speaker ASR, 31% lower DER on diarization, and is competitive on\nsingle- and multi-speaker tasks from SUPERB.\n","authors":["Maryam Fazel-Zarandi","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.11131v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11130v1","updated":"2023-03-20T14:06:32Z","published":"2023-03-20T14:06:32Z","title":"Deep learning automated quantification of lung disease in pulmonary\n  hypertension on CT pulmonary angiography: A preliminary clinical study with\n  external validation","summary":"  Purpose: Lung disease assessment in precapillary pulmonary hypertension (PH)\nis essential for appropriate patient management. This study aims to develop an\nartificial intelligence (AI) deep learning model for lung texture\nclassification in CT Pulmonary Angiography (CTPA), and evaluate its correlation\nwith clinical assessment methods.\n  Materials and Methods: In this retrospective study with external validation,\n122 patients with pre-capillary PH were used to train (n=83), validate (n=17)\nand test (n=10 internal test, n=12 external test) a patch based DenseNet-121\nclassification model. \"Normal\", \"Ground glass\", \"Ground glass with\nreticulation\", \"Honeycombing\", and \"Emphysema\" were classified as per the\nFleishner Society glossary of terms. Ground truth classes were segmented by two\nradiologists with patches extracted from the labelled regions. Proportion of\nlung volume for each texture was calculated by classifying patches throughout\nthe entire lung volume to generate a coarse texture classification mapping\nthroughout the lung parenchyma. AI output was assessed against diffusing\ncapacity of carbon monoxide (DLCO) and specialist radiologist reported disease\nseverity.\n  Results: Micro-average AUCs for the validation, internal test, and external\ntest were 0.92, 0.95, and 0.94, respectively. The model had consistent\nperformance across parenchymal textures, demonstrated strong correlation with\ndiffusing capacity of carbon monoxide (DLCO), and showed good correspondence\nwith disease severity reported by specialist radiologists.\n  Conclusion: The classification model demonstrates excellent performance on\nexternal validation. The clinical utility of its output has been demonstrated.\nThis objective, repeatable measure of disease severity can aid in patient\nmanagement in adjunct to radiological reporting.\n","authors":["Michael J. Sharkey","Krit Dwivedi","Samer Alabed","Andrew J. Swift"],"pdf_url":"https://arxiv.org/pdf/2303.11130v1.pdf","comment":"16 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.13690v3","updated":"2023-03-20T13:57:50Z","published":"2022-10-25T01:20:24Z","title":"Highly Efficient Real-Time Streaming and Fully On-Device Speaker\n  Diarization with Multi-Stage Clustering","summary":"  While recent research advances in speaker diarization mostly focus on\nimproving the quality of diarization results, there is also an increasing\ninterest in improving the efficiency of diarization systems. In this paper, we\ndemonstrate that a multi-stage clustering strategy that uses different\nclustering algorithms for input of different lengths can address multi-faceted\nchallenges of on-device speaker diarization applications. Specifically, a\nfallback clusterer is used to handle short-form inputs; a main clusterer is\nused to handle medium-length inputs; and a pre-clusterer is used to compress\nlong-form inputs before they are processed by the main clusterer. Both the main\nclusterer and the pre-clusterer can be configured with an upper bound of the\ncomputational complexity to adapt to devices with different resource\nconstraints. This multi-stage clustering strategy is critical for streaming\non-device speaker diarization systems, where the budgets of CPU, memory and\nbattery are tight.\n","authors":["Quan Wang","Yiling Huang","Han Lu","Guanlong Zhao","Ignacio Lopez Moreno"],"pdf_url":"https://arxiv.org/pdf/2210.13690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12812v2","updated":"2023-03-20T13:56:49Z","published":"2022-10-23T18:27:04Z","title":"Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning\n  with Parameter Convergence","summary":"  Multi-agent interactions are increasingly important in the context of\nreinforcement learning, and the theoretical foundations of policy gradient\nmethods have attracted surging research interest. We investigate the global\nconvergence of natural policy gradient (NPG) algorithms in multi-agent\nlearning. We first show that vanilla NPG may not have parameter convergence,\ni.e., the convergence of the vector that parameterizes the policy, even when\nthe costs are regularized (which enabled strong convergence guarantees in the\npolicy space in the literature). This non-convergence of parameters leads to\nstability issues in learning, which becomes especially relevant in the function\napproximation setting, where we can only operate on low-dimensional parameters,\ninstead of the high-dimensional policy. We then propose variants of the NPG\nalgorithm, for several standard multi-agent learning scenarios: two-player\nzero-sum matrix and Markov games, and multi-player monotone games, with global\nlast-iterate parameter convergence guarantees. We also generalize the results\nto certain function approximation settings. Note that in our algorithms, the\nagents take symmetric roles. Our results might also be of independent interest\nfor solving nonconvex-nonconcave minimax optimization problems with certain\nstructures. Simulations are also provided to corroborate our theoretical\nfindings.\n","authors":["Sarath Pattathil","Kaiqing Zhang","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2210.12812v2.pdf","comment":"Initially submitted for publication in January 2022"},{"id":"http://arxiv.org/abs/2203.11854v2","updated":"2023-03-20T13:51:38Z","published":"2022-03-22T16:31:44Z","title":"Sionna: An Open-Source Library for Next-Generation Physical Layer\n  Research","summary":"  Sionna is a GPU-accelerated open-source library for link-level simulations\nbased on TensorFlow. It enables the rapid prototyping of complex communication\nsystem architectures and provides native support for the integration of neural\nnetworks. Sionna implements a wide breadth of carefully tested state-of-the-art\nalgorithms that can be used for benchmarking and end-to-end performance\nevaluation. This allows researchers to focus on their research, making it more\nimpactful and reproducible, while saving time implementing components outside\ntheir area of expertise. This white paper provides a brief introduction to\nSionna, explains its design principles and features, as well as future\nextensions, such as integrated ray tracing and custom CUDA kernels. We believe\nthat Sionna is a valuable tool for research on next-generation communication\nsystems, such as 6G, and we welcome contributions from our community.\n","authors":["Jakob Hoydis","Sebastian Cammerer","Fayçal Ait Aoudia","Avinash Vem","Nikolaus Binder","Guillermo Marcus","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2203.11854v2.pdf","comment":"5 pages, 1 figure, 4 code listings"},{"id":"http://arxiv.org/abs/2210.00173v3","updated":"2023-03-20T13:44:46Z","published":"2022-10-01T02:57:37Z","title":"Predictive Inference with Feature Conformal Prediction","summary":"  Conformal prediction is a distribution-free technique for establishing valid\nprediction intervals. Although conventionally people conduct conformal\nprediction in the output space, this is not the only possibility. In this\npaper, we propose feature conformal prediction, which extends the scope of\nconformal prediction to semantic feature spaces by leveraging the inductive\nbias of deep representation learning. From a theoretical perspective, we\ndemonstrate that feature conformal prediction provably outperforms regular\nconformal prediction under mild assumptions. Our approach could be combined\nwith not only vanilla conformal prediction, but also other adaptive conformal\nprediction methods. Apart from experiments on existing predictive inference\nbenchmarks, we also demonstrate the state-of-the-art performance of the\nproposed methods on large-scale tasks such as ImageNet classification and\nCityscapes image segmentation.\n","authors":["Jiaye Teng","Chuan Wen","Dinghuai Zhang","Yoshua Bengio","Yang Gao","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2210.00173v3.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2211.07866v2","updated":"2023-03-20T13:43:44Z","published":"2022-11-15T03:17:11Z","title":"Efficient Estimation for Longitudinal Network via Adaptive Merging","summary":"  Longitudinal network consists of a sequence of temporal edges among multiple\nnodes, where the temporal edges are observed in real time. It has become\nubiquitous with the rise of online social platform and e-commerce, but largely\nunder-investigated in literature. In this paper, we propose an efficient\nestimation framework for longitudinal network, leveraging strengths of adaptive\nnetwork merging, tensor decomposition and point process. It merges neighboring\nsparse networks so as to enlarge the number of observed edges and reduce\nestimation variance, whereas the estimation bias introduced by network merging\nis controlled by exploiting local temporal structures for adaptive network\nneighborhood. A projected gradient descent algorithm is proposed to facilitate\nestimation, where the upper bound of the estimation error in each iteration is\nestablished. A thorough analysis is conducted to quantify the asymptotic\nbehavior of the proposed method, which shows that it can significantly reduce\nthe estimation error and also provides guideline for network merging under\nvarious scenarios. We further demonstrate the advantage of the proposed method\nthrough extensive numerical experiments on synthetic datasets and a militarized\ninterstate dispute dataset.\n","authors":["Haoran Zhang","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2211.07866v2.pdf","comment":"26 pages and 2 figures; the appendix including technical proof will\n  be uploaded later"},{"id":"http://arxiv.org/abs/2301.03364v3","updated":"2023-03-20T13:41:37Z","published":"2022-12-20T15:04:20Z","title":"Towards an AI-enabled Connected Industry: AGV Communication and Sensor\n  Measurement Datasets","summary":"  This paper presents two wireless measurement campaigns in industrial\ntestbeds: industrial Vehicle-to-vehicle (iV2V) and industrial\nVehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the\ntwo captured datasets is provided as well. iV2V covers sidelink communication\nscenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at\nan industrial setting where an autonomous cleaning robot is connected to a\nprivate cellular network. The combination of different communication\ntechnologies, together with a common measurement methodology, provides insights\nthat can be exploited by Machine Learning (ML) for tasks such as\nfingerprinting, line-of-sight detection, prediction of quality of service or\nlink selection. Moreover, the datasets are labelled and pre-filtered for fast\non-boarding and applicability. The corresponding testbeds and measurements are\nalso presented in detail for both datasets.\n","authors":["Rodrigo Hernangómez","Alexandros Palaios","Cara Watermann","Daniel Schäufele","Philipp Geuer","Rafail Ismayilov","Mohammad Parvini","Anton Krause","Martin Kasparick","Thomas Neugebauer","Oscar D. Ramos-Cantor","Hugues Tchouankem","Jose Leon Calvo","Bo Chen","Gerhard Fettweis","Sławomir Stańczak"],"pdf_url":"https://arxiv.org/pdf/2301.03364v3.pdf","comment":"7 pages, 3 figures. Submitted to a special issue magazine. Datasets\n  available at\n  https://ieee-dataport.org/open-access/ai4mobile-industrial-wireless-datasets-iv2v-and-iv2i"},{"id":"http://arxiv.org/abs/2303.11103v1","updated":"2023-03-20T13:40:11Z","published":"2023-03-20T13:40:11Z","title":"Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling","summary":"  Sionna is a GPU-accelerated open-source library for link-level simulations\nbased on TensorFlow. Its latest release (v0.14) integrates a differentiable ray\ntracer (RT) for the simulation of radio wave propagation. This unique feature\nallows for the computation of gradients of the channel impulse response and\nother related quantities with respect to many system and environment\nparameters, such as material properties, antenna patterns, array geometries, as\nwell as transmitter and receiver orientations and positions. In this paper, we\noutline the key components of Sionna RT and showcase example applications such\nas learning radio materials and optimizing transmitter orientations by gradient\ndescent. While classic ray tracing is a crucial tool for 6G research topics\nlike reconfigurable intelligent surfaces, integrated sensing and\ncommunications, as well as user localization, differentiable ray tracing is a\nkey enabler for many novel and exciting research directions, for example,\ndigital twins.\n","authors":["Jakob Hoydis","Fayçal Aït Aoudia","Sebastian Cammerer","Merlin Nimier-David","Nikolaus Binder","Guillermo Marcus","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2303.11103v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.11079v1","updated":"2023-03-20T13:38:58Z","published":"2023-03-20T13:38:58Z","title":"Differentially Private Algorithms for Synthetic Power System Datasets","summary":"  While power systems research relies on the availability of real-world network\ndatasets, data owners (e.g., system operators) are hesitant to share data due\nto security and privacy risks. To control these risks, we develop\nprivacy-preserving algorithms for the synthetic generation of optimization and\nmachine learning datasets. Taking a real-world dataset as input, the algorithms\noutput its noisy, synthetic version, which preserves the accuracy of the real\ndata on a specific downstream model or even a large population of those. We\ncontrol the privacy loss using Laplace and Exponential mechanisms of\ndifferential privacy and preserve data accuracy using a post-processing convex\noptimization. We apply the algorithms to generate synthetic network parameters\nand wind power data.\n","authors":["Vladimir Dvorkin","Audun Botterud"],"pdf_url":"https://arxiv.org/pdf/2303.11079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.11073v1","updated":"2023-03-20T12:59:32Z","published":"2023-03-20T12:59:32Z","title":"Discovering Interpretable Directions in the Semantic Latent Space of\n  Diffusion Models","summary":"  Denoising Diffusion Models (DDMs) have emerged as a strong competitor to\nGenerative Adversarial Networks (GANs). However, despite their widespread use\nin image synthesis and editing applications, their latent space is still not as\nwell understood. Recently, a semantic latent space for DDMs, coined\n`$h$-space', was shown to facilitate semantic image editing in a way\nreminiscent of GANs. The $h$-space is comprised of the bottleneck activations\nin the DDM's denoiser across all timesteps of the diffusion process. In this\npaper, we explore the properties of h-space and propose several novel methods\nfor finding meaningful semantic directions within it. We start by studying\nunsupervised methods for revealing interpretable semantic directions in\npretrained DDMs. Specifically, we show that global latent directions emerge as\nthe principal components in the latent space. Additionally, we provide a novel\nmethod for discovering image-specific semantic directions by spectral analysis\nof the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the\nanalysis by finding directions in a supervised fashion in unconditional DDMs.\nWe demonstrate how such directions can be found by relying on either a labeled\ndata set of real images or by annotating generated samples with a\ndomain-specific attribute classifier. We further show how to semantically\ndisentangle the found direction by simple linear projection. Our approaches are\napplicable without requiring any architectural modifications, text-based\nguidance, CLIP-based optimization, or model fine-tuning.\n","authors":["René Haas","Inbar Huberman-Spiegelglas","Rotem Mulayoff","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2303.11073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05087v5","updated":"2023-03-20T12:43:45Z","published":"2021-06-09T14:06:53Z","title":"Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion\n  Attacks in Deep RL","summary":"  Evaluating the worst-case performance of a reinforcement learning (RL) agent\nunder the strongest/optimal adversarial perturbations on state observations\n(within some constraints) is crucial for understanding the robustness of RL\nagents. However, finding the optimal adversary is challenging, in terms of both\nwhether we can find the optimal attack and how efficiently we can find it.\nExisting works on adversarial RL either use heuristics-based methods that may\nnot find the strongest adversary, or directly train an RL-based adversary by\ntreating the agent as a part of the environment, which can find the optimal\nadversary but may become intractable in a large state space. This paper\nintroduces a novel attacking method to find the optimal attacks through\ncollaboration between a designed function named \"actor\" and an RL-based learner\nnamed \"director\". The actor crafts state perturbations for a given policy\nperturbation direction, and the director learns to propose the best policy\nperturbation directions. Our proposed algorithm, PA-AD, is theoretically\noptimal and significantly more efficient than prior RL-based works in\nenvironments with large state spaces. Empirical results show that our proposed\nPA-AD universally outperforms state-of-the-art attacking methods in various\nAtari and MuJoCo environments. By applying PA-AD to adversarial training, we\nachieve state-of-the-art empirical robustness in multiple tasks under strong\nadversaries. The codebase is released at\nhttps://github.com/umd-huang-lab/paad_adv_rl.\n","authors":["Yanchao Sun","Ruijie Zheng","Yongyuan Liang","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2106.05087v5.pdf","comment":"In the 10th International Conference on Learning Representations\n  (ICLR 2022)"},{"id":"http://arxiv.org/abs/2303.11060v1","updated":"2023-03-20T12:23:31Z","published":"2023-03-20T12:23:31Z","title":"Quantile and moment neural networks for learning functionals of\n  distributions","summary":"  We study news neural networks to approximate function of distributions in a\nprobability space. Two classes of neural networks based on quantile and moment\napproximation are proposed to learn these functions and are theoretically\nsupported by universal approximation theorems. By mixing the quantile and\nmoment features in other new networks, we develop schemes that outperform\nexisting networks on numerical test cases involving univariate distributions.\nFor bivariate distributions, the moment neural network outperforms all other\nnetworks.\n","authors":["Xavier Warin"],"pdf_url":"https://arxiv.org/pdf/2303.11060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11042v1","updated":"2023-03-20T11:48:36Z","published":"2023-03-20T11:48:36Z","title":"Hospitalization Length of Stay Prediction using Patient Event Sequences","summary":"  Predicting patients hospital length of stay (LOS) is essential for improving\nresource allocation and supporting decision-making in healthcare organizations.\nThis paper proposes a novel approach for predicting LOS by modeling patient\ninformation as sequences of events. Specifically, we present a\ntransformer-based model, termed Medic-BERT (M-BERT), for LOS prediction using\nthe unique features describing patients medical event sequences. We performed\nempirical experiments on a cohort of more than 45k emergency care patients from\na large Danish hospital. Experimental results show that M-BERT can achieve high\naccuracy on a variety of LOS problems and outperforms traditional\nnonsequence-based machine learning approaches.\n","authors":["Emil Riis Hansen","Thomas Dyhre Nielsen","Thomas Mulvad","Mads Nibe Strausholm","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2303.11042v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2302.13007v3","updated":"2023-03-20T11:39:47Z","published":"2023-02-25T06:58:16Z","title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation","summary":"  Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.\n","authors":["Haixing Dai","Zhengliang Liu","Wenxiong Liao","Xiaoke Huang","Yihan Cao","Zihao Wu","Lin Zhao","Shaochen Xu","Wei Liu","Ninghao Liu","Sheng Li","Dajiang Zhu","Hongmin Cai","Lichao Sun","Quanzheng Li","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2302.13007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11674v2","updated":"2023-03-20T11:33:18Z","published":"2022-11-21T17:42:42Z","title":"Shape, Pose, and Appearance from a Single Image via Bootstrapped\n  Radiance Field Inversion","summary":"  Neural Radiance Fields (NeRF) coupled with GANs represent a promising\ndirection in the area of 3D reconstruction from a single view, owing to their\nability to efficiently model arbitrary topologies. Recent work in this area,\nhowever, has mostly focused on synthetic datasets where exact ground-truth\nposes are known, and has overlooked pose estimation, which is important for\ncertain downstream applications such as augmented reality (AR) and robotics. We\nintroduce a principled end-to-end reconstruction framework for natural images,\nwhere accurate ground-truth poses are not available. Our approach recovers an\nSDF-parameterized 3D shape, pose, and appearance from a single image of an\nobject, without exploiting multiple views during training. More specifically,\nwe leverage an unconditional 3D-aware generator, to which we apply a hybrid\ninversion scheme where a model produces a first guess of the solution which is\nthen refined via optimization. Our framework can de-render an image in as few\nas 10 steps, enabling its use in practical scenarios. We demonstrate\nstate-of-the-art results on a variety of real and synthetic benchmarks.\n","authors":["Dario Pavllo","David Joseph Tan","Marie-Julie Rakotosaona","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2211.11674v2.pdf","comment":"CVPR 2023. Code and models are available at\n  https://github.com/google-research/nerf-from-image"},{"id":"http://arxiv.org/abs/2303.11028v1","updated":"2023-03-20T11:18:22Z","published":"2023-03-20T11:18:22Z","title":"MAQA: A Quantum Framework for Supervised Learning","summary":"  Quantum Machine Learning has the potential to improve traditional machine\nlearning methods and overcome some of the main limitations imposed by the\nclassical computing paradigm. However, the practical advantages of using\nquantum resources to solve pattern recognition tasks are still to be\ndemonstrated.\n  This work proposes a universal, efficient framework that can reproduce the\noutput of a plethora of classical supervised machine learning algorithms\nexploiting quantum computation's advantages. The proposed framework is named\nMultiple Aggregator Quantum Algorithm (MAQA) due to its capability to combine\nmultiple and diverse functions to solve typical supervised learning problems.\nIn its general formulation, MAQA can be potentially adopted as the quantum\ncounterpart of all those models falling into the scheme of aggregation of\nmultiple functions, such as ensemble algorithms and neural networks. From a\ncomputational point of view, the proposed framework allows generating an\nexponentially large number of different transformations of the input at the\ncost of increasing the depth of the corresponding quantum circuit linearly.\nThus, MAQA produces a model with substantial descriptive power to broaden the\nhorizon of possible applications of quantum machine learning with a\ncomputational advantage over classical methods. As a second meaningful\naddition, we discuss the adoption of the proposed framework as hybrid\nquantum-classical and fault-tolerant quantum algorithm.\n","authors":["Antonio Macaluso","Matthias Klusch","Stefano Lodi","Claudio Sartori"],"pdf_url":"https://arxiv.org/pdf/2303.11028v1.pdf","comment":"1 Figure"},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.11005v1","updated":"2023-03-20T10:33:06Z","published":"2023-03-20T10:33:06Z","title":"Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving","summary":"  Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.\n","authors":["Li Yi"],"pdf_url":"https://arxiv.org/pdf/2303.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11000v1","updated":"2023-03-20T10:29:42Z","published":"2023-03-20T10:29:42Z","title":"Late Meta-learning Fusion Using Representation Learning for Time Series\n  Forecasting","summary":"  Meta-learning, decision fusion, hybrid models, and representation learning\nare topics of investigation with significant traction in time-series\nforecasting research. Of these two specific areas have shown state-of-the-art\nresults in forecasting: hybrid meta-learning models such as Exponential\nSmoothing - Recurrent Neural Network (ES-RNN) and Neural Basis Expansion\nAnalysis (N-BEATS) and feature-based stacking ensembles such as Feature-based\nFORecast Model Averaging (FFORMA). However, a unified taxonomy for model fusion\nand an empirical comparison of these hybrid and feature-based stacking ensemble\napproaches is still missing. This study presents a unified taxonomy\nencompassing these topic areas. Furthermore, the study empirically evaluates\nseveral model fusion approaches and a novel combination of hybrid and feature\nstacking algorithms called Deep-learning FORecast Model Averaging (DeFORMA).\nThe taxonomy contextualises the considered methods. Furthermore, the empirical\nanalysis of the results shows that the proposed model, DeFORMA, can achieve\nstate-of-the-art results in the M4 data set. DeFORMA, increases the mean\nOverall Weighted Average (OWA) in the daily, weekly and yearly subsets with\ncompetitive results in the hourly, monthly and quarterly subsets. The taxonomy\nand empirical results lead us to argue that significant progress is still to be\nmade by continuing to explore the intersection of these research areas.\n","authors":["Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2303.11000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10999v1","updated":"2023-03-20T10:29:35Z","published":"2023-03-20T10:29:35Z","title":"Induced Feature Selection by Structured Pruning","summary":"  The advent of sparsity inducing techniques in neural networks has been of a\ngreat help in the last few years. Indeed, those methods allowed to find lighter\nand faster networks, able to perform more efficiently in resource-constrained\nenvironment such as mobile devices or highly requested servers. Such a sparsity\nis generally imposed on the weights of neural networks, reducing the footprint\nof the architecture. In this work, we go one step further by imposing sparsity\njointly on the weights and on the input data. This can be achieved following a\nthree-step process: 1) impose a certain structured sparsity on the weights of\nthe network; 2) track back input features corresponding to zeroed blocks of\nweight; 3) remove useless weights and input features and retrain the network.\nPerforming pruning both on the network and on input data not only allows for\nextreme reduction in terms of parameters and operations but can also serve as\nan interpretation process. Indeed, with the help of data pruning, we now have\ninformation about which input feature is useful for the network to keep its\nperformance. Experiments conducted on a variety of architectures and datasets:\nMLP validated on MNIST, CIFAR10/100 and ConvNets (VGG16 and ResNet18),\nvalidated on CIFAR10/100 and CALTECH101 respectively, show that it is possible\nto achieve additional gains in terms of total parameters and in FLOPs by\nperforming pruning on input data, while also increasing accuracy.\n","authors":["Nathan Hubens","Victor Delvigne","Matei Mancas","Bernard Gosselin","Marius Preda","Titus Zaharia"],"pdf_url":"https://arxiv.org/pdf/2303.10999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10993v1","updated":"2023-03-20T10:21:29Z","published":"2023-03-20T10:21:29Z","title":"A Survey on Oversmoothing in Graph Neural Networks","summary":"  Node features of graph neural networks (GNNs) tend to become more similar\nwith the increase of the network depth. This effect is known as over-smoothing,\nwhich we axiomatically define as the exponential convergence of suitable\nsimilarity measures on the node features. Our definition unifies previous\napproaches and gives rise to new quantitative measures of over-smoothing.\nMoreover, we empirically demonstrate this behavior for several over-smoothing\nmeasures on different graphs (small-, medium-, and large-scale). We also review\nseveral approaches for mitigating over-smoothing and empirically test their\neffectiveness on real-world graph datasets. Through illustrative examples, we\ndemonstrate that mitigating over-smoothing is a necessary but not sufficient\ncondition for building deep GNNs that are expressive on a wide range of graph\nlearning tasks. Finally, we extend our definition of over-smoothing to the\nrapidly emerging field of continuous-time GNNs.\n","authors":["T. Konstantin Rusch","Michael M. Bronstein","Siddhartha Mishra"],"pdf_url":"https://arxiv.org/pdf/2303.10993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10972v1","updated":"2023-03-20T09:50:07Z","published":"2023-03-20T09:50:07Z","title":"Semantic segmentation of surgical hyperspectral images under geometric\n  domain shifts","summary":"  Robust semantic segmentation of intraoperative image data could pave the way\nfor automatic surgical scene understanding and autonomous robotic surgery.\nGeometric domain shifts, however, although common in real-world open surgeries\ndue to variations in surgical procedures or situs occlusions, remain a topic\nlargely unaddressed in the field. To address this gap in the literature, we (1)\npresent the first analysis of state-of-the-art (SOA) semantic segmentation\nnetworks in the presence of geometric out-of-distribution (OOD) data, and (2)\naddress generalizability with a dedicated augmentation technique termed \"Organ\nTransplantation\" that we adapted from the general computer vision community.\nAccording to a comprehensive validation on six different OOD data sets\ncomprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs\nsemantically annotated with 19 classes, we demonstrate a large performance drop\nof SOA organ segmentation networks applied to geometric OOD data. Surprisingly,\nthis holds true not only for conventional RGB data (drop of Dice similarity\ncoefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the\nlatter's rich information content per pixel. Using our augmentation scheme\nimproves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders\nperformance on par with in-distribution performance on real OOD test data. The\nsimplicity and effectiveness of our augmentation scheme makes it a valuable\nnetwork-independent tool for addressing geometric domain shifts in semantic\nscene segmentation of intraoperative data. Our code and pre-trained models will\nbe made publicly available.\n","authors":["Jan Sellner","Silvia Seidlitz","Alexander Studier-Fischer","Alessandro Motta","Berkin Özdemir","Beat Peter Müller-Stich","Felix Nickel","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.10972v1.pdf","comment":"The first two authors (Jan Sellner and Silvia Seidlitz) contributed\n  equally to this paper"},{"id":"http://arxiv.org/abs/2303.10954v1","updated":"2023-03-20T09:27:58Z","published":"2023-03-20T09:27:58Z","title":"Uncertainty-aware deep learning for digital twin-driven monitoring:\n  Application to fault detection in power lines","summary":"  Deep neural networks (DNNs) are often coupled with physics-based models or\ndata-driven surrogate models to perform fault detection and health monitoring\nof systems in the low data regime. These models serve as digital twins to\ngenerate large quantities of data to train DNNs which would otherwise be\ndifficult to obtain from the real-life system. However, such models can exhibit\nparametric uncertainty that propagates to the generated data. In addition, DNNs\nexhibit uncertainty in the parameters learnt during training. In such a\nscenario, the performance of the DNN model will be influenced by the\nuncertainty in the physics-based model as well as the parameters of the DNN. In\nthis article, we quantify the impact of both these sources of uncertainty on\nthe performance of the DNN. We perform explicit propagation of uncertainty in\ninput data through all layers of the DNN, as well as implicit prediction of\noutput uncertainty to capture the former. Furthermore, we adopt Monte Carlo\ndropout to capture uncertainty in DNN parameters. We demonstrate the approach\nfor fault detection of power lines with a physics-based model, two types of\ninput data and three different neural network architectures. We compare the\nperformance of such uncertainty-aware probabilistic models with their\ndeterministic counterparts. The results show that the probabilistic models\nprovide important information regarding the confidence of predictions, while\nalso delivering an improvement in performance over deterministic models.\n","authors":["Laya Das","Blazhe Gjorgiev","Giovanni Sansavini"],"pdf_url":"https://arxiv.org/pdf/2303.10954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08645v3","updated":"2023-03-20T08:38:19Z","published":"2022-07-18T14:45:55Z","title":"Active Exploration for Inverse Reinforcement Learning","summary":"  Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a\nreward function from expert demonstrations. Many IRL algorithms require a known\ntransition model and sometimes even a known expert policy, or they at least\nrequire access to a generative model. However, these assumptions are too strong\nfor many real-world applications, where the environment can be accessed only\nthrough sequential interaction. We propose a novel IRL algorithm: Active\nexploration for Inverse Reinforcement Learning (AceIRL), which actively\nexplores an unknown environment and expert policy to quickly learn the expert's\nreward function and identify a good policy. AceIRL uses previous observations\nto construct confidence intervals that capture plausible reward functions and\nfind exploration policies that focus on the most informative regions of the\nenvironment. AceIRL is the first approach to active IRL with sample-complexity\nbounds that does not require a generative model of the environment. AceIRL\nmatches the sample complexity of active IRL with a generative model in the\nworst case. Additionally, we establish a problem-dependent bound that relates\nthe sample complexity of AceIRL to the suboptimality gap of a given IRL\nproblem. We empirically evaluate AceIRL in simulations and find that it\nsignificantly outperforms more naive exploration strategies.\n","authors":["David Lindner","Andreas Krause","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2207.08645v3.pdf","comment":"Presented at Conference on Neural Information Processing Systems\n  (NeurIPS), 2022"},{"id":"http://arxiv.org/abs/2303.09232v2","updated":"2023-03-20T08:25:38Z","published":"2023-03-16T11:15:55Z","title":"Generative Adversarial Network for Personalized Art Therapy in Melanoma\n  Disease Management","summary":"  Melanoma is the most lethal type of skin cancer. Patients are vulnerable to\nmental health illnesses which can reduce the effectiveness of the cancer\ntreatment and the patients adherence to drug plans. It is crucial to preserve\nthe mental health of patients while they are receiving treatment. However,\ncurrent art therapy approaches are not personal and unique to the patient. We\naim to provide a well-trained image style transfer model that can quickly\ngenerate unique art from personal dermoscopic melanoma images as an additional\ntool for art therapy in disease management of melanoma. Visual art appreciation\nas a common form of art therapy in disease management that measurably reduces\nthe degree of psychological distress. We developed a network based on the\ncycle-consistent generative adversarial network for style transfer that\ngenerates personalized and unique artworks from dermoscopic melanoma images. We\ndeveloped a model that converts melanoma images into unique flower-themed\nartworks that relate to the shape of the lesion and are therefore personal to\nthe patient. Further, we altered the initial framework and made comparisons and\nevaluations of the results. With this, we increased the options in the toolbox\nfor art therapy in disease management of melanoma. The development of an\neasy-to-use user interface ensures the availability of the approach to\nstakeholders. The transformation of melanoma into flower-themed artworks is\nachieved by the proposed model and the graphical user interface. This\ncontribution opens a new field of GANs in art therapy and could lead to more\npersonalized disease management.\n","authors":["Lennart Jütte","Ning Wang","Bernhard Roth"],"pdf_url":"https://arxiv.org/pdf/2303.09232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10934v1","updated":"2023-03-20T08:17:05Z","published":"2023-03-20T08:17:05Z","title":"EMC2-Net: Joint Equalization and Modulation Classification based on\n  Constellation Network","summary":"  Modulation classification (MC) is the first step performed at the receiver\nside unless the modulation type is explicitly indicated by the transmitter.\nMachine learning techniques have been widely used for MC recently. In this\npaper, we propose a novel MC technique dubbed as Joint Equalization and\nModulation Classification based on Constellation Network (EMC2-Net). Unlike\nprior works that considered the constellation points as an image, the proposed\nEMC2-Net directly uses a set of 2D constellation points to perform MC. In order\nto obtain clear and concrete constellation despite multipath fading channels,\nthe proposed EMC2-Net consists of equalizer and classifier having separate and\nexplainable roles via novel three-phase training and noise-curriculum\npretraining. Numerical results with linear modulation types under different\nchannel models show that the proposed EMC2-Net achieves the performance of\nstate-of-the-art MC techniques with significantly less complexity.\n","authors":["Hyun Ryu","Junil Choi"],"pdf_url":"https://arxiv.org/pdf/2303.10934v1.pdf","comment":"Accepted to ICASSP 2023 (5 pages, 2 figures, 1 table)"},{"id":"http://arxiv.org/abs/2303.10931v1","updated":"2023-03-20T08:09:13Z","published":"2023-03-20T08:09:13Z","title":"Approaching an unknown communication system by latent space exploration\n  and causal inference","summary":"  This paper proposes a methodology for discovering meaningful properties in\ndata by exploring the latent space of unsupervised deep generative models. We\ncombine manipulation of individual latent variables to extreme values outside\nthe training range with methods inspired by causal inference into an approach\nwe call causal disentanglement with extreme values (CDEV) and show that this\napproach yields insights for model interpretability. Using this technique, we\ncan infer what properties of unknown data the model encodes as meaningful. We\napply the methodology to test what is meaningful in the communication system of\nsperm whales, one of the most intriguing and understudied animal communication\nsystems. We train a network that has been shown to learn meaningful\nrepresentations of speech and test whether we can leverage such unsupervised\nlearning to decipher the properties of another vocal communication system for\nwhich we have no ground truth. The proposed technique suggests that sperm\nwhales encode information using the number of clicks in a sequence, the\nregularity of their timing, and audio properties such as the spectral mean and\nthe acoustic regularity of the sequences. Some of these findings are consistent\nwith existing hypotheses, while others are proposed for the first time. We also\nargue that our models uncover rules that govern the structure of communication\nunits in the sperm whale communication system and apply them while generating\ninnovative data not shown during training. This paper suggests that an\ninterpretation of the outputs of deep neural networks with causal methodology\ncan be a viable strategy for approaching data about which little is known and\npresents another case of how deep learning can limit the hypothesis space.\nFinally, the proposed approach combining latent space manipulation and causal\ninference can be extended to other architectures and arbitrary datasets.\n","authors":["Gašper Beguš","Andrej Leban","Shane Gero"],"pdf_url":"https://arxiv.org/pdf/2303.10931v1.pdf","comment":"25 pages, 23 figures"},{"id":"http://arxiv.org/abs/2202.03376v3","updated":"2023-03-20T07:52:57Z","published":"2022-02-07T17:47:46Z","title":"Message Passing Neural PDE Solvers","summary":"  The numerical solution of partial differential equations (PDEs) is difficult,\nhaving led to a century of research so far. Recently, there have been pushes to\nbuild neural--numerical hybrid solvers, which piggy-backs the modern trend\ntowards fully end-to-end learned systems. Most works so far can only generalize\nover a subset of properties to which a generic solver would be faced,\nincluding: resolution, topology, geometry, boundary conditions, domain\ndiscretization regularity, dimensionality, etc. In this work, we build a\nsolver, satisfying these properties, where all the components are based on\nneural message passing, replacing all heuristically designed components in the\ncomputation graph with backprop-optimized neural function approximators. We\nshow that neural message passing solvers representationally contain some\nclassical methods, such as finite differences, finite volumes, and WENO\nschemes. In order to encourage stability in training autoregressive models, we\nput forward a method that is based on the principle of zero-stability, posing\nstability as a domain adaptation problem. We validate our method on various\nfluid-like flow problems, demonstrating fast, stable, and accurate performance\nacross different domain topologies, equation parameters, discretizations, etc.,\nin 1D and 2D.\n","authors":["Johannes Brandstetter","Daniel Worrall","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2202.03376v3.pdf","comment":"Published at ICLR 2022 (Spotlight paper), Github:\n  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers"},{"id":"http://arxiv.org/abs/2303.10912v1","updated":"2023-03-20T07:09:26Z","published":"2023-03-20T07:09:26Z","title":"Exploring Representation Learning for Small-Footprint Keyword Spotting","summary":"  In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.\n","authors":["Fan Cui","Liyong Guo","Quandong Wang","Peng Gao","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06983v3","updated":"2023-03-20T07:01:49Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited in terms of the number of instruments, the length of the\nmusic segments and slow inference. This is partly due to the memory\nrequirements of the lengthy input sequences necessitated by existing\nrepresentations. In this work, we propose a new multitrack music representation\nthat allows a diverse set of instruments while keeping a short sequence length.\nOur proposed Multitrack Music Transformer (MMT) achieves comparable performance\nwith state-of-the-art systems, landing in between two recently proposed models\nin a subjective listening test, while achieving substantial speedups and memory\nreductions over both, making the method attractive for real time improvisation\nor near real time creative applications. Further, we propose a new measure for\nanalyzing musical self-attention and show that the trained model attends more\nto notes that form a consonant interval with the current note and to notes that\nare 4N beats away from the current step.\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v3.pdf","comment":"Accepted by ICASSP 2023. Audio samples available at\n  https://salu133445.github.io/mmt/ . Source code available at\n  https://github.com/salu133445/mmt"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2007.00691v3","updated":"2023-03-20T06:57:52Z","published":"2020-07-01T18:32:05Z","title":"Falsification-Based Robust Adversarial Reinforcement Learning","summary":"  Reinforcement learning (RL) has achieved enormous progress in solving various\nsequential decision-making problems, such as control tasks in robotics. Since\npolicies are overfitted to training environments, RL methods have often failed\nto be generalized to safety-critical test scenarios. Robust adversarial RL\n(RARL) was previously proposed to train an adversarial network that applies\ndisturbances to a system, which improves the robustness in test scenarios.\nHowever, an issue of neural network-based adversaries is that integrating\nsystem requirements without handcrafting sophisticated reward signals are\ndifficult. Safety falsification methods allow one to find a set of initial\nconditions and an input sequence, such that the system violates a given\nproperty formulated in temporal logic. In this paper, we propose\nfalsification-based RARL (FRARL): this is the first generic framework for\nintegrating temporal logic falsification in adversarial learning to improve\npolicy robustness. By applying our falsification method, we do not need to\nconstruct an extra reward function for the adversary. Moreover, we evaluate our\napproach on a braking assistance system and an adaptive cruise control system\nof autonomous vehicles. Our experimental results demonstrate that policies\ntrained with a falsification-based adversary generalize better and show less\nviolation of the safety specification in test scenarios than those trained\nwithout an adversary or with an adversarial network.\n","authors":["Xiao Wang","Saasha Nair","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2007.00691v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10909v1","updated":"2023-03-20T06:57:10Z","published":"2023-03-20T06:57:10Z","title":"Graph Neural Rough Differential Equations for Traffic Forecasting","summary":"  Traffic forecasting is one of the most popular spatio-temporal tasks in the\nfield of machine learning. A prevalent approach in the field is to combine\ngraph convolutional networks and recurrent neural networks for the\nspatio-temporal processing. There has been fierce competition and many novel\nmethods have been proposed. In this paper, we present the method of\nspatio-temporal graph neural rough differential equation (STG-NRDE). Neural\nrough differential equations (NRDEs) are a breakthrough concept for processing\ntime-series data. Their main concept is to use the log-signature transform to\nconvert a time-series sample into a relatively shorter series of feature\nvectors. We extend the concept and design two NRDEs: one for the temporal\nprocessing and the other for the spatial processing. After that, we combine\nthem into a single framework. We conduct experiments with 6 benchmark datasets\nand 21 baselines. STG-NRDE shows the best accuracy in all cases, outperforming\nall those 21 baselines by non-trivial margins.\n","authors":["Jeongwhan Choi","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2303.10909v1.pdf","comment":"Under review by ACM TIST. arXiv admin note: substantial text overlap\n  with arXiv:2112.03558"},{"id":"http://arxiv.org/abs/2205.02910v2","updated":"2023-03-20T06:49:13Z","published":"2022-05-05T20:29:13Z","title":"GANs as Gradient Flows that Converge","summary":"  This paper approaches the unsupervised learning problem by gradient descent\nin the space of probability density functions. A main result shows that along\nthe gradient flow induced by a distribution-dependent ordinary differential\nequation (ODE), the unknown data distribution emerges as the long-time limit.\nThat is, one can uncover the data distribution by simulating the\ndistribution-dependent ODE. Intriguingly, the simulation of the ODE is shown\nequivalent to the training of generative adversarial networks (GANs). This\nequivalence provides a new \"cooperative\" view of GANs and, more importantly,\nsheds new light on the divergence of GANs. In particular, it reveals that the\nGAN algorithm implicitly minimizes the mean squared error (MSE) between two\nsets of samples, and this MSE fitting alone can cause GANs to diverge. To\nconstruct a solution to the distribution-dependent ODE, we first show that the\nassociated nonlinear Fokker-Planck equation has a unique weak solution, by the\nCrandall-Liggett theorem for differential equations in Banach spaces. Based on\nthis solution to the Fokker-Planck equation, we construct a unique solution to\nthe ODE, using Trevisan's superposition principle. The convergence of the\ninduced gradient flow to the data distribution is obtained by analyzing the\nFokker-Planck equation.\n","authors":["Yu-Jui Huang","Yuchong Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.02910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15452v2","updated":"2023-03-20T06:46:22Z","published":"2022-09-30T13:00:33Z","title":"Safe Exploration Method for Reinforcement Learning under Existence of\n  Disturbance","summary":"  Recent rapid developments in reinforcement learning algorithms have been\ngiving us novel possibilities in many fields. However, due to their exploring\nproperty, we have to take the risk into consideration when we apply those\nalgorithms to safety-critical problems especially in real environments. In this\nstudy, we deal with a safe exploration problem in reinforcement learning under\nthe existence of disturbance. We define the safety during learning as\nsatisfaction of the constraint conditions explicitly defined in terms of the\nstate and propose a safe exploration method that uses partial prior knowledge\nof a controlled object and disturbance. The proposed method assures the\nsatisfaction of the explicit state constraints with a pre-specified probability\neven if the controlled object is exposed to a stochastic disturbance following\na normal distribution. As theoretical results, we introduce sufficient\nconditions to construct conservative inputs not containing an exploring aspect\nused in the proposed method and prove that the safety in the above explained\nsense is guaranteed with the proposed method. Furthermore, we illustrate the\nvalidity and effectiveness of the proposed method through numerical simulations\nof an inverted pendulum and a four-bar parallel link robot manipulator.\n","authors":["Yoshihiro Okawa","Tomotake Sasaki","Hitoshi Yanami","Toru Namerikawa"],"pdf_url":"https://arxiv.org/pdf/2209.15452v2.pdf","comment":"Accepted by the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2022.\n  The Version of Record is available at\n  https://doi.org/10.1007/978-3-031-26412-2_9"},{"id":"http://arxiv.org/abs/2303.10898v1","updated":"2023-03-20T06:35:46Z","published":"2023-03-20T06:35:46Z","title":"A Tiny Machine Learning Model for Point Cloud Object Classification","summary":"  The design of a tiny machine learning model, which can be deployed in mobile\nand edge devices, for point cloud object classification is investigated in this\nwork. To achieve this objective, we replace the multi-scale representation of a\npoint cloud object with a single-scale representation for complexity reduction,\nand exploit rich 3D geometric information of a point cloud object for\nperformance improvement. The proposed solution is named Green-PointHop due to\nits low computational complexity. We evaluate the performance of Green-PointHop\non ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of\n64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a\nModelNet40 object of 1024 down-sampled points. Its classification performance\ngaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and\nScanObjectNN, respectively. On the other hand, the model size and inference\ncomplexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively.\n","authors":["Min Zhang","Jintang Xue","Pranav Kadam","Hardik Prajapati","Shan Liu","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.10898v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.10881v1","updated":"2023-03-20T05:47:35Z","published":"2023-03-20T05:47:35Z","title":"Machine Learning Automated Approach for Enormous Synchrotron X-Ray\n  Diffraction Data Interpretation","summary":"  Manual analysis of XRD data is usually laborious and time consuming. The deep\nneural network (DNN) based models trained by synthetic XRD patterns are proved\nto be an automatic, accurate, and high throughput method to analysis common XRD\ndata collected from solid sample in ambient environment. However, it remains\nunknown that whether synthetic XRD based models are capable to solve u-XRD\nmapping data for in-situ experiments involving liquid phase exhibiting lower\nquality with significant artifacts. In this study, we collected u-XRD mapping\ndata from an LaCl3-calcite hydrothermal fluid system and trained two categories\nof models to solve the experimental XRD patterns. The models trained by\nsynthetic XRD patterns show low accuracy (as low as 64%) when solving\nexperimental u-XRD mapping data. The accuracy of the DNN models was\nsignificantly improved (90% or above) when training them with the dataset\ncontaining both synthetic and small number of labeled experimental u-XRD\npatterns. This study highlighted the importance of labeled experimental\npatterns on the training of DNN models to solve u-XRD mapping data from in-situ\nexperiments involving liquid phase.\n","authors":["Xiaodong Zhao","YiXuan Luo","Juejing Liu","Wenjun Liu","Kevin M. Rosso","Xiaofeng Guo","Tong Geng","Ang Li","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10881v1.pdf","comment":"See link below for supporting information\n  https://docs.google.com/document/d/1m2SyaBDej4BhkWCA38GRXJe5Jd7Di7cp/edit?usp=sharing&ouid=108731997922646321851&rtpof=true&sd=true"},{"id":"http://arxiv.org/abs/2303.10880v1","updated":"2023-03-20T05:38:30Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we propose to perform in-hand object rotation using only touching\nwithout seeing the object. Instead of relying on precise tactile sensing in a\nsmall region, we introduce a new system design using dense binary force sensors\n(touch or no touch) overlaying one side of the whole robot hand (palm, finger\nlinks, fingertips). Such a design is low-cost, giving a larger coverage of the\nobject, and minimizing the Sim2Real gap at the same time. We train an in-hand\nrotation policy using Reinforcement Learning on diverse objects in simulation.\nRelying on touch-only sensing, we can directly deploy the policy in a real\nrobot hand and rotate novel objects that are not presented in training.\nExtensive ablations are performed on how tactile information help in-hand\nmanipulation. Our project is available at https://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v1.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2211.15046v3","updated":"2023-03-20T05:18:49Z","published":"2022-11-28T04:08:55Z","title":"PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial\n  Networks for Radar-Based Precipitation Nowcasting","summary":"  The precipitation nowcasting methods have been elaborated over the centuries\nbecause rain has a crucial impact on human life. Not only quantitative\nprecipitation forecast (QPF) models and convolutional long short-term memory\n(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2\nare emerging. In this paper, we propose a paired complementary temporal\ncycle-consistent adversarial networks (PCT-CycleGAN) for radar-based\nprecipitation nowcasting, inspired by cycle-consistent adversarial networks\n(CycleGAN), which shows strong performance in image-to-image translation.\nPCT-CycleGAN generates temporal causality using two generator networks with\nforward and backward temporal dynamics in paired complementary cycles. Each\ngenerator network learns a huge number of one-to-one mappings about\ntime-dependent radar-based precipitation data to approximate a mapping function\nrepresenting the temporal dynamics in each direction. To create robust temporal\ncausality between paired complementary cycles, novel connection loss is\nproposed. The generator network learning forward temporal dynamics in\nPCT-CycleGAN generates radar-based precipitation data 10 minutes from the\ncurrent time. Also, it provides a reliable prediction of up to 2 hours with\niterative forecasting. The superiority of PCT-CycleGAN is demonstrated through\nqualitative and quantitative comparisons with several previous methods.\n","authors":["Jaeho Choi","Yura Kim","Kwang-Ho Kim","Sung-Hwa Jung","Ikhyun Cho"],"pdf_url":"https://arxiv.org/pdf/2211.15046v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10875v1","updated":"2023-03-20T05:18:31Z","published":"2023-03-20T05:18:31Z","title":"Hardware-Aware Graph Neural Network Automated Design for Edge Computing\n  Platforms","summary":"  Graph neural networks (GNNs) have emerged as a popular strategy for handling\nnon-Euclidean data due to their state-of-the-art performance. However, most of\nthe current GNN model designs mainly focus on task accuracy, lacking in\nconsidering hardware resources limitation and real-time requirements of edge\napplication scenarios. Comprehensive profiling of typical GNN models indicates\nthat their execution characteristics are significantly affected across\ndifferent computing platforms, which demands hardware awareness for efficient\nGNN designs. In this work, HGNAS is proposed as the first Hardware-aware Graph\nNeural Architecture Search framework targeting resource constraint edge\ndevices. By decoupling the GNN paradigm, HGNAS constructs a fine-grained design\nspace and leverages an efficient multi-stage search strategy to explore optimal\narchitectures within a few GPU hours. Moreover, HGNAS achieves hardware\nawareness during the GNN architecture design by leveraging a hardware\nperformance predictor, which could balance the GNN model accuracy and\nefficiency corresponding to the characteristics of targeted devices.\nExperimental results show that HGNAS can achieve about $10.6\\times$ speedup and\n$88.2\\%$ peak memory reduction with a negligible accuracy loss compared to\nDGCNN on various edge devices, including Nvidia RTX3080, Jetson TX2, Intel\ni7-8700K and Raspberry Pi 3B+.\n","authors":["Ao Zhou","Jianlei Yang","Yingjie Qi","Yumeng Shi","Tong Qiao","Weisheng Zhao","Chunming Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10875v1.pdf","comment":"Accepted by DAC'23"},{"id":"http://arxiv.org/abs/2209.09004v3","updated":"2023-03-20T04:49:10Z","published":"2022-09-19T13:28:32Z","title":"EcoFormer: Energy-Saving Attention with Linear Complexity","summary":"  Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n","authors":["Jing Liu","Zizheng Pan","Haoyu He","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2209.09004v3.pdf","comment":"NeurIPS 2022 camera ready; First two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.10859v1","updated":"2023-03-20T04:39:39Z","published":"2023-03-20T04:39:39Z","title":"Improved Sample Complexity for Reward-free Reinforcement Learning under\n  Low-rank MDPs","summary":"  In reward-free reinforcement learning (RL), an agent explores the environment\nfirst without any reward information, in order to achieve certain learning\ngoals afterwards for any given reward. In this paper we focus on reward-free RL\nunder low-rank MDP models, in which both the representation and linear weight\nvectors are unknown. Although various algorithms have been proposed for\nreward-free low-rank MDPs, the corresponding sample complexity is still far\nfrom being satisfactory. In this work, we first provide the first known sample\ncomplexity lower bound that holds for any algorithm under low-rank MDPs. This\nlower bound implies it is strictly harder to find a near-optimal policy under\nlow-rank MDPs than under linear MDPs. We then propose a novel model-based\nalgorithm, coined RAFFLE, and show it can both find an $\\epsilon$-optimal\npolicy and achieve an $\\epsilon$-accurate system identification via reward-free\nexploration, with a sample complexity significantly improving the previous\nresults. Such a sample complexity matches our lower bound in the dependence on\n$\\epsilon$, as well as on $K$ in the large $d$ regime, where $d$ and $K$\nrespectively denote the representation dimension and action space cardinality.\nFinally, we provide a planning algorithm (without further interaction with true\nenvironment) for RAFFLE to learn a near-accurate representation, which is the\nfirst known representation learning guarantee under the same setting.\n","authors":["Yuan Cheng","Ruiquan Huang","Jing Yang","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2303.10859v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.10856v1","updated":"2023-03-20T04:30:18Z","published":"2023-03-20T04:30:18Z","title":"Revisiting Realistic Test-Time Training: Sequential Inference and\n  Adaptation by Anchored Clustering Regularized Self-Training","summary":"  Deploying models on target domain data subject to distribution shift requires\nadaptation. Test-time training (TTT) emerges as a solution to this adaptation\nunder a realistic scenario where access to full source domain data is not\navailable, and instant inference on the target domain is required. Despite many\nefforts into TTT, there is a confusion over the experimental settings, thus\nleading to unfair comparisons. In this work, we first revisit TTT assumptions\nand categorize TTT protocols by two key factors. Among the multiple protocols,\nwe adopt a realistic sequential test-time training (sTTT) protocol, under which\nwe develop a test-time anchored clustering (TTAC) approach to enable stronger\ntest-time feature learning. TTAC discovers clusters in both source and target\ndomains and matches the target clusters to the source ones to improve\nadaptation. When source domain information is strictly absent (i.e.\nsource-free) we further develop an efficient method to infer source domain\ndistributions for anchored clustering. Finally, self-training~(ST) has\ndemonstrated great success in learning from unlabeled data and we empirically\nfigure out that applying ST alone to TTT is prone to confirmation bias.\nTherefore, a more effective TTT approach is introduced by regularizing\nself-training with anchored clustering, and the improved model is referred to\nas TTAC++. We demonstrate that, under all TTT protocols, TTAC++ consistently\noutperforms the state-of-the-art methods on five TTT datasets, including\ncorrupted target domain, selected hard samples, synthetic-to-real adaptation\nand adversarially attacked target domain. We hope this work will provide a fair\nbenchmarking of TTT methods, and future research should be compared within\nrespective protocols.\n","authors":["Yongyi Su","Xun Xu","Tianrui Li","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10856v1.pdf","comment":"Test-time training, Self-training. arXiv admin note: substantial text\n  overlap with arXiv:2206.02721"},{"id":"http://arxiv.org/abs/2302.09465v2","updated":"2023-03-20T04:07:09Z","published":"2023-02-19T03:19:40Z","title":"Stochastic Generative Flow Networks","summary":"  Generative Flow Networks (or GFlowNets for short) are a family of\nprobabilistic agents that learn to sample complex combinatorial structures\nthrough the lens of \"inference as control\". They have shown great potential in\ngenerating high-quality and diverse candidates from a given energy landscape.\nHowever, existing GFlowNets can be applied only to deterministic environments,\nand fail in more general tasks with stochastic dynamics, which can limit their\napplicability. To overcome this challenge, this paper introduces Stochastic\nGFlowNets, a new algorithm that extends GFlowNets to stochastic environments.\nBy decomposing state transitions into two steps, Stochastic GFlowNets isolate\nenvironmental stochasticity and learn a dynamics model to capture it. Extensive\nexperimental results demonstrate that Stochastic GFlowNets offer significant\nadvantages over standard GFlowNets as well as MCMC- and RL-based approaches, on\na variety of standard benchmarks with stochastic dynamics.\n","authors":["Ling Pan","Dinghuai Zhang","Moksh Jain","Longbo Huang","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2302.09465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10817v3","updated":"2023-03-20T03:39:51Z","published":"2022-04-22T16:53:39Z","title":"Reward Reports for Reinforcement Learning","summary":"  Building systems that are good for society in the face of complex societal\neffects requires a dynamic approach. Recent approaches to machine learning (ML)\ndocumentation have demonstrated the promise of discursive frameworks for\ndeliberation about these complexities. However, these developments have been\ngrounded in a static ML paradigm, leaving the role of feedback and\npost-deployment performance unexamined. Meanwhile, recent work in reinforcement\nlearning has shown that the effects of feedback and optimization objectives on\nsystem behavior can be wide-ranging and unpredictable. In this paper we sketch\na framework for documenting deployed and iteratively updated learning systems,\nwhich we call Reward Reports. Taking inspiration from various contributions to\nthe technical literature on reinforcement learning, we outline Reward Reports\nas living documents that track updates to design choices and assumptions behind\nwhat a particular automated system is optimizing for. They are intended to\ntrack dynamic phenomena arising from system deployment, rather than merely\nstatic properties of models or data. After presenting the elements of a Reward\nReport, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several\nothers for game-playing (DeepMind's MuZero), content recommendation\n(MovieLens), and traffic control (Project Flow) are included in the appendix.\n","authors":["Thomas Krendl Gilbert","Nathan Lambert","Sarah Dean","Tom Zick","Aaron Snoswell"],"pdf_url":"https://arxiv.org/pdf/2204.10817v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.02398v5","updated":"2023-03-20T03:15:14Z","published":"2021-10-05T23:07:12Z","title":"Approximate Newton policy gradient algorithms","summary":"  Policy gradient algorithms have been widely applied to Markov decision\nprocesses and reinforcement learning problems in recent years. Regularization\nwith various entropy functions is often used to encourage exploration and\nimprove stability. This paper proposes an approximate Newton method for the\npolicy gradient algorithm with entropy regularization. In the case of Shannon\nentropy, the resulting algorithm reproduces the natural policy gradient\nalgorithm. For other entropy functions, this method results in brand-new policy\ngradient algorithms. We prove that all these algorithms enjoy Newton-type\nquadratic convergence and that the corresponding gradient flow converges\nglobally to the optimal solution. We use synthetic and industrial-scale\nexamples to demonstrate that the proposed approximate Newton method typically\nconverges in single-digit iterations, often orders of magnitude faster than\nother state-of-the-art algorithms.\n","authors":["Haoya Li","Samarth Gupta","Hsiangfu Yu","Lexing Ying","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2110.02398v5.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.10840v1","updated":"2023-03-20T03:08:22Z","published":"2023-03-20T03:08:22Z","title":"Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for\n  Multi-View Reconstruction with Reflection","summary":"  Neural implicit surface learning has shown significant progress in multi-view\n3D reconstruction, where an object is represented by multilayer perceptrons\nthat provide continuous implicit surface representation and view-dependent\nradiance. However, current methods often fail to accurately reconstruct\nreflective surfaces, leading to severe ambiguity. To overcome this issue, we\npropose Ref-NeuS, which aims to reduce ambiguity by attenuating the importance\nof reflective surfaces. Specifically, we utilize an anomaly detector to\nestimate an explicit reflection score with the guidance of multi-view context\nto localize reflective surfaces. Afterward, we design a reflection-aware\nphotometric loss that adaptively reduces ambiguity by modeling rendered color\nas a Gaussian distribution, with the reflection score representing the\nvariance. We show that together with a reflection direction-dependent radiance,\nour model achieves high-quality surface reconstruction on reflective surfaces\nand outperforms the state-of-the-arts by a large margin. Besides, our model is\nalso comparable on general surfaces.\n","authors":["Wenhang Ge","Tao Hu","Haoyu Zhao","Shu Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.10840v1.pdf","comment":"Project webpage: https://g3956.github.io/"},{"id":"http://arxiv.org/abs/2303.10837v1","updated":"2023-03-20T02:44:35Z","published":"2023-03-20T02:44:35Z","title":"FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving\n  Federated Learning System","summary":"  Federated Learning (FL) enables machine learning model training on\ndistributed edge devices by aggregating local model updates rather than local\ndata. However, privacy concerns arise as the FL server's access to local model\nupdates can potentially reveal sensitive personal information by performing\nattacks like gradient inversion recovery. To address these concerns,\nprivacy-preserving methods, such as Homomorphic Encryption (HE)-based\napproaches, have been proposed. Despite HE's post-quantum security advantages,\nits applications suffer from impractical overheads. In this paper, we present\nFedML-HE, the first practical system for efficient HE-based secure federated\naggregation that provides a user/device-friendly deployment platform. FL-HE\nutilizes a novel universal overhead optimization scheme, significantly reducing\nboth computation and communication overheads during deployment while providing\ncustomizable privacy guarantees. Our optimized system demonstrates considerable\noverhead reduction, particularly for large models (e.g., ~10x reduction for\nHE-federated training of ResNet-50 and ~40x reduction for BERT), demonstrating\nthe potential for scalable HE-based FL deployment.\n","authors":["Weizhao Jin","Yuhang Yao","Shanshan Han","Carlee Joe-Wong","Srivatsan Ravi","Salman Avestimehr","Chaoyang He"],"pdf_url":"https://arxiv.org/pdf/2303.10837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10834v1","updated":"2023-03-20T02:40:16Z","published":"2023-03-20T02:40:16Z","title":"Object-Centric Slot Diffusion","summary":"  Despite remarkable recent advances, making object-centric learning work for\ncomplex natural scenes remains the main challenge. The recent success of\nadopting the transformer-based image generative model in object-centric\nlearning suggests that having a highly expressive image generator is crucial\nfor dealing with complex scenes. In this paper, inspired by this observation,\nwe aim to answer the following question: can we benefit from the other pillar\nof modern deep generative models, i.e., the diffusion models, for\nobject-centric learning and what are the pros and cons of such a model? To this\nend, we propose a new object-centric learning model, Latent Slot Diffusion\n(LSD). LSD can be seen from two perspectives. From the perspective of\nobject-centric learning, it replaces the conventional slot decoders with a\nlatent diffusion model conditioned on the object slots. Conversely, from the\nperspective of diffusion models, it is the first unsupervised compositional\nconditional diffusion model which, unlike traditional diffusion models, does\nnot require supervised annotation such as a text description to learn to\ncompose. In experiments on various object-centric tasks, including the FFHQ\ndataset for the first time in this line of research, we demonstrate that LSD\nsignificantly outperforms the state-of-the-art transformer-based decoder,\nparticularly when the scene is more complex. We also show a superior quality in\nunsupervised compositional generation.\n","authors":["Jindong Jiang","Fei Deng","Gautam Singh","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.10834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06405v2","updated":"2023-03-20T02:34:18Z","published":"2023-02-03T20:56:01Z","title":"An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of\n  Binary Neural Networks","summary":"  Binary Neural Networks (BNNs) are increasingly preferred over full-precision\nConvolutional Neural Networks(CNNs) to reduce the memory and computational\nrequirements of inference processing with minimal accuracy drop. BNNs convert\nCNN model parameters to 1-bit precision, allowing inference of BNNs to be\nprocessed with simple XNOR and bitcount operations. This makes BNNs amenable to\nhardware acceleration. Several photonic integrated circuits (PICs) based BNN\naccelerators have been proposed. Although these accelerators provide remarkably\nhigher throughput and energy efficiency than their electronic counterparts, the\nutilized XNOR and bitcount circuits in these accelerators need to be further\nenhanced to improve their area, energy efficiency, and throughput. This paper\naims to fulfill this need. For that, we invent a single-MRR-based optical XNOR\ngate (OXG). Moreover, we present a novel design of bitcount circuit which we\nrefer to as Photo-Charge Accumulator (PCA). We employ multiple OXGs in a\ncascaded manner using dense wavelength division multiplexing (DWDM) and connect\nthem to the PCA, to forge a novel Optical XNOR-Bitcount based Binary Neural\nNetwork Accelerator (OXBNN). Our evaluation for the inference of four modern\nBNNs indicates that OXBNN provides improvements of up to 62x and 7.6x in\nframes-per-second (FPS) and FPS/W (energy efficiency), respectively, on\ngeometric mean over two PIC-based BNN accelerators from prior work. We\ndeveloped a transaction-level, event-driven python-based simulator for\nevaluation of accelerators (https://github.com/uky-UCAT/B_ONN_SIM).\n","authors":["Sairam Sri Vatsavai","Venkata Sai Praneeth Karempudi","Ishan Thakkar"],"pdf_url":"https://arxiv.org/pdf/2302.06405v2.pdf","comment":"To Appear at IEEE ISQED 2023"},{"id":"http://arxiv.org/abs/2303.10828v1","updated":"2023-03-20T02:02:50Z","published":"2023-03-20T02:02:50Z","title":"Data Might be Enough: Bridge Real-World Traffic Signal Control Using\n  Offline Reinforcement Learning","summary":"  Applying reinforcement learning (RL) to traffic signal control (TSC) has\nbecome a promising solution. However, most RL-based methods focus solely on\noptimization within simulators and give little thought to deployment issues in\nthe real world. Online RL-based methods, which require interaction with the\nenvironment, are limited in their interactions with the real-world environment.\nAdditionally, acquiring an offline dataset for offline RL is challenging in the\nreal world. Moreover, most real-world intersections prefer a cyclical phase\nstructure. To address these challenges, we propose: (1) a cyclical offline\ndataset (COD), designed based on common real-world scenarios to facilitate easy\ncollection; (2) an offline RL model called DataLight, capable of learning\nsatisfactory control strategies from the COD; and (3) a method called Arbitrary\nTo Cyclical (ATC), which can transform most RL-based methods into cyclical\nsignal control. Extensive experiments using real-world datasets on simulators\ndemonstrate that: (1) DataLight outperforms most existing methods and achieves\ncomparable results with the best-performing method; (2) introducing ATC into\nsome recent RL-based methods achieves satisfactory performance; and (3) COD is\nreliable, with DataLight remaining robust even with a small amount of data.\nThese results suggest that the cyclical offline dataset might be enough for\noffline RL for TSC. Our proposed methods make significant contributions to the\nTSC field and successfully bridge the gap between simulation experiments and\nreal-world applications. Our code is released on Github.\n","authors":["Liang Zhang","Jianming Deng"],"pdf_url":"https://arxiv.org/pdf/2303.10828v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.09863v2","updated":"2023-03-20T01:30:30Z","published":"2023-03-17T10:01:32Z","title":"Deep Nonparametric Estimation of Intrinsic Data Structures by Chart\n  Autoencoders: Generalization Error and Robustness","summary":"  Autoencoders have demonstrated remarkable success in learning low-dimensional\nlatent features of high-dimensional data across various applications. Assuming\nthat data are sampled near a low-dimensional manifold, we employ chart\nautoencoders, which encode data into low-dimensional latent features on a\ncollection of charts, preserving the topology and geometry of the data\nmanifold. Our paper establishes statistical guarantees on the generalization\nerror of chart autoencoders, and we demonstrate their denoising capabilities by\nconsidering $n$ noisy training samples, along with their noise-free\ncounterparts, on a $d$-dimensional manifold. By training autoencoders, we show\nthat chart autoencoders can effectively denoise the input data with normal\nnoise. We prove that, under proper network architectures, chart autoencoders\nachieve a squared generalization error in the order of $\\displaystyle\nn^{-\\frac{2}{d+2}}\\log^4 n$, which depends on the intrinsic dimension of the\nmanifold and only weakly depends on the ambient dimension and noise level. We\nfurther extend our theory on data with noise containing both normal and\ntangential components, where chart autoencoders still exhibit a denoising\neffect for the normal component. As a special case, our theory also applies to\nclassical autoencoders, as long as the data manifold has a global\nparametrization. Our results provide a solid theoretical foundation for the\neffectiveness of autoencoders, which is further validated through several\nnumerical experiments.\n","authors":["Hao Liu","Alex Havrilla","Rongjie Lai","Wenjing Liao"],"pdf_url":"https://arxiv.org/pdf/2303.09863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10211v2","updated":"2023-03-20T00:37:10Z","published":"2022-10-18T23:31:15Z","title":"A Catch-22 of Reservoir Computing","summary":"  Reservoir Computing (RC) is a simple and efficient model-free framework for\nforecasting the behavior of nonlinear dynamical systems from data. Here, we\nshow that there exist commonly-studied systems for which leading RC frameworks\nstruggle to learn the dynamics unless key information about the underlying\nsystem is already known. We focus on the important problem of basin prediction\n-- determining which attractor a system will converge to from its initial\nconditions. First, we show that the predictions of standard RC models (echo\nstate networks) depend critically on warm-up time, requiring a warm-up\ntrajectory containing almost the entire transient in order to identify the\ncorrect attractor even after being trained with optimal hyperparameters.\nAccordingly, we turn to Next-Generation Reservoir Computing (NGRC), an\nattractive variant of RC that requires negligible warm-up time. By\nincorporating the exact nonlinearities in the original equations, we show that\nNGRC can accurately reconstruct intricate and high-dimensional basins of\nattraction, even with sparse training data (e.g., a single transient\ntrajectory). Yet, a tiny uncertainty on the exact nonlinearity can already\nbreak NGRC, rendering the prediction accuracy no better than chance. Our\nresults highlight the challenges faced by data-driven methods in learning the\ndynamics of multistable systems and suggest potential avenues to make these\napproaches more robust.\n","authors":["Yuanzhao Zhang","Sean P. Cornelius"],"pdf_url":"https://arxiv.org/pdf/2210.10211v2.pdf","comment":"added new results on standard RC; expanded intro and discussion"},{"id":"http://arxiv.org/abs/2303.10800v1","updated":"2023-03-20T00:24:05Z","published":"2023-03-20T00:24:05Z","title":"A Global Model Approach to Robust Few-Shot SAR Automatic Target\n  Recognition","summary":"  In real-world scenarios, it may not always be possible to collect hundreds of\nlabeled samples per class for training deep learning-based SAR Automatic Target\nRecognition (ATR) models. This work specifically tackles the few-shot SAR ATR\nproblem, where only a handful of labeled samples may be available to support\nthe task of interest. Our approach is composed of two stages. In the first, a\nglobal representation model is trained via self-supervised learning on a large\npool of diverse and unlabeled SAR data. In the second stage, the global model\nis used as a fixed feature extractor and a classifier is trained to partition\nthe feature space given the few-shot support samples, while simultaneously\nbeing calibrated to detect anomalous inputs. Unlike competing approaches which\nrequire a pristine labeled dataset for pretraining via meta-learning, our\napproach learns highly transferable features from unlabeled data that have\nlittle-to-no relation to the downstream task. We evaluate our method in\nstandard and extended MSTAR operating conditions and find it to achieve high\naccuracy and robust out-of-distribution detection in many different few-shot\nsettings. Our results are particularly significant because they show the merit\nof a global model approach to SAR ATR, which makes minimal assumptions, and\nprovides many axes for extendability.\n","authors":["Nathan Inkawhich"],"pdf_url":"https://arxiv.org/pdf/2303.10800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03660v3","updated":"2023-03-20T00:22:11Z","published":"2021-12-07T12:43:05Z","title":"A generalization gap estimation for overparameterized models via the\n  Langevin functional variance","summary":"  This paper discusses the estimation of the generalization gap, the difference\nbetween generalization performance and training performance, for\noverparameterized models including neural networks. We first show that a\nfunctional variance, a key concept in defining a widely-applicable information\ncriterion, characterizes the generalization gap even in overparameterized\nsettings where a conventional theory cannot be applied. As the computational\ncost of the functional variance is expensive for the overparameterized models,\nwe propose an efficient approximation of the function variance, the Langevin\napproximation of the functional variance (Langevin FV). This method leverages\nonly the $1$st-order gradient of the squared loss function, without referencing\nthe $2$nd-order gradient; this ensures that the computation is efficient and\nthe implementation is consistent with gradient-based optimization algorithms.\nWe demonstrate the Langevin FV numerically by estimating the generalization\ngaps of overparameterized linear regression and non-linear neural network\nmodels, containing more than a thousand of parameters therein.\n","authors":["Akifumi Okuno","Keisuke Yano"],"pdf_url":"https://arxiv.org/pdf/2112.03660v3.pdf","comment":"40 pages, no figure, accepted to Journal of Computational and\n  Graphical Statistics"},{"id":"http://arxiv.org/abs/2206.04119v2","updated":"2023-03-20T00:22:03Z","published":"2022-06-08T18:35:08Z","title":"Diffusion probabilistic modeling of protein backbones in 3D for the\n  motif-scaffolding problem","summary":"  Construction of a scaffold structure that supports a desired motif,\nconferring protein function, shows promise for the design of vaccines and\nenzymes. But a general solution to this motif-scaffolding problem remains open.\nCurrent machine-learning techniques for scaffold design are either limited to\nunrealistically small scaffolds (up to length 20) or struggle to produce\nmultiple diverse scaffolds. We propose to learn a distribution over diverse and\nlonger protein backbone structures via an E(3)-equivariant graph neural\nnetwork. We develop SMCDiff to efficiently sample scaffolds from this\ndistribution conditioned on a given motif; our algorithm is the first to\ntheoretically guarantee conditional samples from a diffusion model in the\nlarge-compute limit. We evaluate our designed backbones by how well they align\nwith AlphaFold2-predicted structures. We show that our method can (1) sample\nscaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for\na fixed motif.\n","authors":["Brian L. Trippe","Jason Yim","Doug Tischer","David Baker","Tamara Broderick","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2206.04119v2.pdf","comment":"Appearing in ICLR 2023. Code available:\n  github.com/blt2114/ProtDiff_SMCDiff"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.12261v2","updated":"2023-03-20T13:34:48Z","published":"2022-07-06T13:56:48Z","title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation\n  Approach for Multimodal Conversational Emotion Recognition","summary":"  Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.12261v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.09858v2","updated":"2023-03-20T13:00:49Z","published":"2023-03-17T09:37:41Z","title":"MedLocker: A Transferable Adversarial Watermarking for Preventing\n  Unauthorized Analysis of Medical Image Dataset","summary":"  The collection of medical image datasets is a demanding and laborious process\nthat requires significant resources. Furthermore, these medical datasets may\ncontain personally identifiable information, necessitating measures to ensure\nthat unauthorized access is prevented. Failure to do so could violate the\nintellectual property rights of the dataset owner and potentially compromise\nthe privacy of patients. As a result, safeguarding medical datasets and\npreventing unauthorized usage by AI diagnostic models is a pressing challenge.\nTo address this challenge, we propose a novel visible adversarial watermarking\nmethod for medical image copyright protection, called MedLocker. Our approach\ninvolves continuously optimizing the position and transparency of a watermark\nlogo, which reduces the performance of the target model, leading to incorrect\npredictions. Importantly, we ensure that our method minimizes the impact on\nclinical visualization by constraining watermark positions using semantical\nmasks (WSM), which are bounding boxes of lesion regions based on semantic\nsegmentation. To ensure the transferability of the watermark across different\nmodels, we verify the cross-model transferability of the watermark generated on\na single model. Additionally, we generate a unique watermark parameter list\neach time, which can be used as a certification to verify the authorization. We\nevaluate the performance of MedLocker on various mainstream backbones and\nvalidate the feasibility of adversarial watermarking for copyright protection\non two widely-used diabetic retinopathy detection datasets. Our results\ndemonstrate that MedLocker can effectively protect the copyright of medical\ndatasets and prevent unauthorized users from analyzing medical images with AI\ndiagnostic models.\n","authors":["Bangzheng Pu","Xingxing Wei","Shiji Zhao","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08453v2","updated":"2023-03-20T10:52:26Z","published":"2023-02-16T17:56:08Z","title":"T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for\n  Text-to-Image Diffusion Models","summary":"  The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out\" the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.\n","authors":["Chong Mou","Xintao Wang","Liangbin Xie","Yanze Wu","Jian Zhang","Zhongang Qi","Ying Shan","Xiaohu Qie"],"pdf_url":"https://arxiv.org/pdf/2302.08453v2.pdf","comment":"Tech Report. GitHub: https://github.com/TencentARC/T2I-Adapter"},{"id":"http://arxiv.org/abs/2303.10961v1","updated":"2023-03-20T09:37:41Z","published":"2023-03-20T09:37:41Z","title":"LFACon: Introducing Anglewise Attention to No-Reference Quality\n  Assessment in Light Field Space","summary":"  Light field imaging can capture both the intensity information and the\ndirection information of light rays. It naturally enables a\nsix-degrees-of-freedom viewing experience and deep user engagement in virtual\nreality. Compared to 2D image assessment, light field image quality assessment\n(LFIQA) needs to consider not only the image quality in the spatial domain but\nalso the quality consistency in the angular domain. However, there is a lack of\nmetrics to effectively reflect the angular consistency and thus the angular\nquality of a light field image (LFI). Furthermore, the existing LFIQA metrics\nsuffer from high computational costs due to the excessive data volume of LFIs.\nIn this paper, we propose a novel concept of \"anglewise attention\" by\nintroducing a multihead self-attention mechanism to the angular domain of an\nLFI. This mechanism better reflects the LFI quality. In particular, we propose\nthree new attention kernels, including anglewise self-attention, anglewise grid\nattention, and anglewise central attention. These attention kernels can realize\nangular self-attention, extract multiangled features globally or selectively,\nand reduce the computational cost of feature extraction. By effectively\nincorporating the proposed kernels, we further propose our light field\nattentional convolutional neural network (LFACon) as an LFIQA metric. Our\nexperimental results show that the proposed LFACon metric significantly\noutperforms the state-of-the-art LFIQA metrics. For the majority of distortion\ntypes, LFACon attains the best performance with lower complexity and less\ncomputational time.\n","authors":["Qiang Qu","Xiaoming Chen","Yuk Ying Chung","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2303.10961v1.pdf","comment":"Accepted for IEEE VR 2023 (TVCG Special Issues) (Early Access)"},{"id":"http://arxiv.org/abs/2207.06983v3","updated":"2023-03-20T07:01:49Z","published":"2022-07-14T15:06:37Z","title":"Multitrack Music Transformer","summary":"  Existing approaches for generating multitrack music with transformer models\nhave been limited in terms of the number of instruments, the length of the\nmusic segments and slow inference. This is partly due to the memory\nrequirements of the lengthy input sequences necessitated by existing\nrepresentations. In this work, we propose a new multitrack music representation\nthat allows a diverse set of instruments while keeping a short sequence length.\nOur proposed Multitrack Music Transformer (MMT) achieves comparable performance\nwith state-of-the-art systems, landing in between two recently proposed models\nin a subjective listening test, while achieving substantial speedups and memory\nreductions over both, making the method attractive for real time improvisation\nor near real time creative applications. Further, we propose a new measure for\nanalyzing musical self-attention and show that the trained model attends more\nto notes that form a consonant interval with the current note and to notes that\nare 4N beats away from the current step.\n","authors":["Hao-Wen Dong","Ke Chen","Shlomo Dubnov","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2207.06983v3.pdf","comment":"Accepted by ICASSP 2023. Audio samples available at\n  https://salu133445.github.io/mmt/ . Source code available at\n  https://github.com/salu133445/mmt"},{"id":"http://arxiv.org/abs/2303.08536v2","updated":"2023-03-20T07:01:45Z","published":"2023-03-15T11:29:36Z","title":"Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring","summary":"  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n","authors":["Joanna Hong","Minsu Kim","Jeongsoo Choi","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2303.08536v2.pdf","comment":"Accepted at CVPR 2023. Implementation available:\n  https://github.com/joannahong/AV-RelScore"},{"id":"http://arxiv.org/abs/2209.05653v4","updated":"2023-03-20T00:53:24Z","published":"2022-09-13T00:01:23Z","title":"Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action\n  Segmentation in Videos","summary":"  Video action segmentation and recognition tasks have been widely applied in\nmany fields. Most previous studies employ large-scale, high computational\nvisual models to understand videos comprehensively. However, few studies\ndirectly employ the graph model to reason about the video. The graph model\nprovides the benefits of fewer parameters, low computational cost, a large\nreceptive field, and flexible neighborhood message aggregation. In this paper,\nwe present a graph-based method named Semantic2Graph, to turn the video action\nsegmentation and recognition problem into node classification of graphs. To\npreserve fine-grained relations in videos, we construct the graph structure of\nvideos at the frame-level and design three types of edges: temporal, semantic,\nand self-loop. We combine visual, structural, and semantic features as node\nattributes. Semantic edges are used to model long-term spatio-temporal\nrelations, while the semantic features are the embedding of the label-text\nbased on the textual prompt. A Graph Neural Networks (GNNs) model is used to\nlearn multi-modal feature fusion. Experimental results show that Semantic2Graph\nachieves improvement on GTEA and 50Salads, compared to the state-of-the-art\nresults. Multiple ablation experiments further confirm the effectiveness of\nsemantic features in improving model performance, and semantic edges enable\nSemantic2Graph to capture long-term dependencies at a low cost.\n","authors":["Junbin Zhang","Pei-Hsuan Tsai","Meng-Hsun Tsai"],"pdf_url":"https://arxiv.org/pdf/2209.05653v4.pdf","comment":"13 pages, 3 figures, 9 tables. This paper was submitted to Springer"}]},"2023-03-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10782v1","updated":"2023-03-19T22:15:05Z","published":"2023-03-19T22:15:05Z","title":"On the Importance of Signer Overlap for Sign Language Detection","summary":"  Sign language detection, identifying if someone is signing or not, is\nbecoming crucially important for its applications in remote conferencing\nsoftware and for selecting useful sign data for training sign language\nrecognition or translation tasks. We argue that the current benchmark data sets\nfor sign language detection estimate overly positive results that do not\ngeneralize well due to signer overlap between train and test partitions. We\nquantify this with a detailed analysis of the effect of signer overlap on\ncurrent sign detection benchmark data sets. Comparing accuracy with and without\noverlap on the DGS corpus and Signing in the Wild, we observed a relative\ndecrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose\nnew data set partitions that are free of overlap and allow for more realistic\nperformance assessment. We hope this work will contribute to improving the\naccuracy and generalization of sign language detection systems.\n","authors":["Abhilash Pal","Stephan Huber","Cyrine Chaabani","Alessandro Manzotti","Oscar Koller"],"pdf_url":"https://arxiv.org/pdf/2303.10782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04088v2","updated":"2023-03-19T20:52:21Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.10699v1","updated":"2023-03-19T16:07:42Z","published":"2023-03-19T16:07:42Z","title":"FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual\n  Question Answering","summary":"  The widely used Fact-based Visual Question Answering (FVQA) dataset contains\nvisually-grounded questions that require information retrieval using common\nsense knowledge graphs to answer. It has been observed that the original\ndataset is highly imbalanced and concentrated on a small portion of its\nassociated knowledge graph. We introduce FVQA 2.0 which contains adversarial\nvariants of test questions to address this imbalance. We show that systems\ntrained with the original FVQA train sets can be vulnerable to adversarial\nsamples and we demonstrate an augmentation scheme to reduce this vulnerability\nwithout human annotations.\n","authors":["Weizhe Lin","Zhilin Wang","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2303.10699v1.pdf","comment":"Accepted to EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.10659v1","updated":"2023-03-19T13:47:56Z","published":"2023-03-19T13:47:56Z","title":"COVID-19 event extraction from Twitter via extractive question answering\n  with continuous prompts","summary":"  As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.\n","authors":["Yuhang Jiang","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2303.10659v1.pdf","comment":"Accepted to appear in MEDINFO 2023. Dataset:\n  https://github.com/viczong/extract_COVID19_events_from_Twitter; Code:\n  https://github.com/bionlproc/twitter-covid-QA-extraction"},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2302.06337v3","updated":"2023-03-19T13:01:47Z","published":"2023-02-13T13:14:23Z","title":"Learning from Noisy Crowd Labels with Logics","summary":"  This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n","authors":["Zhijun Chen","Hailong Sun","Haoqian He","Pengpeng Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06337v3.pdf","comment":"12 pages, 7 figures, accepted by ICDE-2023"},{"id":"http://arxiv.org/abs/2212.12061v2","updated":"2023-03-19T12:10:02Z","published":"2022-12-22T22:27:26Z","title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical\n  Classification","summary":"  This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n","authors":["Alina Petukhova","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2212.12061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08745v3","updated":"2023-03-19T11:53:20Z","published":"2023-01-20T08:51:36Z","title":"Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine","summary":"  This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well and\nshow minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. For distant\nlanguages, we explore an interesting strategy named $\\mathbf{pivot~prompting}$\nthat asks ChatGPT to translate the source sentence into a high-resource pivot\nlanguage before into the target language, which improves the translation\nperformance significantly. As for the translation robustness, ChatGPT does not\nperform as well as the commercial systems on biomedical abstracts or Reddit\ncomments but exhibits good results on spoken language. With the launch of the\nGPT-4 engine, the translation performance of ChatGPT is significantly boosted,\nbecoming comparable to commercial translation products, even for distant\nlanguages. In other words,\n$\\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n","authors":["Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Xing Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2301.08745v3.pdf","comment":"8 pages; added GPT-3 data statistics reference; added GPT-4 results"},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2303.10612v1","updated":"2023-03-19T09:24:48Z","published":"2023-03-19T09:24:48Z","title":"Bangla Grammatical Error Detection Using T5 Transformer Model","summary":"  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n","authors":["H. A. Z. Sameen Shahgir","Khondker Salman Sayeed"],"pdf_url":"https://arxiv.org/pdf/2303.10612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10606v1","updated":"2023-03-19T08:57:39Z","published":"2023-03-19T08:57:39Z","title":"CTRAN: CNN-Transformer-based Network for Natural Language Understanding","summary":"  Intent-detection and slot-filling are the two main tasks in natural language\nunderstanding. In this study, we propose CTRAN, a novel encoder-decoder\nCNN-Transformer-based architecture for intent-detection and slot-filling. In\nthe encoder, we use BERT, followed by several convolutional layers, and\nrearrange the output using window feature sequence. We use stacked Transformer\nencoders after the window feature sequence. For the intent-detection decoder,\nwe utilize self-attention followed by a linear layer. In the slot-filling\ndecoder, we introduce the aligned Transformer decoder, which utilizes a zero\ndiagonal mask, aligning output tags with input tokens. We apply our network on\nATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on\nboth datasets. Furthermore, we incorporate the language model as word\nembeddings, and show that this strategy yields a better result when compared to\nthe language model as an encoder.\n","authors":["Mehrdad Rafiepour","Javad Salimi Sartakhti"],"pdf_url":"https://arxiv.org/pdf/2303.10606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10583v1","updated":"2023-03-19T06:09:52Z","published":"2023-03-19T06:09:52Z","title":"Toward Artificial Empathy for Human-Centered Design: A Framework","summary":"  In the early stages of the design process, designers explore opportunities by\ndiscovering unmet needs and developing innovative concepts as potential\nsolutions. From a human-centered design perspective, designers must develop\nempathy with people to truly understand their needs. However, developing\nempathy is a complex and subjective process that relies heavily on the\ndesigner's empathetic capability. Therefore, the development of empathetic\nunderstanding is intuitive, and the discovery of underlying needs is often\nserendipitous. This paper aims to provide insights from artificial intelligence\nresearch to indicate the future direction of AI-driven human-centered design,\ntaking into account the essential role of empathy. Specifically, we conduct an\ninterdisciplinary investigation of research areas such as data-driven user\nstudies, empathetic understanding development, and artificial empathy. Based on\nthis foundation, we discuss the role that artificial empathy can play in\nhuman-centered design and propose an artificial empathy framework for\nhuman-centered design. Building on the mechanisms behind empathy and insights\nfrom empathetic design research, the framework aims to break down the rather\ncomplex and subjective concept of empathy into components and modules that can\npotentially be modeled computationally. Furthermore, we discuss the expected\nbenefits of developing such systems and identify current research gaps to\nencourage future research efforts.\n","authors":["Qihao Zhu","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2303.10583v1.pdf","comment":"Submitted to IDETC2023"},{"id":"http://arxiv.org/abs/2303.10573v1","updated":"2023-03-19T05:22:12Z","published":"2023-03-19T05:22:12Z","title":"Extracting Incidents, Effects, and Requested Advice from MeToo Posts","summary":"  Survivors of sexual harassment frequently share their experiences on social\nmedia, revealing their feelings and emotions and seeking advice. We observed\nthat on Reddit, survivors regularly share long posts that describe a\ncombination of (i) a sexual harassment incident, (ii) its effect on the\nsurvivor, including their feelings and emotions, and (iii) the advice being\nsought. We term such posts MeToo posts, even though they may not be so tagged\nand may appear in diverse subreddits. A prospective helper (such as a counselor\nor even a casual reader) must understand a survivor's needs from such posts.\nBut long posts can be time-consuming to read and respond to.\n  Accordingly, we address the problem of extracting key information from a long\nMeToo post. We develop a natural language-based model to identify sentences\nfrom a post that describe any of the above three categories.\n  On ten-fold cross-validation of a dataset, our model achieves a macro F1\nscore of 0.82.\n  In addition, we contribute MeThree, a dataset comprising 8,947 labeled\nsentences extracted from Reddit posts. We apply the LIWC-22 toolkit on MeThree\nto understand how different language patterns in sentences of the three\ncategories can reveal differences in emotional tone, authenticity, and other\naspects.\n","authors":["Vaibhav Garg","Jiaqing Yuan","Rujie Xi","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2303.10573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10560v1","updated":"2023-03-19T04:05:10Z","published":"2023-03-19T04:05:10Z","title":"How People Respond to the COVID-19 Pandemic on Twitter: A Comparative\n  Analysis of Emotional Expressions from US and India","summary":"  The COVID-19 pandemic has claimed millions of lives worldwide and elicited\nheightened emotions. This study examines the expression of various emotions\npertaining to COVID-19 in the United States and India as manifested in over 54\nmillion tweets, covering the fifteen-month period from February 2020 through\nApril 2021, a period which includes the beginnings of the huge and disastrous\nincrease in COVID-19 cases that started to ravage India in March 2021.\nEmploying pre-trained emotion analysis and topic modeling algorithms, four\ndistinct types of emotions (fear, anger, happiness, and sadness) and their\ntime- and location-associated variations were examined. Results revealed\nsignificant country differences and temporal changes in the relative\nproportions of fear, anger, and happiness, with fear declining and anger and\nhappiness fluctuating in 2020 until new situations over the first four months\nof 2021 reversed the trends. Detected differences are discussed briefly in\nterms of the latent topics revealed and through the lens of appraisal theories\nof emotions, and the implications of the findings are discussed.\n","authors":["Brandon Siyuan Loh","Raj Kumar Gupta","Ajay Vishwanath","Andrew Ortony","Yinping Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10560v1.pdf","comment":"13 pages, 3 figures, 1 table, 2 appendices"},{"id":"http://arxiv.org/abs/2210.08817v2","updated":"2023-03-19T03:39:16Z","published":"2022-10-17T08:06:56Z","title":"PACIFIC: Towards Proactive Conversational Question Answering over\n  Tabular and Textual Data in Finance","summary":"  To facilitate conversational question answering (CQA) over hybrid contexts in\nfinance, we present a new dataset, named PACIFIC. Compared with existing CQA\ndatasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical\nreasoning, and (iii) hybrid context of tables and text. A new task is defined\naccordingly to study Proactive Conversational Question Answering (PCQA), which\ncombines clarification question generation and CQA. In addition, we propose a\nnovel method, namely UniPCQA, to adapt a hybrid format of input and output\ncontent in PCQA into the Seq2Seq problem, including the reformulation of the\nnumerical reasoning process as code generation. UniPCQA performs multi-task\nlearning over all sub-tasks in PCQA and incorporates a simple ensemble strategy\nto alleviate the error propagation issue in the multi-task learning by\ncross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC\ndataset with extensive baselines and provide comprehensive evaluations on each\nsub-task of PCQA.\n","authors":["Yang Deng","Wenqiang Lei","Wenxuan Zhang","Wai Lam","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2210.08817v2.pdf","comment":"Accepted by EMNLP 2022 (main conference)"},{"id":"http://arxiv.org/abs/2303.10527v1","updated":"2023-03-19T01:40:31Z","published":"2023-03-19T01:40:31Z","title":"Two Kinds of Recall","summary":"  It is an established assumption that pattern-based models are good at\nprecision, while learning based models are better at recall. But is that really\nthe case? I argue that there are two kinds of recall: d-recall, reflecting\ndiversity, and e-recall, reflecting exhaustiveness. I demonstrate through\nexperiments that while neural methods are indeed significantly better at\nd-recall, it is sometimes the case that pattern-based methods are still\nsubstantially better at e-recall. Ideal methods should aim for both kinds, and\nthis ideal should in turn be reflected in our evaluations.\n","authors":["Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2303.10527v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.01994v2","updated":"2023-03-19T20:22:39Z","published":"2023-03-03T15:06:03Z","title":"Discovery and Recognition of Formula Concepts using Machine Learning","summary":"  Citation-based Information Retrieval (IR) methods for scientific documents\nhave proven effective for IR applications, such as Plagiarism Detection or\nLiterature Recommender Systems in academic disciplines that use many\nreferences. In science, technology, engineering, and mathematics, researchers\noften employ mathematical concepts through formula notation to refer to prior\nknowledge. Our long-term goal is to generalize citation-based IR methods and\napply this generalized method to both classical references and mathematical\nconcepts. In this paper, we suggest how mathematical formulas could be cited\nand define a Formula Concept Retrieval task with two subtasks: Formula Concept\nDiscovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the\ndefinition and exploration of a 'Formula Concept' that names bundled equivalent\nrepresentations of a formula, FCR is designed to match a given formula to a\nprior assigned unique mathematical concept identifier. We present machine\nlearning-based approaches to address the FCD and FCR tasks. We then evaluate\nthese approaches on a standardized test collection (NTCIR arXiv dataset). Our\nFCD approach yields a precision of 68% for retrieving equivalent\nrepresentations of frequent formulas and a recall of 72% for extracting the\nformula name from the surrounding text. FCD and FCR enable the citation of\nformulas within mathematical documents and facilitate semantic search and\nquestion answering as well as document similarity assessments for plagiarism\ndetection or recommender systems.\n","authors":["Philipp Scharpf","Moritz Schubotz","Howard S. Cohl","Corinna Breitinger","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.01994v2.pdf","comment":"Accepted by Scientometrics (Springer) journal"},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2205.11857v2","updated":"2023-03-19T10:19:31Z","published":"2022-05-24T07:33:28Z","title":"Comprehensive Privacy Analysis on Federated Recommender System against\n  Attribute Inference Attacks","summary":"  In recent years, recommender systems are crucially important for the delivery\nof personalized services that satisfy users' preferences. With personalized\nrecommendation services, users can enjoy a variety of recommendations such as\nmovies, books, ads, restaurants, and more. Despite the great benefits,\npersonalized recommendations typically require the collection of personal data\nfor user modelling and analysis, which can make users susceptible to attribute\ninference attacks. Specifically, the vulnerability of existing centralized\nrecommenders under attribute inference attacks leaves malicious attackers a\nbackdoor to infer users' private attributes, as the systems remember\ninformation of their training data (i.e., interaction data and side\ninformation). An emerging practice is to implement recommender systems in the\nfederated setting, which enables all user devices to collaboratively learn a\nshared global recommender while keeping all the training data on device.\nHowever, the privacy issues in federated recommender systems have been rarely\nexplored. In this paper, we first design a novel attribute inference attacker\nto perform a comprehensive privacy analysis of the state-of-the-art federated\nrecommender models. The experimental results show that the vulnerability of\neach model component against attribute inference attack is varied, highlighting\nthe need for new defense approaches. Therefore, we propose a novel adaptive\nprivacy-preserving approach to protect users' sensitive data in the presence of\nattribute inference attacks and meanwhile maximize the recommendation accuracy.\nExtensive experimental results on two real-world datasets validate the superior\nperformance of our model on both recommendation effectiveness and resistance to\ninference attacks.\n","authors":["Shijie Zhang","Wei Yuan","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2205.11857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10539v1","updated":"2023-03-19T02:33:19Z","published":"2023-03-19T02:33:19Z","title":"Textless Speech-to-Music Retrieval Using Emotion Similarity","summary":"  We introduce a framework that recommends music based on the emotions of\nspeech. In content creation and daily life, speech contains information about\nhuman emotions, which can be enhanced by music. Our framework focuses on a\ncross-domain retrieval system to bridge the gap between speech and music via\nemotion labels. We explore different speech representations and report their\nimpact on different speech types, including acting voice and wake-up words. We\nalso propose an emotion similarity regularization term in cross-domain\nretrieval tasks. By incorporating the regularization term into training,\nsimilar speech-and-music pairs in the emotion space are closer in the joint\nembedding space. Our comprehensive experimental results show that the proposed\nmodel is effective in textless speech-to-music retrieval.\n","authors":["SeungHeon Doh","Minz Won","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2303.10539v1.pdf","comment":"To Appear IEEE ICASSP 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02218v2","updated":"2023-03-19T23:14:12Z","published":"2022-11-04T02:05:58Z","title":"Fully Bayesian inference for latent variable Gaussian process models","summary":"  Real engineering and scientific applications often involve one or more\nqualitative inputs. Standard Gaussian processes (GPs), however, cannot directly\naccommodate qualitative inputs. The recently introduced latent variable\nGaussian process (LVGP) overcomes this issue by first mapping each qualitative\nfactor to underlying latent variables (LVs), and then uses any standard GP\ncovariance function over these LVs. The LVs are estimated similarly to the\nother GP hyperparameters through maximum likelihood estimation, and then\nplugged into the prediction expressions. However, this plug-in approach will\nnot account for uncertainty in estimation of the LVs, which can be significant\nespecially with limited training data. In this work, we develop a fully\nBayesian approach for the LVGP model and for visualizing the effects of the\nqualitative inputs via their LVs. We also develop approximations for scaling up\nLVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct\nnumerical studies comparing plug-in inference against fully Bayesian inference\nover a few engineering models and material design applications. In contrast to\nprevious studies on standard GP modeling that have largely concluded that a\nfully Bayesian treatment offers limited improvements, our results show that for\nLVGP modeling it offers significant improvements in prediction accuracy and\nuncertainty quantification over the plug-in approach.\n","authors":["Suraj Yerramilli","Akshay Iyer","Wei Chen","Daniel W. Apley"],"pdf_url":"https://arxiv.org/pdf/2211.02218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10789v1","updated":"2023-03-19T23:00:41Z","published":"2023-03-19T23:00:41Z","title":"A hybrid CNN-RNN approach for survival analysis in a Lung Cancer\n  Screening study","summary":"  In this study, we present a hybrid CNN-RNN approach to investigate long-term\nsurvival of subjects in a lung cancer screening study. Subjects who died of\ncardiovascular and respiratory causes were identified whereby the CNN model was\nused to capture imaging features in the CT scans and the RNN model was used to\ninvestigate time series and thus global information. The models were trained on\nsubjects who underwent cardiovascular and respiratory deaths and a control\ncohort matched to participant age, gender, and smoking history. The combined\nmodel can achieve an AUC of 0.76 which outperforms humans at cardiovascular\nmortality prediction. The corresponding F1 and Matthews Correlation Coefficient\nare 0.63 and 0.42 respectively. The generalisability of the model is further\nvalidated on an 'external' cohort. The same models were applied to survival\nanalysis with the Cox Proportional Hazard model. It was demonstrated that\nincorporating the follow-up history can lead to improvement in survival\nprediction. The Cox neural network can achieve an IPCW C-index of 0.75 on the\ninternal dataset and 0.69 on an external dataset. Delineating imaging features\nassociated with long-term survival can help focus preventative interventions\nappropriately, particularly for under-recognised pathologies thereby\npotentially reducing patient morbidity.\n","authors":["Yaozhi Lu","Shahab Aslani","An Zhao","Ahmed Shahin","David Barber","Mark Emberton","Daniel C. Alexander","Joseph Jacob"],"pdf_url":"https://arxiv.org/pdf/2303.10789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06015v2","updated":"2023-03-19T22:36:28Z","published":"2023-02-12T22:12:35Z","title":"A Theoretical Understanding of Shallow Vision Transformers: Learning,\n  Generalization, and Sample Complexity","summary":"  Vision Transformers (ViTs) with self-attention modules have recently achieved\ngreat empirical success in many vision tasks. Due to non-convex interactions\nacross layers, however, theoretical learning and generalization analysis is\nmostly elusive. Based on a data model characterizing both label-relevant and\nlabel-irrelevant tokens, this paper provides the first theoretical analysis of\ntraining a shallow ViT, i.e., one self-attention layer followed by a two-layer\nperceptron, for a classification task. We characterize the sample complexity to\nachieve a zero generalization error. Our sample complexity bound is positively\ncorrelated with the inverse of the fraction of label-relevant tokens, the token\nnoise level, and the initial model error. We also prove that a training process\nusing stochastic gradient descent (SGD) leads to a sparse attention map, which\nis a formal verification of the general intuition about the success of\nattention. Moreover, this paper indicates that a proper token sparsification\ncan improve the test performance by removing label-irrelevant and/or noisy\ntokens, including spurious correlations. Empirical experiments on synthetic\ndata and CIFAR-10 dataset justify our theoretical results and generalize to\ndeeper ViTs.\n","authors":["Hongkang Li","Meng Wang","Sijia Liu","Pin-yu Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02393v2","updated":"2023-03-19T22:30:58Z","published":"2023-03-04T11:53:33Z","title":"Seq-HyGAN: Sequence Classification via Hypergraph Attention Network","summary":"  Sequence classification has a wide range of real-world applications in\ndifferent domains, such as genome classification in health and anomaly\ndetection in business. However, the lack of explicit features in sequence data\nmakes it difficult for machine learning models. While Neural Network (NN)\nmodels address this with learning features automatically, they are limited to\ncapturing adjacent structural connections and ignore global, higher-order\ninformation between the sequences. To address these challenges in the sequence\nclassification problems, we propose a novel Hypergraph Attention Network model,\nnamely Seq-HyGAN. To capture the complex structural similarity between sequence\ndata, we first create a hypergraph where the sequences are depicted as\nhyperedges and subsequences extracted from sequences are depicted as nodes.\nAdditionally, we introduce an attention-based Hypergraph Neural Network model\nthat utilizes a two-level attention mechanism. This model generates a sequence\nrepresentation as a hyperedge while simultaneously learning the crucial\nsubsequences for each sequence. We conduct extensive experiments on four data\nsets to assess and compare our model with several state-of-the-art methods.\nExperimental results demonstrate that our proposed Seq-HyGAN model can\neffectively classify sequence data and significantly outperform the baselines.\nWe also conduct case studies to investigate the contribution of each module in\nSeq-HyGAN.\n","authors":["Khaled Mohammed Saifuddin","Corey May","Farhan Tanvir","Muhammad Ifte Khairul Islam","Esra Akbas"],"pdf_url":"https://arxiv.org/pdf/2303.02393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10780v1","updated":"2023-03-19T22:07:27Z","published":"2023-03-19T22:07:27Z","title":"A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices","summary":"  Biological neural networks continue to inspire breakthroughs in neural\nnetwork performance. And yet, one key area of neural computation that has been\nunder-appreciated and under-investigated is biologically plausible,\nenergy-efficient spiking neural networks, whose potential is especially\nattractive for low-power, mobile, or otherwise hardware-constrained settings.\nWe present a literature review of recent developments in the interpretation,\noptimization, efficiency, and accuracy of spiking neural networks. Key\ncontributions include identification, discussion, and comparison of\ncutting-edge methods in spiking neural network optimization, energy-efficiency,\nand evaluation, starting from first principles so as to be accessible to new\npractitioners.\n","authors":["Kai Malcom","Josue Casco-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2303.10780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10778v1","updated":"2023-03-19T21:58:37Z","published":"2023-03-19T21:58:37Z","title":"Deep Declarative Dynamic Time Warping for End-to-End Learning of\n  Alignment Paths","summary":"  This paper addresses learning end-to-end models for time series data that\ninclude a temporal alignment step via dynamic time warping (DTW). Existing\napproaches to differentiable DTW either differentiate through a fixed warping\npath or apply a differentiable relaxation to the min operator found in the\nrecursive steps used to solve the DTW problem. We instead propose a DTW layer\nbased around bi-level optimisation and deep declarative networks, which we name\nDecDTW. By formulating DTW as a continuous, inequality constrained optimisation\nproblem, we can compute gradients for the solution of the optimal alignment\n(with respect to the underlying time series) using implicit differentiation. An\ninteresting byproduct of this formulation is that DecDTW outputs the optimal\nwarping path between two time series as opposed to a soft approximation,\nrecoverable from Soft-DTW. We show that this property is particularly useful\nfor applications where downstream loss functions are defined on the optimal\nalignment path itself. This naturally occurs, for instance, when learning to\nimprove the accuracy of predicted alignments against ground truth alignments.\nWe evaluate DecDTW on two such applications, namely the audio-to-score\nalignment task in music information retrieval and the visual place recognition\ntask in robotics, demonstrating state-of-the-art results in both.\n","authors":["Ming Xu","Sourav Garg","Michael Milford","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.10778v1.pdf","comment":"ICLR 2023 (Poster)"},{"id":"http://arxiv.org/abs/2303.10774v1","updated":"2023-03-19T21:54:13Z","published":"2023-03-19T21:54:13Z","title":"Cross-GAN Auditing: Unsupervised Identification of Attribute Level\n  Similarities and Differences between Pretrained Generative Models","summary":"  Generative Adversarial Networks (GANs) are notoriously difficult to train\nespecially for complex distributions and with limited data. This has driven the\nneed for tools to audit trained networks in human intelligible format, for\nexample, to identify biases or ensure fairness. Existing GAN audit tools are\nrestricted to coarse-grained, model-data comparisons based on summary\nstatistics such as FID or recall. In this paper, we propose an alternative\napproach that compares a newly developed GAN against a prior baseline. To this\nend, we introduce Cross-GAN Auditing (xGA) that, given an established\n\"reference\" GAN and a newly proposed \"client\" GAN, jointly identifies\nintelligible attributes that are either common across both GANs, novel to the\nclient GAN, or missing from the client GAN. This provides both users and model\ndevelopers an intuitive assessment of similarity and differences between GANs.\nWe introduce novel metrics to evaluate attribute-based GAN auditing approaches\nand use these metrics to demonstrate quantitatively that xGA outperforms\nbaseline approaches. We also include qualitative results that illustrate the\ncommon, novel and missing attributes identified by xGA from GANs trained on a\nvariety of image datasets.\n","authors":["Matthew L. Olson","Shusen Liu","Rushil Anirudh","Jayaraman J. Thiagarajan","Peer-Timo Bremer","Weng-Keen Wong"],"pdf_url":"https://arxiv.org/pdf/2303.10774v1.pdf","comment":"CVPR 2023. Source code is available at\n  https://github.com/mattolson93/cross_gan_auditing"},{"id":"http://arxiv.org/abs/2204.05781v3","updated":"2023-03-19T21:32:59Z","published":"2022-04-06T07:45:05Z","title":"Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis\n  of BERT Classifiers and Weak Supervision","summary":"  Anticipating price developments in financial markets is a topic of continued\ninterest in forecasting. Funneled by advancements in deep learning and natural\nlanguage processing (NLP) together with the availability of vast amounts of\ntextual data in form of news articles, social media postings, etc., an\nincreasing number of studies incorporate text-based predictors in forecasting\nmodels. We contribute to this literature by introducing weak learning, a\nrecently proposed NLP approach to address the problem that text data is\nunlabeled. Without a dependent variable, it is not possible to finetune\npretrained NLP models on a custom corpus. We confirm that finetuning using weak\nlabels enhances the predictive value of text-based features and raises forecast\naccuracy in the context of predicting cryptocurrency returns. More\nfundamentally, the modeling paradigm we present, weak labeling domain-specific\ntext and finetuning pretrained NLP models, is universally applicable in\n(financial) forecasting and unlocks new ways to leverage text data.\n","authors":["Duygu Ider","Stefan Lessmann"],"pdf_url":"https://arxiv.org/pdf/2204.05781v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2212.04088v2","updated":"2023-03-19T20:52:21Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2207.11386v2","updated":"2023-03-19T20:39:59Z","published":"2022-07-23T01:17:23Z","title":"Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks:\n  Resource Usage vs. Latency","summary":"  Designing effective routing strategies for mobile wireless networks is\nchallenging due to the need to seamlessly adapt routing behavior to spatially\ndiverse and temporally changing network conditions. In this work, we use deep\nreinforcement learning (DeepRL) to learn a scalable and generalizable\nsingle-copy routing strategy for such networks. We make the following\ncontributions: i) we design a reward function that enables the DeepRL agent to\nexplicitly trade-off competing network goals, such as minimizing delay vs. the\nnumber of transmissions per packet; ii) we propose a novel set of relational\nneighborhood, path, and context features to characterize mobile wireless\nnetworks and model device mobility independently of a specific network\ntopology; and iii) we use a flexible training approach that allows us to\ncombine data from all packets and devices into a single offline centralized\ntraining set to train a single DeepRL agent. To evaluate generalizeability and\nscalability, we train our DeepRL agent on one mobile network scenario and then\ntest it on other mobile scenarios, varying the number of devices and\ntransmission ranges. Our results show our learned single-copy routing strategy\noutperforms all other strategies in terms of delay except for the optimal\nstrategy, even on scenarios on which the DeepRL agent was not trained.\n","authors":["Victoria Manfredi","Alicia P. Wolfe","Xiaolan Zhang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11386v2.pdf","comment":"39 pages, 9 figures. Added new features as input to DRL model, added\n  new mobility model to evaluation, updated related work, and updated\n  simulations"},{"id":"http://arxiv.org/abs/2303.10761v1","updated":"2023-03-19T20:27:51Z","published":"2023-03-19T20:27:51Z","title":"Calibration of Neural Networks","summary":"  Neural networks solving real-world problems are often required not only to\nmake accurate predictions but also to provide a confidence level in the\nforecast. The calibration of a model indicates how close the estimated\nconfidence is to the true probability. This paper presents a survey of\nconfidence calibration problems in the context of neural networks and provides\nan empirical comparison of calibration methods. We analyze problem statement,\ncalibration definitions, and different approaches to evaluation: visualizations\nand scalar measures that estimate whether the model is well-calibrated. We\nreview modern calibration techniques: based on post-processing or requiring\nchanges in training. Empirical experiments cover various datasets and models,\ncomparing calibration methods according to different criteria.\n","authors":["Ruslan Vasilev","Alexander D'yakonov"],"pdf_url":"https://arxiv.org/pdf/2303.10761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10758v1","updated":"2023-03-19T20:24:33Z","published":"2023-03-19T20:24:33Z","title":"Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex\n  Optimization","summary":"  Recent progress was made in characterizing the generalization error of\ngradient methods for general convex loss by the learning theory community. In\nthis work, we focus on how training longer might affect generalization in\nsmooth stochastic convex optimization (SCO) problems. We first provide tight\nlower bounds for general non-realizable SCO problems. Furthermore, existing\nupper bound results suggest that sample complexity can be improved by assuming\nthe loss is realizable, i.e. an optimal solution simultaneously minimizes all\nthe data points. However, this improvement is compromised when training time is\nlong and lower bounds are lacking. Our paper examines this observation by\nproviding excess risk lower bounds for gradient descent (GD) and stochastic\ngradient descent (SGD) in two realizable settings: 1) realizable with $T =\nO(n)$, and (2) realizable with $T = \\Omega(n)$, where $T$ denotes the number of\ntraining iterations and $n$ is the size of the training dataset. These bounds\nare novel and informative in characterizing the relationship between $T$ and\n$n$. In the first small training horizon case, our lower bounds almost tightly\nmatch and provide the first optimal certificates for the corresponding upper\nbounds. However, for the realizable case with $T = \\Omega(n)$, a gap exists\nbetween the lower and upper bounds. We provide a conjecture to address this\nproblem, that the gap can be closed by improving upper bounds, which is\nsupported by our analyses in one-dimensional and linear regression scenarios.\n","authors":["Peiyuan Zhang","Jiaye Teng","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.10758v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2303.01994v2","updated":"2023-03-19T20:22:39Z","published":"2023-03-03T15:06:03Z","title":"Discovery and Recognition of Formula Concepts using Machine Learning","summary":"  Citation-based Information Retrieval (IR) methods for scientific documents\nhave proven effective for IR applications, such as Plagiarism Detection or\nLiterature Recommender Systems in academic disciplines that use many\nreferences. In science, technology, engineering, and mathematics, researchers\noften employ mathematical concepts through formula notation to refer to prior\nknowledge. Our long-term goal is to generalize citation-based IR methods and\napply this generalized method to both classical references and mathematical\nconcepts. In this paper, we suggest how mathematical formulas could be cited\nand define a Formula Concept Retrieval task with two subtasks: Formula Concept\nDiscovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the\ndefinition and exploration of a 'Formula Concept' that names bundled equivalent\nrepresentations of a formula, FCR is designed to match a given formula to a\nprior assigned unique mathematical concept identifier. We present machine\nlearning-based approaches to address the FCD and FCR tasks. We then evaluate\nthese approaches on a standardized test collection (NTCIR arXiv dataset). Our\nFCD approach yields a precision of 68% for retrieving equivalent\nrepresentations of frequent formulas and a recall of 72% for extracting the\nformula name from the surrounding text. FCD and FCR enable the citation of\nformulas within mathematical documents and facilitate semantic search and\nquestion answering as well as document similarity assessments for plagiarism\ndetection or recommender systems.\n","authors":["Philipp Scharpf","Moritz Schubotz","Howard S. Cohl","Corinna Breitinger","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.01994v2.pdf","comment":"Accepted by Scientometrics (Springer) journal"},{"id":"http://arxiv.org/abs/2303.10757v1","updated":"2023-03-19T20:21:29Z","published":"2023-03-19T20:21:29Z","title":"Multiscale Audio Spectrogram Transformer for Efficient Audio\n  Classification","summary":"  Audio event has a hierarchical architecture in both time and frequency and\ncan be grouped together to construct more abstract semantic audio classes. In\nthis work, we develop a multiscale audio spectrogram Transformer (MAST) that\nemploys hierarchical representation learning for efficient audio\nclassification. Specifically, MAST employs one-dimensional (and\ntwo-dimensional) pooling operators along the time (and frequency domains) in\ndifferent stages, and progressively reduces the number of tokens and increases\nthe feature dimensions. MAST significantly outperforms AST~\\cite{gong2021ast}\nby 22.2\\%, 4.4\\% and 4.7\\% on Kinetics-Sounds, Epic-Kitchens-100 and VGGSound\nin terms of the top-1 accuracy without external training data. On the\ndownloaded AudioSet dataset, which has over 20\\% missing audios, MAST also\nachieves slightly better accuracy than AST. In addition, MAST is 5x more\nefficient in terms of multiply-accumulates (MACs) with 42\\% reduction in the\nnumber of parameters compared to AST. Through clustering metrics and\nvisualizations, we demonstrate that the proposed MAST can learn semantically\nmore separable feature representations from audio signals.\n","authors":["Wentao Zhu","Mohamed Omar"],"pdf_url":"https://arxiv.org/pdf/2303.10757v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.10741v1","updated":"2023-03-19T19:09:41Z","published":"2023-03-19T19:09:41Z","title":"Computer Vision Estimation of Emotion Reaction Intensity in the Wild","summary":"  Emotions play an essential role in human communication. Developing computer\nvision models for automatic recognition of emotion expression can aid in a\nvariety of domains, including robotics, digital behavioral healthcare, and\nmedia analytics. There are three types of emotional representations which are\ntraditionally modeled in affective computing research: Action Units, Valence\nArousal (VA), and Categorical Emotions. As part of an effort to move beyond\nthese representations towards more fine-grained labels, we describe our\nsubmission to the newly introduced Emotional Reaction Intensity (ERI)\nEstimation challenge in the 5th competition for Affective Behavior Analysis\nin-the-Wild (ABAW). We developed four deep neural networks trained in the\nvisual domain and a multimodal model trained with both visual and audio\nfeatures to predict emotion reaction intensity. Our best performing model on\nthe Hume-Reaction dataset achieved an average Pearson correlation coefficient\nof 0.4080 on the test set using a pre-trained ResNet50 model. This work\nprovides a first step towards the development of production-grade models which\npredict emotion reaction intensities rather than discrete emotion categories.\n","authors":["Yang Qian","Ali Kargarandehkordi","Onur Cezmi Mutlu","Saimourya Surabhi","Mohammadmahdi Honarmand","Dennis Paul Wall","Peter Washington"],"pdf_url":"https://arxiv.org/pdf/2303.10741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10738v1","updated":"2023-03-19T18:55:22Z","published":"2023-03-19T18:55:22Z","title":"MIA-3DCNN: COVID-19 Detection Based on a 3D CNN","summary":"  Early and accurate diagnosis of COVID-19 is essential to control the rapid\nspread of the pandemic and mitigate sequelae in the population. Current\ndiagnostic methods, such as RT-PCR, are effective but require time to provide\nresults and can quickly overwhelm clinics, requiring individual laboratory\nanalysis. Automatic detection methods have the potential to significantly\nreduce diagnostic time. To this end, learning-based methods using lung imaging\nhave been explored. Although they require specialized hardware, automatic\nevaluation methods can be performed simultaneously, making diagnosis faster.\nConvolutional neural networks have been widely used to detect pneumonia caused\nby COVID-19 in lung images. This work describes an architecture based on 3D\nconvolutional neural networks for detecting COVID-19 in computed tomography\nimages. Despite the challenging scenario present in the dataset, the results\nobtained with our architecture demonstrated to be quite promising.\n","authors":["Igor Kenzo Ishikawa Oshiro Nakashima","Giovanna Vendramini","Helio Pedrini"],"pdf_url":"https://arxiv.org/pdf/2303.10738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10732v1","updated":"2023-03-19T18:37:18Z","published":"2023-03-19T18:37:18Z","title":"AutoEn: An AutoML method based on ensembles of predefined Machine\n  Learning pipelines for supervised Traffic Forecasting","summary":"  Intelligent Transportation Systems are producing tons of hardly manageable\ntraffic data, which motivates the use of Machine Learning (ML) for data-driven\napplications, such as Traffic Forecasting (TF). TF is gaining relevance due to\nits ability to mitigate traffic congestion by forecasting future traffic\nstates. However, TF poses one big challenge to the ML paradigm, known as the\nModel Selection Problem (MSP): deciding the most suitable combination of data\npreprocessing techniques and ML method for traffic data collected under\ndifferent transportation circumstances. In this context, Automated Machine\nLearning (AutoML), the automation of the ML workflow from data preprocessing to\nmodel validation, arises as a promising strategy to deal with the MSP in\nproblem domains wherein expert ML knowledge is not always an available or\naffordable asset, such as TF. Various AutoML frameworks have been used to\napproach the MSP in TF. Most are based on online optimisation processes to\nsearch for the best-performing pipeline on a given dataset. This online\noptimisation could be complemented with meta-learning to warm-start the search\nphase and/or the construction of ensembles using pipelines derived from the\noptimisation process. However, given the complexity of the search space and the\nhigh computational cost of tuning-evaluating pipelines generated, online\noptimisation is only beneficial when there is a long time to obtain the final\nmodel. Thus, we introduce AutoEn, which is a simple and efficient method for\nautomatically generating multi-classifier ensembles from a predefined set of ML\npipelines. We compare AutoEn against Auto-WEKA and Auto-sklearn, two AutoML\nmethods commonly used in TF. Experimental results demonstrate that AutoEn can\nlead to better or more competitive results in the general-purpose domain and in\nTF.\n","authors":["Juan S. Angarita-Zapata","Antonio D. Masegosa","Isaac Triguero"],"pdf_url":"https://arxiv.org/pdf/2303.10732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10730v1","updated":"2023-03-19T18:29:54Z","published":"2023-03-19T18:29:54Z","title":"Pretrained Vision Models for Predicting High-Risk Breast Cancer Stage","summary":"  Cancer is increasingly a global health issue. Seconding cardiovascular\ndiseases, cancers are the second biggest cause of death in the world with\nmillions of people succumbing to the disease every year. According to the World\nHealth Organization (WHO) report, by the end of 2020, more than 7.8 million\nwomen have been diagnosed with breast cancer, making it the world's most\nprevalent cancer. In this paper, using the Nightingale Open Science dataset of\ndigital pathology (breast biopsy) images, we leverage the capabilities of\npre-trained computer vision models for the breast cancer stage prediction task.\nWhile individual models achieve decent performances, we find out that the\npredictions of an ensemble model are more efficient, and offer a winning\nsolution\\footnote{https://www.nightingalescience.org/updates/hbc1-results}. We\nalso provide analyses of the results and explore pathways for better\ninterpretability and generalization. Our code is open-source at\n\\url{https://github.com/bonaventuredossou/nightingale_winning_solution}\n","authors":["Bonaventure F. P. Dossou","Yenoukoume S. K. Gbenou","Miglanche Ghomsi Nono"],"pdf_url":"https://arxiv.org/pdf/2303.10730v1.pdf","comment":"Accepted at Machine Learning for Global Health Workshop, ICLR 2023"},{"id":"http://arxiv.org/abs/2205.15150v3","updated":"2023-03-19T18:20:39Z","published":"2022-05-30T14:47:27Z","title":"Principal Component Analysis based frameworks for efficient missing data\n  imputation algorithms","summary":"  Missing data is a commonly occurring problem in practice. Many imputation\nmethods have been developed to fill in the missing entries. However, not all of\nthem can scale to high-dimensional data, especially the multiple imputation\ntechniques. Meanwhile, the data nowadays tends toward high-dimensional.\nTherefore, in this work, we propose Principal Component Analysis Imputation\n(PCAI), a simple but versatile framework based on Principal Component Analysis\n(PCA) to speed up the imputation process and alleviate memory issues of many\navailable imputation techniques, without sacrificing the imputation quality in\nterm of MSE. In addition, the frameworks can be used even when some or all of\nthe missing features are categorical, or when the number of missing features is\nlarge. Next, we introduce PCA Imputation - Classification (PIC), an application\nof PCAI for classification problems with some adjustments. We validate our\napproach by experiments on various scenarios, which shows that PCAI and PIC can\nwork with various imputation algorithms, including the state-of-the-art ones\nand improve the imputation speed significantly, while achieving competitive\nmean square error/classification accuracy compared to direct imputation (i.e.,\nimpute directly on the missing data).\n","authors":["Thu Nguyen","Hoang Thien Ly","Michael Alexander Riegler","Pål Halvorsen","Hugo L. Hammer"],"pdf_url":"https://arxiv.org/pdf/2205.15150v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10728v1","updated":"2023-03-19T18:10:15Z","published":"2023-03-19T18:10:15Z","title":"Training Deep Boltzmann Networks with Sparse Ising Machines","summary":"  The slowing down of Moore's law has driven the development of unconventional\ncomputing paradigms, such as specialized Ising machines tailored to solve\ncombinatorial optimization problems. In this paper, we show a new application\ndomain for probabilistic bit (p-bit) based Ising machines by training deep\ngenerative AI models with them. Using sparse, asynchronous, and massively\nparallel Ising machines we train deep Boltzmann networks in a hybrid\nprobabilistic-classical computing setup. We use the full MNIST dataset without\nany downsampling or reduction in hardware-aware network topologies implemented\nin moderately sized Field Programmable Gate Arrays (FPGA). Our machine, which\nuses only 4,264 nodes (p-bits) and about 30,000 parameters, achieves the same\nclassification accuracy (90%) as an optimized software-based restricted\nBoltzmann Machine (RBM) with approximately 3.25 million parameters.\nAdditionally, the sparse deep Boltzmann network can generate new handwritten\ndigits, a task the 3.25 million parameter RBM fails at despite achieving the\nsame accuracy. Our hybrid computer takes a measured 50 to 64 billion\nprobabilistic flips per second, which is at least an order of magnitude faster\nthan superficially similar Graphics and Tensor Processing Unit (GPU/TPU) based\nimplementations. The massively parallel architecture can comfortably perform\nthe contrastive divergence algorithm (CD-n) with up to n = 10 million sweeps\nper update, beyond the capabilities of existing software implementations. These\nresults demonstrate the potential of using Ising machines for traditionally\nhard-to-train deep generative Boltzmann networks, with further possible\nimprovement in nanodevice-based realizations.\n","authors":["Shaila Niazi","Navid Anjum Aadit","Masoud Mohseni","Shuvro Chowdhury","Yao Qin","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2303.10728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10727v1","updated":"2023-03-19T18:08:18Z","published":"2023-03-19T18:08:18Z","title":"ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time\n  Social Ambiance Measurement","summary":"  Social ambiance describes the context in which social interactions happen,\nand can be measured using speech audio by counting the number of concurrent\nspeakers. This measurement has enabled various mental health tracking and\nhuman-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is\nhighly desirable to ensure user privacy and thus facilitate wide adoption of\nthe aforementioned applications, the required computational complexity of\nstate-of-the-art deep neural networks (DNNs) powered SAM solutions stands at\nodds with the often constrained resources on mobile devices. Furthermore, only\nlimited labeled data is available or practical when it comes to SAM under\nclinical settings due to various privacy constraints and the required human\neffort, further challenging the achievable accuracy of on-device SAM solutions.\nTo this end, we propose a dedicated neural architecture search framework for\nEnergy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework\ncan automatically search for DNNs that push forward the achievable accuracy vs.\nhardware efficiency frontier of mobile SAM solutions. For example,\nERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 seconds\nprocessing latency for a 5 seconds audio segment on a Pixel 3 phone, while only\nachieving an error rate of 14.3% on a social ambiance dataset generated by\nLibriSpeech. We can expect that our ERSAM framework can pave the way for\nubiquitous on-device SAM solutions which are in growing demand.\n","authors":["Chaojian Li","Wenwan Chen","Jiayi Yuan"," Yingyan"," Lin","Ashutosh Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2303.10727v1.pdf","comment":"Accepted by ICASSP'23"},{"id":"http://arxiv.org/abs/2303.10725v1","updated":"2023-03-19T17:46:40Z","published":"2023-03-19T17:46:40Z","title":"SIESTA: Efficient Online Continual Learning with Sleep","summary":"  In supervised continual learning, a deep neural network (DNN) is updated with\nan ever-growing data stream. Unlike the offline setting where data is shuffled,\nwe cannot make any distributional assumptions about the data stream. Ideally,\nonly one pass through the dataset is needed for computational efficiency.\nHowever, existing methods are inadequate and make many assumptions that cannot\nbe made for real-world applications, while simultaneously failing to improve\ncomputational efficiency. In this paper, we do not propose a novel method.\nInstead, we present SIESTA, an incremental improvement to the continual\nlearning algorithm REMIND. Unlike REMIND, SIESTA uses a wake/sleep framework\nfor training, which is well aligned to the needs of on-device learning. SIESTA\nis far more computationally efficient than existing methods, enabling continual\nlearning on ImageNet-1K in under 3 hours on a single GPU; moreover, in the\naugmentation-free setting it matches the performance of the offline learner, a\nmilestone critical to driving adoption of continual learning in real-world\napplications.\n","authors":["Md Yousuf Harun","Jhair Gallardo","Tyler L. Hayes","Ronald Kemker","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2303.10725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.13387v2","updated":"2023-03-19T17:46:07Z","published":"2022-01-31T17:52:01Z","title":"L-SVRG and L-Katyusha with Adaptive Sampling","summary":"  Stochastic gradient-based optimization methods, such as L-SVRG and its\naccelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train\nmachine learning models.The theoretical and empirical performance of L-SVRG and\nL-Katyusha can be improved by sampling observations from a non-uniform\ndistribution (Qian et al., 2021). However,designing a desired sampling\ndistribution requires prior knowledge of smoothness constants, which can be\ncomputationally intractable to obtain in practice when the dimension of the\nmodel parameter is high. To address this issue, we propose an adaptive sampling\nstrategy for L-SVRG and L-Katyusha that can learn the sampling distribution\nwith little computational overhead, while allowing it to change with iterates,\nand at the same time does not require any prior knowledge of the problem\nparameters. We prove convergence guarantees for L-SVRG and L-Katyusha for\nconvex objectives when the sampling distribution changes with iterates. Our\nresults show that even without prior information, the proposed adaptive\nsampling strategy matches, and in some cases even surpasses, the performance of\nthe sampling scheme in Qian et al. (2021). Extensive simulations support our\ntheory and the practical utility of the proposed sampling scheme on real data.\n","authors":["Boxin Zhao","Boxiang Lyu","Mladen Kolar"],"pdf_url":"https://arxiv.org/pdf/2201.13387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10722v1","updated":"2023-03-19T17:34:58Z","published":"2023-03-19T17:34:58Z","title":"Q-RBSA: High-Resolution 3D EBSD Map Generation Using An Efficient\n  Quaternion Transformer Network","summary":"  Gathering 3D material microstructural information is time-consuming,\nexpensive, and energy-intensive. Acquisition of 3D data has been accelerated by\ndevelopments in serial sectioning instrument capabilities; however, for\ncrystallographic information, the electron backscatter diffraction (EBSD)\nimaging modality remains rate limiting. We propose a physics-based efficient\ndeep learning framework to reduce the time and cost of collecting 3D EBSD maps.\nOur framework uses a quaternion residual block self-attention network (QRBSA)\nto generate high-resolution 3D EBSD maps from sparsely sectioned EBSD maps. In\nQRBSA, quaternion-valued convolution effectively learns local relations in\norientation space, while self-attention in the quaternion domain captures\nlong-range correlations. We apply our framework to 3D data collected from\ncommercially relevant titanium alloys, showing both qualitatively and\nquantitatively that our method can predict missing samples (EBSD information\nbetween sparsely sectioned mapping points) as compared to high-resolution\nground truth 3D EBSD maps.\n","authors":["Devendra K. Jangid","Neal R. Brodnik","McLean P. Echlin","Tresa M. Pollock","Samantha H. Daly","B. S. Manjunath"],"pdf_url":"https://arxiv.org/pdf/2303.10722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10720v1","updated":"2023-03-19T17:30:44Z","published":"2023-03-19T17:30:44Z","title":"Trainable Projected Gradient Method for Robust Fine-tuning","summary":"  Recent studies on transfer learning have shown that selectively fine-tuning a\nsubset of layers or customizing different learning rates for each layer can\ngreatly improve robustness to out-of-distribution (OOD) data and retain\ngeneralization capability in the pre-trained models. However, most of these\nmethods employ manually crafted heuristics or expensive hyper-parameter\nsearches, which prevent them from scaling up to large datasets and neural\nnetworks. To solve this problem, we propose Trainable Projected Gradient Method\n(TPGM) to automatically learn the constraint imposed for each layer for a\nfine-grained fine-tuning regularization. This is motivated by formulating\nfine-tuning as a bi-level constrained optimization problem. Specifically, TPGM\nmaintains a set of projection radii, i.e., distance constraints between the\nfine-tuned model and the pre-trained model, for each layer, and enforces them\nthrough weight projections. To learn the constraints, we propose a bi-level\noptimization to automatically learn the best set of projection radii in an\nend-to-end manner. Theoretically, we show that the bi-level optimization\nformulation is the key to learning different constraints for each layer.\nEmpirically, with little hyper-parameter search cost, TPGM outperforms existing\nfine-tuning methods in OOD performance while matching the best in-distribution\n(ID) performance. For example, when fine-tuned on DomainNet-Real and ImageNet,\ncompared to vanilla fine-tuning, TPGM shows $22\\%$ and $10\\%$ relative OOD\nimprovement respectively on their sketch counterparts. Code is available at\n\\url{https://github.com/PotatoTian/TPGM}.\n","authors":["Junjiao Tian","Xiaoliang Dai","Chih-Yao Ma","Zecheng He","Yen-Cheng Liu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2303.10720v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.03995v2","updated":"2023-03-19T17:17:11Z","published":"2023-03-07T15:51:03Z","title":"Group conditional validity via multi-group learning","summary":"  We consider the problem of distribution-free conformal prediction and the\ncriterion of group conditional validity. This criterion is motivated by many\npractical scenarios including hidden stratification and group fairness.\nExisting methods achieve such guarantees under either restrictive grouping\nstructure or distributional assumptions, or they are overly-conservative under\nheteroskedastic noise. We propose a simple reduction to the problem of\nachieving validity guarantees for individual populations by leveraging\nalgorithms for a problem called multi-group learning. This allows us to port\ntheoretical guarantees from multi-group learning to obtain obtain sample\ncomplexity guarantees for conformal prediction. We also provide a new algorithm\nfor multi-group learning for groups with hierarchical structure. Using this\nalgorithm in our reduction leads to improved sample complexity guarantees with\na simpler predictor structure.\n","authors":["Samuel Deng","Navid Ardeshir","Daniel Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.03995v2.pdf","comment":"Valid prediction intervals constructed by proposed method do not\n  appear to be any shorter than those constructed by baseline methods"},{"id":"http://arxiv.org/abs/2209.03416v4","updated":"2023-03-19T16:34:47Z","published":"2022-09-07T18:34:48Z","title":"Bispectral Neural Networks","summary":"  We present a neural network architecture, Bispectral Neural Networks (BNNs)\nfor learning representations that are invariant to the actions of compact\ncommutative groups on the space over which a signal is defined. The model\nincorporates the ansatz of the bispectrum, an analytically defined group\ninvariant that is complete -- that is, it preserves all signal structure while\nremoving only the variation due to group actions. Here, we demonstrate that\nBNNs are able to simultaneously learn groups, their irreducible\nrepresentations, and corresponding equivariant and complete-invariant maps\npurely from the symmetries implicit in data. Further, we demonstrate that the\ncompleteness property endows these networks with strong invariance-based\nadversarial robustness. This work establishes Bispectral Neural Networks as a\npowerful computational primitive for robust invariant representation learning\n","authors":["Sophia Sanborn","Christian Shewmake","Bruno Olshausen","Christopher Hillar"],"pdf_url":"https://arxiv.org/pdf/2209.03416v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14305v2","updated":"2023-03-19T16:25:10Z","published":"2022-11-25T18:59:10Z","title":"SpaText: Spatio-Textual Representation for Controllable Image Generation","summary":"  Recent text-to-image diffusion models are able to generate convincing results\nof unprecedented quality. However, it is nearly impossible to control the\nshapes of different regions/objects or their layout in a fine-grained fashion.\nPrevious attempts to provide such controls were hindered by their reliance on a\nfixed set of labels. To this end, we present SpaText - a new method for\ntext-to-image generation using open-vocabulary scene control. In addition to a\nglobal text prompt that describes the entire scene, the user provides a\nsegmentation map where each region of interest is annotated by a free-form\nnatural language description. Due to lack of large-scale datasets that have a\ndetailed textual description for each region in the image, we choose to\nleverage the current large-scale text-to-image datasets and base our approach\non a novel CLIP-based spatio-textual representation, and show its effectiveness\non two state-of-the-art diffusion models: pixel-based and latent-based. In\naddition, we show how to extend the classifier-free guidance method in\ndiffusion models to the multi-conditional case and present an alternative\naccelerated inference algorithm. Finally, we offer several automatic evaluation\nmetrics and use them, in addition to FID scores and a user study, to evaluate\nour method and show that it achieves state-of-the-art results on image\ngeneration with free-form textual scene control.\n","authors":["Omri Avrahami","Thomas Hayes","Oran Gafni","Sonal Gupta","Yaniv Taigman","Devi Parikh","Dani Lischinski","Ohad Fried","Xi Yin"],"pdf_url":"https://arxiv.org/pdf/2211.14305v2.pdf","comment":"CVPR 2023. Project page available at:\n  https://omriavrahami.com/spatext"},{"id":"http://arxiv.org/abs/2303.10702v1","updated":"2023-03-19T16:17:19Z","published":"2023-03-19T16:17:19Z","title":"Evaluation of Convolution Primitives for Embedded Neural Networks on\n  32-bit Microcontrollers","summary":"  Deploying neural networks on constrained hardware platforms such as 32-bit\nmicrocontrollers is a challenging task because of the large memory, computing\nand energy requirements of their inference process. To tackle these issues,\nseveral convolution primitives have been proposed to make the standard\nconvolution more computationally efficient. However, few of these primitives\nare really implemented for 32-bit microcontrollers. In this work, we collect\ndifferent state-of-the-art convolutional primitives and propose an\nimplementation for ARM Cortex-M processor family with an open source deployment\nplatform (NNoM). Then, we carry out experimental characterization tests on\nthese implementations. Our benchmark reveals a linear relationship between\ntheoretical MACs and energy consumption. Thus showing the advantages of using\ncomputationally efficient primitives like shift convolution. We discuss about\nthe significant reduction in latency and energy consumption due to the use of\nSIMD instructions and highlight the importance of data reuse in those\nperformance gains. For reproducibility purpose and further experiments, codes\nand experiments are publicly available.\n","authors":["Baptiste Nguyen","Pierre-Alain Moellic","Sylvain Blayac"],"pdf_url":"https://arxiv.org/pdf/2303.10702v1.pdf","comment":"ISDA 2022"},{"id":"http://arxiv.org/abs/2303.05518v2","updated":"2023-03-19T16:01:30Z","published":"2023-03-09T16:05:10Z","title":"Computably Continuous Reinforcement-Learning Objectives are\n  PAC-learnable","summary":"  In reinforcement learning, the classic objectives of maximizing discounted\nand finite-horizon cumulative rewards are PAC-learnable: There are algorithms\nthat learn a near-optimal policy with high probability using a finite amount of\nsamples and computation. In recent years, researchers have introduced\nobjectives and corresponding reinforcement-learning algorithms beyond the\nclassic cumulative rewards, such as objectives specified as linear temporal\nlogic formulas. However, questions about the PAC-learnability of these new\nobjectives have remained open.\n  This work demonstrates the PAC-learnability of general reinforcement-learning\nobjectives through sufficient conditions for PAC-learnability in two analysis\nsettings. In particular, for the analysis that considers only sample\ncomplexity, we prove that if an objective given as an oracle is uniformly\ncontinuous, then it is PAC-learnable. Further, for the analysis that considers\ncomputational complexity, we prove that if an objective is computable, then it\nis PAC-learnable. In other words, if a procedure computes successive\napproximations of the objective's value, then the objective is PAC-learnable.\n  We give three applications of our condition on objectives from the literature\nwith previously unknown PAC-learnability and prove that these objectives are\nPAC-learnable. Overall, our result helps verify existing objectives'\nPAC-learnability. Also, as some studied objectives that are not uniformly\ncontinuous have been shown to be not PAC-learnable, our results could guide the\ndesign of new PAC-learnable objectives.\n","authors":["Cambridge Yang","Michael Littman","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2303.05518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10695v1","updated":"2023-03-19T15:58:04Z","published":"2023-03-19T15:58:04Z","title":"On the Convergence of Decentralized Federated Learning Under Imperfect\n  Information Sharing","summary":"  Decentralized learning and optimization is a central problem in control that\nencompasses several existing and emerging applications, such as federated\nlearning. While there exists a vast literature on this topic and most methods\ncentered around the celebrated average-consensus paradigm, less attention has\nbeen devoted to scenarios where the communication between the agents may be\nimperfect. To this end, this paper presents three different algorithms of\nDecentralized Federated Learning (DFL) in the presence of imperfect information\nsharing modeled as noisy communication channels. The first algorithm, Federated\nNoisy Decentralized Learning (FedNDL1), comes from the literature, where the\nnoise is added to their parameters to simulate the scenario of the presence of\nnoisy communication channels. This algorithm shares parameters to form a\nconsensus with the clients based on a communication graph topology through a\nnoisy communication channel. The proposed second algorithm (FedNDL2) is similar\nto the first algorithm but with added noise to the parameters, and it performs\nthe gossip averaging before the gradient optimization. The proposed third\nalgorithm\n  (FedNDL3), on the other hand, shares the gradients through noisy\ncommunication channels instead of the parameters. Theoretical and experimental\nresults demonstrate that under imperfect information sharing, the third scheme\nthat mixes gradients is more robust in the presence of a noisy channel compared\nwith the algorithms from the literature that mix the parameters.\n","authors":["Vishnu Pandi Chellapandi","Antesh Upadhyay","Abolfazl Hashemi","Stanislaw H /. Zak"],"pdf_url":"https://arxiv.org/pdf/2303.10695v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.10694v1","updated":"2023-03-19T15:56:50Z","published":"2023-03-19T15:56:50Z","title":"Improving Uncertainty Quantification of Deep Classifiers via\n  Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis","summary":"  Safe deployment of deep neural networks in high-stake real-world applications\nrequires theoretically sound uncertainty quantification. Conformal prediction\n(CP) is a principled framework for uncertainty quantification of deep models in\nthe form of prediction set for classification tasks with a user-specified\ncoverage (i.e., true class label is contained with high probability). This\npaper proposes a novel algorithm referred to as Neighborhood Conformal\nPrediction (NCP) to improve the efficiency of uncertainty quantification from\nCP for deep classifiers (i.e., reduce prediction set size). The key idea behind\nNCP is to use the learned representation of the neural network to identify k\nnearest-neighbors calibration examples for a given testing input and assign\nthem importance weights proportional to their distance to create adaptive\nprediction sets. We theoretically show that if the learned data representation\nof the neural network satisfies some mild conditions, NCP will produce smaller\nprediction sets than traditional CP algorithms. Our comprehensive experiments\non CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural\nnetworks strongly demonstrate that NCP leads to significant reduction in\nprediction set size over prior CP methods.\n","authors":["Subhankar Ghosh","Taha Belkhouja","Yan Yan","Janardhan Rao Doppa"],"pdf_url":"https://arxiv.org/pdf/2303.10694v1.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence, 2023"},{"id":"http://arxiv.org/abs/2303.10681v1","updated":"2023-03-19T15:00:47Z","published":"2023-03-19T15:00:47Z","title":"Generative Adversarial Classification Network with Application to\n  Network Traffic Classification","summary":"  Large datasets in machine learning often contain missing data, which\nnecessitates the imputation of missing data values. In this work, we are\nmotivated by network traffic classification, where traditional data imputation\nmethods do not perform well. We recognize that no existing method directly\naccounts for classification accuracy during data imputation. Therefore, we\npropose a joint data imputation and data classification method, termed\ngenerative adversarial classification network (GACN), whose architecture\ncontains a generator network, a discriminator network, and a classification\nnetwork, which are iteratively optimized toward the ultimate objective of\nclassification accuracy. For the scenario where some data samples are\nunlabeled, we further propose an extension termed semi-supervised GACN\n(SSGACN), which is able to use the partially labeled data to improve\nclassification accuracy. We conduct experiments with real-world network traffic\ndata traces, which demonstrate that GACN and SS-GACN can more accurately impute\ndata features that are more important for classification, and they outperform\nexisting methods in terms of classification accuracy.\n","authors":["Rozhina Ghanavi","Ben Liang","Ali Tizghadam"],"pdf_url":"https://arxiv.org/pdf/2303.10681v1.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.10677v1","updated":"2023-03-19T14:44:37Z","published":"2023-03-19T14:44:37Z","title":"A Survey of Federated Learning for Connected and Automated Vehicles","summary":"  Connected and Automated Vehicles (CAVs) are one of the emerging technologies\nin the automotive domain that has the potential to alleviate the issues of\naccidents, traffic congestion, and pollutant emissions, leading to a safe,\nefficient, and sustainable transportation system. Machine learning-based\nmethods are widely used in CAVs for crucial tasks like perception, motion\nplanning, and motion control, where machine learning models in CAVs are solely\ntrained using the local vehicle data, and the performance is not certain when\nexposed to new environments or unseen conditions. Federated learning (FL) is an\neffective solution for CAVs that enables a collaborative model development with\nmultiple vehicles in a distributed learning framework. FL enables CAVs to learn\nfrom a wide range of driving environments and improve their overall performance\nwhile ensuring the privacy and security of local vehicle data. In this paper,\nwe review the progress accomplished by researchers in applying FL to CAVs. A\nbroader view of the various data modalities and algorithms that have been\nimplemented on CAVs is provided. Specific applications of FL are reviewed in\ndetail, and an analysis of the challenges and future scope of research are\npresented.\n","authors":["Vishnu Pandi Chellapandi","Liangqi Yuan","Stanislaw H /. Zak","Ziran Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10677v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.02384v2","updated":"2023-03-19T14:39:50Z","published":"2023-03-04T11:30:16Z","title":"Hierarchical Training of Deep Neural Networks Using Early Exiting","summary":"  Deep neural networks provide state-of-the-art accuracy for vision tasks but\nthey require significant resources for training. Thus, they are trained on\ncloud servers far from the edge devices that acquire the data. This issue\nincreases communication cost, runtime and privacy concerns. In this study, a\nnovel hierarchical training method for deep neural networks is proposed that\nuses early exits in a divided architecture between edge and cloud workers to\nreduce the communication cost, training runtime and privacy concerns. The\nmethod proposes a brand-new use case for early exits to separate the backward\npass of neural networks between the edge and the cloud during the training\nphase. We address the issues of most available methods that due to the\nsequential nature of the training phase, cannot train the levels of hierarchy\nsimultaneously or they do it with the cost of compromising privacy. In\ncontrast, our method can use both edge and cloud workers simultaneously, does\nnot share the raw input data with the cloud and does not require communication\nduring the backward pass. Several simulations and on-device experiments for\ndifferent neural network architectures demonstrate the effectiveness of this\nmethod. It is shown that the proposed method reduces the training runtime by\n29% and 61% in CIFAR-10 classification experiment for VGG-16 and ResNet-18 when\nthe communication with the cloud is done at a low bit rate channel. This gain\nin the runtime is achieved whilst the accuracy drop is negligible. This method\nis advantageous for online learning of high-accuracy deep neural networks on\nlow-resource devices such as mobile phones or robots as a part of an edge-cloud\nsystem, making them more flexible in facing new tasks and classes of data.\n","authors":["Yamin Sepehri","Pedram Pad","Ahmet Caner Yüzügüler","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2303.02384v2.pdf","comment":"12 pages, 9 figures, 1 Table"},{"id":"http://arxiv.org/abs/2303.10674v1","updated":"2023-03-19T14:33:42Z","published":"2023-03-19T14:33:42Z","title":"URM4DMU: an user represention model for darknet markets users","summary":"  Darknet markets provide a large platform for trading illicit goods and\nservices due to their anonymity. Learning an invariant representation of each\nuser based on their posts on different markets makes it easy to aggregate user\ninformation across different platforms, which helps identify anonymous users.\nTraditional user representation methods mainly rely on modeling the text\ninformation of posts and cannot capture the temporal content and the forum\ninteraction of posts. While recent works mainly use CNN to model the text\ninformation of posts, failing to effectively model posts whose length changes\nfrequently in an episode. To address the above problems, we propose a model\nnamed URM4DMU(User Representation Model for Darknet Markets Users) which mainly\nimproves the post representation by augmenting convolutional operators and\nself-attention with an adaptive gate mechanism. It performs much better when\ncombined with the temporal content and the forum interaction of posts. We\ndemonstrate the effectiveness of URM4DMU on four darknet markets. The average\nimprovements on MRR value and Recall@10 are 22.5% and 25.5% over the\nstate-of-the-art method respectively.\n","authors":["Hongmeng Liu","Jiapeng Zhao","Yixuan Huo","Yuyan Wang","Chun Liao","Liyan Shen","Shiyao Cui","Jinqiao Shi"],"pdf_url":"https://arxiv.org/pdf/2303.10674v1.pdf","comment":"9pages"},{"id":"http://arxiv.org/abs/2303.10665v1","updated":"2023-03-19T14:12:57Z","published":"2023-03-19T14:12:57Z","title":"Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise,\n  Major Agents and Approximation Properties","summary":"  Recently, mean field control (MFC) has provided a tractable and theoretically\nfounded approach to otherwise difficult cooperative multi-agent control.\nHowever, the strict assumption of many independent, homogeneous agents may be\ntoo stringent in practice. In this work, we propose a novel discrete-time\ngeneralization of Markov decision processes and MFC to both many minor agents\nand potentially complex major agents -- major-minor mean field control (M3FC).\nIn contrast to deterministic MFC, M3FC allows for stochastic minor agent\ndistributions with strong correlation between minor agents through the major\nagent state, which can model arbitrary problem details not bound to any agent.\nTheoretically, we give rigorous approximation properties with novel proofs for\nboth M3FC and existing MFC models in the finite multi-agent problem, together\nwith a dynamic programming principle for solving such problems. In the\ninfinite-horizon discounted case, existence of an optimal stationary policy\nfollows. Algorithmically, we propose the major-minor mean field proximal policy\noptimization algorithm (M3FPPO) as a novel multi-agent reinforcement learning\nalgorithm and demonstrate its success in illustrative M3FC-type problems.\n","authors":["Kai Cui","Christian Fabian","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2303.10665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07768v2","updated":"2023-03-19T13:48:05Z","published":"2023-03-14T10:18:31Z","title":"DBSCAN of Multi-Slice Clustering for Third-Order Tensors","summary":"  Several methods for triclustering three-dimensional data require the cluster\nsize or the number of clusters in each dimension to be specified. To address\nthis issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal\nslices that lie in a low dimensional subspace for a rank-one tensor dataset in\norder to find a cluster based on the threshold similarity. We propose an\nextension algorithm called MSC-DBSCAN to extract the different clusters of\nslices that lie in the different subspaces from the data if the dataset is a\nsum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC\nalgorithm and can find the same solution for rank-one tensor data as MSC.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01104v3","updated":"2023-03-19T13:44:49Z","published":"2023-01-03T13:58:39Z","title":"KoopmanLab: machine learning for solving complex physics equations","summary":"  Numerous physics theories are rooted in partial differential equations\n(PDEs). However, the increasingly intricate physics equations, especially those\nthat lack analytic solutions or closed forms, have impeded the further\ndevelopment of physics. Computationally solving PDEs by classic numerical\napproaches suffers from the trade-off between accuracy and efficiency and is\nnot applicable to the empirical data generated by unknown latent PDEs. To\novercome this challenge, we present KoopmanLab, an efficient module of the\nKoopman neural operator family, for learning PDEs without analytic solutions or\nclosed forms. Our module consists of multiple variants of the Koopman neural\noperator (KNO), a kind of mesh-independent neural-network-based PDE solvers\ndeveloped following dynamic system theory. The compact variants of KNO can\naccurately solve PDEs with small model sizes while the large variants of KNO\nare more competitive in predicting highly complicated dynamic systems govern by\nunknown, high-dimensional, and non-linear PDEs. All variants are validated by\nmesh-independent and long-term prediction experiments implemented on\nrepresentative PDEs (e.g., the Navier-Stokes equation and the Bateman-Burgers\nequation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution\nglobal-scale climate data sets in earth physics). These demonstrations suggest\nthe potential of KoopmanLab to be a fundamental tool in diverse physics studies\nrelated to equations or dynamic systems.\n","authors":["Wei Xiong","Muyuan Ma","Xiaomeng Huang","Ziyang Zhang","Pei Sun","Yang Tian"],"pdf_url":"https://arxiv.org/pdf/2301.01104v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07316v3","updated":"2023-03-19T13:37:01Z","published":"2022-10-13T19:42:08Z","title":"MTEB: Massive Text Embedding Benchmark","summary":"  Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n","authors":["Niklas Muennighoff","Nouamane Tazi","Loïc Magne","Nils Reimers"],"pdf_url":"https://arxiv.org/pdf/2210.07316v3.pdf","comment":"24 pages, 14 tables, 6 figures"},{"id":"http://arxiv.org/abs/2303.10653v1","updated":"2023-03-19T13:30:33Z","published":"2023-03-19T13:30:33Z","title":"Randomized Adversarial Training via Taylor Expansion","summary":"  In recent years, there has been an explosion of research into developing more\nrobust deep neural networks against adversarial examples. Adversarial training\nappears as one of the most successful methods. To deal with both the robustness\nagainst adversarial examples and the accuracy over clean examples, many works\ndevelop enhanced adversarial training methods to achieve various trade-offs\nbetween them. Leveraging over the studies that smoothed update on weights\nduring training may help find flat minima and improve generalization, we\nsuggest reconciling the robustness-accuracy trade-off from another perspective,\ni.e., by adding random noise into deterministic weights. The randomized weights\nenable our design of a novel adversarial training method via Taylor expansion\nof a small Gaussian noise, and we show that the new adversarial training method\ncan flatten loss landscape and find flat minima. With PGD, CW, and Auto\nAttacks, an extensive set of experiments demonstrate that our method enhances\nthe state-of-the-art adversarial training methods, boosting both robustness and\nclean accuracy. The code is available at\nhttps://github.com/Alexkael/Randomized-Adversarial-Training.\n","authors":["Gaojie Jin","Xinping Yi","Dengyu Wu","Ronghui Mu","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10653v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10650v1","updated":"2023-03-19T13:03:51Z","published":"2023-03-19T13:03:51Z","title":"Logic of Differentiable Logics: Towards a Uniform Semantics of DL","summary":"  Differentiable logics (DL) have recently been proposed as a method of\ntraining neural networks to satisfy logical specifications. A DL consists of a\nsyntax in which specifications are stated and an interpretation function that\ntranslates expressions in the syntax into loss functions. These loss functions\ncan then be used during training with standard gradient descent algorithms. The\nvariety of existing DLs and the differing levels of formality with which they\nare treated makes a systematic comparative study of their properties and\nimplementations difficult. This paper remedies this problem by suggesting a\nmeta-language for defining DLs that we call the Logic of Differentiable Logics,\nor LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and\nfor the first time introduces the formalism for reasoning about vectors and\nlearners. Semantically, it introduces a general interpretation function that\ncan be instantiated to define loss functions arising from different existing\nDLs. We use LDL to establish several theoretical properties of existing DLs,\nand to conduct their empirical study in neural network verification.\n","authors":["Natalia Ślusarz","Ekaterina Komendantskaya","Matthew L. Daggitt","Robert Stewart","Kathrin Stark"],"pdf_url":"https://arxiv.org/pdf/2303.10650v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2302.06337v3","updated":"2023-03-19T13:01:47Z","published":"2023-02-13T13:14:23Z","title":"Learning from Noisy Crowd Labels with Logics","summary":"  This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n","authors":["Zhijun Chen","Hailong Sun","Haoqian He","Pengpeng Chen"],"pdf_url":"https://arxiv.org/pdf/2302.06337v3.pdf","comment":"12 pages, 7 figures, accepted by ICDE-2023"},{"id":"http://arxiv.org/abs/2206.04959v3","updated":"2023-03-19T12:15:44Z","published":"2022-06-10T09:15:48Z","title":"Merak: An Efficient Distributed DNN Training Framework with Automated 3D\n  Parallelism for Giant Foundation Models","summary":"  Foundation models are becoming the dominant deep learning technologies.\nPretraining a foundation model is always time-consumed due to the large scale\nof both the model parameter and training dataset. Besides being\ncomputing-intensive, the training process is extremely memory-intensive and\ncommunication-intensive. These features make it necessary to apply 3D\nparallelism, which integrates data parallelism, pipeline model parallelism and\ntensor model parallelism, to achieve high training efficiency.\n  To achieve this goal, some custom software frameworks such as Megatron-LM and\nDeepSpeed are developed. However, current 3D parallelism frameworks still meet\ntwo issues: i) they are not transparent to model developers, which need to\nmanually modify the model to parallelize training. ii) their utilization of\ncomputation, GPU memory and network bandwidth are not sufficient. We propose\nMerak, an automated 3D parallelism deep learning training framework with high\nresource utilization. Merak automatically deploys with an automatic model\npartitioner, which uses a graph sharding algorithm on a proxy representation of\nthe model. Merak also presents the non-intrusive API for scaling out foundation\nmodel training with minimal code modification. In addition, we design a\nhigh-performance 3D parallel runtime engine in Merak. It uses several\ntechniques to exploit available training resources, including shifted critical\npath pipeline schedule that brings a higher computation utilization,\nstage-aware recomputation that makes use of idle worker memory, and\nsub-pipelined tensor model parallelism that overlaps communication and\ncomputation. Experiments on 64 GPUs show Merak can speedup the training\nperformance over the state-of-the-art 3D parallelism frameworks of models with\n1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and\n1.61X, respectively.\n","authors":["Zhiquan Lai","Shengwei Li","Xudong Tang","Keshi Ge","Weijie Liu","Yabo Duan","Linbo Qiao","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2206.04959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12061v2","updated":"2023-03-19T12:10:02Z","published":"2022-12-22T22:27:26Z","title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical\n  Classification","summary":"  This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n","authors":["Alina Petukhova","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2212.12061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15214v6","updated":"2023-03-19T11:28:38Z","published":"2022-09-30T04:03:26Z","title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal\n  Business Knowledge Graph","summary":"  Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n","authors":["Shumin Deng","Chengming Wang","Zhoubo Li","Ningyu Zhang","Zelin Dai","Hehong Chen","Feiyu Xiong","Ming Yan","Qiang Chen","Mosha Chen","Jiaoyan Chen","Jeff Z. Pan","Bryan Hooi","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2209.15214v6.pdf","comment":"OpenBG. Accepted by ICDE 2023. The project is released at\n  https://github.com/OpenBGBenchmark/OpenBG . Website: https://kg.alibaba.com/\n  , Leaderboard: https://tianchi.aliyun.com/dataset/dataDetail?dataId=122271"},{"id":"http://arxiv.org/abs/2303.10630v1","updated":"2023-03-19T11:20:43Z","published":"2023-03-19T11:20:43Z","title":"Experimenting with Normalization Layers in Federated Learning on non-IID\n  scenarios","summary":"  Training Deep Learning (DL) models require large, high-quality datasets,\noften assembled with data from different institutions. Federated Learning (FL)\nhas been emerging as a method for privacy-preserving pooling of datasets\nemploying collaborative training from different institutions by iteratively\nglobally aggregating locally trained models. One critical performance challenge\nof FL is operating on datasets not independently and identically distributed\n(non-IID) among the federation participants. Even though this fragility cannot\nbe eliminated, it can be debunked by a suitable optimization of two\nhyper-parameters: layer normalization methods and collaboration frequency\nselection. In this work, we benchmark five different normalization layers for\ntraining Neural Networks (NNs), two families of non-IID data skew, and two\ndatasets. Results show that Batch Normalization, widely employed for\ncentralized DL, is not the best choice for FL, whereas Group and Layer\nNormalization consistently outperform Batch Normalization. Similarly, frequent\nmodel aggregation decreases convergence speed and mode quality.\n","authors":["Bruno Casella","Roberto Esposito","Antonio Sciarappa","Carlo Cavazzoni","Marco Aldinucci"],"pdf_url":"https://arxiv.org/pdf/2303.10630v1.pdf","comment":"10 pages, Submitted to IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2303.10624v1","updated":"2023-03-19T10:38:29Z","published":"2023-03-19T10:38:29Z","title":"PFSL: Personalized & Fair Split Learning with Data & Label Privacy for\n  thin clients","summary":"  The traditional framework of federated learning (FL) requires each client to\nre-train their models in every iteration, making it infeasible for\nresource-constrained mobile devices to train deep-learning (DL) models. Split\nlearning (SL) provides an alternative by using a centralized server to offload\nthe computation of activations and gradients for a subset of the model but\nsuffers from problems of slow convergence and lower accuracy. In this paper, we\nimplement PFSL, a new framework of distributed split learning where a large\nnumber of thin clients perform transfer learning in parallel, starting with a\npre-trained DL model without sharing their data or labels with a central\nserver. We implement a lightweight step of personalization of client models to\nprovide high performance for their respective data distributions. Furthermore,\nwe evaluate performance fairness amongst clients under a work fairness\nconstraint for various scenarios of non-i.i.d. data distributions and unequal\nsample sizes. Our accuracy far exceeds that of current SL algorithms and is\nvery close to that of centralized learning on several real-life benchmarks. It\nhas a very low computation cost compared to FL variants and promises to deliver\nthe full benefits of DL to extremely thin, resource-constrained clients.\n","authors":["Manas Wadhwa","Gagan Raj Gupta","Ashutosh Sahu","Rahul Saini","Vidhi Mittal"],"pdf_url":"https://arxiv.org/pdf/2303.10624v1.pdf","comment":"To be published in : THE 23RD IEEE/ACM INTERNATIONAL SYMPOSIUM ON\n  Cluster, Cloud and Internet Computing. Granted: Open Research Objects (ORO)\n  and Research Objects Reviewed (ROR) badges. See\n  https://www.niso.org/publications/rp-31-2021-badging for definitions of the\n  badges. Code available at: https://github.com/mnswdhw/PFSL"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.10794v1","updated":"2023-03-19T23:41:04Z","published":"2023-03-19T23:41:04Z","title":"PheME: A deep ensemble framework for improving phenotype prediction from\n  multi-modal data","summary":"  Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n","authors":["Shenghan Zhang","Haoxuan Li","Ruixiang Tang","Sirui Ding","Laila Rasmy","Degui Zhi","Na Zou","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.10794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09371v3","updated":"2023-03-19T12:31:20Z","published":"2022-11-17T06:55:49Z","title":"CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal\n  Pre-trained Knowledge","summary":"  Automatically generating textual descriptions for massive unlabeled images on\nthe web can greatly benefit realistic web applications, e.g. multimodal\nretrieval and recommendation. However, existing models suffer from the problem\nof generating ``over-generic'' descriptions, such as their tendency to generate\nrepetitive sentences with common concepts for different images. These generic\ndescriptions fail to provide sufficient textual semantics for ever-changing web\nimages. Inspired by the recent success of Vision-Language Pre-training (VLP)\nmodels that learn diverse image-text concept alignment during pretraining, we\nexplore leveraging their cross-modal pre-trained knowledge to automatically\nenrich the textual semantics of image descriptions. With no need for additional\nhuman annotations, we propose a plug-and-play framework, i.e CapEnrich, to\ncomplement the generic image descriptions with more semantic details.\nSpecifically, we first propose an automatic data-building strategy to get\ndesired training sentences, based on which we then adopt prompting strategies,\ni.e. learnable and template prompts, to incentivize VLP models to generate more\ntextual details. For learnable templates, we fix the whole VLP model and only\ntune the prompt vectors, which leads to two advantages: 1) the pre-training\nknowledge of VLP models can be reserved as much as possible to describe diverse\nvisual concepts; 2) only lightweight trainable parameters are required, so it\nis friendly to low data resources. Extensive experiments show that our method\nsignificantly improves the descriptiveness and diversity of generated sentences\nfor web images. The code is available at https://github.com/yaolinli/CapEnrich.\n","authors":["Linli Yao","Weijing Chen","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2211.09371v3.pdf","comment":"Accepted to WWW2023"},{"id":"http://arxiv.org/abs/2303.10539v1","updated":"2023-03-19T02:33:19Z","published":"2023-03-19T02:33:19Z","title":"Textless Speech-to-Music Retrieval Using Emotion Similarity","summary":"  We introduce a framework that recommends music based on the emotions of\nspeech. In content creation and daily life, speech contains information about\nhuman emotions, which can be enhanced by music. Our framework focuses on a\ncross-domain retrieval system to bridge the gap between speech and music via\nemotion labels. We explore different speech representations and report their\nimpact on different speech types, including acting voice and wake-up words. We\nalso propose an emotion similarity regularization term in cross-domain\nretrieval tasks. By incorporating the regularization term into training,\nsimilar speech-and-music pairs in the emotion space are closer in the joint\nembedding space. Our comprehensive experimental results show that the proposed\nmodel is effective in textless speech-to-music retrieval.\n","authors":["SeungHeon Doh","Minz Won","Keunwoo Choi","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2303.10539v1.pdf","comment":"To Appear IEEE ICASSP 2023"}]},"2023-03-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.10512v1","updated":"2023-03-18T22:36:25Z","published":"2023-03-18T22:36:25Z","title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .\n","authors":["Qingru Zhang","Minshuo Chen","Alexander Bukharin","Pengcheng He","Yu Cheng","Weizhu Chen","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.10512v1.pdf","comment":"The 11th International Conference on Learning Representations (ICLR\n  2023)"},{"id":"http://arxiv.org/abs/2303.10510v1","updated":"2023-03-18T22:19:09Z","published":"2023-03-18T22:19:09Z","title":"A Deep Learning System for Domain-specific speech Recognition","summary":"  As human-machine voice interfaces provide easy access to increasingly\nintelligent machines, many state-of-the-art automatic speech recognition (ASR)\nsystems are proposed. However, commercial ASR systems usually have poor\nperformance on domain-specific speech especially under low-resource settings.\nThe author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to\ndevelop benefit-specific ASR systems. The domain-specific data are collected\nusing proposed semi-supervised learning annotation with little human\nintervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60\nacoustic model with an external KenLM, which surpasses the Google and AWS ASR\nsystems on benefit-specific speech. The viability of using error prone ASR\ntranscriptions as part of spoken language understanding (SLU) is also\ninvestigated. Results of a benefit-specific natural language understanding\n(NLU) task show that the domain-specific fine-tuned ASR system can outperform\nthe commercial ASR systems even when its transcriptions have higher word error\nrate (WER), and the results between fine-tuned ASR and human transcriptions are\nsimilar.\n","authors":["Yanan Jia"],"pdf_url":"https://arxiv.org/pdf/2303.10510v1.pdf","comment":"4th International Conference on Natural Language Processing and\n  Computational Linguistics (NLPCL 2023)"},{"id":"http://arxiv.org/abs/2303.10475v1","updated":"2023-03-18T19:17:47Z","published":"2023-03-18T19:17:47Z","title":"Is Prompt All You Need? No. A Comprehensive and Broader View of\n  Instruction Learning","summary":"  Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n","authors":["Renze Lou","Kai Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.10475v1.pdf","comment":"Work is still in progress. The paper list is available at\n  https://github.com/RenzeLou/awesome-instruction-learning"},{"id":"http://arxiv.org/abs/2202.09791v4","updated":"2023-03-18T18:43:05Z","published":"2022-02-20T11:14:04Z","title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction","summary":"  Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is able to predict multiple kinds of subsumers including\nnamed classes from the same ontology or another ontology, and existential\nrestrictions from the same ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.\n","authors":["Jiaoyan Chen","Yuan He","Yuxia Geng","Ernesto Jimenez-Ruiz","Hang Dong","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2202.09791v4.pdf","comment":"Accepted by World Wide Web Journal"},{"id":"http://arxiv.org/abs/2303.10464v1","updated":"2023-03-18T17:56:01Z","published":"2023-03-18T17:56:01Z","title":"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language\n  Models","summary":"  The pre-training and fine-tuning paradigm has contributed to a number of\nbreakthroughs in Natural Language Processing (NLP). Instead of directly\ntraining on a downstream task, language models are first pre-trained on large\ndatasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then\nfine-tuned on task-specific data (e.g., natural language generation, text\nsummarization, etc.). Scaling the model and dataset size has helped improve the\nperformance of LLMs, but unfortunately, this also leads to highly prohibitive\ncomputational costs. Pre-training LLMs often require orders of magnitude more\nFLOPs than fine-tuning and the model capacity often remains the same between\nthe two phases. To achieve training efficiency w.r.t training FLOPs, we propose\nto decouple the model capacity between the two phases and introduce Sparse\nPre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits\nof using unstructured weight sparsity to train only a subset of weights during\npre-training (Sparse Pre-training) and then recover the representational\ncapacity by allowing the zeroed weights to learn (Dense Fine-tuning). We\ndemonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3\nXL model resulting in a 2.5x reduction in pre-training FLOPs, without a\nsignificant loss in accuracy on the downstream tasks relative to the dense\nbaseline. By rigorously evaluating multiple downstream tasks, we also establish\na relationship between sparsity, task complexity, and dataset size. Our work\npresents a promising direction to train large GPT models at a fraction of the\ntraining FLOPs using weight sparsity while retaining the benefits of\npre-trained textual representations for downstream tasks.\n","authors":["Vithursan Thangarasa","Abhay Gupta","William Marshall","Tianda Li","Kevin Leong","Dennis DeCoste","Sean Lie","Shreyas Saxena"],"pdf_url":"https://arxiv.org/pdf/2303.10464v1.pdf","comment":"Presented at the ICLR 2023 Workshop on Sparsity in Neural Networks"},{"id":"http://arxiv.org/abs/2303.10443v1","updated":"2023-03-18T15:55:49Z","published":"2023-03-18T15:55:49Z","title":"GazeReader: Detecting Unknown Word Using Webcam for English as a Second\n  Language (ESL) Learners","summary":"  Automatic unknown word detection techniques can enable new applications for\nassisting English as a Second Language (ESL) learners, thus improving their\nreading experiences. However, most modern unknown word detection methods\nrequire dedicated eye-tracking devices with high precision that are not easily\naccessible to end-users. In this work, we propose GazeReader, an unknown word\ndetection method only using a webcam. GazeReader tracks the learner's gaze and\nthen applies a transformer-based machine learning model that encodes the text\ninformation to locate the unknown word. We applied knowledge enhancement\nincluding term frequency, part of speech, and named entity recognition to\nimprove the performance. The user study indicates that the accuracy and\nF1-score of our method were 98.09% and 75.73%, respectively. Lastly, we\nexplored the design scope for ESL reading and discussed the findings.\n","authors":["Jiexin Ding","Bowen Zhao","Yuqi Huang","Yuntao Wang","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2303.10443v1.pdf","comment":"This paper has been accepted by ACM CHI 2023"},{"id":"http://arxiv.org/abs/2303.10439v1","updated":"2023-03-18T15:39:23Z","published":"2023-03-18T15:39:23Z","title":"Stop Words for Processing Software Engineering Documents: Do they\n  Matter?","summary":"  Stop words, which are considered non-predictive, are often eliminated in\nnatural language processing tasks. However, the definition of uninformative\nvocabulary is vague, so most algorithms use general knowledge-based stop lists\nto remove stop words. There is an ongoing debate among academics about the\nusefulness of stop word elimination, especially in domain-specific settings. In\nthis work, we investigate the usefulness of stop word removal in a software\nengineering context. To do this, we replicate and experiment with three\nsoftware engineering research tools from related work. Additionally, we\nconstruct a corpus of software engineering domain-related text from 10,000\nStack Overflow questions and identify 200 domain-specific stop words using\ntraditional information-theoretic methods. Our results show that the use of\ndomain-specific stop words significantly improved the performance of research\ntools compared to the use of a general stop list and that 17 out of 19\nevaluation measures showed better performance.\n","authors":["Yaohou Fan","Chetan Arora","Christoph Treude"],"pdf_url":"https://arxiv.org/pdf/2303.10439v1.pdf","comment":"Accepted for publication at the 2nd Intl. Workshop on NL-based\n  Software Engineering (NLBSE 2023)"},{"id":"http://arxiv.org/abs/2303.10430v1","updated":"2023-03-18T14:54:57Z","published":"2023-03-18T14:54:57Z","title":"NoisyHate: Benchmarking Content Moderation Machine Learning Models with\n  Human-Written Perturbations Online","summary":"  Online texts with toxic content are a threat in social media that might cause\ncyber harassment. Although many platforms applied measures, such as machine\nlearning-based hate-speech detection systems, to diminish their effect, those\ntoxic content publishers can still evade the system by modifying the spelling\nof toxic words. Those modified words are also known as human-written text\nperturbations. Many research works developed certain techniques to generate\nadversarial samples to help the machine learning models obtain the ability to\nrecognize those perturbations. However, there is still a gap between those\nmachine-generated perturbations and human-written perturbations. In this paper,\nwe introduce a benchmark test set containing human-written perturbations online\nfor toxic speech detection models. We also recruited a group of workers to\nevaluate the quality of this test set and dropped low-quality samples.\nMeanwhile, to check if our perturbation can be normalized to its clean version,\nwe applied spell corrector algorithms on this dataset. Finally, we test this\ndata on state-of-the-art language models, such as BERT and RoBERTa, and black\nbox APIs, such as perspective API, to demonstrate the adversarial attack with\nreal human-written perturbations is still effective.\n","authors":["Yiran Ye","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2303.10430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10420v1","updated":"2023-03-18T14:02:04Z","published":"2023-03-18T14:02:04Z","title":"A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models","summary":"  GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models'\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models' ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.\n","authors":["Junjie Ye","Xuanting Chen","Nuo Xu","Can Zu","Zekai Shao","Shichun Liu","Yuhan Cui","Zeyang Zhou","Chao Gong","Yang Shen","Jie Zhou","Siming Chen","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10395v1","updated":"2023-03-18T11:15:33Z","published":"2023-03-18T11:15:33Z","title":"A Graph-Guided Reasoning Approach for Open-ended Commonsense Question\n  Answering","summary":"  Recently, end-to-end trained models for multiple-choice commonsense question\nanswering (QA) have delivered promising results. However, such\nquestion-answering systems cannot be directly applied in real-world scenarios\nwhere answer candidates are not provided. Hence, a new benchmark challenge set\nfor open-ended commonsense reasoning (OpenCSR) has been recently released,\nwhich contains natural science questions without any predefined choices. On the\nOpenCSR challenge set, many questions require implicit multi-hop reasoning and\nhave a large decision space, reflecting the difficult nature of this task.\nExisting work on OpenCSR sorely focuses on improving the retrieval process,\nwhich extracts relevant factual sentences from a textual knowledge base,\nleaving the important and non-trivial reasoning task outside the scope. In this\nwork, we extend the scope to include a reasoner that constructs a\nquestion-dependent open knowledge graph based on retrieved supporting facts and\nemploys a sequential subgraph reasoning process to predict the answer. The\nsubgraph can be seen as a concise and compact graphical explanation of the\nprediction. Experiments on two OpenCSR datasets show that the proposed model\nachieves great performance on benchmark OpenCSR datasets.\n","authors":["Zhen Han","Yue Feng","Mingming Sun"],"pdf_url":"https://arxiv.org/pdf/2303.10395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10384v1","updated":"2023-03-18T10:36:33Z","published":"2023-03-18T10:36:33Z","title":"Powerful and Extensible WFST Framework for RNN-Transducer Losses","summary":"  This paper presents a framework based on Weighted Finite-State Transducers\n(WFST) to simplify the development of modifications for RNN-Transducer (RNN-T)\nloss. Existing implementations of RNN-T use CUDA-related code, which is hard to\nextend and debug. WFSTs are easy to construct and extend, and allow debugging\nthrough visualization. We introduce two WFST-powered RNN-T implementations: (1)\n\"Compose-Transducer\", based on a composition of the WFST graphs from acoustic\nand textual schema -- computationally competitive and easy to modify; (2)\n\"Grid-Transducer\", which constructs the lattice directly for further\ncomputations -- most compact, and computationally efficient. We illustrate the\nease of extensibility through introduction of a new W-Transducer loss -- the\nadaptation of the Connectionist Temporal Classification with Wild Cards.\nW-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a\nweakly-supervised data setup with missing parts of transcriptions at the\nbeginning and end of utterances. All RNN-T losses are implemented with the k2\nframework and are available in the NeMo toolkit.\n","authors":["Aleksandr Laptev","Vladimir Bataev","Igor Gitman","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2303.10384v1.pdf","comment":"To appear in Proc. ICASSP 2023, June 04-10, 2023, Rhodes island,\n  Greece. 5 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.10368v1","updated":"2023-03-18T08:57:09Z","published":"2023-03-18T08:57:09Z","title":"An Empirical Study of Pre-trained Language Models in Simple Knowledge\n  Graph Question Answering","summary":"  Large-scale pre-trained language models (PLMs) such as BERT have recently\nachieved great success and become a milestone in natural language processing\n(NLP). It is now the consensus of the NLP community to adopt PLMs as the\nbackbone for downstream tasks. In recent works on knowledge graph question\nanswering (KGQA), BERT or its variants have become necessary in their KGQA\nmodels. However, there is still a lack of comprehensive research and comparison\nof the performance of different PLMs in KGQA. To this end, we summarize two\nbasic KGQA frameworks based on PLMs without additional neural network modules\nto compare the performance of nine PLMs in terms of accuracy and efficiency. In\naddition, we present three benchmarks for larger-scale KGs based on the popular\nSimpleQuestions benchmark to investigate the scalability of PLMs. We carefully\nanalyze the results of all PLMs-based KGQA basic frameworks on these benchmarks\nand two other popular datasets, WebQuestionSP and FreebaseQA, and find that\nknowledge distillation techniques and knowledge enhancement methods in PLMs are\npromising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal\nof attention in the NLP community, demonstrating its impressive capabilities\nand limitations in zero-shot KGQA. We have released the code and benchmarks to\npromote the use of PLMs on KGQA.\n","authors":["Nan Hu","Yike Wu","Guilin Qi","Dehai Min","Jiaoyan Chen","Jeff Z. Pan","Zafar Ali"],"pdf_url":"https://arxiv.org/pdf/2303.10368v1.pdf","comment":"Accepted by World Wide Web Journal"},{"id":"http://arxiv.org/abs/2302.02078v2","updated":"2023-03-18T08:29:52Z","published":"2023-02-04T03:30:07Z","title":"FGSI: Distant Supervision for Relation Extraction method based on\n  Fine-Grained Semantic Information","summary":"  The main purpose of relation extraction is to extract the semantic\nrelationships between tagged pairs of entities in a sentence, which plays an\nimportant role in the semantic understanding of sentences and the construction\nof knowledge graphs. In this paper, we propose that the key semantic\ninformation within a sentence plays a key role in the relationship extraction\nof entities. We propose the hypothesis that the key semantic information inside\nthe sentence plays a key role in entity relationship extraction. And based on\nthis hypothesis, we split the sentence into three segments according to the\nlocation of the entity from the inside of the sentence, and find the\nfine-grained semantic features inside the sentence through the intra-sentence\nattention mechanism to reduce the interference of irrelevant noise information.\nThe proposed relational extraction model can make full use of the available\npositive semantic information. The experimental results show that the proposed\nrelation extraction model improves the accuracy-recall curves and P@N values\ncompared with existing methods, which proves the effectiveness of this model.\n","authors":["Chenghong Sun","Weidong Ji","Guohui Zhou","Hui Guo","Zengxiang Yin","Yuqi Yue"],"pdf_url":"https://arxiv.org/pdf/2302.02078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00465v2","updated":"2023-03-18T06:54:57Z","published":"2023-03-01T12:46:00Z","title":"Uzbek text's correspondence with the educational potential of pupils: a\n  case study of the School corpus","summary":"  One of the major challenges of an educational system is choosing appropriate\ncontent considering pupils' age and intellectual potential. In this article the\nexperiment of primary school grades (from 1st to 4th grades) is considered for\nautomatically determining the correspondence of an educational materials\nrecommended for pupils by using the School corpus where it includes the dataset\nof 25 school textbooks confirmed by the Ministry of preschool and school\neducation of the Republic of Uzbekistan. In this case, TF-IDF scores of the\ntexts are determined, they are converted into a vector representation, and the\ngiven educational materials are compared with the corresponding class of the\nSchool corpus using the cosine similarity algorithm. Based on the results of\nthe calculation, it is determined whether the given educational material is\nappropriate or not appropriate for the pupils' educational potential.\n","authors":["Khabibulla Madatov","Sanatbek Matlatipov","Mersaid Aripov"],"pdf_url":"https://arxiv.org/pdf/2303.00465v2.pdf","comment":"Preprint of the paper accepted to The 10th Language & Technology\n  Conference: Human Language Technologies as a Challenge for Computer Science\n  and Linguistics. April 21-23, 2023, Poznan, Poland"},{"id":"http://arxiv.org/abs/2303.10330v1","updated":"2023-03-18T04:31:07Z","published":"2023-03-18T04:31:07Z","title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking","summary":"  Biomedical entity linking (EL) consists of named entity recognition (NER) and\nnamed entity disambiguation (NED). EL models are trained on corpora labeled by\na predefined KB. However, it is a common scenario that only entities within a\nsubset of the KB are precious to stakeholders. We name this scenario partial\nknowledge base inference: training an EL model with one KB and inferring on the\npart of it without further training. In this work, we give a detailed\ndefinition and evaluation procedures for this practically valuable but\nsignificantly understudied scenario and evaluate methods from three\nrepresentative EL paradigms. We construct partial KB inference benchmarks and\nwitness a catastrophic degradation in EL performance due to dramatically\nprecision drop. Our findings reveal these EL paradigms can not correctly handle\nunlinkable mentions (NIL), so they are not robust to partial KB inference. We\nalso propose two simple-and-effective redemption methods to combat the NIL\nissue with little computational overhead.\n","authors":["Hongyi Yuan","Keming Lu","Zheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.10330v1.pdf","comment":"13 pages, 3 figures. The first two authors are contributed equally"},{"id":"http://arxiv.org/abs/2303.10328v1","updated":"2023-03-18T04:28:01Z","published":"2023-03-18T04:28:01Z","title":"Revisiting Automatic Question Summarization Evaluation in the Biomedical\n  Domain","summary":"  Automatic evaluation metrics have been facilitating the rapid development of\nautomatic summarization methods by providing instant and fair assessments of\nthe quality of summaries. Most metrics have been developed for the general\ndomain, especially news and meeting notes, or other language-generation tasks.\nHowever, these metrics are applied to evaluate summarization systems in\ndifferent domains, such as biomedical question summarization. To better\nunderstand whether commonly used evaluation metrics are capable of evaluating\nautomatic summarization in the biomedical domain, we conduct human evaluations\nof summarization quality from four different aspects of a biomedical question\nsummarization task. Based on human judgments, we identify different noteworthy\nfeatures for current automatic metrics and summarization systems as well. We\nalso release a dataset of our human annotations to aid the research of\nsummarization evaluation metrics in the biomedical domain.\n","authors":["Hongyi Yuan","Yaoyun Zhang","Fei Huang","Songfang Huang"],"pdf_url":"https://arxiv.org/pdf/2303.10328v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2112.03073v3","updated":"2023-03-18T03:55:06Z","published":"2021-11-26T07:58:11Z","title":"Active Learning for Event Extraction with Memory-based Loss Prediction\n  Model","summary":"  Event extraction (EE) plays an important role in many industrial application\nscenarios, and high-quality EE methods require a large amount of manual\nannotation data to train supervised learning models. However, the cost of\nobtaining annotation data is very high, especially for annotation of domain\nevents, which requires the participation of experts from corresponding domain.\nSo we introduce active learning (AL) technology to reduce the cost of event\nannotation. But the existing AL methods have two main problems, which make them\nnot well used for event extraction. Firstly, the existing pool-based selection\nstrategies have limitations in terms of computational cost and sample validity.\nSecondly, the existing evaluation of sample importance lacks the use of local\nsample information. In this paper, we present a novel deep AL method for EE. We\npropose a batch-based selection strategy and a Memory-Based Loss Prediction\nmodel (MBLP) to select unlabeled samples efficiently. During the selection\nprocess, we use an internal-external sample loss ranking method to evaluate the\nsample importance by using local information. Finally, we propose a delayed\ntraining strategy to train the MBLP model. Extensive experiments are performed\non three domain datasets, and our method outperforms other state-of-the-art\nmethods.\n","authors":["Shirong Shen","Zhen Li","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2112.03073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10311v1","updated":"2023-03-18T02:46:49Z","published":"2023-03-18T02:46:49Z","title":"On the rise of fear speech in online social media","summary":"  Recently, social media platforms are heavily moderated to prevent the spread\nof online hate speech, which is usually fertile in toxic words and is directed\ntoward an individual or a community. Owing to such heavy moderation, newer and\nmore subtle techniques are being deployed. One of the most striking among these\nis fear speech. Fear speech, as the name suggests, attempts to incite fear\nabout a target community. Although subtle, it might be highly effective, often\npushing communities toward a physical conflict. Therefore, understanding their\nprevalence in social media is of paramount importance. This article presents a\nlarge-scale study to understand the prevalence of 400K fear speech and over\n700K hate speech posts collected from Gab.com. Remarkably, users posting a\nlarge number of fear speech accrue more followers and occupy more central\npositions in social networks than users posting a large number of hate speech.\nThey can also reach out to benign users more effectively than hate speech users\nthrough replies, reposts, and mentions. This connects to the fact that, unlike\nhate speech, fear speech has almost zero toxic content, making it look\nplausible. Moreover, while fear speech topics mostly portray a community as a\nperpetrator using a (fake) chain of argumentation, hate speech topics hurl\ndirect multitarget insults, thus pointing to why general users could be more\ngullible to fear speech. Our findings transcend even to other platforms\n(Twitter and Facebook) and thus necessitate using sophisticated moderation\npolicies and mass awareness to combat fear speech.\n","authors":["Punyajoy Saha","Kiran Garimella","Narla Komal Kalyan","Saurabh Kumar Pandey","Pauras Mangesh Meher","Binny Mathew","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2303.10311v1.pdf","comment":"16 pages, 9 tables, 15 figures, accepted in Proceedings of the\n  National Academy of Sciences of the United States of America"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.10497v1","updated":"2023-03-18T20:50:33Z","published":"2023-03-18T20:50:33Z","title":"Examining the Potential for Conversational Exploratory Search using a\n  Smart Speaker Digital Assistant","summary":"  Online Digital Assistants, such as Amazon Alexa, Google Assistant, Apple Siri\nare very popular and provide a range or services to their users, a key function\nis their ability to satisfy user information needs from the sources available\nto them. Users may often regard these applications as providing search services\nsimilar to Google type search engines. However, while it is clear that they are\nin general able to answer factoid questions effectively, it is much less\nobvious how well they support less specific or exploratory type search tasks.\nWe describe an investigation examining the behaviour of the standard Amazon\nAlexa for exploratory search tasks. The results of our study show that it not\neffective in addressing these types of information needs. We propose extensions\nto Alexa designed to overcome these shortcomings. Our Custom Alexa application\nextends Alexa's conversational functionality for exploratory search. A user\nstudy shows that our extended Alexa application both enables users to more\nsuccessfully complete exploratory search tasks and is well accepted by our test\nusers.\n","authors":["Abhishek Kaushik","Gareth J. F. Jones"],"pdf_url":"https://arxiv.org/pdf/2303.10497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07145v2","updated":"2023-03-18T08:29:20Z","published":"2023-02-14T15:53:12Z","title":"Practical Cross-System Shilling Attacks with Limited Access to Data","summary":"  In shilling attacks, an adversarial party injects a few fake user profiles\ninto a Recommender System (RS) so that the target item can be promoted or\ndemoted. Although much effort has been devoted to developing shilling attack\nmethods, we find that existing approaches are still far from practical. In this\npaper, we analyze the properties a practical shilling attack method should have\nand propose a new concept of Cross-system Attack. With the idea of Cross-system\nAttack, we design a Practical Cross-system Shilling Attack (PC-Attack)\nframework that requires little information about the victim RS model and the\ntarget RS data for conducting attacks. PC-Attack is trained to capture graph\ntopology knowledge from public RS data in a self-supervised manner. Then, it is\nfine-tuned on a small portion of target data that is easy to access to\nconstruct fake profiles. Extensive experiments have demonstrated the\nsuperiority of PC-Attack over state-of-the-art baselines. Our implementation of\nPC-Attack is available at https://github.com/KDEGroup/PC-Attack.\n","authors":["Meifang Zeng","Ke Li","Bingchuan Jiang","Liujuan Cao","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2302.07145v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08537v2","updated":"2023-03-18T02:30:32Z","published":"2023-03-15T11:30:16Z","title":"Graph-less Collaborative Filtering","summary":"  Graph neural networks (GNNs) have shown the power in representation learning\nover graph-structured user-item interaction data for collaborative filtering\n(CF) task. However, with their inherently recursive message propagation among\nneighboring nodes, existing GNN-based CF models may generate indistinguishable\nand inaccurate user (item) representations due to the over-smoothing and noise\neffect with low-pass Laplacian smoothing operators. In addition, the recursive\ninformation propagation with the stacked aggregators in the entire graph\nstructures may result in poor scalability in practical applications. Motivated\nby these limitations, we propose a simple and effective collaborative filtering\nmodel (SimRec) that marries the power of knowledge distillation and contrastive\nlearning. In SimRec, adaptive transferring knowledge is enabled between the\nteacher GNN model and a lightweight student network, to not only preserve the\nglobal collaborative signals, but also address the over-smoothing issue with\nrepresentation recalibration. Empirical results on public datasets show that\nSimRec archives better efficiency while maintaining superior recommendation\nperformance compared with various strong baselines. Our implementations are\npublicly available at: https://github.com/HKUDS/SimRec.\n","authors":["Lianghao Xia","Chao Huang","Jiao Shi","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08537v2.pdf","comment":"Accepted by ACM WWW 2023"},{"id":"http://arxiv.org/abs/2302.04545v2","updated":"2023-03-18T02:10:10Z","published":"2023-02-09T10:20:23Z","title":"Lorentz Equivariant Model for Knowledge-Enhanced Hyperbolic\n  Collaborative Filtering","summary":"  Introducing prior auxiliary information from the knowledge graph (KG) to\nassist the user-item graph can improve the comprehensive performance of the\nrecommender system. Many recent studies show that the ensemble properties of\nhyperbolic spaces fit the scale-free and hierarchical characteristics exhibited\nin the above two types of graphs well. However, existing hyperbolic methods\nignore the consideration of equivariance, thus they cannot generalize symmetric\nfeatures under given transformations, which seriously limits the capability of\nthe model. Moreover, they cannot balance preserving the heterogeneity and\nmining the high-order entity information to users across two graphs. To fill\nthese gaps, we propose a rigorously Lorentz group equivariant\nknowledge-enhanced collaborative filtering model (LECF). Innovatively, we\njointly update the attribute embeddings (containing the high-order entity\nsignals from the KG) and hyperbolic embeddings (the distance between hyperbolic\nembeddings reveals the recommendation tendency) by the LECF layer with Lorentz\nEquivariant Transformation. Moreover, we propose Hyperbolic Sparse Attention\nMechanism to sample the most informative neighbor nodes. Lorentz equivariance\nis strictly maintained throughout the entire model, and enforcing equivariance\nis proven necessary experimentally. Extensive experiments on three real-world\nbenchmarks demonstrate that LECF remarkably outperforms state-of-the-art\nmethods.\n","authors":["Bosong Huang","Weihao Yu","Ruzhong Xie","Jing Xiao","Jin Huang"],"pdf_url":"https://arxiv.org/pdf/2302.04545v2.pdf","comment":"11 pages, 6 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.10446v1","updated":"2023-03-18T16:09:10Z","published":"2023-03-18T16:09:10Z","title":"A Content Adaptive Learnable Time-Frequency Representation For Audio\n  Signal Processing","summary":"  We propose a learnable content adaptive front end for audio signal\nprocessing. Before the modern advent of deep learning, we used fixed\nrepresentation non-learnable front-ends like spectrogram or mel-spectrogram\nwith/without neural architectures. With convolutional architectures supporting\nvarious applications such as ASR and acoustic scene understanding, a shift to a\nlearnable front ends occurred in which both the type of basis functions and the\nweight were learned from scratch and optimized for the particular task of\ninterest. With the shift to transformer-based architectures with no\nconvolutional blocks present, a linear layer projects small waveform patches\nonto a small latent dimension before feeding them to a transformer\narchitecture. In this work, we propose a way of computing a content-adaptive\nlearnable time-frequency representation. We pass each audio signal through a\nbank of convolutional filters, each giving a fixed-dimensional vector. It is\nakin to learning a bank of finite impulse-response filterbanks and passing the\ninput signal through the optimum filter bank depending on the content of the\ninput signal. A content-adaptive learnable time-frequency representation may be\nmore broadly applicable, beyond the experiments in this paper.\n","authors":["Prateek Verma","Chris Chafe"],"pdf_url":"https://arxiv.org/pdf/2303.10446v1.pdf","comment":"5 pages, 4 figures. 2023 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing, Rhodes, Greece"},{"id":"http://arxiv.org/abs/2303.10372v1","updated":"2023-03-18T09:36:59Z","published":"2023-03-18T09:36:59Z","title":"Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven\n  Approach","summary":"  Just noticeable difference (JND) refers to the maximum visual change that\nhuman eyes cannot perceive, and it has a wide range of applications in\nmultimedia systems. However, most existing JND approaches only focus on a\nsingle modality, and rarely consider the complementary effects of multimodal\ninformation. In this article, we investigate the JND modeling from an\nend-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,\nwe explore three important visually sensitive modalities, including saliency,\ndepth, and segmentation. To better utilize homologous multimodal information,\nwe establish an effective fusion method via summation enhancement and\nsubtractive offset, and align homologous multimodal features based on a\nself-attention driven encoder-decoder paradigm. Extensive experimental results\non eight different benchmark datasets validate the superiority of our hmJND-Net\nover eight representative methods.\n","authors":["Wuyuan Xie","Shukang Wang","Sukun Tian","Lirong Huang","Ye Liu","Miaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10369v1","updated":"2023-03-18T09:04:55Z","published":"2023-03-18T09:04:55Z","title":"Blind Multimodal Quality Assessment: A Brief Survey and A Case Study of\n  Low-light Images","summary":"  Blind image quality assessment (BIQA) aims at automatically and accurately\nforecasting objective scores for visual signals, which has been widely used to\nmonitor product and service quality in low-light applications, covering\nsmartphone photography, video surveillance, autonomous driving, etc. Recent\ndevelopments in this field are dominated by unimodal solutions inconsistent\nwith human subjective rating patterns, where human visual perception is\nsimultaneously reflected by multiple sensory information (e.g., sight and\nhearing). In this article, we present a unique blind multimodal quality\nassessment (BMQA) of low-light images from subjective evaluation to objective\nscore. To investigate the multimodal mechanism, we first establish a multimodal\nlow-light image quality (MLIQ) database with authentic low-light distortions,\ncontaining image and audio modality pairs. Further, we specially design the key\nmodules of BMQA, considering multimodal quality representation, latent feature\nalignment and fusion, and hybrid self-supervised and supervised learning.\nExtensive experiments show that our BMQA yields state-of-the-art accuracy on\nthe proposed MLIQ benchmark database. In particular, we also build an\nindependent single-image modality Dark-4K database, which is used to verify its\napplicability and generalization performance in mainstream unimodal\napplications. Qualitative and quantitative results on Dark-4K show that BMQA\nachieves superior performance to existing BIQA approaches as long as a\npre-trained quality semantic description model is provided. The proposed\nframework and two databases as well as the collected BIQA methods and\nevaluation metrics are made publicly available.\n","authors":["Miaohui Wang","Zhuowei Xu","Mai Xu","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2303.10369v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2303.10335v1","updated":"2023-03-18T04:50:07Z","published":"2023-03-18T04:50:07Z","title":"Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5","summary":"  We used two multimodal models for continuous valence-arousal recognition\nusing visual, audio, and linguistic information. The first model is the same as\nwe used in ABAW2 and ABAW3, which employs the leader-follower attention. The\nsecond model has the same architecture for spatial and temporal encoding. As\nfor the fusion block, it employs a compact and straightforward channel\nattention, borrowed from the End2You toolkit. Unlike our previous attempts that\nuse Vggish feature directly as the audio feature, this time we feed the\npre-trained VGG model using logmel-spectrogram and finetune it during the\ntraining. To make full use of the data and alleviate over-fitting,\ncross-validation is carried out. The fold with the highest concordance\ncorrelation coefficient is selected for submission. The code is to be available\nat https://github.com/sucv/ABAW5.\n","authors":["Su Zhang","Ziyuan Zhao","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2303.10335v1.pdf","comment":"4 pages. arXiv admin note: substantial text overlap with\n  arXiv:2203.13031"},{"id":"http://arxiv.org/abs/2303.10325v1","updated":"2023-03-18T04:01:53Z","published":"2023-03-18T04:01:53Z","title":"Smartbanner: Intelligent banner design framework that strikes a balance\n  between creative freedom and design rules","summary":"  Companies use banners extensively to promote their products, and the\nintelligent automatic synthesis of banners is a challenging event. Under the\npremise of inputting only a small amount of information such as product, text\nand size, it can synthesize styles with high freedom and richness, but at the\nsame time, it must satisfy the design specifications of advertisers for\nadvertising and scenes. We propose an intelligent banner design framework that\nstrikes a balance between creative freedom and design rules, called\nsmartbanner. Smartbanner consists of planner, actuator, adjuster and generator.\nThe banner is synthesized through the combined framework, which fully liberates\nthe designer and reduces the threshold and cost of design. It increases the\nclick-through rate by 30%, improves the human efficiency of designers by 500%\nunder the condition of ensuring the quality of creation, and synthesizes\nhundreds of millions of pictures in batches throughout the year.\n","authors":["Guandong Li","Xian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.10325v1.pdf","comment":null}]}}